{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ffn/core.py:27: MatplotlibDeprecationWarning: The 'warn' parameter of use() is deprecated since Matplotlib 3.1 and will be removed in 3.3.  If any parameter follows 'warn', they should be pass as keyword, not positionally.\n",
      "  matplotlib.use('agg', warn=False)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas_datareader/compat/__init__.py:7: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  from pandas.util.testing import assert_frame_equal\n"
     ]
    }
   ],
   "source": [
    "###  Import librairies and define variables\n",
    "import os\n",
    "import pandas as pd\n",
    "# Principal parameters below\n",
    "# Select company and data ***CHANGE VALUE HERE***\n",
    "compTick = 'AAPL'\n",
    "# Folder in which the sentiment file is\n",
    "root_for_sentiments='CFDS_Tweets_Sentiment'+ os.sep\n",
    "sufix = '_daily_sentiment_tweets.csv'\n",
    "\n",
    "# Load the prices from Bloomberg\n",
    "data_comp = [['ORCL','Oracle'],['MSFT','Microsoft'],['CRM','Salesforce'],\n",
    "             ['AAPL','Apple'],['PYPL','Paypal'],['V','Visa'],['MA','Mastercard'],\n",
    "             ['INTC','Intel Corp'],['CSCO','Cisco Systems'],['NVDA','Nvidia'],['ADBE','Adobe']]\n",
    "companies = pd.DataFrame(data_comp, columns = ['ticker_root','name'])\n",
    "\n",
    "#-*- coding: utf-8 -*-\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import csv\n",
    "from pandas import read_csv\n",
    "import shutil\n",
    "\n",
    "# back testing for trading signals\n",
    "import bt as bt\n",
    "\n",
    "# pytorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torch.utils.data import dataloader\n",
    "\n",
    "# sklearn libraries\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "plt.rcParams['figure.dpi']= 150\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "if not os.path.exists('./data'): os.makedirs('./data')  # create data directory\n",
    "if not os.path.exists('./models'): os.makedirs('./models')  # create trained models directory\n",
    "    \n",
    "# init deterministic seed\n",
    "seed_value = 1234\n",
    "np.random.seed(seed_value) # set numpy seed\n",
    "torch.manual_seed(seed_value) # set pytorch seed CPU\n",
    "\n",
    "\n",
    "market_data_file = 'CoursBloomberg' + os.sep + 'CFDS_prices_all_calendar_days.csv'\n",
    "market_data_file_return = 'CoursBloomberg' + os.sep + 'CFDS_prices_all_calendar_days_return.csv'\n",
    "market_data = read_csv(market_data_file, sep=';', header=0, parse_dates=[0], infer_datetime_format=True)\n",
    "\n",
    "\n",
    "for index, row in companies.iterrows():\n",
    "    headername = row['ticker_root'] + '_Return'\n",
    "    headerticker = row['ticker_root'] + ' US Equity'\n",
    "    market_data[headername] = market_data[headerticker].pct_change()\n",
    "    \n",
    "# Remove first return line\n",
    "market_data = market_data.drop(0)\n",
    "market_data.to_csv(market_data_file_return, sep = \";\", header=True, index=True)\n",
    "market_data = market_data.set_index('Date')\n",
    "\n",
    "# Transform Time-Series Into Sequences\n",
    "\n",
    "\n",
    "# number of timesteps\n",
    "sequence_length = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can loop over companies\n",
    "#for company in companies:\n",
    "#    stock_daily_returns[] = stock_data['RETURN'][1:len(stock_data['RETURN'])]\n",
    "\n",
    "compName = companies[companies.ticker_root==compTick].name.item()\n",
    "\n",
    "fields = ['Date','Comp']\n",
    "sentiment = pd.read_csv(root_for_sentiments + compName + sufix, sep = \"|\", header=0, usecols=fields, index_col='Date')\n",
    "\n",
    "c = sentiment['Comp']\n",
    "#print(c[:5])\n",
    "\n",
    "\n",
    "# plot the sentiment and the return\n",
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(pd.to_datetime(market_data.index).values[0:market_data.shape[0],], market_data[compTick + '_Return'], color='C1', label='Stock return (green)')\n",
    "ax.plot(pd.to_datetime(sentiment.index).values[0:sentiment.shape[0],], sentiment, color='C0', label='sentiment (blue)')\n",
    "\n",
    "# set y-axis limits\n",
    "ax.set_xlim(pd.to_datetime(sentiment.index).values[1000], pd.to_datetime(sentiment.index).values[1185])\n",
    "\n",
    "# set plot legend\n",
    "plt.legend(loc=\"lower right\", numpoints=1, fancybox=True)\n",
    "\n",
    "plt.title('Stock return vs. Sentiment', fontsize=10)\n",
    "plt.xlabel('[time]', fontsize=8)\n",
    "plt.ylabel('[sentiment]', fontsize=8)\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "\n",
    "\n",
    "for i in range(0, c.shape[0] - sequence_length):\n",
    "    single_sentiment_sequence_data = c[i:i + sequence_length].T\n",
    "\n",
    "    if i == 0:\n",
    "        sentiment_sequence_data = np.array(single_sentiment_sequence_data)\n",
    "\n",
    "    else:\n",
    "        sentiment_sequence_data  = np.vstack((sentiment_sequence_data , np.array(single_sentiment_sequence_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.01034704],\n",
       "       [0.01034704, 1.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Having a look at the correlation\n",
    "return_comp = market_data[compTick + '_Return']\n",
    "return_comp = return_comp.drop(pd.to_datetime('2016-12-31'))\n",
    "\n",
    "np.corrcoef(return_comp, sentiment['Comp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sequences for neural network training\n",
    "\n",
    "# 90% of data will be used to train the model, 10% to test\n",
    "split_fraction = 0.9\n",
    "split_row = int(sentiment_sequence_data.shape[0] * split_fraction)\n",
    "\n",
    "train_sequences = sentiment_sequence_data[:split_row,]\n",
    "valid_sequences = sentiment_sequence_data[split_row:,]\n",
    "\n",
    "# Count the number of each population\n",
    "train_sequences.shape\n",
    "valid_sequences.shape\n",
    "\n",
    "# Create inputs and targets of the two sequences\n",
    "train_sequences_input = torch.from_numpy(train_sequences[:, :-1]).float()\n",
    "train_sequences_target = torch.from_numpy(train_sequences[:, 1:]).float()\n",
    "valid_sequences_input = torch.from_numpy(valid_sequences[:, :-1]).float()\n",
    "valid_sequences_target = torch.from_numpy(valid_sequences[:, 1:]).float()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define daily sentiment dataset\n",
    "class DailySentimentDataset(data.Dataset):\n",
    "\n",
    "    # define the class constructor\n",
    "    def __init__(self, sequences, targets):\n",
    "\n",
    "        # init sequences and corresponding targets\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    # define the length method \n",
    "    def __len__(self):\n",
    "\n",
    "        # returns the number of samples\n",
    "        return len(self.targets)\n",
    "\n",
    "    # define the get item method\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # determine single sequence and corresponding target\n",
    "        sequence = self.sequences[index, :]\n",
    "        target = self.targets[index, :]\n",
    "\n",
    "        # return sequences and target\n",
    "        return sequence, target\n",
    "\n",
    "train_dataset = DailySentimentDataset(train_sequences_input, train_sequences_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network implementation\n",
    "\n",
    "# implement the LSTMNet network architecture\n",
    "class LSTMNet(nn.Module):\n",
    "\n",
    "    # define class constructor\n",
    "    def __init__(self):\n",
    "\n",
    "        super(LSTMNet, self).__init__()\n",
    "\n",
    "        # define lstm nn architecture\n",
    "        self.lstm1 = nn.LSTMCell(1, 51)  # first lstm layer\n",
    "        self.lstm2 = nn.LSTMCell(51, 51)  # second lstm layer\n",
    "        self.linear = nn.Linear(51, 1)  # final linear layer\n",
    "\n",
    "    # define network forward pass\n",
    "    def forward(self, input):\n",
    "\n",
    "        # init predictions\n",
    "        predictions = []\n",
    "\n",
    "        # init the lstm hidden states\n",
    "        h_t = torch.zeros(input.size(0), 51, dtype=torch.float)\n",
    "        h_t2 = torch.zeros(input.size(0), 51, dtype=torch.float)\n",
    "\n",
    "        # init the lstm cell states\n",
    "        c_t = torch.zeros(input.size(0), 51, dtype=torch.float)\n",
    "        c_t2 = torch.zeros(input.size(0), 51, dtype=torch.float)\n",
    "        \n",
    "        # iterate over distinct time steps\n",
    "        for i, input_t in enumerate(input.chunk(input.size(1), dim=1)):\n",
    "\n",
    "            # propagate through time step data\n",
    "            h_t, c_t = self.lstm1(input_t, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            prediction = self.linear(h_t2)\n",
    "            \n",
    "            # collect predictions\n",
    "            predictions += [prediction]\n",
    "\n",
    "        # stack predictions\n",
    "        predictions = torch.stack(predictions, 1).squeeze(2)\n",
    "\n",
    "        # return predictions\n",
    "        return predictions\n",
    "\n",
    "lstm_model = LSTMNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing loss function\n",
    "loss_function = nn.MSELoss()\n",
    "learning_rate = 1e-06 # set constant learning rate\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=learning_rate) # define optimization technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-13:53:05] epoch: 0 train-loss: 0.010606143106189039\n",
      "[LOG 20200502-13:53:05] epoch: 1 train-loss: 0.010605756400360001\n",
      "[LOG 20200502-13:53:05] epoch: 2 train-loss: 0.010605371246735254\n",
      "[LOG 20200502-13:53:05] epoch: 3 train-loss: 0.01060498733487394\n",
      "[LOG 20200502-13:53:06] epoch: 4 train-loss: 0.010604605596098635\n",
      "[LOG 20200502-13:53:06] epoch: 5 train-loss: 0.010604224788645903\n",
      "[LOG 20200502-13:53:06] epoch: 6 train-loss: 0.010603845429917177\n",
      "[LOG 20200502-13:53:06] epoch: 7 train-loss: 0.010603467726873027\n",
      "[LOG 20200502-13:53:06] epoch: 8 train-loss: 0.010603092196914885\n",
      "[LOG 20200502-13:53:07] epoch: 9 train-loss: 0.010602717908720175\n",
      "[LOG 20200502-13:53:07] epoch: 10 train-loss: 0.010602344655328326\n",
      "[LOG 20200502-13:53:07] epoch: 10 new best train-loss: 0.010602344655328326 found\n",
      "[LOG 20200502-13:53:07] epoch: 11 train-loss: 0.0106019734715422\n",
      "[LOG 20200502-13:53:07] epoch: 12 train-loss: 0.010601603322558932\n",
      "[LOG 20200502-13:53:07] epoch: 13 train-loss: 0.010601234725779958\n",
      "[LOG 20200502-13:53:08] epoch: 14 train-loss: 0.010600868302086989\n",
      "[LOG 20200502-13:53:08] epoch: 15 train-loss: 0.01060050270623631\n",
      "[LOG 20200502-13:53:08] epoch: 16 train-loss: 0.010600139386951923\n",
      "[LOG 20200502-13:53:08] epoch: 17 train-loss: 0.010599777205950685\n",
      "[LOG 20200502-13:53:08] epoch: 18 train-loss: 0.010599417094555166\n",
      "[LOG 20200502-13:53:09] epoch: 19 train-loss: 0.010599059259725941\n",
      "[LOG 20200502-13:53:09] epoch: 20 train-loss: 0.010598702977101008\n",
      "[LOG 20200502-13:53:09] epoch: 20 new best train-loss: 0.010598702977101008 found\n",
      "[LOG 20200502-13:53:09] epoch: 21 train-loss: 0.010598347936239507\n",
      "[LOG 20200502-13:53:09] epoch: 22 train-loss: 0.010597994240621725\n",
      "[LOG 20200502-13:53:09] epoch: 23 train-loss: 0.010597642407649092\n",
      "[LOG 20200502-13:53:09] epoch: 24 train-loss: 0.010597292230361037\n",
      "[LOG 20200502-13:53:10] epoch: 25 train-loss: 0.01059694319135613\n",
      "[LOG 20200502-13:53:10] epoch: 26 train-loss: 0.010596595808035798\n",
      "[LOG 20200502-13:53:10] epoch: 27 train-loss: 0.01059625070128176\n",
      "[LOG 20200502-13:53:10] epoch: 28 train-loss: 0.010595906732810868\n",
      "[LOG 20200502-13:53:10] epoch: 29 train-loss: 0.010595564109583696\n",
      "[LOG 20200502-13:53:11] epoch: 30 train-loss: 0.010595223452481959\n",
      "[LOG 20200502-13:53:11] epoch: 30 new best train-loss: 0.010595223452481959 found\n",
      "[LOG 20200502-13:53:11] epoch: 31 train-loss: 0.01059488414062394\n",
      "[LOG 20200502-13:53:11] epoch: 32 train-loss: 0.010594545863568783\n",
      "[LOG 20200502-13:53:11] epoch: 33 train-loss: 0.01059421007004049\n",
      "[LOG 20200502-13:53:11] epoch: 34 train-loss: 0.010593876242637634\n",
      "[LOG 20200502-13:53:12] epoch: 35 train-loss: 0.010593543243077066\n",
      "[LOG 20200502-13:53:12] epoch: 36 train-loss: 0.010593212002681361\n",
      "[LOG 20200502-13:53:12] epoch: 37 train-loss: 0.01059288221100966\n",
      "[LOG 20200502-13:53:12] epoch: 38 train-loss: 0.010592554178502824\n",
      "[LOG 20200502-13:53:12] epoch: 39 train-loss: 0.010592228112121424\n",
      "[LOG 20200502-13:53:13] epoch: 40 train-loss: 0.010591902770102024\n",
      "[LOG 20200502-13:53:13] epoch: 40 new best train-loss: 0.010591902770102024 found\n",
      "[LOG 20200502-13:53:13] epoch: 41 train-loss: 0.010591579290727774\n",
      "[LOG 20200502-13:53:13] epoch: 42 train-loss: 0.010591257570518387\n",
      "[LOG 20200502-13:53:13] epoch: 43 train-loss: 0.010590938126875294\n",
      "[LOG 20200502-13:53:13] epoch: 44 train-loss: 0.01059062085631821\n",
      "[LOG 20200502-13:53:14] epoch: 45 train-loss: 0.010590304517083697\n",
      "[LOG 20200502-13:53:14] epoch: 46 train-loss: 0.010589990143974623\n",
      "[LOG 20200502-13:53:14] epoch: 47 train-loss: 0.010589677530030409\n",
      "[LOG 20200502-13:53:14] epoch: 48 train-loss: 0.010589366571770774\n",
      "[LOG 20200502-13:53:14] epoch: 49 train-loss: 0.010589056751794286\n",
      "[LOG 20200502-13:53:15] epoch: 50 train-loss: 0.010588748484022088\n",
      "[LOG 20200502-13:53:15] epoch: 50 new best train-loss: 0.010588748484022088 found\n",
      "[LOG 20200502-13:53:15] epoch: 51 train-loss: 0.010588442389335897\n",
      "[LOG 20200502-13:53:15] epoch: 52 train-loss: 0.01058813753641314\n",
      "[LOG 20200502-13:53:15] epoch: 53 train-loss: 0.010587833821773529\n",
      "[LOG 20200502-13:53:15] epoch: 54 train-loss: 0.010587533004581928\n",
      "[LOG 20200502-13:53:16] epoch: 55 train-loss: 0.010587232808272043\n",
      "[LOG 20200502-13:53:16] epoch: 56 train-loss: 0.010586934578087594\n",
      "[LOG 20200502-13:53:16] epoch: 57 train-loss: 0.01058663779662715\n",
      "[LOG 20200502-13:53:16] epoch: 58 train-loss: 0.010586342049969567\n",
      "[LOG 20200502-13:53:17] epoch: 59 train-loss: 0.010586048683358563\n",
      "[LOG 20200502-13:53:17] epoch: 60 train-loss: 0.010585757282872995\n",
      "[LOG 20200502-13:53:17] epoch: 60 new best train-loss: 0.010585757282872995 found\n",
      "[LOG 20200502-13:53:17] epoch: 61 train-loss: 0.01058546681371\n",
      "[LOG 20200502-13:53:17] epoch: 62 train-loss: 0.010585178724593587\n",
      "[LOG 20200502-13:53:17] epoch: 63 train-loss: 0.010584891566799747\n",
      "[LOG 20200502-13:53:18] epoch: 64 train-loss: 0.010584606478611628\n",
      "[LOG 20200502-13:53:18] epoch: 65 train-loss: 0.0105843229426278\n",
      "[LOG 20200502-13:53:18] epoch: 66 train-loss: 0.01058404054492712\n",
      "[LOG 20200502-13:53:18] epoch: 67 train-loss: 0.010583760216832161\n",
      "[LOG 20200502-13:53:18] epoch: 68 train-loss: 0.01058348206182321\n",
      "[LOG 20200502-13:53:19] epoch: 69 train-loss: 0.010583205459018549\n",
      "[LOG 20200502-13:53:19] epoch: 70 train-loss: 0.010582930511898465\n",
      "[LOG 20200502-13:53:19] epoch: 70 new best train-loss: 0.010582930511898465 found\n",
      "[LOG 20200502-13:53:19] epoch: 71 train-loss: 0.010582657220462957\n",
      "[LOG 20200502-13:53:19] epoch: 72 train-loss: 0.0105823857916726\n",
      "[LOG 20200502-13:53:19] epoch: 73 train-loss: 0.010582115708125962\n",
      "[LOG 20200502-13:53:20] epoch: 74 train-loss: 0.010581847176783614\n",
      "[LOG 20200502-13:53:20] epoch: 75 train-loss: 0.010581580197645558\n",
      "[LOG 20200502-13:53:20] epoch: 76 train-loss: 0.010581315081152651\n",
      "[LOG 20200502-13:53:20] epoch: 77 train-loss: 0.01058105193078518\n",
      "[LOG 20200502-13:53:20] epoch: 78 train-loss: 0.010580789815220568\n",
      "[LOG 20200502-13:53:21] epoch: 79 train-loss: 0.010580530079702536\n",
      "[LOG 20200502-13:53:21] epoch: 80 train-loss: 0.010580271172026793\n",
      "[LOG 20200502-13:53:21] epoch: 80 new best train-loss: 0.010580271172026793 found\n",
      "[LOG 20200502-13:53:21] epoch: 81 train-loss: 0.01058001433395677\n",
      "[LOG 20200502-13:53:21] epoch: 82 train-loss: 0.010579759048091041\n",
      "[LOG 20200502-13:53:21] epoch: 83 train-loss: 0.010579505314429602\n",
      "[LOG 20200502-13:53:21] epoch: 84 train-loss: 0.01057925344341331\n",
      "[LOG 20200502-13:53:22] epoch: 85 train-loss: 0.010579003228081597\n",
      "[LOG 20200502-13:53:22] epoch: 86 train-loss: 0.010578754564954175\n",
      "[LOG 20200502-13:53:22] epoch: 87 train-loss: 0.010578507040109899\n",
      "[LOG 20200502-13:53:22] epoch: 88 train-loss: 0.010578261791831918\n",
      "[LOG 20200502-13:53:22] epoch: 89 train-loss: 0.010578017267915938\n",
      "[LOG 20200502-13:53:23] epoch: 90 train-loss: 0.010577776055369113\n",
      "[LOG 20200502-13:53:23] epoch: 90 new best train-loss: 0.010577776055369113 found\n",
      "[LOG 20200502-13:53:23] epoch: 91 train-loss: 0.010577534842822287\n",
      "[LOG 20200502-13:53:23] epoch: 92 train-loss: 0.010577296113802327\n",
      "[LOG 20200502-13:53:23] epoch: 93 train-loss: 0.010577059350907803\n",
      "[LOG 20200502-13:53:23] epoch: 94 train-loss: 0.010576824243697856\n",
      "[LOG 20200502-13:53:24] epoch: 95 train-loss: 0.010576590481731627\n",
      "[LOG 20200502-13:53:24] epoch: 96 train-loss: 0.010576358685890833\n",
      "[LOG 20200502-13:53:24] epoch: 97 train-loss: 0.010576128235293759\n",
      "[LOG 20200502-13:53:24] epoch: 98 train-loss: 0.010575899647341834\n",
      "[LOG 20200502-13:53:25] epoch: 99 train-loss: 0.010575672715074487\n",
      "[LOG 20200502-13:53:25] epoch: 100 train-loss: 0.010575447128050856\n",
      "[LOG 20200502-13:53:25] epoch: 100 new best train-loss: 0.010575447128050856 found\n",
      "[LOG 20200502-13:53:25] epoch: 101 train-loss: 0.010575223714113235\n",
      "[LOG 20200502-13:53:25] epoch: 102 train-loss: 0.010575001334978474\n",
      "[LOG 20200502-13:53:25] epoch: 103 train-loss: 0.010574781128929721\n",
      "[LOG 20200502-13:53:26] epoch: 104 train-loss: 0.0105745621646444\n",
      "[LOG 20200502-13:53:26] epoch: 105 train-loss: 0.010574344959523942\n",
      "[LOG 20200502-13:53:26] epoch: 106 train-loss: 0.010574128996166918\n",
      "[LOG 20200502-13:53:26] epoch: 107 train-loss: 0.0105739152058959\n",
      "[LOG 20200502-13:53:26] epoch: 108 train-loss: 0.010573702657388316\n",
      "[LOG 20200502-13:53:27] epoch: 109 train-loss: 0.010573491868045595\n",
      "[LOG 20200502-13:53:27] epoch: 110 train-loss: 0.010573283044828309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-13:53:27] epoch: 110 new best train-loss: 0.010573283044828309 found\n",
      "[LOG 20200502-13:53:27] epoch: 111 train-loss: 0.010573075256413884\n",
      "[LOG 20200502-13:53:27] epoch: 112 train-loss: 0.010572869020203749\n",
      "[LOG 20200502-13:53:27] epoch: 113 train-loss: 0.010572664853599336\n",
      "[LOG 20200502-13:53:28] epoch: 114 train-loss: 0.010572462239199214\n",
      "[LOG 20200502-13:53:28] epoch: 115 train-loss: 0.010572261177003384\n",
      "[LOG 20200502-13:53:28] epoch: 116 train-loss: 0.010572061873972416\n",
      "[LOG 20200502-13:53:28] epoch: 117 train-loss: 0.01057186360574431\n",
      "[LOG 20200502-13:53:28] epoch: 118 train-loss: 0.010571667924523354\n",
      "[LOG 20200502-13:53:29] epoch: 119 train-loss: 0.010571473898986975\n",
      "[LOG 20200502-13:53:29] epoch: 120 train-loss: 0.010571280701292885\n",
      "[LOG 20200502-13:53:29] epoch: 120 new best train-loss: 0.010571280701292885 found\n",
      "[LOG 20200502-13:53:29] epoch: 121 train-loss: 0.010571089676684804\n",
      "[LOG 20200502-13:53:29] epoch: 122 train-loss: 0.01057090051472187\n",
      "[LOG 20200502-13:53:29] epoch: 123 train-loss: 0.010570712801482942\n",
      "[LOG 20200502-13:53:30] epoch: 124 train-loss: 0.01057052705436945\n",
      "[LOG 20200502-13:53:30] epoch: 125 train-loss: 0.010570342652499676\n",
      "[LOG 20200502-13:53:30] epoch: 126 train-loss: 0.01057015959587362\n",
      "[LOG 20200502-13:53:30] epoch: 127 train-loss: 0.01056997798797157\n",
      "[LOG 20200502-13:53:30] epoch: 128 train-loss: 0.010569797932273813\n",
      "[LOG 20200502-13:53:31] epoch: 129 train-loss: 0.010569619946181774\n",
      "[LOG 20200502-13:53:31] epoch: 130 train-loss: 0.010569443615774313\n",
      "[LOG 20200502-13:53:31] epoch: 130 new best train-loss: 0.010569443615774313 found\n",
      "[LOG 20200502-13:53:31] epoch: 131 train-loss: 0.010569268527130285\n",
      "[LOG 20200502-13:53:31] epoch: 132 train-loss: 0.010569095404611694\n",
      "[LOG 20200502-13:53:31] epoch: 133 train-loss: 0.010568923213415675\n",
      "[LOG 20200502-13:53:32] epoch: 134 train-loss: 0.01056875360922681\n",
      "[LOG 20200502-13:53:32] epoch: 135 train-loss: 0.01056858462591966\n",
      "[LOG 20200502-13:53:32] epoch: 136 train-loss: 0.010568417298297087\n",
      "[LOG 20200502-13:53:32] epoch: 137 train-loss: 0.010568252143760523\n",
      "[LOG 20200502-13:53:32] epoch: 138 train-loss: 0.01056808823098739\n",
      "[LOG 20200502-13:53:33] epoch: 139 train-loss: 0.010567926284339692\n",
      "[LOG 20200502-13:53:33] epoch: 140 train-loss: 0.010567765165534284\n",
      "[LOG 20200502-13:53:33] epoch: 140 new best train-loss: 0.010567765165534284 found\n",
      "[LOG 20200502-13:53:33] epoch: 141 train-loss: 0.010567606012854312\n",
      "[LOG 20200502-13:53:33] epoch: 142 train-loss: 0.010567448619339202\n",
      "[LOG 20200502-13:53:33] epoch: 143 train-loss: 0.01056729257106781\n",
      "[LOG 20200502-13:53:34] epoch: 144 train-loss: 0.010567137868040137\n",
      "[LOG 20200502-13:53:34] epoch: 145 train-loss: 0.010566985441578759\n",
      "[LOG 20200502-13:53:34] epoch: 146 train-loss: 0.010566834256880812\n",
      "[LOG 20200502-13:53:34] epoch: 147 train-loss: 0.010566685141788589\n",
      "[LOG 20200502-13:53:34] epoch: 148 train-loss: 0.01056653716497951\n",
      "[LOG 20200502-13:53:35] epoch: 149 train-loss: 0.010566390636894438\n",
      "[LOG 20200502-13:53:35] epoch: 150 train-loss: 0.010566245764493942\n",
      "[LOG 20200502-13:53:35] epoch: 150 new best train-loss: 0.010566245764493942 found\n",
      "[LOG 20200502-13:53:35] epoch: 151 train-loss: 0.010566102444297738\n",
      "[LOG 20200502-13:53:35] epoch: 152 train-loss: 0.010565960883266397\n",
      "[LOG 20200502-13:53:35] epoch: 153 train-loss: 0.010565821081399918\n",
      "[LOG 20200502-13:53:36] epoch: 154 train-loss: 0.010565682003895441\n",
      "[LOG 20200502-13:53:36] epoch: 155 train-loss: 0.010565545409917831\n",
      "[LOG 20200502-13:53:36] epoch: 156 train-loss: 0.010565409540302224\n",
      "[LOG 20200502-13:53:36] epoch: 157 train-loss: 0.010565275843772624\n",
      "[LOG 20200502-13:53:37] epoch: 158 train-loss: 0.010565143182045884\n",
      "[LOG 20200502-13:53:37] epoch: 159 train-loss: 0.010565012589924865\n",
      "[LOG 20200502-13:53:37] epoch: 160 train-loss: 0.010564882825646136\n",
      "[LOG 20200502-13:53:37] epoch: 160 new best train-loss: 0.010564882825646136 found\n",
      "[LOG 20200502-13:53:37] epoch: 161 train-loss: 0.01056475554489427\n",
      "[LOG 20200502-13:53:37] epoch: 162 train-loss: 0.010564628678063551\n",
      "[LOG 20200502-13:53:38] epoch: 163 train-loss: 0.010564504191279411\n",
      "[LOG 20200502-13:53:38] epoch: 164 train-loss: 0.01056438104973899\n",
      "[LOG 20200502-13:53:38] epoch: 165 train-loss: 0.010564258529080285\n",
      "[LOG 20200502-13:53:38] epoch: 166 train-loss: 0.010564138595428731\n",
      "[LOG 20200502-13:53:38] epoch: 167 train-loss: 0.010564019903540611\n",
      "[LOG 20200502-13:53:39] epoch: 168 train-loss: 0.010563902349935638\n",
      "[LOG 20200502-13:53:39] epoch: 169 train-loss: 0.010563786555495527\n",
      "[LOG 20200502-13:53:39] epoch: 170 train-loss: 0.010563672313259708\n",
      "[LOG 20200502-13:53:39] epoch: 170 new best train-loss: 0.010563672313259708 found\n",
      "[LOG 20200502-13:53:39] epoch: 171 train-loss: 0.010563559105826749\n",
      "[LOG 20200502-13:53:39] epoch: 172 train-loss: 0.010563448071479797\n",
      "[LOG 20200502-13:53:40] epoch: 173 train-loss: 0.010563338692817423\n",
      "[LOG 20200502-13:53:40] epoch: 174 train-loss: 0.010563230969839625\n",
      "[LOG 20200502-13:53:40] epoch: 175 train-loss: 0.010563123660782972\n",
      "[LOG 20200502-13:53:40] epoch: 176 train-loss: 0.0105630187317729\n",
      "[LOG 20200502-13:53:40] epoch: 177 train-loss: 0.01056291556192769\n",
      "[LOG 20200502-13:53:41] epoch: 178 train-loss: 0.010562813323405053\n",
      "[LOG 20200502-13:53:41] epoch: 179 train-loss: 0.01056271263708671\n",
      "[LOG 20200502-13:53:41] epoch: 180 train-loss: 0.010562612985571226\n",
      "[LOG 20200502-13:53:41] epoch: 180 new best train-loss: 0.010562612985571226 found\n",
      "[LOG 20200502-13:53:41] epoch: 181 train-loss: 0.010562515093220605\n",
      "[LOG 20200502-13:53:41] epoch: 182 train-loss: 0.010562418442633417\n",
      "[LOG 20200502-13:53:42] epoch: 183 train-loss: 0.010562324068612523\n",
      "[LOG 20200502-13:53:42] epoch: 184 train-loss: 0.010562230625914203\n",
      "[LOG 20200502-13:53:42] epoch: 185 train-loss: 0.010562138735420175\n",
      "[LOG 20200502-13:53:42] epoch: 186 train-loss: 0.010562047983209292\n",
      "[LOG 20200502-13:53:42] epoch: 187 train-loss: 0.010561958990163274\n",
      "[LOG 20200502-13:53:43] epoch: 188 train-loss: 0.010561871549321545\n",
      "[LOG 20200502-13:53:43] epoch: 189 train-loss: 0.010561784729361534\n",
      "[LOG 20200502-13:53:43] epoch: 190 train-loss: 0.010561700082487531\n",
      "[LOG 20200502-13:53:43] epoch: 190 new best train-loss: 0.010561700082487531 found\n",
      "[LOG 20200502-13:53:43] epoch: 191 train-loss: 0.010561616884337531\n",
      "[LOG 20200502-13:53:43] epoch: 192 train-loss: 0.010561534617510106\n",
      "[LOG 20200502-13:53:44] epoch: 193 train-loss: 0.010561454213327832\n",
      "[LOG 20200502-13:53:44] epoch: 194 train-loss: 0.010561374843948416\n",
      "[LOG 20200502-13:53:44] epoch: 195 train-loss: 0.010561297233733866\n",
      "[LOG 20200502-13:53:44] epoch: 196 train-loss: 0.010561220658322176\n",
      "[LOG 20200502-13:53:44] epoch: 197 train-loss: 0.010561146255996492\n",
      "[LOG 20200502-13:53:45] epoch: 198 train-loss: 0.010561072681513097\n",
      "[LOG 20200502-13:53:45] epoch: 199 train-loss: 0.010561000245312849\n",
      "[LOG 20200502-13:53:45] epoch: 200 train-loss: 0.010560929568277465\n",
      "[LOG 20200502-13:53:45] epoch: 200 new best train-loss: 0.010560929568277465 found\n",
      "[LOG 20200502-13:53:45] epoch: 201 train-loss: 0.010560860546926657\n",
      "[LOG 20200502-13:53:45] epoch: 202 train-loss: 0.010560792767339282\n",
      "[LOG 20200502-13:53:46] epoch: 203 train-loss: 0.010560726126035055\n",
      "[LOG 20200502-13:53:46] epoch: 204 train-loss: 0.010560661347375976\n",
      "[LOG 20200502-13:53:46] epoch: 205 train-loss: 0.010560597396559186\n",
      "[LOG 20200502-13:53:46] epoch: 206 train-loss: 0.010560535204907259\n",
      "[LOG 20200502-13:53:46] epoch: 207 train-loss: 0.010560474048058191\n",
      "[LOG 20200502-13:53:47] epoch: 208 train-loss: 0.010560414443413416\n",
      "[LOG 20200502-13:53:47] epoch: 209 train-loss: 0.01056035618401236\n",
      "[LOG 20200502-13:53:47] epoch: 210 train-loss: 0.010560299269855022\n",
      "[LOG 20200502-13:53:47] epoch: 210 new best train-loss: 0.010560299269855022 found\n",
      "[LOG 20200502-13:53:47] epoch: 211 train-loss: 0.01056024380442169\n",
      "[LOG 20200502-13:53:47] epoch: 212 train-loss: 0.010560189787712362\n",
      "[LOG 20200502-13:53:48] epoch: 213 train-loss: 0.010560136288404465\n",
      "[LOG 20200502-13:53:48] epoch: 214 train-loss: 0.010560085272623433\n",
      "[LOG 20200502-13:53:48] epoch: 215 train-loss: 0.010560034670763545\n",
      "[LOG 20200502-13:53:48] epoch: 216 train-loss: 0.01055998613850938\n",
      "[LOG 20200502-13:53:48] epoch: 217 train-loss: 0.010559938123656644\n",
      "[LOG 20200502-13:53:49] epoch: 218 train-loss: 0.010559891764488485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-13:53:49] epoch: 219 train-loss: 0.010559846957524618\n",
      "[LOG 20200502-13:53:49] epoch: 220 train-loss: 0.010559803392324183\n",
      "[LOG 20200502-13:53:49] epoch: 220 new best train-loss: 0.010559803392324183 found\n",
      "[LOG 20200502-13:53:49] epoch: 221 train-loss: 0.010559761172367467\n",
      "[LOG 20200502-13:53:49] epoch: 222 train-loss: 0.010559719883733325\n",
      "[LOG 20200502-13:53:50] epoch: 223 train-loss: 0.010559679836862616\n",
      "[LOG 20200502-13:53:50] epoch: 224 train-loss: 0.010559641652637057\n",
      "[LOG 20200502-13:53:50] epoch: 225 train-loss: 0.0105596041927735\n",
      "[LOG 20200502-13:53:50] epoch: 226 train-loss: 0.010559568492074808\n",
      "[LOG 20200502-13:53:50] epoch: 227 train-loss: 0.010559533515738117\n",
      "[LOG 20200502-13:53:51] epoch: 228 train-loss: 0.010559500091605716\n",
      "[LOG 20200502-13:53:51] epoch: 229 train-loss: 0.010559467805756463\n",
      "[LOG 20200502-13:53:51] epoch: 230 train-loss: 0.010559436865150928\n",
      "[LOG 20200502-13:53:51] epoch: 230 new best train-loss: 0.010559436865150928 found\n",
      "[LOG 20200502-13:53:51] epoch: 231 train-loss: 0.010559407890670829\n",
      "[LOG 20200502-13:53:51] epoch: 232 train-loss: 0.010559379640552733\n",
      "[LOG 20200502-13:53:52] epoch: 233 train-loss: 0.010559352735678354\n",
      "[LOG 20200502-13:53:52] epoch: 234 train-loss: 0.010559326865606837\n",
      "[LOG 20200502-13:53:52] epoch: 235 train-loss: 0.010559302444259325\n",
      "[LOG 20200502-13:53:52] epoch: 236 train-loss: 0.010559278954234388\n",
      "[LOG 20200502-13:53:52] epoch: 237 train-loss: 0.010559256705972884\n",
      "[LOG 20200502-13:53:53] epoch: 238 train-loss: 0.010559235595994525\n",
      "[LOG 20200502-13:53:53] epoch: 239 train-loss: 0.010559216348661317\n",
      "[LOG 20200502-13:53:53] epoch: 240 train-loss: 0.010559197204808394\n",
      "[LOG 20200502-13:53:53] epoch: 240 new best train-loss: 0.010559197204808394 found\n",
      "[LOG 20200502-13:53:53] epoch: 241 train-loss: 0.01055918044100205\n",
      "[LOG 20200502-13:53:53] epoch: 242 train-loss: 0.010559164298077425\n",
      "[LOG 20200502-13:53:54] epoch: 243 train-loss: 0.010559149707357088\n",
      "[LOG 20200502-13:53:54] epoch: 244 train-loss: 0.010559135840998756\n",
      "[LOG 20200502-13:53:54] epoch: 245 train-loss: 0.010559122905962996\n",
      "[LOG 20200502-13:53:54] epoch: 246 train-loss: 0.010559111523131529\n",
      "[LOG 20200502-13:53:54] epoch: 247 train-loss: 0.010559101278583208\n",
      "[LOG 20200502-13:53:55] epoch: 248 train-loss: 0.010559092172318034\n",
      "[LOG 20200502-13:53:55] epoch: 249 train-loss: 0.01055908441129658\n",
      "[LOG 20200502-13:53:55] epoch: 250 train-loss: 0.01055907778855827\n",
      "[LOG 20200502-13:53:55] epoch: 250 new best train-loss: 0.01055907778855827 found\n",
      "[LOG 20200502-13:53:55] epoch: 251 train-loss: 0.010559072511063682\n",
      "[LOG 20200502-13:53:55] epoch: 252 train-loss: 0.01055906806141138\n",
      "[LOG 20200502-13:53:56] epoch: 253 train-loss: 0.010559064543081654\n",
      "[LOG 20200502-13:53:56] epoch: 254 train-loss: 0.01055906257695622\n",
      "[LOG 20200502-13:53:56] epoch: 255 train-loss: 0.01055906174911393\n",
      "[LOG 20200502-13:53:56] epoch: 256 train-loss: 0.010559061852594217\n",
      "[LOG 20200502-13:53:57] epoch: 257 train-loss: 0.010559063094357649\n",
      "[LOG 20200502-13:53:57] epoch: 258 train-loss: 0.010559065784845088\n",
      "[LOG 20200502-13:53:57] epoch: 259 train-loss: 0.010559069303174814\n",
      "[LOG 20200502-13:53:57] epoch: 260 train-loss: 0.010559073959787687\n",
      "[LOG 20200502-13:53:57] epoch: 260 new best train-loss: 0.010559073959787687 found\n",
      "[LOG 20200502-13:53:57] epoch: 261 train-loss: 0.010559079754683707\n",
      "[LOG 20200502-13:53:58] epoch: 262 train-loss: 0.010559086584382586\n",
      "[LOG 20200502-13:53:58] epoch: 263 train-loss: 0.010559094034963183\n",
      "[LOG 20200502-13:53:58] epoch: 264 train-loss: 0.010559103451669216\n",
      "[LOG 20200502-13:53:58] epoch: 265 train-loss: 0.010559113696217537\n",
      "[LOG 20200502-13:53:58] epoch: 266 train-loss: 0.010559124458167288\n",
      "[LOG 20200502-13:53:59] epoch: 267 train-loss: 0.010559136358400186\n",
      "[LOG 20200502-13:53:59] epoch: 268 train-loss: 0.010559150535199378\n",
      "[LOG 20200502-13:53:59] epoch: 269 train-loss: 0.010559164505037997\n",
      "[LOG 20200502-13:53:59] epoch: 270 train-loss: 0.010559179820120335\n",
      "[LOG 20200502-13:53:59] epoch: 271 train-loss: 0.010559196170005534\n",
      "[LOG 20200502-13:54:00] epoch: 272 train-loss: 0.010559213554693593\n",
      "[LOG 20200502-13:54:00] epoch: 273 train-loss: 0.010559232491585944\n",
      "[LOG 20200502-13:54:00] epoch: 274 train-loss: 0.010559251324998008\n",
      "[LOG 20200502-13:54:00] epoch: 275 train-loss: 0.010559272641936937\n",
      "[LOG 20200502-13:54:01] epoch: 276 train-loss: 0.010559293234513866\n",
      "[LOG 20200502-13:54:01] epoch: 277 train-loss: 0.010559316414097944\n",
      "[LOG 20200502-13:54:01] epoch: 278 train-loss: 0.010559340111083455\n",
      "[LOG 20200502-13:54:01] epoch: 279 train-loss: 0.010559364221990108\n",
      "[LOG 20200502-13:54:01] epoch: 280 train-loss: 0.010559390092061626\n",
      "[LOG 20200502-13:54:02] epoch: 281 train-loss: 0.010559417203896575\n",
      "[LOG 20200502-13:54:02] epoch: 282 train-loss: 0.010559444315731525\n",
      "[LOG 20200502-13:54:02] epoch: 283 train-loss: 0.010559472669329908\n",
      "[LOG 20200502-13:54:02] epoch: 284 train-loss: 0.010559502678612867\n",
      "[LOG 20200502-13:54:02] epoch: 285 train-loss: 0.01055953341225783\n",
      "[LOG 20200502-13:54:02] epoch: 286 train-loss: 0.01055956476678451\n",
      "[LOG 20200502-13:54:03] epoch: 287 train-loss: 0.010559597259594334\n",
      "[LOG 20200502-13:54:03] epoch: 288 train-loss: 0.010559630166325305\n",
      "[LOG 20200502-13:54:03] epoch: 289 train-loss: 0.010559664935701422\n",
      "[LOG 20200502-13:54:03] epoch: 290 train-loss: 0.010559699912038114\n",
      "[LOG 20200502-13:54:04] epoch: 291 train-loss: 0.010559736751019955\n",
      "[LOG 20200502-13:54:04] epoch: 292 train-loss: 0.01055977369348208\n",
      "[LOG 20200502-13:54:04] epoch: 293 train-loss: 0.010559811463786496\n",
      "[LOG 20200502-13:54:04] epoch: 294 train-loss: 0.010559850682814917\n",
      "[LOG 20200502-13:54:04] epoch: 295 train-loss: 0.010559890419244766\n",
      "[LOG 20200502-13:54:05] epoch: 296 train-loss: 0.010559931293957762\n",
      "[LOG 20200502-13:54:05] epoch: 297 train-loss: 0.01055997320347362\n",
      "[LOG 20200502-13:54:05] epoch: 298 train-loss: 0.01056001480254862\n",
      "[LOG 20200502-13:54:05] epoch: 299 train-loss: 0.010560058885150485\n",
      "[LOG 20200502-13:54:05] epoch: 300 train-loss: 0.01056010245035092\n",
      "[LOG 20200502-13:54:06] epoch: 301 train-loss: 0.010560148085157076\n",
      "[LOG 20200502-13:54:06] epoch: 302 train-loss: 0.010560193926923804\n",
      "[LOG 20200502-13:54:06] epoch: 303 train-loss: 0.01056024090697368\n",
      "[LOG 20200502-13:54:06] epoch: 304 train-loss: 0.010560288714865843\n",
      "[LOG 20200502-13:54:06] epoch: 305 train-loss: 0.010560336626238294\n",
      "[LOG 20200502-13:54:07] epoch: 306 train-loss: 0.010560386503736178\n",
      "[LOG 20200502-13:54:07] epoch: 307 train-loss: 0.01056043648471435\n",
      "[LOG 20200502-13:54:07] epoch: 308 train-loss: 0.010560487190054523\n",
      "[LOG 20200502-13:54:07] epoch: 309 train-loss: 0.010560538930197557\n",
      "[LOG 20200502-13:54:07] epoch: 310 train-loss: 0.01056059201558431\n",
      "[LOG 20200502-13:54:08] epoch: 311 train-loss: 0.010560645100971064\n",
      "[LOG 20200502-13:54:08] epoch: 312 train-loss: 0.010560699531601535\n",
      "[LOG 20200502-13:54:08] epoch: 313 train-loss: 0.010560753962232007\n",
      "[LOG 20200502-13:54:08] epoch: 314 train-loss: 0.0105608104624682\n",
      "[LOG 20200502-13:54:08] epoch: 315 train-loss: 0.010560867066184679\n",
      "[LOG 20200502-13:54:09] epoch: 316 train-loss: 0.010560923980342017\n",
      "[LOG 20200502-13:54:09] epoch: 317 train-loss: 0.010560982136262788\n",
      "[LOG 20200502-13:54:09] epoch: 318 train-loss: 0.010561041430466704\n",
      "[LOG 20200502-13:54:09] epoch: 319 train-loss: 0.01056110155251291\n",
      "[LOG 20200502-13:54:09] epoch: 320 train-loss: 0.010561161778039403\n",
      "[LOG 20200502-13:54:10] epoch: 321 train-loss: 0.010561223348809613\n",
      "[LOG 20200502-13:54:10] epoch: 322 train-loss: 0.010561284919579824\n",
      "[LOG 20200502-13:54:10] epoch: 323 train-loss: 0.01056134793907404\n",
      "[LOG 20200502-13:54:10] epoch: 324 train-loss: 0.0105614113724894\n",
      "[LOG 20200502-13:54:10] epoch: 325 train-loss: 0.010561475426786475\n",
      "[LOG 20200502-13:54:11] epoch: 326 train-loss: 0.010561539895004697\n",
      "[LOG 20200502-13:54:11] epoch: 327 train-loss: 0.010561605191065205\n",
      "[LOG 20200502-13:54:11] epoch: 328 train-loss: 0.010561671004527144\n",
      "[LOG 20200502-13:54:11] epoch: 329 train-loss: 0.010561738059752517\n",
      "[LOG 20200502-13:54:11] epoch: 330 train-loss: 0.01056180563237932\n",
      "[LOG 20200502-13:54:12] epoch: 331 train-loss: 0.010561873515446981\n",
      "[LOG 20200502-13:54:12] epoch: 332 train-loss: 0.010561942329837216\n",
      "[LOG 20200502-13:54:12] epoch: 333 train-loss: 0.010562011765109168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-13:54:12] epoch: 334 train-loss: 0.010562082028223408\n",
      "[LOG 20200502-13:54:12] epoch: 335 train-loss: 0.01056215249829822\n",
      "[LOG 20200502-13:54:13] epoch: 336 train-loss: 0.010562224313616753\n",
      "[LOG 20200502-13:54:13] epoch: 337 train-loss: 0.010562295921974711\n",
      "[LOG 20200502-13:54:13] epoch: 338 train-loss: 0.010562368772096105\n",
      "[LOG 20200502-13:54:13] epoch: 339 train-loss: 0.010562442657020357\n",
      "[LOG 20200502-13:54:13] epoch: 340 train-loss: 0.010562516334984038\n",
      "[LOG 20200502-13:54:14] epoch: 341 train-loss: 0.01056259104775058\n",
      "[LOG 20200502-13:54:14] epoch: 342 train-loss: 0.01056266607095798\n",
      "[LOG 20200502-13:54:14] epoch: 343 train-loss: 0.01056274181852738\n",
      "[LOG 20200502-13:54:14] epoch: 344 train-loss: 0.01056281839393907\n",
      "[LOG 20200502-13:54:14] epoch: 345 train-loss: 0.010562895176311335\n",
      "[LOG 20200502-13:54:15] epoch: 346 train-loss: 0.010562973303927315\n",
      "[LOG 20200502-13:54:15] epoch: 347 train-loss: 0.010563051017622152\n",
      "[LOG 20200502-13:54:15] epoch: 348 train-loss: 0.010563129559159279\n",
      "[LOG 20200502-13:54:15] epoch: 349 train-loss: 0.010563209445940124\n",
      "[LOG 20200502-13:54:15] epoch: 350 train-loss: 0.010563288815319538\n",
      "[LOG 20200502-13:54:16] epoch: 351 train-loss: 0.010563369426462386\n",
      "[LOG 20200502-13:54:16] epoch: 352 train-loss: 0.01056345034804609\n",
      "[LOG 20200502-13:54:16] epoch: 353 train-loss: 0.010563531890511513\n",
      "[LOG 20200502-13:54:16] epoch: 354 train-loss: 0.01056361353645722\n",
      "[LOG 20200502-13:54:16] epoch: 355 train-loss: 0.010563696113725504\n",
      "[LOG 20200502-13:54:17] epoch: 356 train-loss: 0.01056377910491493\n",
      "[LOG 20200502-13:54:17] epoch: 357 train-loss: 0.010563862716986073\n",
      "[LOG 20200502-13:54:17] epoch: 358 train-loss: 0.010563946846458647\n",
      "[LOG 20200502-13:54:17] epoch: 359 train-loss: 0.010564030562010076\n",
      "[LOG 20200502-13:54:17] epoch: 360 train-loss: 0.010564115622805225\n",
      "[LOG 20200502-13:54:18] epoch: 361 train-loss: 0.010564200890560945\n",
      "[LOG 20200502-13:54:18] epoch: 362 train-loss: 0.01056428657223781\n",
      "[LOG 20200502-13:54:18] epoch: 363 train-loss: 0.010564372564355532\n",
      "[LOG 20200502-13:54:18] epoch: 364 train-loss: 0.010564459798236689\n",
      "[LOG 20200502-13:54:18] epoch: 365 train-loss: 0.010564546204275556\n",
      "[LOG 20200502-13:54:19] epoch: 366 train-loss: 0.010564633645117283\n",
      "[LOG 20200502-13:54:19] epoch: 367 train-loss: 0.010564722120761871\n",
      "[LOG 20200502-13:54:19] epoch: 368 train-loss: 0.010564810492926173\n",
      "[LOG 20200502-13:54:19] epoch: 369 train-loss: 0.010564899072051048\n",
      "[LOG 20200502-13:54:19] epoch: 370 train-loss: 0.01056498878945907\n",
      "[LOG 20200502-13:54:20] epoch: 371 train-loss: 0.010565078403386805\n",
      "[LOG 20200502-13:54:20] epoch: 372 train-loss: 0.010565168741676543\n",
      "[LOG 20200502-13:54:20] epoch: 373 train-loss: 0.010565258769525422\n",
      "[LOG 20200502-13:54:20] epoch: 374 train-loss: 0.010565350142618021\n",
      "[LOG 20200502-13:54:20] epoch: 375 train-loss: 0.010565441412230333\n",
      "[LOG 20200502-13:54:21] epoch: 376 train-loss: 0.010565533406204648\n",
      "[LOG 20200502-13:54:21] epoch: 377 train-loss: 0.010565625089738104\n",
      "[LOG 20200502-13:54:21] epoch: 378 train-loss: 0.010565717808074422\n",
      "[LOG 20200502-13:54:21] epoch: 379 train-loss: 0.010565811147292456\n",
      "[LOG 20200502-13:54:21] epoch: 380 train-loss: 0.010565903969109058\n",
      "[LOG 20200502-13:54:22] epoch: 381 train-loss: 0.010565997308327092\n",
      "[LOG 20200502-13:54:22] epoch: 382 train-loss: 0.010566091164946556\n",
      "[LOG 20200502-13:54:22] epoch: 383 train-loss: 0.010566185849408308\n",
      "[LOG 20200502-13:54:22] epoch: 384 train-loss: 0.01056628032690949\n",
      "[LOG 20200502-13:54:22] epoch: 385 train-loss: 0.01056637583921353\n",
      "[LOG 20200502-13:54:23] epoch: 386 train-loss: 0.01056647083411614\n",
      "[LOG 20200502-13:54:23] epoch: 387 train-loss: 0.01056656665686104\n",
      "[LOG 20200502-13:54:23] epoch: 388 train-loss: 0.010566662583086226\n",
      "[LOG 20200502-13:54:23] epoch: 389 train-loss: 0.010566758819752269\n",
      "[LOG 20200502-13:54:23] epoch: 390 train-loss: 0.010566855263378885\n",
      "[LOG 20200502-13:54:24] epoch: 391 train-loss: 0.010566951810485788\n",
      "[LOG 20200502-13:54:24] epoch: 392 train-loss: 0.010567048978474405\n",
      "[LOG 20200502-13:54:24] epoch: 393 train-loss: 0.010567145939502452\n",
      "[LOG 20200502-13:54:24] epoch: 394 train-loss: 0.010567243314451642\n",
      "[LOG 20200502-13:54:24] epoch: 395 train-loss: 0.010567341103321977\n",
      "[LOG 20200502-13:54:25] epoch: 396 train-loss: 0.010567438581751453\n",
      "[LOG 20200502-13:54:25] epoch: 397 train-loss: 0.010567537508904934\n",
      "[LOG 20200502-13:54:25] epoch: 398 train-loss: 0.0105676358151767\n",
      "[LOG 20200502-13:54:25] epoch: 399 train-loss: 0.010567733914487891\n",
      "[LOG 20200502-13:54:25] epoch: 400 train-loss: 0.010567833152082231\n",
      "[LOG 20200502-13:54:26] epoch: 401 train-loss: 0.010567932286196284\n",
      "[LOG 20200502-13:54:26] epoch: 402 train-loss: 0.010568031523790624\n",
      "[LOG 20200502-13:54:26] epoch: 403 train-loss: 0.010568131278786395\n",
      "[LOG 20200502-13:54:26] epoch: 404 train-loss: 0.01056823113726245\n",
      "[LOG 20200502-13:54:26] epoch: 405 train-loss: 0.010568330788777934\n",
      "[LOG 20200502-13:54:27] epoch: 406 train-loss: 0.010568431682056852\n",
      "[LOG 20200502-13:54:27] epoch: 407 train-loss: 0.010568532057934336\n",
      "[LOG 20200502-13:54:27] epoch: 408 train-loss: 0.01056863253729211\n",
      "[LOG 20200502-13:54:27] epoch: 409 train-loss: 0.010568733430571027\n",
      "[LOG 20200502-13:54:27] epoch: 410 train-loss: 0.010568834841251373\n",
      "[LOG 20200502-13:54:28] epoch: 411 train-loss: 0.010568936355412006\n",
      "[LOG 20200502-13:54:28] epoch: 412 train-loss: 0.010569037248690924\n",
      "[LOG 20200502-13:54:28] epoch: 413 train-loss: 0.0105691391767727\n",
      "[LOG 20200502-13:54:28] epoch: 414 train-loss: 0.010569241104854478\n",
      "[LOG 20200502-13:54:28] epoch: 415 train-loss: 0.010569343032936255\n",
      "[LOG 20200502-13:54:29] epoch: 416 train-loss: 0.01056944527145889\n",
      "[LOG 20200502-13:54:29] epoch: 417 train-loss: 0.010569547406501241\n",
      "[LOG 20200502-13:54:29] epoch: 418 train-loss: 0.010569649851984449\n",
      "[LOG 20200502-13:54:29] epoch: 419 train-loss: 0.010569751987026798\n",
      "[LOG 20200502-13:54:30] epoch: 420 train-loss: 0.010569855674273439\n",
      "[LOG 20200502-13:54:30] epoch: 421 train-loss: 0.010569958430197503\n",
      "[LOG 20200502-13:54:30] epoch: 422 train-loss: 0.01057006118612157\n",
      "[LOG 20200502-13:54:30] epoch: 423 train-loss: 0.010570164149006208\n",
      "[LOG 20200502-13:54:30] epoch: 424 train-loss: 0.010570267732772563\n",
      "[LOG 20200502-13:54:31] epoch: 425 train-loss: 0.010570371420019202\n",
      "[LOG 20200502-13:54:31] epoch: 426 train-loss: 0.0105704746933447\n",
      "[LOG 20200502-13:54:31] epoch: 427 train-loss: 0.010570578277111053\n",
      "[LOG 20200502-13:54:31] epoch: 428 train-loss: 0.010570681550436549\n",
      "[LOG 20200502-13:54:31] epoch: 429 train-loss: 0.010570784823762046\n",
      "[LOG 20200502-13:54:32] epoch: 430 train-loss: 0.010570888200567828\n",
      "[LOG 20200502-13:54:32] epoch: 431 train-loss: 0.010570991784334183\n",
      "[LOG 20200502-13:54:32] epoch: 432 train-loss: 0.010571095368100537\n",
      "[LOG 20200502-13:54:32] epoch: 433 train-loss: 0.010571199055347178\n",
      "[LOG 20200502-13:54:32] epoch: 434 train-loss: 0.010571302535633246\n",
      "[LOG 20200502-13:54:33] epoch: 435 train-loss: 0.01057140694724189\n",
      "[LOG 20200502-13:54:33] epoch: 436 train-loss: 0.010571510841449102\n",
      "[LOG 20200502-13:54:33] epoch: 437 train-loss: 0.010571614942616887\n",
      "[LOG 20200502-13:54:33] epoch: 438 train-loss: 0.01057171935422553\n",
      "[LOG 20200502-13:54:33] epoch: 439 train-loss: 0.010571823144952456\n",
      "[LOG 20200502-13:54:34] epoch: 440 train-loss: 0.010571927039159669\n",
      "[LOG 20200502-13:54:34] epoch: 441 train-loss: 0.010572031554248598\n",
      "[LOG 20200502-13:54:34] epoch: 442 train-loss: 0.010572135551936097\n",
      "[LOG 20200502-13:54:34] epoch: 443 train-loss: 0.010572240170505311\n",
      "[LOG 20200502-13:54:34] epoch: 444 train-loss: 0.010572343754271666\n",
      "[LOG 20200502-13:54:35] epoch: 445 train-loss: 0.010572449304163456\n",
      "[LOG 20200502-13:54:35] epoch: 446 train-loss: 0.01057255288792981\n",
      "[LOG 20200502-13:54:35] epoch: 447 train-loss: 0.010572657403018739\n",
      "[LOG 20200502-13:54:35] epoch: 448 train-loss: 0.010572761814627383\n",
      "[LOG 20200502-13:54:35] epoch: 449 train-loss: 0.01057286612275574\n",
      "[LOG 20200502-13:54:36] epoch: 450 train-loss: 0.01057297084480524\n",
      "[LOG 20200502-13:54:36] epoch: 451 train-loss: 0.010573074945973026\n",
      "[LOG 20200502-13:54:36] epoch: 452 train-loss: 0.010573179668022526\n",
      "[LOG 20200502-13:54:36] epoch: 453 train-loss: 0.010573284390072027\n",
      "[LOG 20200502-13:54:36] epoch: 454 train-loss: 0.010573388594720099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-13:54:37] epoch: 455 train-loss: 0.010573492902848456\n",
      "[LOG 20200502-13:54:37] epoch: 456 train-loss: 0.01057359752141767\n",
      "[LOG 20200502-13:54:37] epoch: 457 train-loss: 0.010573701933026314\n",
      "[LOG 20200502-13:54:37] epoch: 458 train-loss: 0.010573806448115243\n",
      "[LOG 20200502-13:54:37] epoch: 459 train-loss: 0.010573911066684458\n",
      "[LOG 20200502-13:54:38] epoch: 460 train-loss: 0.010574015064371957\n",
      "[LOG 20200502-13:54:38] epoch: 461 train-loss: 0.010574120096862316\n",
      "[LOG 20200502-13:54:38] epoch: 462 train-loss: 0.010574224094549814\n",
      "[LOG 20200502-13:54:38] epoch: 463 train-loss: 0.010574327988757027\n",
      "[LOG 20200502-13:54:38] epoch: 464 train-loss: 0.010574432089924812\n",
      "[LOG 20200502-13:54:39] epoch: 465 train-loss: 0.010574536191092597\n",
      "[LOG 20200502-13:54:39] epoch: 466 train-loss: 0.010574639671378665\n",
      "[LOG 20200502-13:54:39] epoch: 467 train-loss: 0.01057474325514502\n",
      "[LOG 20200502-13:54:39] epoch: 468 train-loss: 0.010574846735431088\n",
      "[LOG 20200502-13:54:39] epoch: 469 train-loss: 0.010574950215717157\n",
      "[LOG 20200502-13:54:40] epoch: 470 train-loss: 0.010575053075121509\n",
      "[LOG 20200502-13:54:40] epoch: 471 train-loss: 0.010575156451927291\n",
      "[LOG 20200502-13:54:40] epoch: 472 train-loss: 0.01057525941481193\n",
      "[LOG 20200502-13:54:40] epoch: 473 train-loss: 0.010575363102058569\n",
      "[LOG 20200502-13:54:41] epoch: 474 train-loss: 0.01057546627190378\n",
      "[LOG 20200502-13:54:41] epoch: 475 train-loss: 0.010575569234788418\n",
      "[LOG 20200502-13:54:41] epoch: 476 train-loss: 0.010575672197673056\n",
      "[LOG 20200502-13:54:41] epoch: 477 train-loss: 0.010575775470998552\n",
      "[LOG 20200502-13:54:41] epoch: 478 train-loss: 0.010575878330402903\n",
      "[LOG 20200502-13:54:42] epoch: 479 train-loss: 0.010575981189807257\n",
      "[LOG 20200502-13:54:42] epoch: 480 train-loss: 0.010576083945731321\n",
      "[LOG 20200502-13:54:42] epoch: 481 train-loss: 0.010576186184253957\n",
      "[LOG 20200502-13:54:42] epoch: 482 train-loss: 0.010576288836697737\n",
      "[LOG 20200502-13:54:42] epoch: 483 train-loss: 0.01057639169610209\n",
      "[LOG 20200502-13:54:43] epoch: 484 train-loss: 0.010576493934624724\n",
      "[LOG 20200502-13:54:43] epoch: 485 train-loss: 0.010576596380107932\n",
      "[LOG 20200502-13:54:43] epoch: 486 train-loss: 0.010576698515150283\n",
      "[LOG 20200502-13:54:43] epoch: 487 train-loss: 0.010576800546712346\n",
      "[LOG 20200502-13:54:43] epoch: 488 train-loss: 0.010576902578274408\n",
      "[LOG 20200502-13:54:44] epoch: 489 train-loss: 0.010577005023757616\n",
      "[LOG 20200502-13:54:44] epoch: 490 train-loss: 0.01057710653791825\n",
      "[LOG 20200502-13:54:44] epoch: 491 train-loss: 0.010577208776440885\n",
      "[LOG 20200502-13:54:44] epoch: 492 train-loss: 0.010577309669719802\n",
      "[LOG 20200502-13:54:44] epoch: 493 train-loss: 0.01057741159780158\n",
      "[LOG 20200502-13:54:45] epoch: 494 train-loss: 0.01057751290500164\n",
      "[LOG 20200502-13:54:45] epoch: 495 train-loss: 0.010577614212201701\n",
      "[LOG 20200502-13:54:45] epoch: 496 train-loss: 0.010577715519401763\n",
      "[LOG 20200502-13:54:45] epoch: 497 train-loss: 0.010577816309200393\n",
      "[LOG 20200502-13:54:45] epoch: 498 train-loss: 0.010577917512920167\n",
      "[LOG 20200502-13:54:46] epoch: 499 train-loss: 0.010578017785317369\n",
      "[LOG 20200502-13:54:46] epoch: 500 train-loss: 0.010578118678596284\n",
      "[LOG 20200502-13:54:46] epoch: 501 train-loss: 0.01057821936491463\n",
      "[LOG 20200502-13:54:46] epoch: 502 train-loss: 0.010578319430351257\n",
      "[LOG 20200502-13:54:46] epoch: 503 train-loss: 0.010578419599268172\n",
      "[LOG 20200502-13:54:47] epoch: 504 train-loss: 0.0105785196647048\n",
      "[LOG 20200502-13:54:47] epoch: 505 train-loss: 0.010578618798818853\n",
      "[LOG 20200502-13:54:47] epoch: 506 train-loss: 0.010578717622492049\n",
      "[LOG 20200502-13:54:47] epoch: 507 train-loss: 0.010578817274007533\n",
      "[LOG 20200502-13:54:47] epoch: 508 train-loss: 0.010578915890720155\n",
      "[LOG 20200502-13:54:48] epoch: 509 train-loss: 0.010579014610913064\n",
      "[LOG 20200502-13:54:48] epoch: 510 train-loss: 0.010579112399783399\n",
      "[LOG 20200502-13:54:48] epoch: 511 train-loss: 0.010579210188653734\n",
      "[LOG 20200502-13:54:48] epoch: 512 train-loss: 0.01057930901232693\n",
      "[LOG 20200502-13:54:48] epoch: 513 train-loss: 0.010579406697716977\n",
      "[LOG 20200502-13:54:49] epoch: 514 train-loss: 0.010579503969185881\n",
      "[LOG 20200502-13:54:49] epoch: 515 train-loss: 0.010579602171977362\n",
      "[LOG 20200502-13:54:49] epoch: 516 train-loss: 0.010579699133005407\n",
      "[LOG 20200502-13:54:49] epoch: 517 train-loss: 0.01057979640447431\n",
      "[LOG 20200502-13:54:49] epoch: 518 train-loss: 0.010579893468982644\n",
      "[LOG 20200502-13:54:50] epoch: 519 train-loss: 0.010579990326530404\n",
      "[LOG 20200502-13:54:50] epoch: 520 train-loss: 0.010580086873637306\n",
      "[LOG 20200502-13:54:50] epoch: 521 train-loss: 0.010580183731185066\n",
      "[LOG 20200502-13:54:50] epoch: 522 train-loss: 0.010580279760890536\n",
      "[LOG 20200502-13:54:50] epoch: 523 train-loss: 0.010580375583635436\n",
      "[LOG 20200502-13:54:51] epoch: 524 train-loss: 0.010580471613340907\n",
      "[LOG 20200502-13:54:51] epoch: 525 train-loss: 0.010580567229125235\n",
      "[LOG 20200502-13:54:51] epoch: 526 train-loss: 0.010580662327508131\n",
      "[LOG 20200502-13:54:51] epoch: 527 train-loss: 0.010580757322410742\n",
      "[LOG 20200502-13:54:51] epoch: 528 train-loss: 0.010580852524273925\n",
      "[LOG 20200502-13:54:52] epoch: 529 train-loss: 0.010580947312215963\n",
      "[LOG 20200502-13:54:52] epoch: 530 train-loss: 0.010581041272315714\n",
      "[LOG 20200502-13:54:52] epoch: 531 train-loss: 0.010581135542856323\n",
      "[LOG 20200502-13:54:52] epoch: 532 train-loss: 0.010581229502956072\n",
      "[LOG 20200502-13:54:52] epoch: 533 train-loss: 0.010581323049134679\n",
      "[LOG 20200502-13:54:53] epoch: 534 train-loss: 0.010581416905754142\n",
      "[LOG 20200502-13:54:53] epoch: 535 train-loss: 0.010581510038011603\n",
      "[LOG 20200502-13:54:53] epoch: 536 train-loss: 0.01058160275634792\n",
      "[LOG 20200502-13:54:53] epoch: 537 train-loss: 0.010581695371203952\n",
      "[LOG 20200502-13:54:54] epoch: 538 train-loss: 0.010581787986059984\n",
      "[LOG 20200502-13:54:54] epoch: 539 train-loss: 0.010581880083514584\n",
      "[LOG 20200502-13:54:54] epoch: 540 train-loss: 0.0105819720774889\n",
      "[LOG 20200502-13:54:54] epoch: 541 train-loss: 0.010582063554061783\n",
      "[LOG 20200502-13:54:54] epoch: 542 train-loss: 0.010582155030634668\n",
      "[LOG 20200502-13:54:55] epoch: 543 train-loss: 0.010582246093286408\n",
      "[LOG 20200502-13:54:55] epoch: 544 train-loss: 0.010582337569859292\n",
      "[LOG 20200502-13:54:55] epoch: 545 train-loss: 0.010582427804668745\n",
      "[LOG 20200502-13:54:55] epoch: 546 train-loss: 0.010582518349919055\n",
      "[LOG 20200502-13:54:55] epoch: 547 train-loss: 0.01058260796384679\n",
      "[LOG 20200502-13:54:56] epoch: 548 train-loss: 0.010582696853412522\n",
      "[LOG 20200502-13:54:56] epoch: 549 train-loss: 0.010582785225576825\n",
      "[LOG 20200502-13:54:56] epoch: 550 train-loss: 0.010582874218622843\n",
      "[LOG 20200502-13:54:56] epoch: 551 train-loss: 0.010582962383826574\n",
      "[LOG 20200502-13:54:56] epoch: 552 train-loss: 0.01058305065251059\n",
      "[LOG 20200502-13:54:57] epoch: 553 train-loss: 0.010583138507273462\n",
      "[LOG 20200502-13:54:57] epoch: 554 train-loss: 0.010583225637674332\n",
      "[LOG 20200502-13:54:57] epoch: 555 train-loss: 0.010583312975035774\n",
      "[LOG 20200502-13:54:57] epoch: 556 train-loss: 0.010583400001956357\n",
      "[LOG 20200502-13:54:57] epoch: 557 train-loss: 0.010583486407995224\n",
      "[LOG 20200502-13:54:58] epoch: 558 train-loss: 0.010583572710553804\n",
      "[LOG 20200502-13:54:58] epoch: 559 train-loss: 0.010583658185270097\n",
      "[LOG 20200502-13:54:58] epoch: 560 train-loss: 0.010583743970427249\n",
      "[LOG 20200502-13:54:58] epoch: 561 train-loss: 0.010583829755584398\n",
      "[LOG 20200502-13:54:58] epoch: 562 train-loss: 0.010583914609418975\n",
      "[LOG 20200502-13:54:59] epoch: 563 train-loss: 0.01058399946325355\n",
      "[LOG 20200502-13:54:59] epoch: 564 train-loss: 0.010584084006647268\n",
      "[LOG 20200502-13:54:59] epoch: 565 train-loss: 0.010584168032639556\n",
      "[LOG 20200502-13:54:59] epoch: 566 train-loss: 0.010584251541230414\n",
      "[LOG 20200502-13:55:00] epoch: 567 train-loss: 0.010584335256781843\n",
      "[LOG 20200502-13:55:00] epoch: 568 train-loss: 0.010584418558412127\n",
      "[LOG 20200502-13:55:00] epoch: 569 train-loss: 0.010584501549601555\n",
      "[LOG 20200502-13:55:00] epoch: 570 train-loss: 0.010584584023389552\n",
      "[LOG 20200502-13:55:00] epoch: 571 train-loss: 0.010584666600657834\n",
      "[LOG 20200502-13:55:01] epoch: 572 train-loss: 0.010584748453564115\n",
      "[LOG 20200502-13:55:01] epoch: 573 train-loss: 0.010584830099509822\n",
      "[LOG 20200502-13:55:01] epoch: 574 train-loss: 0.010584911848935816\n",
      "[LOG 20200502-13:55:01] epoch: 575 train-loss: 0.010584992356598377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-13:55:02] epoch: 576 train-loss: 0.01058507358862294\n",
      "[LOG 20200502-13:55:02] epoch: 577 train-loss: 0.010585154096285502\n",
      "[LOG 20200502-13:55:02] epoch: 578 train-loss: 0.010585234293507205\n",
      "[LOG 20200502-13:55:02] epoch: 579 train-loss: 0.010585313973327478\n",
      "[LOG 20200502-13:55:03] epoch: 580 train-loss: 0.010585393549667465\n",
      "[LOG 20200502-13:55:03] epoch: 581 train-loss: 0.01058547260860602\n",
      "[LOG 20200502-13:55:03] epoch: 582 train-loss: 0.010585551771024862\n",
      "[LOG 20200502-13:55:03] epoch: 583 train-loss: 0.010585630312561989\n",
      "[LOG 20200502-13:55:03] epoch: 584 train-loss: 0.010585708543658257\n",
      "[LOG 20200502-13:55:04] epoch: 585 train-loss: 0.010585786567793952\n",
      "[LOG 20200502-13:55:04] epoch: 586 train-loss: 0.010585864902370505\n",
      "[LOG 20200502-13:55:04] epoch: 587 train-loss: 0.010585941891703341\n",
      "[LOG 20200502-13:55:04] epoch: 588 train-loss: 0.01058601929495732\n",
      "[LOG 20200502-13:55:05] epoch: 589 train-loss: 0.010586095973849297\n",
      "[LOG 20200502-13:55:05] epoch: 590 train-loss: 0.010586172342300415\n",
      "[LOG 20200502-13:55:05] epoch: 591 train-loss: 0.010586248193350103\n",
      "[LOG 20200502-13:55:05] epoch: 592 train-loss: 0.01058632435484065\n",
      "[LOG 20200502-13:55:05] epoch: 593 train-loss: 0.010586398757166333\n",
      "[LOG 20200502-13:55:06] epoch: 594 train-loss: 0.010586473469932875\n",
      "[LOG 20200502-13:55:06] epoch: 595 train-loss: 0.010586547354857126\n",
      "[LOG 20200502-13:55:06] epoch: 596 train-loss: 0.010586621550222238\n",
      "[LOG 20200502-13:55:06] epoch: 597 train-loss: 0.010586695124705633\n",
      "[LOG 20200502-13:55:06] epoch: 598 train-loss: 0.010586768285267882\n",
      "[LOG 20200502-13:55:07] epoch: 599 train-loss: 0.010586841445830133\n",
      "[LOG 20200502-13:55:07] epoch: 600 train-loss: 0.010586913985510668\n",
      "[LOG 20200502-13:55:07] epoch: 601 train-loss: 0.01058698600778977\n",
      "[LOG 20200502-13:55:07] epoch: 602 train-loss: 0.010587058857911162\n",
      "[LOG 20200502-13:55:07] epoch: 603 train-loss: 0.010587130569749408\n",
      "[LOG 20200502-13:55:08] epoch: 604 train-loss: 0.010587202074627081\n",
      "[LOG 20200502-13:55:08] epoch: 605 train-loss: 0.010587273062103324\n",
      "[LOG 20200502-13:55:08] epoch: 606 train-loss: 0.01058734425654014\n",
      "[LOG 20200502-13:55:08] epoch: 607 train-loss: 0.010587414933575524\n",
      "[LOG 20200502-13:55:08] epoch: 608 train-loss: 0.010587484161886904\n",
      "[LOG 20200502-13:55:09] epoch: 609 train-loss: 0.010587554631961716\n",
      "[LOG 20200502-13:55:09] epoch: 610 train-loss: 0.01058762427419424\n",
      "[LOG 20200502-13:55:09] epoch: 611 train-loss: 0.010587693399025334\n",
      "[LOG 20200502-13:55:09] epoch: 612 train-loss: 0.010587762316895856\n",
      "[LOG 20200502-13:55:09] epoch: 613 train-loss: 0.010587831338246664\n",
      "[LOG 20200502-13:55:10] epoch: 614 train-loss: 0.010587899945676327\n",
      "[LOG 20200502-13:55:10] epoch: 615 train-loss: 0.010587968139184846\n",
      "[LOG 20200502-13:55:10] epoch: 616 train-loss: 0.01058803591877222\n",
      "[LOG 20200502-13:55:10] epoch: 617 train-loss: 0.010588103284438452\n",
      "[LOG 20200502-13:55:10] epoch: 618 train-loss: 0.010588170443144109\n",
      "[LOG 20200502-13:55:11] epoch: 619 train-loss: 0.010588237084448338\n",
      "[LOG 20200502-13:55:11] epoch: 620 train-loss: 0.010588303932713138\n",
      "[LOG 20200502-13:55:11] epoch: 621 train-loss: 0.010588370367056794\n",
      "[LOG 20200502-13:55:11] epoch: 622 train-loss: 0.010588436904880736\n",
      "[LOG 20200502-13:55:11] epoch: 623 train-loss: 0.010588502097460959\n",
      "[LOG 20200502-13:55:12] epoch: 624 train-loss: 0.01058856811788347\n",
      "[LOG 20200502-13:55:12] epoch: 625 train-loss: 0.010588633206983408\n",
      "[LOG 20200502-13:55:12] epoch: 626 train-loss: 0.010588697261280484\n",
      "[LOG 20200502-13:55:12] epoch: 627 train-loss: 0.01058876235038042\n",
      "[LOG 20200502-13:55:12] epoch: 628 train-loss: 0.010588826094236638\n",
      "[LOG 20200502-13:55:13] epoch: 629 train-loss: 0.010588890665935146\n",
      "[LOG 20200502-13:55:13] epoch: 630 train-loss: 0.010588954202830791\n",
      "[LOG 20200502-13:55:13] epoch: 631 train-loss: 0.010589017739726437\n",
      "[LOG 20200502-13:55:13] epoch: 632 train-loss: 0.010589080448779795\n",
      "[LOG 20200502-13:55:13] epoch: 633 train-loss: 0.010589143157833152\n",
      "[LOG 20200502-13:55:14] epoch: 634 train-loss: 0.010589205763406224\n",
      "[LOG 20200502-13:55:14] epoch: 635 train-loss: 0.010589268162018724\n",
      "[LOG 20200502-13:55:14] epoch: 636 train-loss: 0.010589329629308648\n",
      "[LOG 20200502-13:55:14] epoch: 637 train-loss: 0.01058939140703943\n",
      "[LOG 20200502-13:55:14] epoch: 638 train-loss: 0.01058945277084907\n",
      "[LOG 20200502-13:55:15] epoch: 639 train-loss: 0.010589513617257277\n",
      "[LOG 20200502-13:55:15] epoch: 640 train-loss: 0.010589573635823198\n",
      "[LOG 20200502-13:55:15] epoch: 641 train-loss: 0.01058963406831026\n",
      "[LOG 20200502-13:55:15] epoch: 642 train-loss: 0.010589694190356467\n",
      "[LOG 20200502-13:55:15] epoch: 643 train-loss: 0.010589752967158953\n",
      "[LOG 20200502-13:55:16] epoch: 644 train-loss: 0.010589811433520582\n",
      "[LOG 20200502-13:55:16] epoch: 645 train-loss: 0.010589869692921638\n",
      "[LOG 20200502-13:55:16] epoch: 646 train-loss: 0.010589927331440978\n",
      "[LOG 20200502-13:55:16] epoch: 647 train-loss: 0.010589984866480032\n",
      "[LOG 20200502-13:55:16] epoch: 648 train-loss: 0.010590042298038801\n",
      "[LOG 20200502-13:55:17] epoch: 649 train-loss: 0.01059009941915671\n",
      "[LOG 20200502-13:55:17] epoch: 650 train-loss: 0.010590156229833761\n",
      "[LOG 20200502-13:55:17] epoch: 651 train-loss: 0.01059021231614881\n",
      "[LOG 20200502-13:55:17] epoch: 652 train-loss: 0.01059026840246386\n",
      "[LOG 20200502-13:55:17] epoch: 653 train-loss: 0.010590324385298623\n",
      "[LOG 20200502-13:55:18] epoch: 654 train-loss: 0.010590379747251669\n",
      "[LOG 20200502-13:55:18] epoch: 655 train-loss: 0.010590434695283571\n",
      "[LOG 20200502-13:55:18] epoch: 656 train-loss: 0.010590489850276046\n",
      "[LOG 20200502-13:55:18] epoch: 657 train-loss: 0.010590543763505088\n",
      "[LOG 20200502-13:55:18] epoch: 658 train-loss: 0.010590598401096132\n",
      "[LOG 20200502-13:55:19] epoch: 659 train-loss: 0.010590652314325174\n",
      "[LOG 20200502-13:55:19] epoch: 660 train-loss: 0.010590705813633071\n",
      "[LOG 20200502-13:55:19] epoch: 661 train-loss: 0.01059075951990154\n",
      "[LOG 20200502-13:55:19] epoch: 662 train-loss: 0.01059081219136715\n",
      "[LOG 20200502-13:55:19] epoch: 663 train-loss: 0.010590864966313044\n",
      "[LOG 20200502-13:55:20] epoch: 664 train-loss: 0.010590917223857509\n",
      "[LOG 20200502-13:55:20] epoch: 665 train-loss: 0.010590969688362546\n",
      "[LOG 20200502-13:55:20] epoch: 666 train-loss: 0.010591021738946438\n",
      "[LOG 20200502-13:55:20] epoch: 667 train-loss: 0.01059107275472747\n",
      "[LOG 20200502-13:55:21] epoch: 668 train-loss: 0.010591124184429646\n",
      "[LOG 20200502-13:55:21] epoch: 669 train-loss: 0.010591175510651536\n",
      "[LOG 20200502-13:55:21] epoch: 670 train-loss: 0.010591226526432566\n",
      "[LOG 20200502-13:55:21] epoch: 671 train-loss: 0.010591276817851596\n",
      "[LOG 20200502-13:55:21] epoch: 672 train-loss: 0.010591326488388909\n",
      "[LOG 20200502-13:55:22] epoch: 673 train-loss: 0.010591376883288225\n",
      "[LOG 20200502-13:55:22] epoch: 674 train-loss: 0.01059142624338468\n",
      "[LOG 20200502-13:55:22] epoch: 675 train-loss: 0.01059147622436285\n",
      "[LOG 20200502-13:55:22] epoch: 676 train-loss: 0.010591524860097302\n",
      "[LOG 20200502-13:55:22] epoch: 677 train-loss: 0.010591573702792326\n",
      "[LOG 20200502-13:55:23] epoch: 678 train-loss: 0.010591622442007065\n",
      "[LOG 20200502-13:55:23] epoch: 679 train-loss: 0.010591670456859801\n",
      "[LOG 20200502-13:55:23] epoch: 680 train-loss: 0.010591718161271678\n",
      "[LOG 20200502-13:55:23] epoch: 681 train-loss: 0.010591765451762412\n",
      "[LOG 20200502-13:55:23] epoch: 682 train-loss: 0.010591813259654574\n",
      "[LOG 20200502-13:55:24] epoch: 683 train-loss: 0.010591859929263592\n",
      "[LOG 20200502-13:55:24] epoch: 684 train-loss: 0.010591906702352894\n",
      "[LOG 20200502-13:55:24] epoch: 685 train-loss: 0.010591952958040766\n",
      "[LOG 20200502-13:55:24] epoch: 686 train-loss: 0.010591998903287781\n",
      "[LOG 20200502-13:55:24] epoch: 687 train-loss: 0.010592045055495368\n",
      "[LOG 20200502-13:55:25] epoch: 688 train-loss: 0.01059209027638038\n",
      "[LOG 20200502-13:55:25] epoch: 689 train-loss: 0.010592135186824534\n",
      "[LOG 20200502-13:55:25] epoch: 690 train-loss: 0.010592180200748973\n",
      "[LOG 20200502-13:55:25] epoch: 691 train-loss: 0.010592224904232554\n",
      "[LOG 20200502-13:55:25] epoch: 692 train-loss: 0.010592269090314707\n",
      "[LOG 20200502-13:55:26] epoch: 693 train-loss: 0.010592313069436286\n",
      "[LOG 20200502-13:55:26] epoch: 694 train-loss: 0.010592357462479008\n",
      "[LOG 20200502-13:55:26] epoch: 695 train-loss: 0.010592400096356869\n",
      "[LOG 20200502-13:55:26] epoch: 696 train-loss: 0.010592443351116445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-13:55:26] epoch: 697 train-loss: 0.010592486295435164\n",
      "[LOG 20200502-13:55:27] epoch: 698 train-loss: 0.010592528204951022\n",
      "[LOG 20200502-13:55:27] epoch: 699 train-loss: 0.010592569907506308\n",
      "[LOG 20200502-13:55:27] epoch: 700 train-loss: 0.010592611196140448\n",
      "[LOG 20200502-13:55:27] epoch: 701 train-loss: 0.010592651863892874\n",
      "[LOG 20200502-13:55:27] epoch: 702 train-loss: 0.010592692842086157\n",
      "[LOG 20200502-13:55:28] epoch: 703 train-loss: 0.010592733199397722\n",
      "[LOG 20200502-13:55:28] epoch: 704 train-loss: 0.010592773142788146\n",
      "[LOG 20200502-13:55:28] epoch: 705 train-loss: 0.010592813396619426\n",
      "[LOG 20200502-13:55:28] epoch: 706 train-loss: 0.010592852408687273\n",
      "[LOG 20200502-13:55:28] epoch: 707 train-loss: 0.010592891938156553\n",
      "[LOG 20200502-13:55:29] epoch: 708 train-loss: 0.010592931157184972\n",
      "[LOG 20200502-13:55:29] epoch: 709 train-loss: 0.010592969858811961\n",
      "[LOG 20200502-13:55:29] epoch: 710 train-loss: 0.010593007629116377\n",
      "[LOG 20200502-13:55:29] epoch: 711 train-loss: 0.010593046537703939\n",
      "[LOG 20200502-13:55:29] epoch: 712 train-loss: 0.010593084308008352\n",
      "[LOG 20200502-13:55:30] epoch: 713 train-loss: 0.010593122078312768\n",
      "[LOG 20200502-13:55:30] epoch: 714 train-loss: 0.010593159848617183\n",
      "[LOG 20200502-13:55:30] epoch: 715 train-loss: 0.010593197205000453\n",
      "[LOG 20200502-13:55:30] epoch: 716 train-loss: 0.010593233423100578\n",
      "[LOG 20200502-13:55:30] epoch: 717 train-loss: 0.010593270262082418\n",
      "[LOG 20200502-13:55:31] epoch: 718 train-loss: 0.01059330627322197\n",
      "[LOG 20200502-13:55:31] epoch: 719 train-loss: 0.01059334259480238\n",
      "[LOG 20200502-13:55:31] epoch: 720 train-loss: 0.010593378295501074\n",
      "[LOG 20200502-13:55:31] epoch: 721 train-loss: 0.01059341368575891\n",
      "[LOG 20200502-13:55:31] epoch: 722 train-loss: 0.010593448869056173\n",
      "[LOG 20200502-13:55:32] epoch: 723 train-loss: 0.01059348394887315\n",
      "[LOG 20200502-13:55:32] epoch: 724 train-loss: 0.010593518821729554\n",
      "[LOG 20200502-13:55:32] epoch: 725 train-loss: 0.010593553073704243\n",
      "[LOG 20200502-13:55:32] epoch: 726 train-loss: 0.010593587843080362\n",
      "[LOG 20200502-13:55:32] epoch: 727 train-loss: 0.010593621267212762\n",
      "[LOG 20200502-13:55:33] epoch: 728 train-loss: 0.010593654691345163\n",
      "[LOG 20200502-13:55:33] epoch: 729 train-loss: 0.010593688218957849\n",
      "[LOG 20200502-13:55:33] epoch: 730 train-loss: 0.010593721953531107\n",
      "[LOG 20200502-13:55:33] epoch: 731 train-loss: 0.01059375423938036\n",
      "[LOG 20200502-13:55:33] epoch: 732 train-loss: 0.010593786732190184\n",
      "[LOG 20200502-13:55:34] epoch: 733 train-loss: 0.010593819742401442\n",
      "[LOG 20200502-13:55:34] epoch: 734 train-loss: 0.01059385109692812\n",
      "[LOG 20200502-13:55:34] epoch: 735 train-loss: 0.010593882865375943\n",
      "[LOG 20200502-13:55:34] epoch: 736 train-loss: 0.010593914323382907\n",
      "[LOG 20200502-13:55:34] epoch: 737 train-loss: 0.010593945781389872\n",
      "[LOG 20200502-13:55:35] epoch: 738 train-loss: 0.010593977239396837\n",
      "[LOG 20200502-13:55:35] epoch: 739 train-loss: 0.010594007869561514\n",
      "[LOG 20200502-13:55:35] epoch: 740 train-loss: 0.01059403818928533\n",
      "[LOG 20200502-13:55:35] epoch: 741 train-loss: 0.01059406902641058\n",
      "[LOG 20200502-13:55:35] epoch: 742 train-loss: 0.010594098932213254\n",
      "[LOG 20200502-13:55:36] epoch: 743 train-loss: 0.010594129148456786\n",
      "[LOG 20200502-13:55:36] epoch: 744 train-loss: 0.010594158950779173\n",
      "[LOG 20200502-13:55:36] epoch: 745 train-loss: 0.010594188132219844\n",
      "[LOG 20200502-13:55:36] epoch: 746 train-loss: 0.010594216796259085\n",
      "[LOG 20200502-13:55:36] epoch: 747 train-loss: 0.010594246081180043\n",
      "[LOG 20200502-13:55:37] epoch: 748 train-loss: 0.010594274641738998\n",
      "[LOG 20200502-13:55:37] epoch: 749 train-loss: 0.010594302684896521\n",
      "[LOG 20200502-13:55:37] epoch: 750 train-loss: 0.010594330728054047\n",
      "[LOG 20200502-13:55:37] epoch: 751 train-loss: 0.010594358150329854\n",
      "[LOG 20200502-13:55:37] epoch: 752 train-loss: 0.010594386090007093\n",
      "[LOG 20200502-13:55:38] epoch: 753 train-loss: 0.010594413305322329\n",
      "[LOG 20200502-13:55:38] epoch: 754 train-loss: 0.01059444093455871\n",
      "[LOG 20200502-13:55:38] epoch: 755 train-loss: 0.010594467218551371\n",
      "[LOG 20200502-13:55:38] epoch: 756 train-loss: 0.010594493916465176\n",
      "[LOG 20200502-13:55:38] epoch: 757 train-loss: 0.01059451989001698\n",
      "[LOG 20200502-13:55:39] epoch: 758 train-loss: 0.010594545449647639\n",
      "[LOG 20200502-13:55:39] epoch: 759 train-loss: 0.01059456987099515\n",
      "[LOG 20200502-13:55:39] epoch: 760 train-loss: 0.01059459439582295\n",
      "[LOG 20200502-13:55:39] epoch: 761 train-loss: 0.010594619127611319\n",
      "[LOG 20200502-13:55:39] epoch: 762 train-loss: 0.010594643135037687\n",
      "[LOG 20200502-13:55:40] epoch: 763 train-loss: 0.01059466703898377\n",
      "[LOG 20200502-13:55:40] epoch: 764 train-loss: 0.010594690529008707\n",
      "[LOG 20200502-13:55:40] epoch: 765 train-loss: 0.010594713708592786\n",
      "[LOG 20200502-13:55:40] epoch: 766 train-loss: 0.010594737405578295\n",
      "[LOG 20200502-13:55:40] epoch: 767 train-loss: 0.010594759860800372\n",
      "[LOG 20200502-13:55:41] epoch: 768 train-loss: 0.010594782729943594\n",
      "[LOG 20200502-13:55:41] epoch: 769 train-loss: 0.010594804978205098\n",
      "[LOG 20200502-13:55:41] epoch: 770 train-loss: 0.010594827226466604\n",
      "[LOG 20200502-13:55:41] epoch: 771 train-loss: 0.010594848957326677\n",
      "[LOG 20200502-13:55:41] epoch: 772 train-loss: 0.010594870170785321\n",
      "[LOG 20200502-13:55:42] epoch: 773 train-loss: 0.010594891591204537\n",
      "[LOG 20200502-13:55:42] epoch: 774 train-loss: 0.010594913011623753\n",
      "[LOG 20200502-13:55:42] epoch: 775 train-loss: 0.010594933914641539\n",
      "[LOG 20200502-13:55:42] epoch: 776 train-loss: 0.010594954507218467\n",
      "[LOG 20200502-13:55:42] epoch: 777 train-loss: 0.01059497520327568\n",
      "[LOG 20200502-13:55:43] epoch: 778 train-loss: 0.010594994761049747\n",
      "[LOG 20200502-13:55:43] epoch: 779 train-loss: 0.010595014836225245\n",
      "[LOG 20200502-13:55:43] epoch: 780 train-loss: 0.010595034600959884\n",
      "[LOG 20200502-13:55:43] epoch: 781 train-loss: 0.010595053434371948\n",
      "[LOG 20200502-13:55:43] epoch: 782 train-loss: 0.010595072785185443\n",
      "[LOG 20200502-13:55:44] epoch: 783 train-loss: 0.010595091204676364\n",
      "[LOG 20200502-13:55:44] epoch: 784 train-loss: 0.010595109417206712\n",
      "[LOG 20200502-13:55:44] epoch: 785 train-loss: 0.010595127526256774\n",
      "[LOG 20200502-13:55:44] epoch: 786 train-loss: 0.010595145842267407\n",
      "[LOG 20200502-13:55:44] epoch: 787 train-loss: 0.01059516343391604\n",
      "[LOG 20200502-13:55:45] epoch: 788 train-loss: 0.010595180611643527\n",
      "[LOG 20200502-13:55:45] epoch: 789 train-loss: 0.010595198306772444\n",
      "[LOG 20200502-13:55:45] epoch: 790 train-loss: 0.010595215381019644\n",
      "[LOG 20200502-13:55:45] epoch: 791 train-loss: 0.0105952313169837\n",
      "[LOG 20200502-13:55:45] epoch: 792 train-loss: 0.010595247977309756\n",
      "[LOG 20200502-13:55:46] epoch: 793 train-loss: 0.010595264327194955\n",
      "[LOG 20200502-13:55:46] epoch: 794 train-loss: 0.010595280573599868\n",
      "[LOG 20200502-13:55:46] epoch: 795 train-loss: 0.010595295888682207\n",
      "[LOG 20200502-13:55:46] epoch: 796 train-loss: 0.010595311928126547\n",
      "[LOG 20200502-13:55:46] epoch: 797 train-loss: 0.010595326415366597\n",
      "[LOG 20200502-13:55:47] epoch: 798 train-loss: 0.01059534183392922\n",
      "[LOG 20200502-13:55:47] epoch: 799 train-loss: 0.010595356735090414\n",
      "[LOG 20200502-13:55:47] epoch: 800 train-loss: 0.010595371429291036\n",
      "[LOG 20200502-13:55:47] epoch: 801 train-loss: 0.01059538529564937\n",
      "[LOG 20200502-13:55:47] epoch: 802 train-loss: 0.010595399679409133\n",
      "[LOG 20200502-13:55:48] epoch: 803 train-loss: 0.010595412614444891\n",
      "[LOG 20200502-13:55:48] epoch: 804 train-loss: 0.010595426273842653\n",
      "[LOG 20200502-13:55:48] epoch: 805 train-loss: 0.010595439415838983\n",
      "[LOG 20200502-13:55:48] epoch: 806 train-loss: 0.010595452350874742\n",
      "[LOG 20200502-13:55:48] epoch: 807 train-loss: 0.01059546476850907\n",
      "[LOG 20200502-13:55:49] epoch: 808 train-loss: 0.010595477289623685\n",
      "[LOG 20200502-13:55:49] epoch: 809 train-loss: 0.010595489396817155\n",
      "[LOG 20200502-13:55:49] epoch: 810 train-loss: 0.010595501504010625\n",
      "[LOG 20200502-13:55:49] epoch: 811 train-loss: 0.010595512576401234\n",
      "[LOG 20200502-13:55:49] epoch: 812 train-loss: 0.010595524062712988\n",
      "[LOG 20200502-13:55:50] epoch: 813 train-loss: 0.010595535445544455\n",
      "[LOG 20200502-13:55:50] epoch: 814 train-loss: 0.010595545793573061\n",
      "[LOG 20200502-13:55:50] epoch: 815 train-loss: 0.010595555934641097\n",
      "[LOG 20200502-13:55:50] epoch: 816 train-loss: 0.010595565972228846\n",
      "[LOG 20200502-13:55:50] epoch: 817 train-loss: 0.010595576423737738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-13:55:51] epoch: 818 train-loss: 0.010595586047404341\n",
      "[LOG 20200502-13:55:51] epoch: 819 train-loss: 0.010595595153669516\n",
      "[LOG 20200502-13:55:51] epoch: 820 train-loss: 0.01059560425993469\n",
      "[LOG 20200502-13:55:51] epoch: 821 train-loss: 0.010595613055759005\n",
      "[LOG 20200502-13:55:51] epoch: 822 train-loss: 0.01059562112722132\n",
      "[LOG 20200502-13:55:52] epoch: 823 train-loss: 0.010595629509124491\n",
      "[LOG 20200502-13:55:52] epoch: 824 train-loss: 0.010595637477106519\n",
      "[LOG 20200502-13:55:52] epoch: 825 train-loss: 0.010595644203325113\n",
      "[LOG 20200502-13:55:52] epoch: 826 train-loss: 0.01059565082606342\n",
      "[LOG 20200502-13:55:52] epoch: 827 train-loss: 0.010595656000077724\n",
      "[LOG 20200502-13:55:53] epoch: 828 train-loss: 0.010595662208894888\n",
      "[LOG 20200502-13:55:53] epoch: 829 train-loss: 0.010595668107271194\n",
      "[LOG 20200502-13:55:53] epoch: 830 train-loss: 0.010595673177805211\n",
      "[LOG 20200502-13:55:53] epoch: 831 train-loss: 0.010595678351819515\n",
      "[LOG 20200502-13:55:53] epoch: 832 train-loss: 0.010595683008432388\n",
      "[LOG 20200502-13:55:54] epoch: 833 train-loss: 0.010595687251124117\n",
      "[LOG 20200502-13:55:54] epoch: 834 train-loss: 0.010595691183374988\n",
      "[LOG 20200502-13:55:54] epoch: 835 train-loss: 0.01059569532258643\n",
      "[LOG 20200502-13:55:54] epoch: 836 train-loss: 0.010595698633955585\n",
      "[LOG 20200502-13:55:54] epoch: 837 train-loss: 0.010595701738364167\n",
      "[LOG 20200502-13:55:55] epoch: 838 train-loss: 0.010595704739292463\n",
      "[LOG 20200502-13:55:55] epoch: 839 train-loss: 0.010595707119339041\n",
      "[LOG 20200502-13:55:55] epoch: 840 train-loss: 0.010595709913306765\n",
      "[LOG 20200502-13:55:55] epoch: 841 train-loss: 0.010595711982912488\n",
      "[LOG 20200502-13:55:55] epoch: 842 train-loss: 0.010595713535116779\n",
      "[LOG 20200502-13:55:56] epoch: 843 train-loss: 0.010595715501242213\n",
      "[LOG 20200502-13:55:56] epoch: 844 train-loss: 0.010595716743005646\n",
      "[LOG 20200502-13:55:56] epoch: 845 train-loss: 0.010595716949966218\n",
      "[LOG 20200502-13:55:56] epoch: 846 train-loss: 0.010595717570847936\n",
      "[LOG 20200502-13:55:56] epoch: 847 train-loss: 0.01059571715692679\n",
      "[LOG 20200502-13:55:57] epoch: 848 train-loss: 0.01059571715692679\n",
      "[LOG 20200502-13:55:57] epoch: 849 train-loss: 0.010595716432564788\n",
      "[LOG 20200502-13:55:57] epoch: 850 train-loss: 0.010595715501242213\n",
      "[LOG 20200502-13:55:57] epoch: 851 train-loss: 0.010595714466439353\n",
      "[LOG 20200502-13:55:57] epoch: 852 train-loss: 0.010595713017715348\n",
      "[LOG 20200502-13:55:58] epoch: 853 train-loss: 0.010595710637668768\n",
      "[LOG 20200502-13:55:58] epoch: 854 train-loss: 0.01059570825762219\n",
      "[LOG 20200502-13:55:58] epoch: 855 train-loss: 0.010595705774095323\n",
      "[LOG 20200502-13:55:58] epoch: 856 train-loss: 0.010595702876647314\n",
      "[LOG 20200502-13:55:58] epoch: 857 train-loss: 0.010595699772238731\n",
      "[LOG 20200502-13:55:59] epoch: 858 train-loss: 0.010595696046948433\n",
      "[LOG 20200502-13:55:59] epoch: 859 train-loss: 0.010595692114697563\n",
      "[LOG 20200502-13:55:59] epoch: 860 train-loss: 0.010595687665045261\n",
      "[LOG 20200502-13:55:59] epoch: 861 train-loss: 0.010595683422353532\n",
      "[LOG 20200502-13:55:59] epoch: 862 train-loss: 0.010595677937898371\n",
      "[LOG 20200502-13:56:00] epoch: 863 train-loss: 0.010595672867364354\n",
      "[LOG 20200502-13:56:00] epoch: 864 train-loss: 0.010595666968988048\n",
      "[LOG 20200502-13:56:00] epoch: 865 train-loss: 0.010595661484532885\n",
      "[LOG 20200502-13:56:00] epoch: 866 train-loss: 0.010595654758314291\n",
      "[LOG 20200502-13:56:00] epoch: 867 train-loss: 0.010595648032095697\n",
      "[LOG 20200502-13:56:01] epoch: 868 train-loss: 0.010595640995436244\n",
      "[LOG 20200502-13:56:01] epoch: 869 train-loss: 0.010595633337895075\n",
      "[LOG 20200502-13:56:01] epoch: 870 train-loss: 0.01059562609427505\n",
      "[LOG 20200502-13:56:01] epoch: 871 train-loss: 0.010595617401931021\n",
      "[LOG 20200502-13:56:01] epoch: 872 train-loss: 0.010595608709586991\n",
      "[LOG 20200502-13:56:02] epoch: 873 train-loss: 0.010595599706802104\n",
      "[LOG 20200502-13:56:02] epoch: 874 train-loss: 0.01059559060053693\n",
      "[LOG 20200502-13:56:02] epoch: 875 train-loss: 0.010595581080350611\n",
      "[LOG 20200502-13:56:02] epoch: 876 train-loss: 0.010595570732322004\n",
      "[LOG 20200502-13:56:02] epoch: 877 train-loss: 0.010595561315615972\n",
      "[LOG 20200502-13:56:03] epoch: 878 train-loss: 0.01059555003626479\n",
      "[LOG 20200502-13:56:03] epoch: 879 train-loss: 0.010595538963874182\n",
      "[LOG 20200502-13:56:03] epoch: 880 train-loss: 0.010595527891483571\n",
      "[LOG 20200502-13:56:03] epoch: 881 train-loss: 0.010595515784290101\n",
      "[LOG 20200502-13:56:03] epoch: 882 train-loss: 0.010595503263175488\n",
      "[LOG 20200502-13:56:04] epoch: 883 train-loss: 0.01059549105250173\n",
      "[LOG 20200502-13:56:04] epoch: 884 train-loss: 0.010595477496584257\n",
      "[LOG 20200502-13:56:04] epoch: 885 train-loss: 0.01059546425110764\n",
      "[LOG 20200502-13:56:04] epoch: 886 train-loss: 0.01059545079867045\n",
      "[LOG 20200502-13:56:04] epoch: 887 train-loss: 0.010595436621871259\n",
      "[LOG 20200502-13:56:05] epoch: 888 train-loss: 0.010595422134631209\n",
      "[LOG 20200502-13:56:05] epoch: 889 train-loss: 0.010595406716068586\n",
      "[LOG 20200502-13:56:05] epoch: 890 train-loss: 0.010595390987065103\n",
      "[LOG 20200502-13:56:05] epoch: 891 train-loss: 0.010595375465022193\n",
      "[LOG 20200502-13:56:05] epoch: 892 train-loss: 0.010595359011656709\n",
      "[LOG 20200502-13:56:06] epoch: 893 train-loss: 0.010595342972212367\n",
      "[LOG 20200502-13:56:06] epoch: 894 train-loss: 0.010595325691004595\n",
      "[LOG 20200502-13:56:06] epoch: 895 train-loss: 0.01059530789239539\n",
      "[LOG 20200502-13:56:06] epoch: 896 train-loss: 0.010595289576384757\n",
      "[LOG 20200502-13:56:06] epoch: 897 train-loss: 0.010595271984736124\n",
      "[LOG 20200502-13:56:07] epoch: 898 train-loss: 0.010595253358284632\n",
      "[LOG 20200502-13:56:07] epoch: 899 train-loss: 0.01059523473183314\n",
      "[LOG 20200502-13:56:07] epoch: 900 train-loss: 0.010595215070578787\n",
      "[LOG 20200502-13:56:07] epoch: 901 train-loss: 0.010595195305844149\n",
      "[LOG 20200502-13:56:07] epoch: 902 train-loss: 0.010595173781944646\n",
      "[LOG 20200502-13:56:08] epoch: 903 train-loss: 0.010595151844123999\n",
      "[LOG 20200502-13:56:08] epoch: 904 train-loss: 0.010595129802823067\n",
      "[LOG 20200502-13:56:08] epoch: 905 train-loss: 0.010595106933679845\n",
      "[LOG 20200502-13:56:08] epoch: 906 train-loss: 0.010595084064536624\n",
      "[LOG 20200502-13:56:08] epoch: 907 train-loss: 0.010595060988432832\n",
      "[LOG 20200502-13:56:09] epoch: 908 train-loss: 0.010595036360124746\n",
      "[LOG 20200502-13:56:09] epoch: 909 train-loss: 0.010595011938777234\n",
      "[LOG 20200502-13:56:09] epoch: 910 train-loss: 0.010594987413949437\n",
      "[LOG 20200502-13:56:09] epoch: 911 train-loss: 0.010594962164759636\n",
      "[LOG 20200502-13:56:09] epoch: 912 train-loss: 0.010594936398168405\n",
      "[LOG 20200502-13:56:10] epoch: 913 train-loss: 0.010594910114175744\n",
      "[LOG 20200502-13:56:10] epoch: 914 train-loss: 0.010594883519742224\n",
      "[LOG 20200502-13:56:10] epoch: 915 train-loss: 0.010594856304426989\n",
      "[LOG 20200502-13:56:10] epoch: 916 train-loss: 0.010594828675190607\n",
      "[LOG 20200502-13:56:11] epoch: 917 train-loss: 0.01059480094247394\n",
      "[LOG 20200502-13:56:11] epoch: 918 train-loss: 0.010594772485395273\n",
      "[LOG 20200502-13:56:11] epoch: 919 train-loss: 0.010594743303954601\n",
      "[LOG 20200502-13:56:11] epoch: 920 train-loss: 0.0105947136051125\n",
      "[LOG 20200502-13:56:11] epoch: 921 train-loss: 0.01059468411323097\n",
      "[LOG 20200502-13:56:12] epoch: 922 train-loss: 0.010594653793507151\n",
      "[LOG 20200502-13:56:12] epoch: 923 train-loss: 0.010594622852901617\n",
      "[LOG 20200502-13:56:12] epoch: 924 train-loss: 0.010594591705335511\n",
      "[LOG 20200502-13:56:12] epoch: 925 train-loss: 0.01059456014384826\n",
      "[LOG 20200502-13:56:12] epoch: 926 train-loss: 0.010594527961479293\n",
      "[LOG 20200502-13:56:13] epoch: 927 train-loss: 0.010594494951268038\n",
      "[LOG 20200502-13:56:13] epoch: 928 train-loss: 0.010594462044537067\n",
      "[LOG 20200502-13:56:13] epoch: 929 train-loss: 0.01059442830996381\n",
      "[LOG 20200502-13:56:13] epoch: 930 train-loss: 0.010594394471910264\n",
      "[LOG 20200502-13:56:13] epoch: 931 train-loss: 0.010594359806014432\n",
      "[LOG 20200502-13:56:14] epoch: 932 train-loss: 0.01059432482967774\n",
      "[LOG 20200502-13:56:14] epoch: 933 train-loss: 0.010594289439419905\n",
      "[LOG 20200502-13:56:14] epoch: 934 train-loss: 0.010594253324800067\n",
      "[LOG 20200502-13:56:14] epoch: 935 train-loss: 0.01059421721018023\n",
      "[LOG 20200502-13:56:14] epoch: 936 train-loss: 0.010594180060757531\n",
      "[LOG 20200502-13:56:15] epoch: 937 train-loss: 0.010594142911334833\n",
      "[LOG 20200502-13:56:15] epoch: 938 train-loss: 0.010594104520148702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-13:56:15] epoch: 939 train-loss: 0.010594067060285144\n",
      "[LOG 20200502-13:56:15] epoch: 940 train-loss: 0.010594027737776438\n",
      "[LOG 20200502-13:56:15] epoch: 941 train-loss: 0.010593988311787447\n",
      "[LOG 20200502-13:56:16] epoch: 942 train-loss: 0.010593949092759026\n",
      "[LOG 20200502-13:56:16] epoch: 943 train-loss: 0.010593908631967174\n",
      "[LOG 20200502-13:56:16] epoch: 944 train-loss: 0.010593867757254176\n",
      "[LOG 20200502-13:56:16] epoch: 945 train-loss: 0.010593826261659464\n",
      "[LOG 20200502-13:56:16] epoch: 946 train-loss: 0.010593784455623891\n",
      "[LOG 20200502-13:56:17] epoch: 947 train-loss: 0.010593742235667176\n",
      "[LOG 20200502-13:56:17] epoch: 948 train-loss: 0.010593699601789316\n",
      "[LOG 20200502-13:56:17] epoch: 949 train-loss: 0.010593656243549453\n",
      "[LOG 20200502-13:56:17] epoch: 950 train-loss: 0.010593612471388446\n",
      "[LOG 20200502-13:56:17] epoch: 951 train-loss: 0.010593568906188011\n",
      "[LOG 20200502-13:56:18] epoch: 952 train-loss: 0.01059352389226357\n",
      "[LOG 20200502-13:56:18] epoch: 953 train-loss: 0.010593478981819417\n",
      "[LOG 20200502-13:56:18] epoch: 954 train-loss: 0.010593433450493548\n",
      "[LOG 20200502-13:56:18] epoch: 955 train-loss: 0.01059338709132539\n",
      "[LOG 20200502-13:56:18] epoch: 956 train-loss: 0.010593340525196658\n",
      "[LOG 20200502-13:56:19] epoch: 957 train-loss: 0.010593293027745353\n",
      "[LOG 20200502-13:56:19] epoch: 958 train-loss: 0.01059324573725462\n",
      "[LOG 20200502-13:56:19] epoch: 959 train-loss: 0.010593198032842742\n",
      "[LOG 20200502-13:56:19] epoch: 960 train-loss: 0.010593149086667432\n",
      "[LOG 20200502-13:56:19] epoch: 961 train-loss: 0.010593100037011836\n",
      "[LOG 20200502-13:56:20] epoch: 962 train-loss: 0.010593050780395666\n",
      "[LOG 20200502-13:56:20] epoch: 963 train-loss: 0.010593000592456924\n",
      "[LOG 20200502-13:56:20] epoch: 964 train-loss: 0.010592950301037895\n",
      "[LOG 20200502-13:56:20] epoch: 965 train-loss: 0.010592898871335719\n",
      "[LOG 20200502-13:56:20] epoch: 966 train-loss: 0.010592847545113828\n",
      "[LOG 20200502-13:56:21] epoch: 967 train-loss: 0.010592795804970793\n",
      "[LOG 20200502-13:56:21] epoch: 968 train-loss: 0.010592743133505186\n",
      "[LOG 20200502-13:56:21] epoch: 969 train-loss: 0.010592690255079005\n",
      "[LOG 20200502-13:56:21] epoch: 970 train-loss: 0.010592637066211965\n",
      "[LOG 20200502-13:56:21] epoch: 971 train-loss: 0.010592583152982924\n",
      "[LOG 20200502-13:56:22] epoch: 972 train-loss: 0.010592528825832738\n",
      "[LOG 20200502-13:56:22] epoch: 973 train-loss: 0.01059247429172198\n",
      "[LOG 20200502-13:56:22] epoch: 974 train-loss: 0.010592419343690077\n",
      "[LOG 20200502-13:56:22] epoch: 975 train-loss: 0.010592363360855315\n",
      "[LOG 20200502-13:56:22] epoch: 976 train-loss: 0.010592307067579694\n",
      "[LOG 20200502-13:56:23] epoch: 977 train-loss: 0.010592250774304071\n",
      "[LOG 20200502-13:56:23] epoch: 978 train-loss: 0.010592193653186163\n",
      "[LOG 20200502-13:56:23] epoch: 979 train-loss: 0.010592135911186537\n",
      "[LOG 20200502-13:56:23] epoch: 980 train-loss: 0.01059207765178548\n",
      "[LOG 20200502-13:56:23] epoch: 981 train-loss: 0.010592019702825282\n",
      "[LOG 20200502-13:56:24] epoch: 982 train-loss: 0.010591960719062222\n",
      "[LOG 20200502-13:56:24] epoch: 983 train-loss: 0.010591901631818878\n",
      "[LOG 20200502-13:56:24] epoch: 984 train-loss: 0.010591841509772671\n",
      "[LOG 20200502-13:56:24] epoch: 985 train-loss: 0.010591781077285608\n",
      "[LOG 20200502-13:56:24] epoch: 986 train-loss: 0.01059172074827883\n",
      "[LOG 20200502-13:56:25] epoch: 987 train-loss: 0.010591659487949477\n",
      "[LOG 20200502-13:56:25] epoch: 988 train-loss: 0.010591597813698981\n",
      "[LOG 20200502-13:56:25] epoch: 989 train-loss: 0.010591535622047054\n",
      "[LOG 20200502-13:56:25] epoch: 990 train-loss: 0.010591473016473982\n",
      "[LOG 20200502-13:56:25] epoch: 991 train-loss: 0.01059141020394034\n",
      "[LOG 20200502-13:56:26] epoch: 992 train-loss: 0.010591346874005265\n",
      "[LOG 20200502-13:56:26] epoch: 993 train-loss: 0.010591283026668761\n",
      "[LOG 20200502-13:56:26] epoch: 994 train-loss: 0.01059121855845054\n",
      "[LOG 20200502-13:56:26] epoch: 995 train-loss: 0.010591154193712605\n",
      "[LOG 20200502-13:56:26] epoch: 996 train-loss: 0.01059108879417181\n",
      "[LOG 20200502-13:56:27] epoch: 997 train-loss: 0.010591023808552159\n",
      "[LOG 20200502-13:56:27] epoch: 998 train-loss: 0.010590957684649361\n",
      "[LOG 20200502-13:56:27] epoch: 999 train-loss: 0.010590891250305705\n",
      "[LOG 20200502-13:56:27] epoch: 1000 train-loss: 0.01059082429856062\n",
      "[LOG 20200502-13:56:27] epoch: 1001 train-loss: 0.010590757036374675\n",
      "[LOG 20200502-13:56:28] epoch: 1002 train-loss: 0.010590689670708444\n",
      "[LOG 20200502-13:56:28] epoch: 1003 train-loss: 0.01059062137371964\n",
      "[LOG 20200502-13:56:28] epoch: 1004 train-loss: 0.010590552766289976\n",
      "[LOG 20200502-13:56:28] epoch: 1005 train-loss: 0.010590483020577166\n",
      "[LOG 20200502-13:56:28] epoch: 1006 train-loss: 0.010590412964423498\n",
      "[LOG 20200502-13:56:29] epoch: 1007 train-loss: 0.010590342597828971\n",
      "[LOG 20200502-13:56:29] epoch: 1008 train-loss: 0.010590271610352728\n",
      "[LOG 20200502-13:56:29] epoch: 1009 train-loss: 0.010590200105475055\n",
      "[LOG 20200502-13:56:29] epoch: 1010 train-loss: 0.010590128290156523\n",
      "[LOG 20200502-13:56:29] epoch: 1011 train-loss: 0.010590056371357705\n",
      "[LOG 20200502-13:56:30] epoch: 1012 train-loss: 0.010589984038637744\n",
      "[LOG 20200502-13:56:30] epoch: 1013 train-loss: 0.01058991098155578\n",
      "[LOG 20200502-13:56:30] epoch: 1014 train-loss: 0.010589837200111814\n",
      "[LOG 20200502-13:56:30] epoch: 1015 train-loss: 0.010589763936069276\n",
      "[LOG 20200502-13:56:30] epoch: 1016 train-loss: 0.01058969015462531\n",
      "[LOG 20200502-13:56:31] epoch: 1017 train-loss: 0.01058961564881934\n",
      "[LOG 20200502-13:56:31] epoch: 1018 train-loss: 0.010589541349973943\n",
      "[LOG 20200502-13:56:31] epoch: 1019 train-loss: 0.010589466223286258\n",
      "[LOG 20200502-13:56:31] epoch: 1020 train-loss: 0.010589390889638\n",
      "[LOG 20200502-13:56:31] epoch: 1021 train-loss: 0.010589314728147455\n",
      "[LOG 20200502-13:56:32] epoch: 1022 train-loss: 0.010589238670137193\n",
      "[LOG 20200502-13:56:32] epoch: 1023 train-loss: 0.01058916219820579\n",
      "[LOG 20200502-13:56:32] epoch: 1024 train-loss: 0.010589085415833525\n",
      "[LOG 20200502-13:56:32] epoch: 1025 train-loss: 0.010589008633461263\n",
      "[LOG 20200502-13:56:32] epoch: 1026 train-loss: 0.010588931023246713\n",
      "[LOG 20200502-13:56:33] epoch: 1027 train-loss: 0.010588853413032161\n",
      "[LOG 20200502-13:56:33] epoch: 1028 train-loss: 0.010588774974975321\n",
      "[LOG 20200502-13:56:33] epoch: 1029 train-loss: 0.010588696640398767\n",
      "[LOG 20200502-13:56:33] epoch: 1030 train-loss: 0.010588617995381355\n",
      "[LOG 20200502-13:56:33] epoch: 1031 train-loss: 0.010588538832962513\n",
      "[LOG 20200502-13:56:34] epoch: 1032 train-loss: 0.010588459360102812\n",
      "[LOG 20200502-13:56:34] epoch: 1033 train-loss: 0.010588379887243113\n",
      "[LOG 20200502-13:56:34] epoch: 1034 train-loss: 0.01058829937958055\n",
      "[LOG 20200502-13:56:34] epoch: 1035 train-loss: 0.010588219596279992\n",
      "[LOG 20200502-13:56:34] epoch: 1036 train-loss: 0.010588139606018862\n",
      "[LOG 20200502-13:56:35] epoch: 1037 train-loss: 0.010588058270514011\n",
      "[LOG 20200502-13:56:35] epoch: 1038 train-loss: 0.010587977659371164\n",
      "[LOG 20200502-13:56:35] epoch: 1039 train-loss: 0.010587896116905741\n",
      "[LOG 20200502-13:56:35] epoch: 1040 train-loss: 0.010587814263999462\n",
      "[LOG 20200502-13:56:35] epoch: 1041 train-loss: 0.010587732307612896\n",
      "[LOG 20200502-13:56:36] epoch: 1042 train-loss: 0.010587649937305186\n",
      "[LOG 20200502-13:56:36] epoch: 1043 train-loss: 0.010587567360036902\n",
      "[LOG 20200502-13:56:36] epoch: 1044 train-loss: 0.010587484575808048\n",
      "[LOG 20200502-13:56:36] epoch: 1045 train-loss: 0.010587401688098907\n",
      "[LOG 20200502-13:56:37] epoch: 1046 train-loss: 0.01058731869690948\n",
      "[LOG 20200502-13:56:37] epoch: 1047 train-loss: 0.01058723477439748\n",
      "[LOG 20200502-13:56:37] epoch: 1048 train-loss: 0.010587151369286908\n",
      "[LOG 20200502-13:56:37] epoch: 1049 train-loss: 0.010587067136334049\n",
      "[LOG 20200502-13:56:37] epoch: 1050 train-loss: 0.010586983110341761\n",
      "[LOG 20200502-13:56:38] epoch: 1051 train-loss: 0.0105868981530269\n",
      "[LOG 20200502-13:56:38] epoch: 1052 train-loss: 0.010586814023554325\n",
      "[LOG 20200502-13:56:38] epoch: 1053 train-loss: 0.010586728445357747\n",
      "[LOG 20200502-13:56:38] epoch: 1054 train-loss: 0.010586644005444314\n",
      "[LOG 20200502-13:56:38] epoch: 1055 train-loss: 0.010586558737688594\n",
      "[LOG 20200502-13:56:39] epoch: 1056 train-loss: 0.010586473056011729\n",
      "[LOG 20200502-13:56:39] epoch: 1057 train-loss: 0.010586387063894007\n",
      "[LOG 20200502-13:56:39] epoch: 1058 train-loss: 0.01058630117525657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-13:56:39] epoch: 1059 train-loss: 0.010586215286619134\n",
      "[LOG 20200502-13:56:39] epoch: 1060 train-loss: 0.010586129087540839\n",
      "[LOG 20200502-13:56:40] epoch: 1061 train-loss: 0.010586041853659682\n",
      "[LOG 20200502-13:56:40] epoch: 1062 train-loss: 0.010585955137179958\n",
      "[LOG 20200502-13:56:40] epoch: 1063 train-loss: 0.010585868317219947\n",
      "[LOG 20200502-13:56:40] epoch: 1064 train-loss: 0.010585780979858505\n",
      "[LOG 20200502-13:56:40] epoch: 1065 train-loss: 0.010585694056418207\n",
      "[LOG 20200502-13:56:41] epoch: 1066 train-loss: 0.010585606822537051\n",
      "[LOG 20200502-13:56:41] epoch: 1067 train-loss: 0.010585518864293894\n",
      "[LOG 20200502-13:56:41] epoch: 1068 train-loss: 0.01058543131997188\n",
      "[LOG 20200502-13:56:41] epoch: 1069 train-loss: 0.010585343258248435\n",
      "[LOG 20200502-13:56:41] epoch: 1070 train-loss: 0.010585255300005278\n",
      "[LOG 20200502-13:56:42] epoch: 1071 train-loss: 0.010585167548722692\n",
      "[LOG 20200502-13:56:42] epoch: 1072 train-loss: 0.010585079073078103\n",
      "[LOG 20200502-13:56:42] epoch: 1073 train-loss: 0.010584991011354659\n",
      "[LOG 20200502-13:56:42] epoch: 1074 train-loss: 0.010584902328749498\n",
      "[LOG 20200502-13:56:42] epoch: 1075 train-loss: 0.010584813542664051\n",
      "[LOG 20200502-13:56:43] epoch: 1076 train-loss: 0.010584725067019463\n",
      "[LOG 20200502-13:56:43] epoch: 1077 train-loss: 0.01058463617745373\n",
      "[LOG 20200502-13:56:43] epoch: 1078 train-loss: 0.010584547287887998\n",
      "[LOG 20200502-13:56:43] epoch: 1079 train-loss: 0.01058445850180255\n",
      "[LOG 20200502-13:56:43] epoch: 1080 train-loss: 0.010584368887874816\n",
      "[LOG 20200502-13:56:44] epoch: 1081 train-loss: 0.010584279894828796\n",
      "[LOG 20200502-13:56:44] epoch: 1082 train-loss: 0.010584190694822205\n",
      "[LOG 20200502-13:56:44] epoch: 1083 train-loss: 0.010584101184374757\n",
      "[LOG 20200502-13:56:44] epoch: 1084 train-loss: 0.010584011984368166\n",
      "[LOG 20200502-13:56:44] epoch: 1085 train-loss: 0.010583922266960144\n",
      "[LOG 20200502-13:56:45] epoch: 1086 train-loss: 0.010583832859992981\n",
      "[LOG 20200502-13:56:45] epoch: 1087 train-loss: 0.010583743246065246\n",
      "[LOG 20200502-13:56:45] epoch: 1088 train-loss: 0.010583653632137511\n",
      "[LOG 20200502-13:56:45] epoch: 1089 train-loss: 0.010583563914729489\n",
      "[LOG 20200502-13:56:45] epoch: 1090 train-loss: 0.010583473990360895\n",
      "[LOG 20200502-13:56:46] epoch: 1091 train-loss: 0.01058338437643316\n",
      "[LOG 20200502-13:56:46] epoch: 1092 train-loss: 0.010583294452064566\n",
      "[LOG 20200502-13:56:46] epoch: 1093 train-loss: 0.010583204320735402\n",
      "[LOG 20200502-13:56:46] epoch: 1094 train-loss: 0.01058311460332738\n",
      "[LOG 20200502-13:56:46] epoch: 1095 train-loss: 0.010583024265037643\n",
      "[LOG 20200502-13:56:47] epoch: 1096 train-loss: 0.010582934547629621\n",
      "[LOG 20200502-13:56:47] epoch: 1097 train-loss: 0.0105828448302216\n",
      "[LOG 20200502-13:56:47] epoch: 1098 train-loss: 0.010582754388451576\n",
      "[LOG 20200502-13:56:47] epoch: 1099 train-loss: 0.010582664464082982\n",
      "[LOG 20200502-13:56:47] epoch: 1100 train-loss: 0.010582574436234104\n",
      "[LOG 20200502-13:56:48] epoch: 1101 train-loss: 0.010582484201424651\n",
      "[LOG 20200502-13:56:48] epoch: 1102 train-loss: 0.010582394173575772\n",
      "[LOG 20200502-13:56:48] epoch: 1103 train-loss: 0.010582304145726893\n",
      "[LOG 20200502-13:56:48] epoch: 1104 train-loss: 0.0105822142213583\n",
      "[LOG 20200502-13:56:48] epoch: 1105 train-loss: 0.010582124090029133\n",
      "[LOG 20200502-13:56:49] epoch: 1106 train-loss: 0.010582034372621112\n",
      "[LOG 20200502-13:56:49] epoch: 1107 train-loss: 0.010581944344772233\n",
      "[LOG 20200502-13:56:49] epoch: 1108 train-loss: 0.010581854006482495\n",
      "[LOG 20200502-13:56:49] epoch: 1109 train-loss: 0.010581764185594188\n",
      "[LOG 20200502-13:56:49] epoch: 1110 train-loss: 0.010581674468186166\n",
      "[LOG 20200502-13:56:50] epoch: 1111 train-loss: 0.010581584543817572\n",
      "[LOG 20200502-13:56:50] epoch: 1112 train-loss: 0.010581495033370124\n",
      "[LOG 20200502-13:56:50] epoch: 1113 train-loss: 0.010581405419442389\n",
      "[LOG 20200502-13:56:50] epoch: 1114 train-loss: 0.010581315598554082\n",
      "[LOG 20200502-13:56:50] epoch: 1115 train-loss: 0.010581225777665773\n",
      "[LOG 20200502-13:56:51] epoch: 1116 train-loss: 0.010581136163738038\n",
      "[LOG 20200502-13:56:51] epoch: 1117 train-loss: 0.010581046963731447\n",
      "[LOG 20200502-13:56:51] epoch: 1118 train-loss: 0.01058095714284314\n",
      "[LOG 20200502-13:56:51] epoch: 1119 train-loss: 0.01058086846023798\n",
      "[LOG 20200502-13:56:51] epoch: 1120 train-loss: 0.010580778742829958\n",
      "[LOG 20200502-13:56:52] epoch: 1121 train-loss: 0.01058068974978394\n",
      "[LOG 20200502-13:56:52] epoch: 1122 train-loss: 0.010580600653257635\n",
      "[LOG 20200502-13:56:52] epoch: 1123 train-loss: 0.010580511349770758\n",
      "[LOG 20200502-13:56:52] epoch: 1124 train-loss: 0.010580422460205026\n",
      "[LOG 20200502-13:56:52] epoch: 1125 train-loss: 0.01058033388108015\n",
      "[LOG 20200502-13:56:53] epoch: 1126 train-loss: 0.01058024519847499\n",
      "[LOG 20200502-13:56:53] epoch: 1127 train-loss: 0.010580156412389543\n",
      "[LOG 20200502-13:56:53] epoch: 1128 train-loss: 0.010580067936744954\n",
      "[LOG 20200502-13:56:53] epoch: 1129 train-loss: 0.010579979564580653\n",
      "[LOG 20200502-13:56:53] epoch: 1130 train-loss: 0.010579891399376921\n",
      "[LOG 20200502-13:56:54] epoch: 1131 train-loss: 0.010579803337653479\n",
      "[LOG 20200502-13:56:54] epoch: 1132 train-loss: 0.010579715068969462\n",
      "[LOG 20200502-13:56:54] epoch: 1133 train-loss: 0.010579626593324874\n",
      "[LOG 20200502-13:56:54] epoch: 1134 train-loss: 0.010579539152483145\n",
      "[LOG 20200502-13:56:54] epoch: 1135 train-loss: 0.010579451090759702\n",
      "[LOG 20200502-13:56:55] epoch: 1136 train-loss: 0.010579363960358832\n",
      "[LOG 20200502-13:56:55] epoch: 1137 train-loss: 0.010579276209076246\n",
      "[LOG 20200502-13:56:55] epoch: 1138 train-loss: 0.010579189078675376\n",
      "[LOG 20200502-13:56:55] epoch: 1139 train-loss: 0.010579101637833647\n",
      "[LOG 20200502-13:56:55] epoch: 1140 train-loss: 0.010579014610913064\n",
      "[LOG 20200502-13:56:56] epoch: 1141 train-loss: 0.010578928101393912\n",
      "[LOG 20200502-13:56:56] epoch: 1142 train-loss: 0.010578840970993042\n",
      "[LOG 20200502-13:56:56] epoch: 1143 train-loss: 0.010578754564954175\n",
      "[LOG 20200502-13:56:56] epoch: 1144 train-loss: 0.010578668055435022\n",
      "[LOG 20200502-13:56:56] epoch: 1145 train-loss: 0.010578582063317299\n",
      "[LOG 20200502-13:56:57] epoch: 1146 train-loss: 0.010578495657278432\n",
      "[LOG 20200502-13:56:57] epoch: 1147 train-loss: 0.010578410182562139\n",
      "[LOG 20200502-13:56:57] epoch: 1148 train-loss: 0.010578323983483844\n",
      "[LOG 20200502-13:56:57] epoch: 1149 train-loss: 0.010578238508767553\n",
      "[LOG 20200502-13:56:57] epoch: 1150 train-loss: 0.010578152930570973\n",
      "[LOG 20200502-13:56:58] epoch: 1151 train-loss: 0.010578067973256111\n",
      "[LOG 20200502-13:56:58] epoch: 1152 train-loss: 0.01057798270550039\n",
      "[LOG 20200502-13:56:58] epoch: 1153 train-loss: 0.010577897230784098\n",
      "[LOG 20200502-13:56:58] epoch: 1154 train-loss: 0.010577812376949523\n",
      "[LOG 20200502-13:56:58] epoch: 1155 train-loss: 0.01057772793703609\n",
      "[LOG 20200502-13:56:59] epoch: 1156 train-loss: 0.010577643497122658\n",
      "[LOG 20200502-13:56:59] epoch: 1157 train-loss: 0.010577559160689512\n",
      "[LOG 20200502-13:56:59] epoch: 1158 train-loss: 0.01057747503121694\n",
      "[LOG 20200502-13:56:59] epoch: 1159 train-loss: 0.010577390487823222\n",
      "[LOG 20200502-13:56:59] epoch: 1160 train-loss: 0.010577307289673222\n",
      "[LOG 20200502-13:57:00] epoch: 1161 train-loss: 0.01057722419500351\n",
      "[LOG 20200502-13:57:00] epoch: 1162 train-loss: 0.010577140375971794\n",
      "[LOG 20200502-13:57:00] epoch: 1163 train-loss: 0.010577057177821795\n",
      "[LOG 20200502-13:57:00] epoch: 1164 train-loss: 0.010576974186632369\n",
      "[LOG 20200502-13:57:00] epoch: 1165 train-loss: 0.010576891298923228\n",
      "[LOG 20200502-13:57:01] epoch: 1166 train-loss: 0.010576808825135231\n",
      "[LOG 20200502-13:57:01] epoch: 1167 train-loss: 0.010576726351347234\n",
      "[LOG 20200502-13:57:01] epoch: 1168 train-loss: 0.010576644394960668\n",
      "[LOG 20200502-13:57:01] epoch: 1169 train-loss: 0.01057656223161353\n",
      "[LOG 20200502-13:57:01] epoch: 1170 train-loss: 0.010576480068266392\n",
      "[LOG 20200502-13:57:02] epoch: 1171 train-loss: 0.010576398732761541\n",
      "[LOG 20200502-13:57:02] epoch: 1172 train-loss: 0.010576317397256693\n",
      "[LOG 20200502-13:57:02] epoch: 1173 train-loss: 0.010576236061751842\n",
      "[LOG 20200502-13:57:02] epoch: 1174 train-loss: 0.010576155140168138\n",
      "[LOG 20200502-13:57:02] epoch: 1175 train-loss: 0.010576074425545003\n",
      "[LOG 20200502-13:57:03] epoch: 1176 train-loss: 0.010575994228323301\n",
      "[LOG 20200502-13:57:03] epoch: 1177 train-loss: 0.010575913617180454\n",
      "[LOG 20200502-13:57:03] epoch: 1178 train-loss: 0.010575833523439037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-13:57:03] epoch: 1179 train-loss: 0.010575753843618764\n",
      "[LOG 20200502-13:57:03] epoch: 1180 train-loss: 0.010575674681199921\n",
      "[LOG 20200502-13:57:04] epoch: 1181 train-loss: 0.010575594897899363\n",
      "[LOG 20200502-13:57:04] epoch: 1182 train-loss: 0.010575515528519949\n",
      "[LOG 20200502-13:57:04] epoch: 1183 train-loss: 0.010575436469581392\n",
      "[LOG 20200502-13:57:04] epoch: 1184 train-loss: 0.010575358238485124\n",
      "[LOG 20200502-13:57:04] epoch: 1185 train-loss: 0.010575279489987426\n",
      "[LOG 20200502-13:57:05] epoch: 1186 train-loss: 0.010575201879772875\n",
      "[LOG 20200502-13:57:05] epoch: 1187 train-loss: 0.010575123234755464\n",
      "[LOG 20200502-13:57:05] epoch: 1188 train-loss: 0.01057504541758034\n",
      "[LOG 20200502-13:57:05] epoch: 1189 train-loss: 0.010574967703885503\n",
      "[LOG 20200502-13:57:05] epoch: 1190 train-loss: 0.010574890714552667\n",
      "[LOG 20200502-13:57:06] epoch: 1191 train-loss: 0.010574813207818402\n",
      "[LOG 20200502-13:57:06] epoch: 1192 train-loss: 0.010574736011524996\n",
      "[LOG 20200502-13:57:06] epoch: 1193 train-loss: 0.010574659643073877\n",
      "[LOG 20200502-13:57:06] epoch: 1194 train-loss: 0.010574584723346762\n",
      "[LOG 20200502-13:57:06] epoch: 1195 train-loss: 0.010574508665336503\n",
      "[LOG 20200502-13:57:07] epoch: 1196 train-loss: 0.010574433124727674\n",
      "[LOG 20200502-13:57:07] epoch: 1197 train-loss: 0.010574358205000559\n",
      "[LOG 20200502-13:57:07] epoch: 1198 train-loss: 0.010574283388753733\n",
      "[LOG 20200502-13:57:07] epoch: 1199 train-loss: 0.010574208675987191\n",
      "[LOG 20200502-13:57:07] epoch: 1200 train-loss: 0.010574134791062938\n",
      "[LOG 20200502-13:57:08] epoch: 1201 train-loss: 0.010574060699178113\n",
      "[LOG 20200502-13:57:08] epoch: 1202 train-loss: 0.010573986710773574\n",
      "[LOG 20200502-13:57:08] epoch: 1203 train-loss: 0.01057391313629018\n",
      "[LOG 20200502-13:57:08] epoch: 1204 train-loss: 0.010573839768767357\n",
      "[LOG 20200502-13:57:08] epoch: 1205 train-loss: 0.010573766711685393\n",
      "[LOG 20200502-13:57:09] epoch: 1206 train-loss: 0.010573693861564001\n",
      "[LOG 20200502-13:57:09] epoch: 1207 train-loss: 0.010573621114922894\n",
      "[LOG 20200502-13:57:09] epoch: 1208 train-loss: 0.010573548471762074\n",
      "[LOG 20200502-13:57:09] epoch: 1209 train-loss: 0.010573476346002685\n",
      "[LOG 20200502-13:57:09] epoch: 1210 train-loss: 0.010573404944605298\n",
      "[LOG 20200502-13:57:10] epoch: 1211 train-loss: 0.010573333439727625\n",
      "[LOG 20200502-13:57:10] epoch: 1212 train-loss: 0.01057326172788938\n",
      "[LOG 20200502-13:57:10] epoch: 1213 train-loss: 0.010573190843893422\n",
      "[LOG 20200502-13:57:10] epoch: 1214 train-loss: 0.010573119752936892\n",
      "[LOG 20200502-13:57:10] epoch: 1215 train-loss: 0.010573049593302939\n",
      "[LOG 20200502-13:57:11] epoch: 1216 train-loss: 0.010572979537149271\n",
      "[LOG 20200502-13:57:11] epoch: 1217 train-loss: 0.010572909480995603\n",
      "[LOG 20200502-13:57:11] epoch: 1218 train-loss: 0.010572839735282792\n",
      "[LOG 20200502-13:57:11] epoch: 1219 train-loss: 0.01057276978260941\n",
      "[LOG 20200502-13:57:12] epoch: 1220 train-loss: 0.010572700864738889\n",
      "[LOG 20200502-13:57:12] epoch: 1221 train-loss: 0.010572631739907794\n",
      "[LOG 20200502-13:57:12] epoch: 1222 train-loss: 0.010572563028997846\n",
      "[LOG 20200502-13:57:12] epoch: 1223 train-loss: 0.010572494835489325\n",
      "[LOG 20200502-13:57:12] epoch: 1224 train-loss: 0.010572426124579377\n",
      "[LOG 20200502-13:57:13] epoch: 1225 train-loss: 0.010572357931070857\n",
      "[LOG 20200502-13:57:13] epoch: 1226 train-loss: 0.010572290565404627\n",
      "[LOG 20200502-13:57:13] epoch: 1227 train-loss: 0.010572223199738396\n",
      "[LOG 20200502-13:57:13] epoch: 1228 train-loss: 0.010572156041032739\n",
      "[LOG 20200502-13:57:14] epoch: 1229 train-loss: 0.010572088675366508\n",
      "[LOG 20200502-13:57:14] epoch: 1230 train-loss: 0.010572021930581994\n",
      "[LOG 20200502-13:57:14] epoch: 1231 train-loss: 0.01057195622060034\n",
      "[LOG 20200502-13:57:14] epoch: 1232 train-loss: 0.010571889682776399\n",
      "[LOG 20200502-13:57:14] epoch: 1233 train-loss: 0.010571823765834173\n",
      "[LOG 20200502-13:57:15] epoch: 1234 train-loss: 0.010571758676734235\n",
      "[LOG 20200502-13:57:15] epoch: 1235 train-loss: 0.010571692656311724\n",
      "[LOG 20200502-13:57:15] epoch: 1236 train-loss: 0.010571627981132932\n",
      "[LOG 20200502-13:57:15] epoch: 1237 train-loss: 0.010571562685072422\n",
      "[LOG 20200502-13:57:15] epoch: 1238 train-loss: 0.010571498837735917\n",
      "[LOG 20200502-13:57:16] epoch: 1239 train-loss: 0.010571434266037412\n",
      "[LOG 20200502-13:57:16] epoch: 1240 train-loss: 0.010571370211740335\n",
      "[LOG 20200502-13:57:16] epoch: 1241 train-loss: 0.010571306881805262\n",
      "[LOG 20200502-13:57:16] epoch: 1242 train-loss: 0.010571243862311045\n",
      "[LOG 20200502-13:57:16] epoch: 1243 train-loss: 0.010571180635856258\n",
      "[LOG 20200502-13:57:17] epoch: 1244 train-loss: 0.010571117098960612\n",
      "[LOG 20200502-13:57:17] epoch: 1245 train-loss: 0.010571054596867826\n",
      "[LOG 20200502-13:57:17] epoch: 1246 train-loss: 0.010570992301735613\n",
      "[LOG 20200502-13:57:17] epoch: 1247 train-loss: 0.010570930110083686\n",
      "[LOG 20200502-13:57:17] epoch: 1248 train-loss: 0.01057086843583319\n",
      "[LOG 20200502-13:57:18] epoch: 1249 train-loss: 0.010570806865062978\n",
      "[LOG 20200502-13:57:18] epoch: 1250 train-loss: 0.010570745915174484\n",
      "[LOG 20200502-13:57:18] epoch: 1251 train-loss: 0.010570684551364846\n",
      "[LOG 20200502-13:57:18] epoch: 1252 train-loss: 0.010570624325838354\n",
      "[LOG 20200502-13:57:18] epoch: 1253 train-loss: 0.010570563582910432\n",
      "[LOG 20200502-13:57:19] epoch: 1254 train-loss: 0.010570503150423368\n",
      "[LOG 20200502-13:57:19] epoch: 1255 train-loss: 0.010570443545778593\n",
      "[LOG 20200502-13:57:19] epoch: 1256 train-loss: 0.010570384251574675\n",
      "[LOG 20200502-13:57:19] epoch: 1257 train-loss: 0.010570324853890471\n",
      "[LOG 20200502-13:57:20] epoch: 1258 train-loss: 0.010570265973607698\n",
      "[LOG 20200502-13:57:20] epoch: 1259 train-loss: 0.010570206782884069\n",
      "[LOG 20200502-13:57:20] epoch: 1260 train-loss: 0.010570148213042153\n",
      "[LOG 20200502-13:57:20] epoch: 1261 train-loss: 0.010570089953641096\n",
      "[LOG 20200502-13:57:20] epoch: 1262 train-loss: 0.010570032108161185\n",
      "[LOG 20200502-13:57:21] epoch: 1263 train-loss: 0.010569974469641844\n",
      "[LOG 20200502-13:57:21] epoch: 1264 train-loss: 0.010569916624161933\n",
      "[LOG 20200502-13:57:21] epoch: 1265 train-loss: 0.01056985929608345\n",
      "[LOG 20200502-13:57:21] epoch: 1266 train-loss: 0.010569802899327543\n",
      "[LOG 20200502-13:57:21] epoch: 1267 train-loss: 0.010569745985170206\n",
      "[LOG 20200502-13:57:21] epoch: 1268 train-loss: 0.010569689691894583\n",
      "[LOG 20200502-13:57:22] epoch: 1269 train-loss: 0.01056963350209925\n",
      "[LOG 20200502-13:57:22] epoch: 1270 train-loss: 0.010569578036665916\n",
      "[LOG 20200502-13:57:22] epoch: 1271 train-loss: 0.010569522985153727\n",
      "[LOG 20200502-13:57:22] epoch: 1272 train-loss: 0.010569467209279537\n",
      "[LOG 20200502-13:57:23] epoch: 1273 train-loss: 0.010569412468208207\n",
      "[LOG 20200502-13:57:23] epoch: 1274 train-loss: 0.010569357934097448\n",
      "[LOG 20200502-13:57:23] epoch: 1275 train-loss: 0.010569303813907836\n",
      "[LOG 20200502-13:57:23] epoch: 1276 train-loss: 0.010569249279797077\n",
      "[LOG 20200502-13:57:23] epoch: 1277 train-loss: 0.010569195677008893\n",
      "[LOG 20200502-13:57:24] epoch: 1278 train-loss: 0.010569142177700996\n",
      "[LOG 20200502-13:57:24] epoch: 1279 train-loss: 0.010569088471432527\n",
      "[LOG 20200502-13:57:24] epoch: 1280 train-loss: 0.010569034972124629\n",
      "[LOG 20200502-13:57:24] epoch: 1281 train-loss: 0.010568983335461881\n",
      "[LOG 20200502-13:57:24] epoch: 1282 train-loss: 0.010568930250075128\n",
      "[LOG 20200502-13:57:25] epoch: 1283 train-loss: 0.010568878406451808\n",
      "[LOG 20200502-13:57:25] epoch: 1284 train-loss: 0.01056882625238763\n",
      "[LOG 20200502-13:57:25] epoch: 1285 train-loss: 0.010568774719205167\n",
      "[LOG 20200502-13:57:25] epoch: 1286 train-loss: 0.010568723496463563\n",
      "[LOG 20200502-13:57:25] epoch: 1287 train-loss: 0.010568672584162818\n",
      "[LOG 20200502-13:57:26] epoch: 1288 train-loss: 0.01056862198230293\n",
      "[LOG 20200502-13:57:26] epoch: 1289 train-loss: 0.010568572311765619\n",
      "[LOG 20200502-13:57:26] epoch: 1290 train-loss: 0.01056852202034659\n",
      "[LOG 20200502-13:57:26] epoch: 1291 train-loss: 0.010568472867210707\n",
      "[LOG 20200502-13:57:26] epoch: 1292 train-loss: 0.010568423610594537\n",
      "[LOG 20200502-13:57:27] epoch: 1293 train-loss: 0.010568374664419226\n",
      "[LOG 20200502-13:57:27] epoch: 1294 train-loss: 0.010568325718243917\n",
      "[LOG 20200502-13:57:27] epoch: 1295 train-loss: 0.010568277289470037\n",
      "[LOG 20200502-13:57:27] epoch: 1296 train-loss: 0.01056822906765673\n",
      "[LOG 20200502-13:57:27] epoch: 1297 train-loss: 0.010568181363244852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-13:57:28] epoch: 1298 train-loss: 0.010568133865793547\n",
      "[LOG 20200502-13:57:28] epoch: 1299 train-loss: 0.010568086264861954\n",
      "[LOG 20200502-13:57:28] epoch: 1300 train-loss: 0.010568039698733224\n",
      "[LOG 20200502-13:57:28] epoch: 1301 train-loss: 0.010567992511722777\n",
      "[LOG 20200502-13:57:28] epoch: 1302 train-loss: 0.010567946462995477\n",
      "[LOG 20200502-13:57:29] epoch: 1303 train-loss: 0.010567900207307603\n",
      "[LOG 20200502-13:57:29] epoch: 1304 train-loss: 0.010567854469021162\n",
      "[LOG 20200502-13:57:29] epoch: 1305 train-loss: 0.01056780842029386\n",
      "[LOG 20200502-13:57:29] epoch: 1306 train-loss: 0.010567762992448278\n",
      "[LOG 20200502-13:57:29] epoch: 1307 train-loss: 0.010567718082004122\n",
      "[LOG 20200502-13:57:30] epoch: 1308 train-loss: 0.010567672861119112\n",
      "[LOG 20200502-13:57:30] epoch: 1309 train-loss: 0.010567628571556674\n",
      "[LOG 20200502-13:57:30] epoch: 1310 train-loss: 0.010567584385474523\n",
      "[LOG 20200502-13:57:30] epoch: 1311 train-loss: 0.010567540302872658\n",
      "[LOG 20200502-13:57:30] epoch: 1312 train-loss: 0.010567496737672223\n",
      "[LOG 20200502-13:57:31] epoch: 1313 train-loss: 0.010567452551590072\n",
      "[LOG 20200502-13:57:31] epoch: 1314 train-loss: 0.010567409607271353\n",
      "[LOG 20200502-13:57:31] epoch: 1315 train-loss: 0.010567366249031492\n",
      "[LOG 20200502-13:57:31] epoch: 1316 train-loss: 0.010567323822114203\n",
      "[LOG 20200502-13:57:31] epoch: 1317 train-loss: 0.010567281498677202\n",
      "[LOG 20200502-13:57:32] epoch: 1318 train-loss: 0.01056723938220077\n",
      "[LOG 20200502-13:57:32] epoch: 1319 train-loss: 0.010567197472684912\n",
      "[LOG 20200502-13:57:32] epoch: 1320 train-loss: 0.010567155563169055\n",
      "[LOG 20200502-13:57:32] epoch: 1321 train-loss: 0.0105671143780152\n",
      "[LOG 20200502-13:57:32] epoch: 1322 train-loss: 0.010567073606782489\n",
      "[LOG 20200502-13:57:33] epoch: 1323 train-loss: 0.010567032939030064\n",
      "[LOG 20200502-13:57:33] epoch: 1324 train-loss: 0.010566992478238212\n",
      "[LOG 20200502-13:57:33] epoch: 1325 train-loss: 0.010566952224406932\n",
      "[LOG 20200502-13:57:33] epoch: 1326 train-loss: 0.010566912177536223\n",
      "[LOG 20200502-13:57:33] epoch: 1327 train-loss: 0.010566872648066945\n",
      "[LOG 20200502-13:57:34] epoch: 1328 train-loss: 0.010566832808156809\n",
      "[LOG 20200502-13:57:34] epoch: 1329 train-loss: 0.010566793692608675\n",
      "[LOG 20200502-13:57:34] epoch: 1330 train-loss: 0.010566754887501398\n",
      "[LOG 20200502-13:57:34] epoch: 1331 train-loss: 0.010566716082394123\n",
      "[LOG 20200502-13:57:34] epoch: 1332 train-loss: 0.010566677587727705\n",
      "[LOG 20200502-13:57:35] epoch: 1333 train-loss: 0.010566640024383863\n",
      "[LOG 20200502-13:57:35] epoch: 1334 train-loss: 0.010566601840158304\n",
      "[LOG 20200502-13:57:35] epoch: 1335 train-loss: 0.01056656427681446\n",
      "[LOG 20200502-13:57:35] epoch: 1336 train-loss: 0.010566527023911476\n",
      "[LOG 20200502-13:57:36] epoch: 1337 train-loss: 0.01056649008144935\n",
      "[LOG 20200502-13:57:36] epoch: 1338 train-loss: 0.01056645324246751\n",
      "[LOG 20200502-13:57:36] epoch: 1339 train-loss: 0.0105664169208871\n",
      "[LOG 20200502-13:57:36] epoch: 1340 train-loss: 0.010566381013227833\n",
      "[LOG 20200502-13:57:36] epoch: 1341 train-loss: 0.010566344588167138\n",
      "[LOG 20200502-13:57:37] epoch: 1342 train-loss: 0.010566309094429016\n",
      "[LOG 20200502-13:57:37] epoch: 1343 train-loss: 0.010566273807651468\n",
      "[LOG 20200502-13:57:37] epoch: 1344 train-loss: 0.01056623872783449\n",
      "[LOG 20200502-13:57:37] epoch: 1345 train-loss: 0.010566203854978085\n",
      "[LOG 20200502-13:57:37] epoch: 1346 train-loss: 0.010566169396042824\n",
      "[LOG 20200502-13:57:38] epoch: 1347 train-loss: 0.010566135247548422\n",
      "[LOG 20200502-13:57:38] epoch: 1348 train-loss: 0.010566101409494877\n",
      "[LOG 20200502-13:57:38] epoch: 1349 train-loss: 0.010566067054039903\n",
      "[LOG 20200502-13:57:38] epoch: 1350 train-loss: 0.01056603394034836\n",
      "[LOG 20200502-13:57:38] epoch: 1351 train-loss: 0.010566000930137105\n",
      "[LOG 20200502-13:57:39] epoch: 1352 train-loss: 0.01056596864428785\n",
      "[LOG 20200502-13:57:39] epoch: 1353 train-loss: 0.010565936461918883\n",
      "[LOG 20200502-13:57:39] epoch: 1354 train-loss: 0.010565904383030202\n",
      "[LOG 20200502-13:57:39] epoch: 1355 train-loss: 0.010565872925023237\n",
      "[LOG 20200502-13:57:39] epoch: 1356 train-loss: 0.010565840846134556\n",
      "[LOG 20200502-13:57:40] epoch: 1357 train-loss: 0.010565810215969881\n",
      "[LOG 20200502-13:57:40] epoch: 1358 train-loss: 0.01056577865448263\n",
      "[LOG 20200502-13:57:40] epoch: 1359 train-loss: 0.01056574761039681\n",
      "[LOG 20200502-13:57:40] epoch: 1360 train-loss: 0.010565717704594135\n",
      "[LOG 20200502-13:57:40] epoch: 1361 train-loss: 0.010565688005752034\n",
      "[LOG 20200502-13:57:41] epoch: 1362 train-loss: 0.01056565758254793\n",
      "[LOG 20200502-13:57:41] epoch: 1363 train-loss: 0.010565627987186113\n",
      "[LOG 20200502-13:57:41] epoch: 1364 train-loss: 0.010565598598784871\n",
      "[LOG 20200502-13:57:41] epoch: 1365 train-loss: 0.010565569210383628\n",
      "[LOG 20200502-13:57:41] epoch: 1366 train-loss: 0.010565540649824672\n",
      "[LOG 20200502-13:57:42] epoch: 1367 train-loss: 0.010565512192746004\n",
      "[LOG 20200502-13:57:42] epoch: 1368 train-loss: 0.01056548383914762\n",
      "[LOG 20200502-13:57:42] epoch: 1369 train-loss: 0.010565455795990096\n",
      "[LOG 20200502-13:57:42] epoch: 1370 train-loss: 0.010565428477194574\n",
      "[LOG 20200502-13:57:42] epoch: 1371 train-loss: 0.01056540043403705\n",
      "[LOG 20200502-13:57:43] epoch: 1372 train-loss: 0.010565373529162671\n",
      "[LOG 20200502-13:57:43] epoch: 1373 train-loss: 0.01056534672776858\n",
      "[LOG 20200502-13:57:43] epoch: 1374 train-loss: 0.010565319926374488\n",
      "[LOG 20200502-13:57:43] epoch: 1375 train-loss: 0.010565293642381826\n",
      "[LOG 20200502-13:57:43] epoch: 1376 train-loss: 0.010565267565349737\n",
      "[LOG 20200502-13:57:44] epoch: 1377 train-loss: 0.01056524169527822\n",
      "[LOG 20200502-13:57:44] epoch: 1378 train-loss: 0.010565216239127848\n",
      "[LOG 20200502-13:57:44] epoch: 1379 train-loss: 0.010565190886457762\n",
      "[LOG 20200502-13:57:44] epoch: 1380 train-loss: 0.010565165430307388\n",
      "[LOG 20200502-13:57:44] epoch: 1381 train-loss: 0.01056514090547959\n",
      "[LOG 20200502-13:57:45] epoch: 1382 train-loss: 0.01056511617369122\n",
      "[LOG 20200502-13:57:45] epoch: 1383 train-loss: 0.010565092683666281\n",
      "[LOG 20200502-13:57:45] epoch: 1384 train-loss: 0.010565067951877912\n",
      "[LOG 20200502-13:57:45] epoch: 1385 train-loss: 0.010565044254892401\n",
      "[LOG 20200502-13:57:45] epoch: 1386 train-loss: 0.010565020971828036\n",
      "[LOG 20200502-13:57:46] epoch: 1387 train-loss: 0.01056499768876367\n",
      "[LOG 20200502-13:57:46] epoch: 1388 train-loss: 0.010564975130061308\n",
      "[LOG 20200502-13:57:46] epoch: 1389 train-loss: 0.010564952053957515\n",
      "[LOG 20200502-13:57:46] epoch: 1390 train-loss: 0.010564930323097441\n",
      "[LOG 20200502-13:57:46] epoch: 1391 train-loss: 0.010564907660914792\n",
      "[LOG 20200502-13:57:47] epoch: 1392 train-loss: 0.01056488613701529\n",
      "[LOG 20200502-13:57:47] epoch: 1393 train-loss: 0.010564864302674929\n",
      "[LOG 20200502-13:57:47] epoch: 1394 train-loss: 0.010564843296176858\n",
      "[LOG 20200502-13:57:47] epoch: 1395 train-loss: 0.010564822082718214\n",
      "[LOG 20200502-13:57:47] epoch: 1396 train-loss: 0.010564801593621572\n",
      "[LOG 20200502-13:57:48] epoch: 1397 train-loss: 0.010564780794084072\n",
      "[LOG 20200502-13:57:48] epoch: 1398 train-loss: 0.010564760511948003\n",
      "[LOG 20200502-13:57:48] epoch: 1399 train-loss: 0.010564740643733077\n",
      "[LOG 20200502-13:57:48] epoch: 1400 train-loss: 0.01056472160336044\n",
      "[LOG 20200502-13:57:48] epoch: 1401 train-loss: 0.01056470163166523\n",
      "[LOG 20200502-13:57:49] epoch: 1402 train-loss: 0.010564682487812307\n",
      "[LOG 20200502-13:57:49] epoch: 1403 train-loss: 0.01056466344743967\n",
      "[LOG 20200502-13:57:49] epoch: 1404 train-loss: 0.010564645131429037\n",
      "[LOG 20200502-13:57:49] epoch: 1405 train-loss: 0.010564627022378974\n",
      "[LOG 20200502-13:57:49] epoch: 1406 train-loss: 0.010564609327250056\n",
      "[LOG 20200502-13:57:50] epoch: 1407 train-loss: 0.010564592046042284\n",
      "[LOG 20200502-13:57:50] epoch: 1408 train-loss: 0.010564574971795082\n",
      "[LOG 20200502-13:57:50] epoch: 1409 train-loss: 0.010564558104508452\n",
      "[LOG 20200502-13:57:50] epoch: 1410 train-loss: 0.010564541444182396\n",
      "[LOG 20200502-13:57:51] epoch: 1411 train-loss: 0.010564525094297197\n",
      "[LOG 20200502-13:57:51] epoch: 1412 train-loss: 0.010564509054852856\n",
      "[LOG 20200502-13:57:51] epoch: 1413 train-loss: 0.010564492601487372\n",
      "[LOG 20200502-13:57:51] epoch: 1414 train-loss: 0.010564476975964176\n",
      "[LOG 20200502-13:57:51] epoch: 1415 train-loss: 0.010564462074802982\n",
      "[LOG 20200502-13:57:52] epoch: 1416 train-loss: 0.010564446863200929\n",
      "[LOG 20200502-13:57:52] epoch: 1417 train-loss: 0.010564431237677733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-13:57:52] epoch: 1418 train-loss: 0.010564417578279972\n",
      "[LOG 20200502-13:57:52] epoch: 1419 train-loss: 0.01056440288407935\n",
      "[LOG 20200502-13:57:52] epoch: 1420 train-loss: 0.01056438922468159\n",
      "[LOG 20200502-13:57:53] epoch: 1421 train-loss: 0.010564374944402112\n",
      "[LOG 20200502-13:57:53] epoch: 1422 train-loss: 0.010564362112846639\n",
      "[LOG 20200502-13:57:53] epoch: 1423 train-loss: 0.010564348867370022\n",
      "[LOG 20200502-13:57:53] epoch: 1424 train-loss: 0.010564335932334265\n",
      "[LOG 20200502-13:57:53] epoch: 1425 train-loss: 0.010564322997298505\n",
      "[LOG 20200502-13:57:54] epoch: 1426 train-loss: 0.010564310683144463\n",
      "[LOG 20200502-13:57:54] epoch: 1427 train-loss: 0.010564298472470708\n",
      "[LOG 20200502-13:57:54] epoch: 1428 train-loss: 0.010564286779198382\n",
      "[LOG 20200502-13:57:54] epoch: 1429 train-loss: 0.010564275189406343\n",
      "[LOG 20200502-13:57:54] epoch: 1430 train-loss: 0.010564264013535447\n",
      "[LOG 20200502-13:57:55] epoch: 1431 train-loss: 0.010564253251585696\n",
      "[LOG 20200502-13:57:55] epoch: 1432 train-loss: 0.010564242282675372\n",
      "[LOG 20200502-13:57:55] epoch: 1433 train-loss: 0.010564232141607337\n",
      "[LOG 20200502-13:57:55] epoch: 1434 train-loss: 0.010564221586618159\n",
      "[LOG 20200502-13:57:55] epoch: 1435 train-loss: 0.010564211652510695\n",
      "[LOG 20200502-13:57:56] epoch: 1436 train-loss: 0.010564202132324377\n",
      "[LOG 20200502-13:57:56] epoch: 1437 train-loss: 0.010564192094736628\n",
      "[LOG 20200502-13:57:56] epoch: 1438 train-loss: 0.0105641834023926\n",
      "[LOG 20200502-13:57:56] epoch: 1439 train-loss: 0.010564174089166854\n",
      "[LOG 20200502-13:57:56] epoch: 1440 train-loss: 0.010564165707263682\n",
      "[LOG 20200502-13:57:57] epoch: 1441 train-loss: 0.010564157221880224\n",
      "[LOG 20200502-13:57:57] epoch: 1442 train-loss: 0.010564148943457339\n",
      "[LOG 20200502-13:57:57] epoch: 1443 train-loss: 0.010564141182435883\n",
      "[LOG 20200502-13:57:57] epoch: 1444 train-loss: 0.010564133317934142\n",
      "[LOG 20200502-13:57:57] epoch: 1445 train-loss: 0.010564125867353545\n",
      "[LOG 20200502-13:57:58] epoch: 1446 train-loss: 0.010564119244615236\n",
      "[LOG 20200502-13:57:58] epoch: 1447 train-loss: 0.010564112414916357\n",
      "[LOG 20200502-13:57:58] epoch: 1448 train-loss: 0.010564105585217476\n",
      "[LOG 20200502-13:57:58] epoch: 1449 train-loss: 0.01056409968684117\n",
      "[LOG 20200502-13:57:58] epoch: 1450 train-loss: 0.010564093064102862\n",
      "[LOG 20200502-13:57:59] epoch: 1451 train-loss: 0.010564086958765984\n",
      "[LOG 20200502-13:57:59] epoch: 1452 train-loss: 0.010564082095192539\n",
      "[LOG 20200502-13:57:59] epoch: 1453 train-loss: 0.010564076921178235\n",
      "[LOG 20200502-13:57:59] epoch: 1454 train-loss: 0.01056407205760479\n",
      "[LOG 20200502-13:57:59] epoch: 1455 train-loss: 0.010564067918393347\n",
      "[LOG 20200502-13:58:00] epoch: 1456 train-loss: 0.01056406336526076\n",
      "[LOG 20200502-13:58:00] epoch: 1457 train-loss: 0.010564059329529604\n",
      "[LOG 20200502-13:58:00] epoch: 1458 train-loss: 0.01056405570771959\n",
      "[LOG 20200502-13:58:00] epoch: 1459 train-loss: 0.010564051982429292\n",
      "[LOG 20200502-13:58:00] epoch: 1460 train-loss: 0.010564048774540424\n",
      "[LOG 20200502-13:58:01] epoch: 1461 train-loss: 0.010564045670131842\n",
      "[LOG 20200502-13:58:01] epoch: 1462 train-loss: 0.010564042979644405\n",
      "[LOG 20200502-13:58:01] epoch: 1463 train-loss: 0.010564040392637253\n",
      "[LOG 20200502-13:58:01] epoch: 1464 train-loss: 0.010564038323031532\n",
      "[LOG 20200502-13:58:02] epoch: 1465 train-loss: 0.010564036460386382\n",
      "[LOG 20200502-13:58:02] epoch: 1466 train-loss: 0.010564034597741233\n",
      "[LOG 20200502-13:58:02] epoch: 1467 train-loss: 0.010564033459458087\n",
      "[LOG 20200502-13:58:02] epoch: 1468 train-loss: 0.010564031596812937\n",
      "[LOG 20200502-13:58:02] epoch: 1469 train-loss: 0.010564030768970648\n",
      "[LOG 20200502-13:58:03] epoch: 1470 train-loss: 0.010564029837648073\n",
      "[LOG 20200502-13:58:03] epoch: 1471 train-loss: 0.010564029734167788\n",
      "[LOG 20200502-13:58:03] epoch: 1472 train-loss: 0.010564029527207216\n",
      "[LOG 20200502-13:58:03] epoch: 1473 train-loss: 0.01056402911328607\n",
      "[LOG 20200502-13:58:03] epoch: 1474 train-loss: 0.010564029527207216\n",
      "[LOG 20200502-13:58:04] epoch: 1475 train-loss: 0.01056402994112836\n",
      "[LOG 20200502-13:58:04] epoch: 1476 train-loss: 0.01056403045852979\n",
      "[LOG 20200502-13:58:04] epoch: 1477 train-loss: 0.01056403180377351\n",
      "[LOG 20200502-13:58:04] epoch: 1478 train-loss: 0.010564033252497515\n",
      "[LOG 20200502-13:58:04] epoch: 1479 train-loss: 0.010564034494260946\n",
      "[LOG 20200502-13:58:05] epoch: 1480 train-loss: 0.010564036563866668\n",
      "[LOG 20200502-13:58:05] epoch: 1481 train-loss: 0.010564038323031532\n",
      "[LOG 20200502-13:58:05] epoch: 1482 train-loss: 0.010564040082196394\n",
      "[LOG 20200502-13:58:05] epoch: 1483 train-loss: 0.010564042358762689\n",
      "[LOG 20200502-13:58:05] epoch: 1484 train-loss: 0.01056404442836841\n",
      "[LOG 20200502-13:58:06] epoch: 1485 train-loss: 0.010564047946698137\n",
      "[LOG 20200502-13:58:06] epoch: 1486 train-loss: 0.010564051671988435\n",
      "[LOG 20200502-13:58:06] epoch: 1487 train-loss: 0.01056405467291673\n",
      "[LOG 20200502-13:58:06] epoch: 1488 train-loss: 0.010564058191246457\n",
      "[LOG 20200502-13:58:06] epoch: 1489 train-loss: 0.010564062226977613\n",
      "[LOG 20200502-13:58:07] epoch: 1490 train-loss: 0.010564065848787626\n",
      "[LOG 20200502-13:58:07] epoch: 1491 train-loss: 0.0105640705054005\n",
      "[LOG 20200502-13:58:07] epoch: 1492 train-loss: 0.010564075368973944\n",
      "[LOG 20200502-13:58:07] epoch: 1493 train-loss: 0.010564080129067102\n",
      "[LOG 20200502-13:58:07] epoch: 1494 train-loss: 0.010564084889160262\n",
      "[LOG 20200502-13:58:08] epoch: 1495 train-loss: 0.01056408964925342\n",
      "[LOG 20200502-13:58:08] epoch: 1496 train-loss: 0.010564095651110014\n",
      "[LOG 20200502-13:58:08] epoch: 1497 train-loss: 0.010564101342525747\n",
      "[LOG 20200502-13:58:08] epoch: 1498 train-loss: 0.010564107654823197\n",
      "[LOG 20200502-13:58:08] epoch: 1499 train-loss: 0.01056411386364036\n",
      "[LOG 20200502-13:58:09] epoch: 1500 train-loss: 0.010564119865496954\n",
      "[LOG 20200502-13:58:09] epoch: 1501 train-loss: 0.010564126695195833\n",
      "[LOG 20200502-13:58:09] epoch: 1502 train-loss: 0.010564133628375001\n",
      "[LOG 20200502-13:58:09] epoch: 1503 train-loss: 0.010564140871995024\n",
      "[LOG 20200502-13:58:09] epoch: 1504 train-loss: 0.010564148322575621\n",
      "[LOG 20200502-13:58:10] epoch: 1505 train-loss: 0.010564156497518221\n",
      "[LOG 20200502-13:58:10] epoch: 1506 train-loss: 0.01056416415505939\n",
      "[LOG 20200502-13:58:10] epoch: 1507 train-loss: 0.01056417233000199\n",
      "[LOG 20200502-13:58:10] epoch: 1508 train-loss: 0.010564180401464304\n",
      "[LOG 20200502-13:58:11] epoch: 1509 train-loss: 0.010564188783367475\n",
      "[LOG 20200502-13:58:11] epoch: 1510 train-loss: 0.010564196751349501\n",
      "[LOG 20200502-13:58:11] epoch: 1511 train-loss: 0.010564205961094962\n",
      "[LOG 20200502-13:58:11] epoch: 1512 train-loss: 0.010564215377800994\n",
      "[LOG 20200502-13:58:11] epoch: 1513 train-loss: 0.010564225001467599\n",
      "[LOG 20200502-13:58:12] epoch: 1514 train-loss: 0.010564234004252486\n",
      "[LOG 20200502-13:58:12] epoch: 1515 train-loss: 0.010564243731399378\n",
      "[LOG 20200502-13:58:12] epoch: 1516 train-loss: 0.010564254286388556\n",
      "[LOG 20200502-13:58:12] epoch: 1517 train-loss: 0.010564264323976304\n",
      "[LOG 20200502-13:58:12] epoch: 1518 train-loss: 0.010564274258083768\n",
      "[LOG 20200502-13:58:13] epoch: 1519 train-loss: 0.010564285537434949\n",
      "[LOG 20200502-13:58:13] epoch: 1520 train-loss: 0.010564295988943841\n",
      "[LOG 20200502-13:58:13] epoch: 1521 train-loss: 0.010564307061334452\n",
      "[LOG 20200502-13:58:13] epoch: 1522 train-loss: 0.010564318030244775\n",
      "[LOG 20200502-13:58:13] epoch: 1523 train-loss: 0.010564329413076242\n",
      "[LOG 20200502-13:58:14] epoch: 1524 train-loss: 0.010564341002868282\n",
      "[LOG 20200502-13:58:14] epoch: 1525 train-loss: 0.010564352799620893\n",
      "[LOG 20200502-13:58:14] epoch: 1526 train-loss: 0.01056436501029465\n",
      "[LOG 20200502-13:58:14] epoch: 1527 train-loss: 0.01056437711748812\n",
      "[LOG 20200502-13:58:14] epoch: 1528 train-loss: 0.010564389431642162\n",
      "[LOG 20200502-13:58:15] epoch: 1529 train-loss: 0.01056440133187506\n",
      "[LOG 20200502-13:58:15] epoch: 1530 train-loss: 0.010564414266910818\n",
      "[LOG 20200502-13:58:15] epoch: 1531 train-loss: 0.010564427512387434\n",
      "[LOG 20200502-13:58:15] epoch: 1532 train-loss: 0.010564440343942907\n",
      "[LOG 20200502-13:58:16] epoch: 1533 train-loss: 0.010564453485939238\n",
      "[LOG 20200502-13:58:16] epoch: 1534 train-loss: 0.010564467041856714\n",
      "[LOG 20200502-13:58:16] epoch: 1535 train-loss: 0.010564480701254474\n",
      "[LOG 20200502-13:58:16] epoch: 1536 train-loss: 0.010564494774573378\n",
      "[LOG 20200502-13:58:16] epoch: 1537 train-loss: 0.010564509158333143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-13:58:17] epoch: 1538 train-loss: 0.010564522817730904\n",
      "[LOG 20200502-13:58:17] epoch: 1539 train-loss: 0.010564537201490667\n",
      "[LOG 20200502-13:58:17] epoch: 1540 train-loss: 0.010564551688730717\n",
      "[LOG 20200502-13:58:17] epoch: 1541 train-loss: 0.010564566175970767\n",
      "[LOG 20200502-13:58:17] epoch: 1542 train-loss: 0.010564581698013676\n",
      "[LOG 20200502-13:58:18] epoch: 1543 train-loss: 0.010564597116576301\n",
      "[LOG 20200502-13:58:18] epoch: 1544 train-loss: 0.010564612328178354\n",
      "[LOG 20200502-13:58:18] epoch: 1545 train-loss: 0.010564628264142407\n",
      "[LOG 20200502-13:58:18] epoch: 1546 train-loss: 0.010564644096626176\n",
      "[LOG 20200502-13:58:18] epoch: 1547 train-loss: 0.010564660136070516\n",
      "[LOG 20200502-13:58:19] epoch: 1548 train-loss: 0.010564676175514856\n",
      "[LOG 20200502-13:58:19] epoch: 1549 train-loss: 0.010564692214959197\n",
      "[LOG 20200502-13:58:19] epoch: 1550 train-loss: 0.010564708357883824\n",
      "[LOG 20200502-13:58:19] epoch: 1551 train-loss: 0.010564725018209882\n",
      "[LOG 20200502-13:58:19] epoch: 1552 train-loss: 0.010564742092457082\n",
      "[LOG 20200502-13:58:20] epoch: 1553 train-loss: 0.010564758545822568\n",
      "[LOG 20200502-13:58:20] epoch: 1554 train-loss: 0.010564775413109196\n",
      "[LOG 20200502-13:58:20] epoch: 1555 train-loss: 0.010564793108238114\n",
      "[LOG 20200502-13:58:20] epoch: 1556 train-loss: 0.01056481059640646\n",
      "[LOG 20200502-13:58:21] epoch: 1557 train-loss: 0.010564827774133947\n",
      "[LOG 20200502-13:58:21] epoch: 1558 train-loss: 0.01056484536578258\n",
      "[LOG 20200502-13:58:21] epoch: 1559 train-loss: 0.010564863267872069\n",
      "[LOG 20200502-13:58:21] epoch: 1560 train-loss: 0.01056488137692213\n",
      "[LOG 20200502-13:58:21] epoch: 1561 train-loss: 0.010564899589452479\n",
      "[LOG 20200502-13:58:22] epoch: 1562 train-loss: 0.0105649180089434\n",
      "[LOG 20200502-13:58:22] epoch: 1563 train-loss: 0.010564936324954033\n",
      "[LOG 20200502-13:58:22] epoch: 1564 train-loss: 0.010564954847925238\n",
      "[LOG 20200502-13:58:22] epoch: 1565 train-loss: 0.010564973681337304\n",
      "[LOG 20200502-13:58:22] epoch: 1566 train-loss: 0.010564992514749369\n",
      "[LOG 20200502-13:58:23] epoch: 1567 train-loss: 0.010565011555122005\n",
      "[LOG 20200502-13:58:23] epoch: 1568 train-loss: 0.010565031319856644\n",
      "[LOG 20200502-13:58:23] epoch: 1569 train-loss: 0.010565050153268708\n",
      "[LOG 20200502-13:58:23] epoch: 1570 train-loss: 0.010565069504082203\n",
      "[LOG 20200502-13:58:23] epoch: 1571 train-loss: 0.010565089268816842\n",
      "[LOG 20200502-13:58:24] epoch: 1572 train-loss: 0.010565108723110624\n",
      "[LOG 20200502-13:58:24] epoch: 1573 train-loss: 0.010565128694805834\n",
      "[LOG 20200502-13:58:24] epoch: 1574 train-loss: 0.010565148976941904\n",
      "[LOG 20200502-13:58:24] epoch: 1575 train-loss: 0.010565168741676543\n",
      "[LOG 20200502-13:58:24] epoch: 1576 train-loss: 0.010565189023812612\n",
      "[LOG 20200502-13:58:25] epoch: 1577 train-loss: 0.010565209616389539\n",
      "[LOG 20200502-13:58:25] epoch: 1578 train-loss: 0.010565230208966468\n",
      "[LOG 20200502-13:58:25] epoch: 1579 train-loss: 0.010565250594582822\n",
      "[LOG 20200502-13:58:25] epoch: 1580 train-loss: 0.010565271394120323\n",
      "[LOG 20200502-13:58:25] epoch: 1581 train-loss: 0.010565292400618395\n",
      "[LOG 20200502-13:58:26] epoch: 1582 train-loss: 0.010565313200155893\n",
      "[LOG 20200502-13:58:26] epoch: 1583 train-loss: 0.010565334310134252\n",
      "[LOG 20200502-13:58:26] epoch: 1584 train-loss: 0.010565355834033754\n",
      "[LOG 20200502-13:58:26] epoch: 1585 train-loss: 0.010565377564893829\n",
      "[LOG 20200502-13:58:27] epoch: 1586 train-loss: 0.010565398985313045\n",
      "[LOG 20200502-13:58:27] epoch: 1587 train-loss: 0.010565420819653405\n",
      "[LOG 20200502-13:58:27] epoch: 1588 train-loss: 0.010565442447033193\n",
      "[LOG 20200502-13:58:27] epoch: 1589 train-loss: 0.010565464591814412\n",
      "[LOG 20200502-13:58:27] epoch: 1590 train-loss: 0.01056548673659563\n",
      "[LOG 20200502-13:58:28] epoch: 1591 train-loss: 0.010565508777896563\n",
      "[LOG 20200502-13:58:28] epoch: 1592 train-loss: 0.010565531440079212\n",
      "[LOG 20200502-13:58:28] epoch: 1593 train-loss: 0.010565553998781575\n",
      "[LOG 20200502-13:58:28] epoch: 1594 train-loss: 0.010565576350523366\n",
      "[LOG 20200502-13:58:28] epoch: 1595 train-loss: 0.010565598495304585\n",
      "[LOG 20200502-13:58:29] epoch: 1596 train-loss: 0.010565621674888663\n",
      "[LOG 20200502-13:58:29] epoch: 1597 train-loss: 0.010565644026630454\n",
      "[LOG 20200502-13:58:29] epoch: 1598 train-loss: 0.010565667413175106\n",
      "[LOG 20200502-13:58:29] epoch: 1599 train-loss: 0.010565690903200043\n",
      "[LOG 20200502-13:58:29] epoch: 1600 train-loss: 0.010565713772343265\n",
      "[LOG 20200502-13:58:30] epoch: 1601 train-loss: 0.010565737158887915\n",
      "[LOG 20200502-13:58:30] epoch: 1602 train-loss: 0.01056576044195228\n",
      "[LOG 20200502-13:58:30] epoch: 1603 train-loss: 0.010565784035457505\n",
      "[LOG 20200502-13:58:30] epoch: 1604 train-loss: 0.010565808042883873\n",
      "[LOG 20200502-13:58:30] epoch: 1605 train-loss: 0.010565831429428525\n",
      "[LOG 20200502-13:58:31] epoch: 1606 train-loss: 0.010565855126414034\n",
      "[LOG 20200502-13:58:31] epoch: 1607 train-loss: 0.010565879030360116\n",
      "[LOG 20200502-13:58:31] epoch: 1608 train-loss: 0.010565902520385053\n",
      "[LOG 20200502-13:58:31] epoch: 1609 train-loss: 0.010565927045212852\n",
      "[LOG 20200502-13:58:31] epoch: 1610 train-loss: 0.010565950949158933\n",
      "[LOG 20200502-13:58:32] epoch: 1611 train-loss: 0.010565975473986732\n",
      "[LOG 20200502-13:58:32] epoch: 1612 train-loss: 0.010565999377932813\n",
      "[LOG 20200502-13:58:32] epoch: 1613 train-loss: 0.01056602369580004\n",
      "[LOG 20200502-13:58:32] epoch: 1614 train-loss: 0.010566048531068696\n",
      "[LOG 20200502-13:58:32] epoch: 1615 train-loss: 0.010566073055896495\n",
      "[LOG 20200502-13:58:33] epoch: 1616 train-loss: 0.010566097684204578\n",
      "[LOG 20200502-13:58:33] epoch: 1617 train-loss: 0.010566122726433806\n",
      "[LOG 20200502-13:58:33] epoch: 1618 train-loss: 0.01056614714778132\n",
      "[LOG 20200502-13:58:33] epoch: 1619 train-loss: 0.01056617208653026\n",
      "[LOG 20200502-13:58:33] epoch: 1620 train-loss: 0.010566197025279203\n",
      "[LOG 20200502-13:58:34] epoch: 1621 train-loss: 0.010566222584909864\n",
      "[LOG 20200502-13:58:34] epoch: 1622 train-loss: 0.010566247316698233\n",
      "[LOG 20200502-13:58:34] epoch: 1623 train-loss: 0.01056627215196689\n",
      "[LOG 20200502-13:58:34] epoch: 1624 train-loss: 0.01056629822899898\n",
      "[LOG 20200502-13:58:34] epoch: 1625 train-loss: 0.010566323374708494\n",
      "[LOG 20200502-13:58:35] epoch: 1626 train-loss: 0.010566348416937722\n",
      "[LOG 20200502-13:58:35] epoch: 1627 train-loss: 0.01056637397656838\n",
      "[LOG 20200502-13:58:35] epoch: 1628 train-loss: 0.010566400364041328\n",
      "[LOG 20200502-13:58:35] epoch: 1629 train-loss: 0.010566425509750843\n",
      "[LOG 20200502-13:58:35] epoch: 1630 train-loss: 0.010566451069381502\n",
      "[LOG 20200502-13:58:36] epoch: 1631 train-loss: 0.010566477042933306\n",
      "[LOG 20200502-13:58:36] epoch: 1632 train-loss: 0.010566503016485108\n",
      "[LOG 20200502-13:58:36] epoch: 1633 train-loss: 0.010566528783076339\n",
      "[LOG 20200502-13:58:36] epoch: 1634 train-loss: 0.010566554756628143\n",
      "[LOG 20200502-13:58:36] epoch: 1635 train-loss: 0.010566580730179945\n",
      "[LOG 20200502-13:58:37] epoch: 1636 train-loss: 0.010566606910692321\n",
      "[LOG 20200502-13:58:37] epoch: 1637 train-loss: 0.010566633712086413\n",
      "[LOG 20200502-13:58:37] epoch: 1638 train-loss: 0.01056665958215793\n",
      "[LOG 20200502-13:58:37] epoch: 1639 train-loss: 0.010566686073111163\n",
      "[LOG 20200502-13:58:37] epoch: 1640 train-loss: 0.010566712253623538\n",
      "[LOG 20200502-13:58:38] epoch: 1641 train-loss: 0.010566738641096486\n",
      "[LOG 20200502-13:58:38] epoch: 1642 train-loss: 0.010566765132049719\n",
      "[LOG 20200502-13:58:38] epoch: 1643 train-loss: 0.010566791623002954\n",
      "[LOG 20200502-13:58:38] epoch: 1644 train-loss: 0.01056681883831819\n",
      "[LOG 20200502-13:58:39] epoch: 1645 train-loss: 0.010566845018830564\n",
      "[LOG 20200502-13:58:39] epoch: 1646 train-loss: 0.010566872648066945\n",
      "[LOG 20200502-13:58:39] epoch: 1647 train-loss: 0.010566898414658176\n",
      "[LOG 20200502-13:58:39] epoch: 1648 train-loss: 0.010566925733453698\n",
      "[LOG 20200502-13:58:39] epoch: 1649 train-loss: 0.010566952638328075\n",
      "[LOG 20200502-13:58:40] epoch: 1650 train-loss: 0.010566979439722167\n",
      "[LOG 20200502-13:58:40] epoch: 1651 train-loss: 0.010567006344596544\n",
      "[LOG 20200502-13:58:40] epoch: 1652 train-loss: 0.010567033249470923\n",
      "[LOG 20200502-13:58:40] epoch: 1653 train-loss: 0.010567060361305872\n",
      "[LOG 20200502-13:58:40] epoch: 1654 train-loss: 0.010567087473140823\n",
      "[LOG 20200502-13:58:41] epoch: 1655 train-loss: 0.0105671143780152\n",
      "[LOG 20200502-13:58:41] epoch: 1656 train-loss: 0.010567142007251581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-13:58:41] epoch: 1657 train-loss: 0.010567168808645673\n",
      "[LOG 20200502-13:58:41] epoch: 1658 train-loss: 0.01056719623092148\n",
      "[LOG 20200502-13:58:41] epoch: 1659 train-loss: 0.01056722386015786\n",
      "[LOG 20200502-13:58:42] epoch: 1660 train-loss: 0.010567250558071666\n",
      "[LOG 20200502-13:58:42] epoch: 1661 train-loss: 0.01056727860122919\n",
      "[LOG 20200502-13:58:42] epoch: 1662 train-loss: 0.010567306023504999\n",
      "[LOG 20200502-13:58:42] epoch: 1663 train-loss: 0.010567333342300521\n",
      "[LOG 20200502-13:58:42] epoch: 1664 train-loss: 0.010567360350655185\n",
      "[LOG 20200502-13:58:43] epoch: 1665 train-loss: 0.010567388186852137\n",
      "[LOG 20200502-13:58:43] epoch: 1666 train-loss: 0.01056741550564766\n",
      "[LOG 20200502-13:58:43] epoch: 1667 train-loss: 0.010567443341844611\n",
      "[LOG 20200502-13:58:43] epoch: 1668 train-loss: 0.010567470660640134\n",
      "[LOG 20200502-13:58:43] epoch: 1669 train-loss: 0.010567498600317372\n",
      "[LOG 20200502-13:58:44] epoch: 1670 train-loss: 0.010567526126073467\n",
      "[LOG 20200502-13:58:44] epoch: 1671 train-loss: 0.010567554065750705\n",
      "[LOG 20200502-13:58:44] epoch: 1672 train-loss: 0.010567581901947657\n",
      "[LOG 20200502-13:58:44] epoch: 1673 train-loss: 0.01056760922074318\n",
      "[LOG 20200502-13:58:44] epoch: 1674 train-loss: 0.010567636746499274\n",
      "[LOG 20200502-13:58:45] epoch: 1675 train-loss: 0.010567664686176512\n",
      "[LOG 20200502-13:58:45] epoch: 1676 train-loss: 0.010567692729334036\n",
      "[LOG 20200502-13:58:45] epoch: 1677 train-loss: 0.010567720358570417\n",
      "[LOG 20200502-13:58:45] epoch: 1678 train-loss: 0.010567748401727941\n",
      "[LOG 20200502-13:58:45] epoch: 1679 train-loss: 0.010567775927484035\n",
      "[LOG 20200502-13:58:46] epoch: 1680 train-loss: 0.01056780448804299\n",
      "[LOG 20200502-13:58:46] epoch: 1681 train-loss: 0.010567832117279371\n",
      "[LOG 20200502-13:58:46] epoch: 1682 train-loss: 0.010567859953476323\n",
      "[LOG 20200502-13:58:46] epoch: 1683 train-loss: 0.010567888307074705\n",
      "[LOG 20200502-13:58:46] epoch: 1684 train-loss: 0.010567916143271659\n",
      "[LOG 20200502-13:58:47] epoch: 1685 train-loss: 0.010567944600350328\n",
      "[LOG 20200502-13:58:47] epoch: 1686 train-loss: 0.010567972333066993\n",
      "[LOG 20200502-13:58:47] epoch: 1687 train-loss: 0.010568000376224518\n",
      "[LOG 20200502-13:58:47] epoch: 1688 train-loss: 0.010568028315901756\n",
      "[LOG 20200502-13:58:48] epoch: 1689 train-loss: 0.010568056566019854\n",
      "[LOG 20200502-13:58:48] epoch: 1690 train-loss: 0.010568084712657664\n",
      "[LOG 20200502-13:58:48] epoch: 1691 train-loss: 0.010568113066256046\n",
      "[LOG 20200502-13:58:48] epoch: 1692 train-loss: 0.010568140798972713\n",
      "[LOG 20200502-13:58:48] epoch: 1693 train-loss: 0.01056816904909081\n",
      "[LOG 20200502-13:58:49] epoch: 1694 train-loss: 0.010568197402689192\n",
      "[LOG 20200502-13:58:49] epoch: 1695 train-loss: 0.010568225135405859\n",
      "[LOG 20200502-13:58:49] epoch: 1696 train-loss: 0.010568252868122525\n",
      "[LOG 20200502-13:58:49] epoch: 1697 train-loss: 0.01056828194608291\n",
      "[LOG 20200502-13:58:49] epoch: 1698 train-loss: 0.010568309264878431\n",
      "[LOG 20200502-13:58:50] epoch: 1699 train-loss: 0.010568337825437387\n",
      "[LOG 20200502-13:58:50] epoch: 1700 train-loss: 0.010568366282516055\n",
      "[LOG 20200502-13:58:50] epoch: 1701 train-loss: 0.010568394118713008\n",
      "[LOG 20200502-13:58:50] epoch: 1702 train-loss: 0.010568422368831105\n",
      "[LOG 20200502-13:58:50] epoch: 1703 train-loss: 0.010568450722429488\n",
      "[LOG 20200502-13:58:51] epoch: 1704 train-loss: 0.010568478869067298\n",
      "[LOG 20200502-13:58:51] epoch: 1705 train-loss: 0.010568506394823393\n",
      "[LOG 20200502-13:58:51] epoch: 1706 train-loss: 0.010568535265823206\n",
      "[LOG 20200502-13:58:51] epoch: 1707 train-loss: 0.010568563205500444\n",
      "[LOG 20200502-13:58:51] epoch: 1708 train-loss: 0.010568591352138255\n",
      "[LOG 20200502-13:58:52] epoch: 1709 train-loss: 0.010568620016177496\n",
      "[LOG 20200502-13:58:52] epoch: 1710 train-loss: 0.010568647645413876\n",
      "[LOG 20200502-13:58:52] epoch: 1711 train-loss: 0.010568675688571401\n",
      "[LOG 20200502-13:58:52] epoch: 1712 train-loss: 0.010568704559571214\n",
      "[LOG 20200502-13:58:52] epoch: 1713 train-loss: 0.010568732913169596\n",
      "[LOG 20200502-13:58:53] epoch: 1714 train-loss: 0.010568761163287692\n",
      "[LOG 20200502-13:58:53] epoch: 1715 train-loss: 0.010568788999484645\n",
      "[LOG 20200502-13:58:53] epoch: 1716 train-loss: 0.010568817249602742\n",
      "[LOG 20200502-13:58:53] epoch: 1717 train-loss: 0.010568845292760266\n",
      "[LOG 20200502-13:58:53] epoch: 1718 train-loss: 0.010568873232437504\n",
      "[LOG 20200502-13:58:54] epoch: 1719 train-loss: 0.010568901482555602\n",
      "[LOG 20200502-13:58:54] epoch: 1720 train-loss: 0.010568929836153984\n",
      "[LOG 20200502-13:58:54] epoch: 1721 train-loss: 0.010568957982791794\n",
      "[LOG 20200502-13:58:54] epoch: 1722 train-loss: 0.010568986543350749\n",
      "[LOG 20200502-13:58:54] epoch: 1723 train-loss: 0.010569014483027987\n",
      "[LOG 20200502-13:58:55] epoch: 1724 train-loss: 0.010569042526185513\n",
      "[LOG 20200502-13:58:55] epoch: 1725 train-loss: 0.010569070155421892\n",
      "[LOG 20200502-13:58:55] epoch: 1726 train-loss: 0.010569098715980848\n",
      "[LOG 20200502-13:58:55] epoch: 1727 train-loss: 0.0105691265521778\n",
      "[LOG 20200502-13:58:55] epoch: 1728 train-loss: 0.010569154388374753\n",
      "[LOG 20200502-13:58:56] epoch: 1729 train-loss: 0.010569182638492849\n",
      "[LOG 20200502-13:58:56] epoch: 1730 train-loss: 0.01056921026772923\n",
      "[LOG 20200502-13:58:56] epoch: 1731 train-loss: 0.010569238621327613\n",
      "[LOG 20200502-13:58:56] epoch: 1732 train-loss: 0.010569266767965423\n",
      "[LOG 20200502-13:58:56] epoch: 1733 train-loss: 0.010569294811122946\n",
      "[LOG 20200502-13:58:57] epoch: 1734 train-loss: 0.010569322854280472\n",
      "[LOG 20200502-13:58:57] epoch: 1735 train-loss: 0.010569351000918282\n",
      "[LOG 20200502-13:58:57] epoch: 1736 train-loss: 0.010569378733634949\n",
      "[LOG 20200502-13:58:57] epoch: 1737 train-loss: 0.010569406776792474\n",
      "[LOG 20200502-13:58:57] epoch: 1738 train-loss: 0.010569434716469712\n",
      "[LOG 20200502-13:58:58] epoch: 1739 train-loss: 0.010569462345706092\n",
      "[LOG 20200502-13:58:58] epoch: 1740 train-loss: 0.010569490078422759\n",
      "[LOG 20200502-13:58:58] epoch: 1741 train-loss: 0.010569518535501428\n",
      "[LOG 20200502-13:58:58] epoch: 1742 train-loss: 0.010569546164737808\n",
      "[LOG 20200502-13:58:58] epoch: 1743 train-loss: 0.01056957400093476\n",
      "[LOG 20200502-13:58:59] epoch: 1744 train-loss: 0.010569602044092285\n",
      "[LOG 20200502-13:58:59] epoch: 1745 train-loss: 0.010569629776808951\n",
      "[LOG 20200502-13:58:59] epoch: 1746 train-loss: 0.010569657509525618\n",
      "[LOG 20200502-13:58:59] epoch: 1747 train-loss: 0.010569685138761997\n",
      "[LOG 20200502-13:58:59] epoch: 1748 train-loss: 0.010569712974958949\n",
      "[LOG 20200502-13:59:00] epoch: 1749 train-loss: 0.010569741018116474\n",
      "[LOG 20200502-13:59:00] epoch: 1750 train-loss: 0.010569768543872569\n",
      "[LOG 20200502-13:59:00] epoch: 1751 train-loss: 0.010569795759187805\n",
      "[LOG 20200502-13:59:00] epoch: 1752 train-loss: 0.010569823491904471\n",
      "[LOG 20200502-13:59:01] epoch: 1753 train-loss: 0.010569851224621138\n",
      "[LOG 20200502-13:59:01] epoch: 1754 train-loss: 0.010569878957337804\n",
      "[LOG 20200502-13:59:01] epoch: 1755 train-loss: 0.010569906586574184\n",
      "[LOG 20200502-13:59:01] epoch: 1756 train-loss: 0.01056993431929085\n",
      "[LOG 20200502-13:59:01] epoch: 1757 train-loss: 0.010569961845046945\n",
      "[LOG 20200502-13:59:02] epoch: 1758 train-loss: 0.01056998906036218\n",
      "[LOG 20200502-13:59:02] epoch: 1759 train-loss: 0.01057001668959856\n",
      "[LOG 20200502-13:59:02] epoch: 1760 train-loss: 0.010570044111874368\n",
      "[LOG 20200502-13:59:02] epoch: 1761 train-loss: 0.01057007174111075\n",
      "[LOG 20200502-13:59:02] epoch: 1762 train-loss: 0.010570098852945698\n",
      "[LOG 20200502-13:59:03] epoch: 1763 train-loss: 0.010570126171741221\n",
      "[LOG 20200502-13:59:03] epoch: 1764 train-loss: 0.010570153490536742\n",
      "[LOG 20200502-13:59:03] epoch: 1765 train-loss: 0.010570181326733695\n",
      "[LOG 20200502-13:59:03] epoch: 1766 train-loss: 0.010570208438568644\n",
      "[LOG 20200502-13:59:03] epoch: 1767 train-loss: 0.010570235550403595\n",
      "[LOG 20200502-13:59:04] epoch: 1768 train-loss: 0.010570263179639975\n",
      "[LOG 20200502-13:59:04] epoch: 1769 train-loss: 0.010570290084514353\n",
      "[LOG 20200502-13:59:04] epoch: 1770 train-loss: 0.01057031698938873\n",
      "[LOG 20200502-13:59:04] epoch: 1771 train-loss: 0.010570344411664538\n",
      "[LOG 20200502-13:59:04] epoch: 1772 train-loss: 0.010570371626979776\n",
      "[LOG 20200502-13:59:05] epoch: 1773 train-loss: 0.01057039863533444\n",
      "[LOG 20200502-13:59:05] epoch: 1774 train-loss: 0.010570425850649675\n",
      "[LOG 20200502-13:59:05] epoch: 1775 train-loss: 0.010570452962484624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-13:59:05] epoch: 1776 train-loss: 0.010570479970839288\n",
      "[LOG 20200502-13:59:05] epoch: 1777 train-loss: 0.01057050677223338\n",
      "[LOG 20200502-13:59:06] epoch: 1778 train-loss: 0.010570534608430333\n",
      "[LOG 20200502-13:59:06] epoch: 1779 train-loss: 0.010570560892422995\n",
      "[LOG 20200502-13:59:06] epoch: 1780 train-loss: 0.010570587693817086\n",
      "[LOG 20200502-13:59:06] epoch: 1781 train-loss: 0.01057061521957318\n",
      "[LOG 20200502-13:59:06] epoch: 1782 train-loss: 0.010570641400085555\n",
      "[LOG 20200502-13:59:07] epoch: 1783 train-loss: 0.010570668511920504\n",
      "[LOG 20200502-13:59:07] epoch: 1784 train-loss: 0.010570695520275168\n",
      "[LOG 20200502-13:59:07] epoch: 1785 train-loss: 0.010570722011228403\n",
      "[LOG 20200502-13:59:07] epoch: 1786 train-loss: 0.010570748812622495\n",
      "[LOG 20200502-13:59:07] epoch: 1787 train-loss: 0.0105707755105363\n",
      "[LOG 20200502-13:59:08] epoch: 1788 train-loss: 0.010570801898009248\n",
      "[LOG 20200502-13:59:08] epoch: 1789 train-loss: 0.010570828492442766\n",
      "[LOG 20200502-13:59:08] epoch: 1790 train-loss: 0.010570854569474855\n",
      "[LOG 20200502-13:59:08] epoch: 1791 train-loss: 0.010570880749987232\n",
      "[LOG 20200502-13:59:08] epoch: 1792 train-loss: 0.010570907240940465\n",
      "[LOG 20200502-13:59:09] epoch: 1793 train-loss: 0.01057093393885427\n",
      "[LOG 20200502-13:59:09] epoch: 1794 train-loss: 0.01057096053328779\n",
      "[LOG 20200502-13:59:09] epoch: 1795 train-loss: 0.010570986713800166\n",
      "[LOG 20200502-13:59:09] epoch: 1796 train-loss: 0.01057101289431254\n",
      "[LOG 20200502-13:59:09] epoch: 1797 train-loss: 0.010571039178305201\n",
      "[LOG 20200502-13:59:10] epoch: 1798 train-loss: 0.010571065462297864\n",
      "[LOG 20200502-13:59:10] epoch: 1799 train-loss: 0.010571091228889095\n",
      "[LOG 20200502-13:59:10] epoch: 1800 train-loss: 0.010571117823322615\n",
      "[LOG 20200502-13:59:10] epoch: 1801 train-loss: 0.010571144003834989\n",
      "[LOG 20200502-13:59:11] epoch: 1802 train-loss: 0.010571169977386793\n",
      "[LOG 20200502-13:59:11] epoch: 1803 train-loss: 0.01057119584745831\n",
      "[LOG 20200502-13:59:11] epoch: 1804 train-loss: 0.0105712219244904\n",
      "[LOG 20200502-13:59:11] epoch: 1805 train-loss: 0.010571248001522489\n",
      "[LOG 20200502-13:59:11] epoch: 1806 train-loss: 0.01057127428551515\n",
      "[LOG 20200502-13:59:12] epoch: 1807 train-loss: 0.010571300259066952\n",
      "[LOG 20200502-13:59:12] epoch: 1808 train-loss: 0.010571325715217326\n",
      "[LOG 20200502-13:59:12] epoch: 1809 train-loss: 0.010571351792249415\n",
      "[LOG 20200502-13:59:12] epoch: 1810 train-loss: 0.010571377869281504\n",
      "[LOG 20200502-13:59:12] epoch: 1811 train-loss: 0.010571403325431876\n",
      "[LOG 20200502-13:59:13] epoch: 1812 train-loss: 0.010571428678101964\n",
      "[LOG 20200502-13:59:13] epoch: 1813 train-loss: 0.010571454444693195\n",
      "[LOG 20200502-13:59:13] epoch: 1814 train-loss: 0.010571480418244997\n",
      "[LOG 20200502-13:59:13] epoch: 1815 train-loss: 0.010571505667434799\n",
      "[LOG 20200502-13:59:13] epoch: 1816 train-loss: 0.010571531434026029\n",
      "[LOG 20200502-13:59:14] epoch: 1817 train-loss: 0.010571556476255259\n",
      "[LOG 20200502-13:59:14] epoch: 1818 train-loss: 0.010571581828925345\n",
      "[LOG 20200502-13:59:14] epoch: 1819 train-loss: 0.010571607595516576\n",
      "[LOG 20200502-13:59:14] epoch: 1820 train-loss: 0.010571633155147234\n",
      "[LOG 20200502-13:59:14] epoch: 1821 train-loss: 0.010571658093896177\n",
      "[LOG 20200502-13:59:15] epoch: 1822 train-loss: 0.010571684170928266\n",
      "[LOG 20200502-13:59:15] epoch: 1823 train-loss: 0.01057170931663778\n",
      "[LOG 20200502-13:59:15] epoch: 1824 train-loss: 0.010571734151906438\n",
      "[LOG 20200502-13:59:15] epoch: 1825 train-loss: 0.01057176012545824\n",
      "[LOG 20200502-13:59:15] epoch: 1826 train-loss: 0.010571784753766324\n",
      "[LOG 20200502-13:59:16] epoch: 1827 train-loss: 0.010571809485554695\n",
      "[LOG 20200502-13:59:16] epoch: 1828 train-loss: 0.010571834941705069\n",
      "[LOG 20200502-13:59:16] epoch: 1829 train-loss: 0.010571859570013152\n",
      "[LOG 20200502-13:59:16] epoch: 1830 train-loss: 0.010571884715722667\n",
      "[LOG 20200502-13:59:16] epoch: 1831 train-loss: 0.010571909550991323\n",
      "[LOG 20200502-13:59:17] epoch: 1832 train-loss: 0.010571934593220552\n",
      "[LOG 20200502-13:59:17] epoch: 1833 train-loss: 0.010571959428489208\n",
      "[LOG 20200502-13:59:17] epoch: 1834 train-loss: 0.010571984263757864\n",
      "[LOG 20200502-13:59:17] epoch: 1835 train-loss: 0.010572009099026522\n",
      "[LOG 20200502-13:59:17] epoch: 1836 train-loss: 0.010572033623854319\n",
      "[LOG 20200502-13:59:18] epoch: 1837 train-loss: 0.010572058459122976\n",
      "[LOG 20200502-13:59:18] epoch: 1838 train-loss: 0.01057208308743106\n",
      "[LOG 20200502-13:59:18] epoch: 1839 train-loss: 0.01057210781921943\n",
      "[LOG 20200502-13:59:18] epoch: 1840 train-loss: 0.010572132344047228\n",
      "[LOG 20200502-13:59:18] epoch: 1841 train-loss: 0.010572157075835599\n",
      "[LOG 20200502-13:59:19] epoch: 1842 train-loss: 0.01057218097978168\n",
      "[LOG 20200502-13:59:19] epoch: 1843 train-loss: 0.010572206125491194\n",
      "[LOG 20200502-13:59:19] epoch: 1844 train-loss: 0.010572230546838708\n",
      "[LOG 20200502-13:59:19] epoch: 1845 train-loss: 0.010572254761225648\n",
      "[LOG 20200502-13:59:19] epoch: 1846 train-loss: 0.010572278768652014\n",
      "[LOG 20200502-13:59:20] epoch: 1847 train-loss: 0.010572303914361529\n",
      "[LOG 20200502-13:59:20] epoch: 1848 train-loss: 0.010572327404386468\n",
      "[LOG 20200502-13:59:20] epoch: 1849 train-loss: 0.01057235130833255\n",
      "[LOG 20200502-13:59:20] epoch: 1850 train-loss: 0.010572375936640633\n",
      "[LOG 20200502-13:59:20] epoch: 1851 train-loss: 0.010572399944067001\n",
      "[LOG 20200502-13:59:21] epoch: 1852 train-loss: 0.010572423744532797\n",
      "[LOG 20200502-13:59:21] epoch: 1853 train-loss: 0.010572447751959166\n",
      "[LOG 20200502-13:59:21] epoch: 1854 train-loss: 0.010572471655905247\n",
      "[LOG 20200502-13:59:21] epoch: 1855 train-loss: 0.010572495352890756\n",
      "[LOG 20200502-13:59:21] epoch: 1856 train-loss: 0.010572519153356552\n",
      "[LOG 20200502-13:59:22] epoch: 1857 train-loss: 0.010572543264263205\n",
      "[LOG 20200502-13:59:22] epoch: 1858 train-loss: 0.010572567064729001\n",
      "[LOG 20200502-13:59:22] epoch: 1859 train-loss: 0.010572590968675084\n",
      "[LOG 20200502-13:59:22] epoch: 1860 train-loss: 0.010572614355219735\n",
      "[LOG 20200502-13:59:22] epoch: 1861 train-loss: 0.010572638362646103\n",
      "[LOG 20200502-13:59:23] epoch: 1862 train-loss: 0.010572661438749896\n",
      "[LOG 20200502-13:59:23] epoch: 1863 train-loss: 0.010572685342695978\n",
      "[LOG 20200502-13:59:23] epoch: 1864 train-loss: 0.010572708832720915\n",
      "[LOG 20200502-13:59:23] epoch: 1865 train-loss: 0.010572732633186711\n",
      "[LOG 20200502-13:59:24] epoch: 1866 train-loss: 0.010572755916251076\n",
      "[LOG 20200502-13:59:24] epoch: 1867 train-loss: 0.010572779302795729\n",
      "[LOG 20200502-13:59:24] epoch: 1868 train-loss: 0.010572802171938948\n",
      "[LOG 20200502-13:59:24] epoch: 1869 train-loss: 0.01057282586892446\n",
      "[LOG 20200502-13:59:24] epoch: 1870 train-loss: 0.01057284925546911\n",
      "[LOG 20200502-13:59:25] epoch: 1871 train-loss: 0.010572871814171473\n",
      "[LOG 20200502-13:59:25] epoch: 1872 train-loss: 0.010572894476354122\n",
      "[LOG 20200502-13:59:25] epoch: 1873 train-loss: 0.0105729176559382\n",
      "[LOG 20200502-13:59:25] epoch: 1874 train-loss: 0.01057294114596314\n",
      "[LOG 20200502-13:59:26] epoch: 1875 train-loss: 0.01057296349770493\n",
      "[LOG 20200502-13:59:26] epoch: 1876 train-loss: 0.010572986573808722\n",
      "[LOG 20200502-13:59:26] epoch: 1877 train-loss: 0.0105730090290308\n",
      "[LOG 20200502-13:59:26] epoch: 1878 train-loss: 0.010573032208614878\n",
      "[LOG 20200502-13:59:26] epoch: 1879 train-loss: 0.010573054767317243\n",
      "[LOG 20200502-13:59:27] epoch: 1880 train-loss: 0.010573077222539319\n",
      "[LOG 20200502-13:59:27] epoch: 1881 train-loss: 0.010573099988202253\n",
      "[LOG 20200502-13:59:27] epoch: 1882 train-loss: 0.010573122236463759\n",
      "[LOG 20200502-13:59:27] epoch: 1883 train-loss: 0.010573145209087266\n",
      "[LOG 20200502-13:59:28] epoch: 1884 train-loss: 0.010573167457348771\n",
      "[LOG 20200502-13:59:28] epoch: 1885 train-loss: 0.010573190016051134\n",
      "[LOG 20200502-13:59:28] epoch: 1886 train-loss: 0.01057321195387178\n",
      "[LOG 20200502-13:59:28] epoch: 1887 train-loss: 0.010573234409093857\n",
      "[LOG 20200502-13:59:28] epoch: 1888 train-loss: 0.01057325696779622\n",
      "[LOG 20200502-13:59:29] epoch: 1889 train-loss: 0.010573278905616866\n",
      "[LOG 20200502-13:59:29] epoch: 1890 train-loss: 0.010573301360838942\n",
      "[LOG 20200502-13:59:29] epoch: 1891 train-loss: 0.01057332329865959\n",
      "[LOG 20200502-13:59:29] epoch: 1892 train-loss: 0.010573345339960523\n",
      "[LOG 20200502-13:59:29] epoch: 1893 train-loss: 0.01057336748474174\n",
      "[LOG 20200502-13:59:30] epoch: 1894 train-loss: 0.010573389319082102\n",
      "[LOG 20200502-13:59:30] epoch: 1895 train-loss: 0.010573411567343606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-13:59:30] epoch: 1896 train-loss: 0.010573433194723394\n",
      "[LOG 20200502-13:59:30] epoch: 1897 train-loss: 0.010573455236024328\n",
      "[LOG 20200502-13:59:30] epoch: 1898 train-loss: 0.010573476759923829\n",
      "[LOG 20200502-13:59:31] epoch: 1899 train-loss: 0.010573498387303617\n",
      "[LOG 20200502-13:59:31] epoch: 1900 train-loss: 0.010573520014683405\n",
      "[LOG 20200502-13:59:31] epoch: 1901 train-loss: 0.01057354174554348\n",
      "[LOG 20200502-13:59:31] epoch: 1902 train-loss: 0.01057356306248241\n",
      "[LOG 20200502-13:59:31] epoch: 1903 train-loss: 0.010573584586381912\n",
      "[LOG 20200502-13:59:32] epoch: 1904 train-loss: 0.010573606420722272\n",
      "[LOG 20200502-13:59:32] epoch: 1905 train-loss: 0.010573627944621775\n",
      "[LOG 20200502-13:59:32] epoch: 1906 train-loss: 0.010573649054600133\n",
      "[LOG 20200502-13:59:32] epoch: 1907 train-loss: 0.010573670268058777\n",
      "[LOG 20200502-13:59:33] epoch: 1908 train-loss: 0.010573691584997706\n",
      "[LOG 20200502-13:59:33] epoch: 1909 train-loss: 0.010573712488015493\n",
      "[LOG 20200502-13:59:33] epoch: 1910 train-loss: 0.010573734011914995\n",
      "[LOG 20200502-13:59:33] epoch: 1911 train-loss: 0.010573754811452495\n",
      "[LOG 20200502-13:59:33] epoch: 1912 train-loss: 0.010573776231871711\n",
      "[LOG 20200502-13:59:34] epoch: 1913 train-loss: 0.010573796927928925\n",
      "[LOG 20200502-13:59:34] epoch: 1914 train-loss: 0.010573818555308713\n",
      "[LOG 20200502-13:59:34] epoch: 1915 train-loss: 0.010573838940925069\n",
      "[LOG 20200502-13:59:34] epoch: 1916 train-loss: 0.01057385994742314\n",
      "[LOG 20200502-13:59:34] epoch: 1917 train-loss: 0.01057388074696064\n",
      "[LOG 20200502-13:59:35] epoch: 1918 train-loss: 0.01057390102909671\n",
      "[LOG 20200502-13:59:35] epoch: 1919 train-loss: 0.010573921725153923\n",
      "[LOG 20200502-13:59:35] epoch: 1920 train-loss: 0.010573942524691423\n",
      "[LOG 20200502-13:59:35] epoch: 1921 train-loss: 0.010573963324228922\n",
      "[LOG 20200502-13:59:35] epoch: 1922 train-loss: 0.010573983606364992\n",
      "[LOG 20200502-13:59:36] epoch: 1923 train-loss: 0.010574003785020776\n",
      "[LOG 20200502-13:59:36] epoch: 1924 train-loss: 0.01057402448107799\n",
      "[LOG 20200502-13:59:36] epoch: 1925 train-loss: 0.010574044659733772\n",
      "[LOG 20200502-13:59:36] epoch: 1926 train-loss: 0.010574065148830414\n",
      "[LOG 20200502-13:59:37] epoch: 1927 train-loss: 0.01057408501704534\n",
      "[LOG 20200502-13:59:37] epoch: 1928 train-loss: 0.010574105402661694\n",
      "[LOG 20200502-13:59:37] epoch: 1929 train-loss: 0.010574125167396333\n",
      "[LOG 20200502-13:59:37] epoch: 1930 train-loss: 0.010574145759973261\n",
      "[LOG 20200502-13:59:37] epoch: 1931 train-loss: 0.010574165421227614\n",
      "[LOG 20200502-13:59:38] epoch: 1932 train-loss: 0.010574185392922826\n",
      "[LOG 20200502-13:59:38] epoch: 1933 train-loss: 0.010574205364618037\n",
      "[LOG 20200502-13:59:38] epoch: 1934 train-loss: 0.010574224922392104\n",
      "[LOG 20200502-13:59:38] epoch: 1935 train-loss: 0.010574245101047887\n",
      "[LOG 20200502-13:59:38] epoch: 1936 train-loss: 0.010574264348381095\n",
      "[LOG 20200502-13:59:39] epoch: 1937 train-loss: 0.010574284423556592\n",
      "[LOG 20200502-13:59:39] epoch: 1938 train-loss: 0.010574303774370087\n",
      "[LOG 20200502-13:59:39] epoch: 1939 train-loss: 0.010574323125183582\n",
      "[LOG 20200502-13:59:39] epoch: 1940 train-loss: 0.010574342889918221\n",
      "[LOG 20200502-13:59:39] epoch: 1941 train-loss: 0.010574362551172575\n",
      "[LOG 20200502-13:59:40] epoch: 1942 train-loss: 0.010574382108946642\n",
      "[LOG 20200502-13:59:40] epoch: 1943 train-loss: 0.010574401563240422\n",
      "[LOG 20200502-13:59:40] epoch: 1944 train-loss: 0.010574420293172201\n",
      "[LOG 20200502-13:59:40] epoch: 1945 train-loss: 0.010574439954426553\n",
      "[LOG 20200502-13:59:40] epoch: 1946 train-loss: 0.01057445899479919\n",
      "[LOG 20200502-13:59:41] epoch: 1947 train-loss: 0.010574478345612684\n",
      "[LOG 20200502-13:59:41] epoch: 1948 train-loss: 0.01057449769642618\n",
      "[LOG 20200502-13:59:41] epoch: 1949 train-loss: 0.010574516529838244\n",
      "[LOG 20200502-13:59:41] epoch: 1950 train-loss: 0.010574535570210881\n",
      "[LOG 20200502-13:59:41] epoch: 1951 train-loss: 0.010574554093182087\n",
      "[LOG 20200502-13:59:42] epoch: 1952 train-loss: 0.010574573547475867\n",
      "[LOG 20200502-13:59:42] epoch: 1953 train-loss: 0.010574591449565358\n",
      "[LOG 20200502-13:59:42] epoch: 1954 train-loss: 0.010574611007339425\n",
      "[LOG 20200502-13:59:42] epoch: 1955 train-loss: 0.01057462984075149\n",
      "[LOG 20200502-13:59:42] epoch: 1956 train-loss: 0.010574648156762123\n",
      "[LOG 20200502-13:59:43] epoch: 1957 train-loss: 0.010574666783213615\n",
      "[LOG 20200502-13:59:43] epoch: 1958 train-loss: 0.010574685409665108\n",
      "[LOG 20200502-13:59:43] epoch: 1959 train-loss: 0.010574703725675741\n",
      "[LOG 20200502-13:59:43] epoch: 1960 train-loss: 0.010574722145166662\n",
      "[LOG 20200502-13:59:44] epoch: 1961 train-loss: 0.010574740668137869\n",
      "[LOG 20200502-13:59:44] epoch: 1962 train-loss: 0.010574758984148502\n",
      "[LOG 20200502-13:59:44] epoch: 1963 train-loss: 0.010574777507119708\n",
      "[LOG 20200502-13:59:44] epoch: 1964 train-loss: 0.0105747954092092\n",
      "[LOG 20200502-13:59:44] epoch: 1965 train-loss: 0.010574812897377543\n",
      "[LOG 20200502-13:59:45] epoch: 1966 train-loss: 0.010574831627309322\n",
      "[LOG 20200502-13:59:45] epoch: 1967 train-loss: 0.010574850150280528\n",
      "[LOG 20200502-13:59:45] epoch: 1968 train-loss: 0.010574867845409446\n",
      "[LOG 20200502-13:59:45] epoch: 1969 train-loss: 0.010574885747498937\n",
      "[LOG 20200502-13:59:45] epoch: 1970 train-loss: 0.010574903753068712\n",
      "[LOG 20200502-13:59:46] epoch: 1971 train-loss: 0.010574921758638488\n",
      "[LOG 20200502-13:59:46] epoch: 1972 train-loss: 0.010574939660727978\n",
      "[LOG 20200502-13:59:46] epoch: 1973 train-loss: 0.010574957252376609\n",
      "[LOG 20200502-13:59:46] epoch: 1974 train-loss: 0.010574974844025241\n",
      "[LOG 20200502-13:59:46] epoch: 1975 train-loss: 0.010574992332193587\n",
      "[LOG 20200502-13:59:47] epoch: 1976 train-loss: 0.010575009923842218\n",
      "[LOG 20200502-13:59:47] epoch: 1977 train-loss: 0.010575027825931707\n",
      "[LOG 20200502-13:59:47] epoch: 1978 train-loss: 0.010575045107139481\n",
      "[LOG 20200502-13:59:47] epoch: 1979 train-loss: 0.010575062595307827\n",
      "[LOG 20200502-13:59:47] epoch: 1980 train-loss: 0.010575079462594457\n",
      "[LOG 20200502-13:59:48] epoch: 1981 train-loss: 0.010575097364683947\n",
      "[LOG 20200502-13:59:48] epoch: 1982 train-loss: 0.010575114335450862\n",
      "[LOG 20200502-13:59:48] epoch: 1983 train-loss: 0.010575131927099492\n",
      "[LOG 20200502-13:59:48] epoch: 1984 train-loss: 0.010575148897866407\n",
      "[LOG 20200502-13:59:48] epoch: 1985 train-loss: 0.01057516597211361\n",
      "[LOG 20200502-13:59:49] epoch: 1986 train-loss: 0.010575182632439666\n",
      "[LOG 20200502-13:59:49] epoch: 1987 train-loss: 0.010575200327568583\n",
      "[LOG 20200502-13:59:49] epoch: 1988 train-loss: 0.010575217091374926\n",
      "[LOG 20200502-13:59:49] epoch: 1989 train-loss: 0.01057523385518127\n",
      "[LOG 20200502-13:59:50] epoch: 1990 train-loss: 0.010575250929428471\n",
      "[LOG 20200502-13:59:50] epoch: 1991 train-loss: 0.01057526758975453\n",
      "[LOG 20200502-13:59:50] epoch: 1992 train-loss: 0.010575284250080585\n",
      "[LOG 20200502-13:59:50] epoch: 1993 train-loss: 0.01057530070344607\n",
      "[LOG 20200502-13:59:50] epoch: 1994 train-loss: 0.010575317467252413\n",
      "[LOG 20200502-13:59:51] epoch: 1995 train-loss: 0.010575333817137612\n",
      "[LOG 20200502-13:59:51] epoch: 1996 train-loss: 0.010575350580943955\n",
      "[LOG 20200502-13:59:51] epoch: 1997 train-loss: 0.010575367137789726\n",
      "[LOG 20200502-13:59:51] epoch: 1998 train-loss: 0.010575382970273495\n",
      "[LOG 20200502-13:59:51] epoch: 1999 train-loss: 0.010575399734079838\n",
      "[LOG 20200502-13:59:52] epoch: 2000 train-loss: 0.010575415877004465\n",
      "[LOG 20200502-13:59:52] epoch: 2001 train-loss: 0.01057543233036995\n",
      "[LOG 20200502-13:59:52] epoch: 2002 train-loss: 0.010575447645452287\n",
      "[LOG 20200502-13:59:52] epoch: 2003 train-loss: 0.010575464202298058\n",
      "[LOG 20200502-13:59:53] epoch: 2004 train-loss: 0.010575480552183257\n",
      "[LOG 20200502-13:59:53] epoch: 2005 train-loss: 0.010575496177706454\n",
      "[LOG 20200502-13:59:53] epoch: 2006 train-loss: 0.010575512424111366\n",
      "[LOG 20200502-13:59:53] epoch: 2007 train-loss: 0.010575528153114848\n",
      "[LOG 20200502-13:59:53] epoch: 2008 train-loss: 0.01057554367515776\n",
      "[LOG 20200502-13:59:54] epoch: 2009 train-loss: 0.010575559921562672\n",
      "[LOG 20200502-13:59:54] epoch: 2010 train-loss: 0.01057557575404644\n",
      "[LOG 20200502-13:59:54] epoch: 2011 train-loss: 0.010575591069128778\n",
      "[LOG 20200502-13:59:54] epoch: 2012 train-loss: 0.010575606487691402\n",
      "[LOG 20200502-13:59:54] epoch: 2013 train-loss: 0.01057562180277374\n",
      "[LOG 20200502-13:59:55] epoch: 2014 train-loss: 0.010575637324816652\n",
      "[LOG 20200502-13:59:55] epoch: 2015 train-loss: 0.01057565315730042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-13:59:55] epoch: 2016 train-loss: 0.010575667644540468\n",
      "[LOG 20200502-13:59:55] epoch: 2017 train-loss: 0.010575683477024237\n",
      "[LOG 20200502-13:59:55] epoch: 2018 train-loss: 0.010575698274705145\n",
      "[LOG 20200502-13:59:56] epoch: 2019 train-loss: 0.010575713796748055\n",
      "[LOG 20200502-13:59:56] epoch: 2020 train-loss: 0.010575728904869821\n",
      "[LOG 20200502-13:59:56] epoch: 2021 train-loss: 0.010575744012991587\n",
      "[LOG 20200502-13:59:56] epoch: 2022 train-loss: 0.010575759431554211\n",
      "[LOG 20200502-13:59:57] epoch: 2023 train-loss: 0.010575773918794261\n",
      "[LOG 20200502-13:59:57] epoch: 2024 train-loss: 0.010575789233876599\n",
      "[LOG 20200502-13:59:57] epoch: 2025 train-loss: 0.010575803824596934\n",
      "[LOG 20200502-13:59:57] epoch: 2026 train-loss: 0.010575818932718702\n",
      "[LOG 20200502-13:59:57] epoch: 2027 train-loss: 0.01057583393736018\n",
      "[LOG 20200502-13:59:58] epoch: 2028 train-loss: 0.01057584842460023\n",
      "[LOG 20200502-13:59:58] epoch: 2029 train-loss: 0.010575863118800852\n",
      "[LOG 20200502-13:59:58] epoch: 2030 train-loss: 0.010575877399080329\n",
      "[LOG 20200502-13:59:58] epoch: 2031 train-loss: 0.010575892300241523\n",
      "[LOG 20200502-13:59:58] epoch: 2032 train-loss: 0.010575906787481573\n",
      "[LOG 20200502-13:59:59] epoch: 2033 train-loss: 0.010575921171241336\n",
      "[LOG 20200502-13:59:59] epoch: 2034 train-loss: 0.010575935865441958\n",
      "[LOG 20200502-13:59:59] epoch: 2035 train-loss: 0.010575950249201722\n",
      "[LOG 20200502-13:59:59] epoch: 2036 train-loss: 0.010575964322520627\n",
      "[LOG 20200502-13:59:59] epoch: 2037 train-loss: 0.010575978499319818\n",
      "[LOG 20200502-14:00:00] epoch: 2038 train-loss: 0.010575992572638724\n",
      "[LOG 20200502-14:00:00] epoch: 2039 train-loss: 0.0105760068529182\n",
      "[LOG 20200502-14:00:00] epoch: 2040 train-loss: 0.010576021236677965\n",
      "[LOG 20200502-14:00:00] epoch: 2041 train-loss: 0.010576035206516584\n",
      "[LOG 20200502-14:00:00] epoch: 2042 train-loss: 0.010576049176355204\n",
      "[LOG 20200502-14:00:01] epoch: 2043 train-loss: 0.010576062835752964\n",
      "[LOG 20200502-14:00:01] epoch: 2044 train-loss: 0.010576076805591583\n",
      "[LOG 20200502-14:00:01] epoch: 2045 train-loss: 0.010576090982390774\n",
      "[LOG 20200502-14:00:01] epoch: 2046 train-loss: 0.01057610453830825\n",
      "[LOG 20200502-14:00:02] epoch: 2047 train-loss: 0.010576118094225725\n",
      "[LOG 20200502-14:00:02] epoch: 2048 train-loss: 0.010576132064064344\n",
      "[LOG 20200502-14:00:02] epoch: 2049 train-loss: 0.010576145516501533\n",
      "[LOG 20200502-14:00:02] epoch: 2050 train-loss: 0.010576159175899293\n",
      "[LOG 20200502-14:00:02] epoch: 2051 train-loss: 0.01057617242137591\n",
      "[LOG 20200502-14:00:03] epoch: 2052 train-loss: 0.010576186184253957\n",
      "[LOG 20200502-14:00:03] epoch: 2053 train-loss: 0.010576199222770002\n",
      "[LOG 20200502-14:00:03] epoch: 2054 train-loss: 0.010576212778687477\n",
      "[LOG 20200502-14:00:03] epoch: 2055 train-loss: 0.010576225920683809\n",
      "[LOG 20200502-14:00:03] epoch: 2056 train-loss: 0.01057623926964071\n",
      "[LOG 20200502-14:00:04] epoch: 2057 train-loss: 0.010576252515117327\n",
      "[LOG 20200502-14:00:04] epoch: 2058 train-loss: 0.010576265657113658\n",
      "[LOG 20200502-14:00:04] epoch: 2059 train-loss: 0.010576279006070562\n",
      "[LOG 20200502-14:00:04] epoch: 2060 train-loss: 0.010576291734145747\n",
      "[LOG 20200502-14:00:05] epoch: 2061 train-loss: 0.01057630456570122\n",
      "[LOG 20200502-14:00:05] epoch: 2062 train-loss: 0.010576318018138409\n",
      "[LOG 20200502-14:00:05] epoch: 2063 train-loss: 0.010576330539253023\n",
      "[LOG 20200502-14:00:05] epoch: 2064 train-loss: 0.010576343474288782\n",
      "[LOG 20200502-14:00:05] epoch: 2065 train-loss: 0.010576355995403396\n",
      "[LOG 20200502-14:00:06] epoch: 2066 train-loss: 0.010576368930439154\n",
      "[LOG 20200502-14:00:06] epoch: 2067 train-loss: 0.01057638145155377\n",
      "[LOG 20200502-14:00:06] epoch: 2068 train-loss: 0.010576394386589527\n",
      "[LOG 20200502-14:00:06] epoch: 2069 train-loss: 0.010576406700743569\n",
      "[LOG 20200502-14:00:06] epoch: 2070 train-loss: 0.010576419221858183\n",
      "[LOG 20200502-14:00:07] epoch: 2071 train-loss: 0.01057643143253194\n",
      "[LOG 20200502-14:00:07] epoch: 2072 train-loss: 0.010576443746685982\n",
      "[LOG 20200502-14:00:07] epoch: 2073 train-loss: 0.010576455543438593\n",
      "[LOG 20200502-14:00:07] epoch: 2074 train-loss: 0.01057646827151378\n",
      "[LOG 20200502-14:00:07] epoch: 2075 train-loss: 0.010576480585667822\n",
      "[LOG 20200502-14:00:08] epoch: 2076 train-loss: 0.010576492899821864\n",
      "[LOG 20200502-14:00:08] epoch: 2077 train-loss: 0.010576504800054762\n",
      "[LOG 20200502-14:00:08] epoch: 2078 train-loss: 0.010576516907248232\n",
      "[LOG 20200502-14:00:08] epoch: 2079 train-loss: 0.01057652880748113\n",
      "[LOG 20200502-14:00:09] epoch: 2080 train-loss: 0.010576540500753455\n",
      "[LOG 20200502-14:00:09] epoch: 2081 train-loss: 0.010576553125348356\n",
      "[LOG 20200502-14:00:09] epoch: 2082 train-loss: 0.01057656461166011\n",
      "[LOG 20200502-14:00:09] epoch: 2083 train-loss: 0.010576576511893008\n",
      "[LOG 20200502-14:00:09] epoch: 2084 train-loss: 0.010576588308645619\n",
      "[LOG 20200502-14:00:10] epoch: 2085 train-loss: 0.010576600001917945\n",
      "[LOG 20200502-14:00:10] epoch: 2086 train-loss: 0.010576611074308554\n",
      "[LOG 20200502-14:00:10] epoch: 2087 train-loss: 0.010576623078021739\n",
      "[LOG 20200502-14:00:10] epoch: 2088 train-loss: 0.010576634667813778\n",
      "[LOG 20200502-14:00:10] epoch: 2089 train-loss: 0.010576645843684673\n",
      "[LOG 20200502-14:00:11] epoch: 2090 train-loss: 0.010576657433476713\n",
      "[LOG 20200502-14:00:11] epoch: 2091 train-loss: 0.010576668712827895\n",
      "[LOG 20200502-14:00:11] epoch: 2092 train-loss: 0.010576680095659362\n",
      "[LOG 20200502-14:00:11] epoch: 2093 train-loss: 0.010576691788931688\n",
      "[LOG 20200502-14:00:11] epoch: 2094 train-loss: 0.010576702550881438\n",
      "[LOG 20200502-14:00:12] epoch: 2095 train-loss: 0.010576714037193192\n",
      "[LOG 20200502-14:00:12] epoch: 2096 train-loss: 0.010576724902623229\n",
      "[LOG 20200502-14:00:12] epoch: 2097 train-loss: 0.010576735975013839\n",
      "[LOG 20200502-14:00:12] epoch: 2098 train-loss: 0.010576746943924163\n",
      "[LOG 20200502-14:00:13] epoch: 2099 train-loss: 0.010576758430235915\n",
      "[LOG 20200502-14:00:13] epoch: 2100 train-loss: 0.010576768985225094\n",
      "[LOG 20200502-14:00:13] epoch: 2101 train-loss: 0.010576779954135418\n",
      "[LOG 20200502-14:00:13] epoch: 2102 train-loss: 0.01057679071608517\n",
      "[LOG 20200502-14:00:13] epoch: 2103 train-loss: 0.010576801271074347\n",
      "[LOG 20200502-14:00:14] epoch: 2104 train-loss: 0.010576811826063527\n",
      "[LOG 20200502-14:00:14] epoch: 2105 train-loss: 0.010576822484532991\n",
      "[LOG 20200502-14:00:14] epoch: 2106 train-loss: 0.01057683334996303\n",
      "[LOG 20200502-14:00:14] epoch: 2107 train-loss: 0.01057684411191278\n",
      "[LOG 20200502-14:00:14] epoch: 2108 train-loss: 0.010576853942539956\n",
      "[LOG 20200502-14:00:15] epoch: 2109 train-loss: 0.010576864807969995\n",
      "[LOG 20200502-14:00:15] epoch: 2110 train-loss: 0.010576875052518316\n",
      "[LOG 20200502-14:00:15] epoch: 2111 train-loss: 0.010576885400546921\n",
      "[LOG 20200502-14:00:15] epoch: 2112 train-loss: 0.010576895541614957\n",
      "[LOG 20200502-14:00:15] epoch: 2113 train-loss: 0.01057690599312385\n",
      "[LOG 20200502-14:00:16] epoch: 2114 train-loss: 0.010576916030711599\n",
      "[LOG 20200502-14:00:16] epoch: 2115 train-loss: 0.010576926482220491\n",
      "[LOG 20200502-14:00:16] epoch: 2116 train-loss: 0.010576936312847666\n",
      "[LOG 20200502-14:00:16] epoch: 2117 train-loss: 0.010576946143474843\n",
      "[LOG 20200502-14:00:16] epoch: 2118 train-loss: 0.010576956181062592\n",
      "[LOG 20200502-14:00:17] epoch: 2119 train-loss: 0.010576966425610913\n",
      "[LOG 20200502-14:00:17] epoch: 2120 train-loss: 0.010576976049277518\n",
      "[LOG 20200502-14:00:17] epoch: 2121 train-loss: 0.010576985879904695\n",
      "[LOG 20200502-14:00:17] epoch: 2122 train-loss: 0.010576995814012157\n",
      "[LOG 20200502-14:00:18] epoch: 2123 train-loss: 0.010577005230718188\n",
      "[LOG 20200502-14:00:18] epoch: 2124 train-loss: 0.010577014854384793\n",
      "[LOG 20200502-14:00:18] epoch: 2125 train-loss: 0.010577024478051398\n",
      "[LOG 20200502-14:00:18] epoch: 2126 train-loss: 0.010577034205198288\n",
      "[LOG 20200502-14:00:18] epoch: 2127 train-loss: 0.010577043311463462\n",
      "[LOG 20200502-14:00:19] epoch: 2128 train-loss: 0.010577052935130067\n",
      "[LOG 20200502-14:00:19] epoch: 2129 train-loss: 0.010577061937914954\n",
      "[LOG 20200502-14:00:19] epoch: 2130 train-loss: 0.010577071768542131\n",
      "[LOG 20200502-14:00:19] epoch: 2131 train-loss: 0.010577080874807306\n",
      "[LOG 20200502-14:00:19] epoch: 2132 train-loss: 0.010577090291513337\n",
      "[LOG 20200502-14:00:20] epoch: 2133 train-loss: 0.010577099190817939\n",
      "[LOG 20200502-14:00:20] epoch: 2134 train-loss: 0.010577108090122541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:00:20] epoch: 2135 train-loss: 0.010577117196387716\n",
      "[LOG 20200502-14:00:20] epoch: 2136 train-loss: 0.010577126613093747\n",
      "[LOG 20200502-14:00:21] epoch: 2137 train-loss: 0.010577134788036346\n",
      "[LOG 20200502-14:00:21] epoch: 2138 train-loss: 0.010577143997781806\n",
      "[LOG 20200502-14:00:21] epoch: 2139 train-loss: 0.010577152690125836\n",
      "[LOG 20200502-14:00:21] epoch: 2140 train-loss: 0.010577161589430438\n",
      "[LOG 20200502-14:00:21] epoch: 2141 train-loss: 0.01057717048873504\n",
      "[LOG 20200502-14:00:22] epoch: 2142 train-loss: 0.010577178974118497\n",
      "[LOG 20200502-14:00:22] epoch: 2143 train-loss: 0.010577187976903386\n",
      "[LOG 20200502-14:00:22] epoch: 2144 train-loss: 0.010577196255326271\n",
      "[LOG 20200502-14:00:22] epoch: 2145 train-loss: 0.010577204947670301\n",
      "[LOG 20200502-14:00:23] epoch: 2146 train-loss: 0.010577213226093186\n",
      "[LOG 20200502-14:00:23] epoch: 2147 train-loss: 0.010577221607996358\n",
      "[LOG 20200502-14:00:23] epoch: 2148 train-loss: 0.010577230403820673\n",
      "[LOG 20200502-14:00:23] epoch: 2149 train-loss: 0.010577238785723845\n",
      "[LOG 20200502-14:00:24] epoch: 2150 train-loss: 0.010577246753705872\n",
      "[LOG 20200502-14:00:24] epoch: 2151 train-loss: 0.010577255135609044\n",
      "[LOG 20200502-14:00:24] epoch: 2152 train-loss: 0.010577263620992502\n",
      "[LOG 20200502-14:00:24] epoch: 2153 train-loss: 0.010577271071573099\n",
      "[LOG 20200502-14:00:24] epoch: 2154 train-loss: 0.010577279349995984\n",
      "[LOG 20200502-14:00:25] epoch: 2155 train-loss: 0.010577286904056868\n",
      "[LOG 20200502-14:00:25] epoch: 2156 train-loss: 0.010577295285960039\n",
      "[LOG 20200502-14:00:25] epoch: 2157 train-loss: 0.010577303460902639\n",
      "[LOG 20200502-14:00:25] epoch: 2158 train-loss: 0.010577311221924093\n",
      "[LOG 20200502-14:00:25] epoch: 2159 train-loss: 0.010577318982945548\n",
      "[LOG 20200502-14:00:26] epoch: 2160 train-loss: 0.010577326433526145\n",
      "[LOG 20200502-14:00:26] epoch: 2161 train-loss: 0.010577334401508173\n",
      "[LOG 20200502-14:00:26] epoch: 2162 train-loss: 0.010577342369490199\n",
      "[LOG 20200502-14:00:26] epoch: 2163 train-loss: 0.010577349613110224\n",
      "[LOG 20200502-14:00:27] epoch: 2164 train-loss: 0.010577357477611966\n",
      "[LOG 20200502-14:00:27] epoch: 2165 train-loss: 0.010577365135153135\n",
      "[LOG 20200502-14:00:27] epoch: 2166 train-loss: 0.010577372171812587\n",
      "[LOG 20200502-14:00:27] epoch: 2167 train-loss: 0.01057737972587347\n",
      "[LOG 20200502-14:00:27] epoch: 2168 train-loss: 0.010577387279934354\n",
      "[LOG 20200502-14:00:28] epoch: 2169 train-loss: 0.010577394627034664\n",
      "[LOG 20200502-14:00:28] epoch: 2170 train-loss: 0.010577401974134974\n",
      "[LOG 20200502-14:00:28] epoch: 2171 train-loss: 0.01057740973515643\n",
      "[LOG 20200502-14:00:28] epoch: 2172 train-loss: 0.010577416564855311\n",
      "[LOG 20200502-14:00:28] epoch: 2173 train-loss: 0.010577423187593618\n",
      "[LOG 20200502-14:00:29] epoch: 2174 train-loss: 0.010577431155575646\n",
      "[LOG 20200502-14:00:29] epoch: 2175 train-loss: 0.01057743736439281\n",
      "[LOG 20200502-14:00:29] epoch: 2176 train-loss: 0.01057744419409169\n",
      "[LOG 20200502-14:00:29] epoch: 2177 train-loss: 0.010577451437711716\n",
      "[LOG 20200502-14:00:29] epoch: 2178 train-loss: 0.01057745816393031\n",
      "[LOG 20200502-14:00:30] epoch: 2179 train-loss: 0.010577465407550335\n",
      "[LOG 20200502-14:00:30] epoch: 2180 train-loss: 0.01057747213376893\n",
      "[LOG 20200502-14:00:30] epoch: 2181 train-loss: 0.010577478342586093\n",
      "[LOG 20200502-14:00:30] epoch: 2182 train-loss: 0.010577485482725833\n",
      "[LOG 20200502-14:00:31] epoch: 2183 train-loss: 0.010577492208944427\n",
      "[LOG 20200502-14:00:31] epoch: 2184 train-loss: 0.010577498935163021\n",
      "[LOG 20200502-14:00:31] epoch: 2185 train-loss: 0.010577504937019613\n",
      "[LOG 20200502-14:00:31] epoch: 2186 train-loss: 0.010577511559757922\n",
      "[LOG 20200502-14:00:32] epoch: 2187 train-loss: 0.01057751818249623\n",
      "[LOG 20200502-14:00:32] epoch: 2188 train-loss: 0.010577524598273966\n",
      "[LOG 20200502-14:00:32] epoch: 2189 train-loss: 0.010577531427972846\n",
      "[LOG 20200502-14:00:32] epoch: 2190 train-loss: 0.010577537222868867\n",
      "[LOG 20200502-14:00:32] epoch: 2191 train-loss: 0.010577543535166316\n",
      "[LOG 20200502-14:00:33] epoch: 2192 train-loss: 0.010577549640503194\n",
      "[LOG 20200502-14:00:33] epoch: 2193 train-loss: 0.010577556056280931\n",
      "[LOG 20200502-14:00:33] epoch: 2194 train-loss: 0.010577561851176951\n",
      "[LOG 20200502-14:00:33] epoch: 2195 train-loss: 0.010577567749553256\n",
      "[LOG 20200502-14:00:34] epoch: 2196 train-loss: 0.010577574268811278\n",
      "[LOG 20200502-14:00:34] epoch: 2197 train-loss: 0.010577579960227013\n",
      "[LOG 20200502-14:00:34] epoch: 2198 train-loss: 0.010577585858603319\n",
      "[LOG 20200502-14:00:34] epoch: 2199 train-loss: 0.01057759186045991\n",
      "[LOG 20200502-14:00:34] epoch: 2200 train-loss: 0.010577597965796789\n",
      "[LOG 20200502-14:00:35] epoch: 2201 train-loss: 0.010577603553732237\n",
      "[LOG 20200502-14:00:35] epoch: 2202 train-loss: 0.010577609659069113\n",
      "[LOG 20200502-14:00:35] epoch: 2203 train-loss: 0.010577615660925707\n",
      "[LOG 20200502-14:00:35] epoch: 2204 train-loss: 0.010577620834940009\n",
      "[LOG 20200502-14:00:35] epoch: 2205 train-loss: 0.010577626319395171\n",
      "[LOG 20200502-14:00:36] epoch: 2206 train-loss: 0.010577631803850332\n",
      "[LOG 20200502-14:00:36] epoch: 2207 train-loss: 0.010577637495266067\n",
      "[LOG 20200502-14:00:36] epoch: 2208 train-loss: 0.010577642772760656\n",
      "[LOG 20200502-14:00:36] epoch: 2209 train-loss: 0.010577648360696103\n",
      "[LOG 20200502-14:00:36] epoch: 2210 train-loss: 0.010577654362552695\n",
      "[LOG 20200502-14:00:37] epoch: 2211 train-loss: 0.010577659640047286\n",
      "[LOG 20200502-14:00:37] epoch: 2212 train-loss: 0.010577664607101016\n",
      "[LOG 20200502-14:00:37] epoch: 2213 train-loss: 0.010577669367194176\n",
      "[LOG 20200502-14:00:37] epoch: 2214 train-loss: 0.01057767505860991\n",
      "[LOG 20200502-14:00:37] epoch: 2215 train-loss: 0.010577680543065071\n",
      "[LOG 20200502-14:00:38] epoch: 2216 train-loss: 0.010577685096197657\n",
      "[LOG 20200502-14:00:38] epoch: 2217 train-loss: 0.010577690166731676\n",
      "[LOG 20200502-14:00:38] epoch: 2218 train-loss: 0.010577695340745978\n",
      "[LOG 20200502-14:00:38] epoch: 2219 train-loss: 0.010577700411279997\n",
      "[LOG 20200502-14:00:39] epoch: 2220 train-loss: 0.010577705378333727\n",
      "[LOG 20200502-14:00:39] epoch: 2221 train-loss: 0.01057771055234803\n",
      "[LOG 20200502-14:00:39] epoch: 2222 train-loss: 0.010577715208960904\n",
      "[LOG 20200502-14:00:39] epoch: 2223 train-loss: 0.010577720382975208\n",
      "[LOG 20200502-14:00:39] epoch: 2224 train-loss: 0.010577724625666937\n",
      "[LOG 20200502-14:00:40] epoch: 2225 train-loss: 0.010577729903161526\n",
      "[LOG 20200502-14:00:40] epoch: 2226 train-loss: 0.010577734352813827\n",
      "[LOG 20200502-14:00:40] epoch: 2227 train-loss: 0.01057773952682813\n",
      "[LOG 20200502-14:00:40] epoch: 2228 train-loss: 0.010577743459079001\n",
      "[LOG 20200502-14:00:40] epoch: 2229 train-loss: 0.010577748115691874\n",
      "[LOG 20200502-14:00:41] epoch: 2230 train-loss: 0.010577752461863888\n",
      "[LOG 20200502-14:00:41] epoch: 2231 train-loss: 0.01057775742891762\n",
      "[LOG 20200502-14:00:41] epoch: 2232 train-loss: 0.010577761775089635\n",
      "[LOG 20200502-14:00:41] epoch: 2233 train-loss: 0.010577766535182795\n",
      "[LOG 20200502-14:00:41] epoch: 2234 train-loss: 0.010577770363953378\n",
      "[LOG 20200502-14:00:42] epoch: 2235 train-loss: 0.010577774710125394\n",
      "[LOG 20200502-14:00:42] epoch: 2236 train-loss: 0.010577779366738267\n",
      "[LOG 20200502-14:00:42] epoch: 2237 train-loss: 0.010577783298989138\n",
      "[LOG 20200502-14:00:42] epoch: 2238 train-loss: 0.010577787748641439\n",
      "[LOG 20200502-14:00:42] epoch: 2239 train-loss: 0.010577792198293738\n",
      "[LOG 20200502-14:00:43] epoch: 2240 train-loss: 0.010577795820103751\n",
      "[LOG 20200502-14:00:43] epoch: 2241 train-loss: 0.010577799752354622\n",
      "[LOG 20200502-14:00:43] epoch: 2242 train-loss: 0.010577803995046351\n",
      "[LOG 20200502-14:00:43] epoch: 2243 train-loss: 0.010577808134257793\n",
      "[LOG 20200502-14:00:44] epoch: 2244 train-loss: 0.010577812376949523\n",
      "[LOG 20200502-14:00:44] epoch: 2245 train-loss: 0.010577816412680678\n",
      "[LOG 20200502-14:00:44] epoch: 2246 train-loss: 0.01057782034493155\n",
      "[LOG 20200502-14:00:44] epoch: 2247 train-loss: 0.010577823552820418\n",
      "[LOG 20200502-14:00:44] epoch: 2248 train-loss: 0.01057782769203186\n",
      "[LOG 20200502-14:00:45] epoch: 2249 train-loss: 0.010577831417322159\n",
      "[LOG 20200502-14:00:45] epoch: 2250 train-loss: 0.01057783534957303\n",
      "[LOG 20200502-14:00:45] epoch: 2251 train-loss: 0.010577838867902756\n",
      "[LOG 20200502-14:00:45] epoch: 2252 train-loss: 0.01057784217927191\n",
      "[LOG 20200502-14:00:45] epoch: 2253 train-loss: 0.01057784642196364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:00:46] epoch: 2254 train-loss: 0.010577850250734223\n",
      "[LOG 20200502-14:00:46] epoch: 2255 train-loss: 0.010577853044701947\n",
      "[LOG 20200502-14:00:46] epoch: 2256 train-loss: 0.01057785666651196\n",
      "[LOG 20200502-14:00:46] epoch: 2257 train-loss: 0.010577860391802259\n",
      "[LOG 20200502-14:00:46] epoch: 2258 train-loss: 0.010577863806651698\n",
      "[LOG 20200502-14:00:47] epoch: 2259 train-loss: 0.010577867014540566\n",
      "[LOG 20200502-14:00:47] epoch: 2260 train-loss: 0.010577870843311151\n",
      "[LOG 20200502-14:00:47] epoch: 2261 train-loss: 0.010577873637278875\n",
      "[LOG 20200502-14:00:47] epoch: 2262 train-loss: 0.010577876948648028\n",
      "[LOG 20200502-14:00:47] epoch: 2263 train-loss: 0.010577880466977755\n",
      "[LOG 20200502-14:00:48] epoch: 2264 train-loss: 0.010577883985307481\n",
      "[LOG 20200502-14:00:48] epoch: 2265 train-loss: 0.010577886261873774\n",
      "[LOG 20200502-14:00:48] epoch: 2266 train-loss: 0.01057789009064436\n",
      "[LOG 20200502-14:00:48] epoch: 2267 train-loss: 0.010577893195052942\n",
      "[LOG 20200502-14:00:48] epoch: 2268 train-loss: 0.01057789640294181\n",
      "[LOG 20200502-14:00:49] epoch: 2269 train-loss: 0.010577899196909534\n",
      "[LOG 20200502-14:00:49] epoch: 2270 train-loss: 0.010577902508278688\n",
      "[LOG 20200502-14:00:49] epoch: 2271 train-loss: 0.01057790561268727\n",
      "[LOG 20200502-14:00:49] epoch: 2272 train-loss: 0.01057790851013528\n",
      "[LOG 20200502-14:00:50] epoch: 2273 train-loss: 0.010577911718024148\n",
      "[LOG 20200502-14:00:50] epoch: 2274 train-loss: 0.010577914718952443\n",
      "[LOG 20200502-14:00:50] epoch: 2275 train-loss: 0.010577917823361026\n",
      "[LOG 20200502-14:00:50] epoch: 2276 train-loss: 0.010577920203407606\n",
      "[LOG 20200502-14:00:50] epoch: 2277 train-loss: 0.010577923411296474\n",
      "[LOG 20200502-14:00:51] epoch: 2278 train-loss: 0.01057792610178391\n",
      "[LOG 20200502-14:00:51] epoch: 2279 train-loss: 0.010577928895751635\n",
      "[LOG 20200502-14:00:51] epoch: 2280 train-loss: 0.01057793117231793\n",
      "[LOG 20200502-14:00:51] epoch: 2281 train-loss: 0.010577933862805367\n",
      "[LOG 20200502-14:00:52] epoch: 2282 train-loss: 0.010577936760253377\n",
      "[LOG 20200502-14:00:52] epoch: 2283 train-loss: 0.010577939657701386\n",
      "[LOG 20200502-14:00:52] epoch: 2284 train-loss: 0.010577941727307107\n",
      "[LOG 20200502-14:00:52] epoch: 2285 train-loss: 0.010577944728235403\n",
      "[LOG 20200502-14:00:52] epoch: 2286 train-loss: 0.010577947315242555\n",
      "[LOG 20200502-14:00:53] epoch: 2287 train-loss: 0.010577949695289135\n",
      "[LOG 20200502-14:00:53] epoch: 2288 train-loss: 0.010577952075335715\n",
      "[LOG 20200502-14:00:53] epoch: 2289 train-loss: 0.010577954248421721\n",
      "[LOG 20200502-14:00:53] epoch: 2290 train-loss: 0.010577956835428873\n",
      "[LOG 20200502-14:00:53] epoch: 2291 train-loss: 0.010577959215475453\n",
      "[LOG 20200502-14:00:54] epoch: 2292 train-loss: 0.010577961492041746\n",
      "[LOG 20200502-14:00:54] epoch: 2293 train-loss: 0.010577963665127754\n",
      "[LOG 20200502-14:00:54] epoch: 2294 train-loss: 0.01057796614865462\n",
      "[LOG 20200502-14:00:54] epoch: 2295 train-loss: 0.010577968114780055\n",
      "[LOG 20200502-14:00:54] epoch: 2296 train-loss: 0.010577970184385777\n",
      "[LOG 20200502-14:00:55] epoch: 2297 train-loss: 0.010577972564432356\n",
      "[LOG 20200502-14:00:55] epoch: 2298 train-loss: 0.010577974634038078\n",
      "[LOG 20200502-14:00:55] epoch: 2299 train-loss: 0.010577976703643799\n",
      "[LOG 20200502-14:00:55] epoch: 2300 train-loss: 0.010577978980210092\n",
      "[LOG 20200502-14:00:55] epoch: 2301 train-loss: 0.010577981049815813\n",
      "[LOG 20200502-14:00:56] epoch: 2302 train-loss: 0.01057798301594125\n",
      "[LOG 20200502-14:00:56] epoch: 2303 train-loss: 0.010577985499468114\n",
      "[LOG 20200502-14:00:56] epoch: 2304 train-loss: 0.01057798694819212\n",
      "[LOG 20200502-14:00:56] epoch: 2305 train-loss: 0.01057798901779784\n",
      "[LOG 20200502-14:00:56] epoch: 2306 train-loss: 0.010577991294364134\n",
      "[LOG 20200502-14:00:57] epoch: 2307 train-loss: 0.010577992846568426\n",
      "[LOG 20200502-14:00:57] epoch: 2308 train-loss: 0.010577994709213575\n",
      "[LOG 20200502-14:00:57] epoch: 2309 train-loss: 0.010577996571858725\n",
      "[LOG 20200502-14:00:57] epoch: 2310 train-loss: 0.010577998124063015\n",
      "[LOG 20200502-14:00:57] epoch: 2311 train-loss: 0.010578000400629308\n",
      "[LOG 20200502-14:00:58] epoch: 2312 train-loss: 0.010578002056313885\n",
      "[LOG 20200502-14:00:58] epoch: 2313 train-loss: 0.010578003711998463\n",
      "[LOG 20200502-14:00:58] epoch: 2314 train-loss: 0.010578005160722468\n",
      "[LOG 20200502-14:00:58] epoch: 2315 train-loss: 0.010578007126847902\n",
      "[LOG 20200502-14:00:59] epoch: 2316 train-loss: 0.010578008679052195\n",
      "[LOG 20200502-14:00:59] epoch: 2317 train-loss: 0.010578010438217057\n",
      "[LOG 20200502-14:00:59] epoch: 2318 train-loss: 0.010578012093901634\n",
      "[LOG 20200502-14:00:59] epoch: 2319 train-loss: 0.010578013853066497\n",
      "[LOG 20200502-14:00:59] epoch: 2320 train-loss: 0.010578014991349645\n",
      "[LOG 20200502-14:01:00] epoch: 2321 train-loss: 0.01057801695747508\n",
      "[LOG 20200502-14:01:00] epoch: 2322 train-loss: 0.010578018406199085\n",
      "[LOG 20200502-14:01:00] epoch: 2323 train-loss: 0.01057801985492309\n",
      "[LOG 20200502-14:01:00] epoch: 2324 train-loss: 0.01057802171756824\n",
      "[LOG 20200502-14:01:00] epoch: 2325 train-loss: 0.010578023062811958\n",
      "[LOG 20200502-14:01:01] epoch: 2326 train-loss: 0.010578024201095104\n",
      "[LOG 20200502-14:01:01] epoch: 2327 train-loss: 0.010578025546338823\n",
      "[LOG 20200502-14:01:01] epoch: 2328 train-loss: 0.010578026995062828\n",
      "[LOG 20200502-14:01:01] epoch: 2329 train-loss: 0.010578028133345975\n",
      "[LOG 20200502-14:01:01] epoch: 2330 train-loss: 0.01057802958206998\n",
      "[LOG 20200502-14:01:02] epoch: 2331 train-loss: 0.010578031237754557\n",
      "[LOG 20200502-14:01:02] epoch: 2332 train-loss: 0.010578032272557417\n",
      "[LOG 20200502-14:01:02] epoch: 2333 train-loss: 0.010578033928241994\n",
      "[LOG 20200502-14:01:02] epoch: 2334 train-loss: 0.010578034859564569\n",
      "[LOG 20200502-14:01:02] epoch: 2335 train-loss: 0.010578036204808287\n",
      "[LOG 20200502-14:01:03] epoch: 2336 train-loss: 0.010578037136130862\n",
      "[LOG 20200502-14:01:03] epoch: 2337 train-loss: 0.010578038998776011\n",
      "[LOG 20200502-14:01:03] epoch: 2338 train-loss: 0.010578039619657729\n",
      "[LOG 20200502-14:01:03] epoch: 2339 train-loss: 0.010578041068381734\n",
      "[LOG 20200502-14:01:04] epoch: 2340 train-loss: 0.01057804220666488\n",
      "[LOG 20200502-14:01:04] epoch: 2341 train-loss: 0.010578043448428312\n",
      "[LOG 20200502-14:01:04] epoch: 2342 train-loss: 0.010578044793672033\n",
      "[LOG 20200502-14:01:04] epoch: 2343 train-loss: 0.010578045828474892\n",
      "[LOG 20200502-14:01:04] epoch: 2344 train-loss: 0.010578046863277754\n",
      "[LOG 20200502-14:01:05] epoch: 2345 train-loss: 0.010578047794600328\n",
      "[LOG 20200502-14:01:05] epoch: 2346 train-loss: 0.010578048622442616\n",
      "[LOG 20200502-14:01:05] epoch: 2347 train-loss: 0.010578049760725763\n",
      "[LOG 20200502-14:01:05] epoch: 2348 train-loss: 0.010578051209449768\n",
      "[LOG 20200502-14:01:05] epoch: 2349 train-loss: 0.010578051623370912\n",
      "[LOG 20200502-14:01:06] epoch: 2350 train-loss: 0.010578053072094917\n",
      "[LOG 20200502-14:01:06] epoch: 2351 train-loss: 0.010578053899937205\n",
      "[LOG 20200502-14:01:06] epoch: 2352 train-loss: 0.010578055038220353\n",
      "[LOG 20200502-14:01:06] epoch: 2353 train-loss: 0.01057805565910207\n",
      "[LOG 20200502-14:01:06] epoch: 2354 train-loss: 0.010578056797385216\n",
      "[LOG 20200502-14:01:07] epoch: 2355 train-loss: 0.010578057625227504\n",
      "[LOG 20200502-14:01:07] epoch: 2356 train-loss: 0.010578058349589506\n",
      "[LOG 20200502-14:01:07] epoch: 2357 train-loss: 0.010578059384392368\n",
      "[LOG 20200502-14:01:07] epoch: 2358 train-loss: 0.010578060419195227\n",
      "[LOG 20200502-14:01:08] epoch: 2359 train-loss: 0.010578060729636086\n",
      "[LOG 20200502-14:01:08] epoch: 2360 train-loss: 0.010578061764438948\n",
      "[LOG 20200502-14:01:08] epoch: 2361 train-loss: 0.010578062695761522\n",
      "[LOG 20200502-14:01:08] epoch: 2362 train-loss: 0.010578063420123525\n",
      "[LOG 20200502-14:01:08] epoch: 2363 train-loss: 0.0105780643514461\n",
      "[LOG 20200502-14:01:09] epoch: 2364 train-loss: 0.010578064972327815\n",
      "[LOG 20200502-14:01:09] epoch: 2365 train-loss: 0.010578066007130675\n",
      "[LOG 20200502-14:01:09] epoch: 2366 train-loss: 0.010578066524532106\n",
      "[LOG 20200502-14:01:09] epoch: 2367 train-loss: 0.01057806745585468\n",
      "[LOG 20200502-14:01:09] epoch: 2368 train-loss: 0.010578067973256111\n",
      "[LOG 20200502-14:01:10] epoch: 2369 train-loss: 0.010578068904578686\n",
      "[LOG 20200502-14:01:10] epoch: 2370 train-loss: 0.01057806931849983\n",
      "[LOG 20200502-14:01:10] epoch: 2371 train-loss: 0.01057807014634212\n",
      "[LOG 20200502-14:01:10] epoch: 2372 train-loss: 0.010578070974184407\n",
      "[LOG 20200502-14:01:10] epoch: 2373 train-loss: 0.01057807138810555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:01:11] epoch: 2374 train-loss: 0.010578072526388697\n",
      "[LOG 20200502-14:01:11] epoch: 2375 train-loss: 0.010578072733349271\n",
      "[LOG 20200502-14:01:11] epoch: 2376 train-loss: 0.010578073457711272\n",
      "[LOG 20200502-14:01:11] epoch: 2377 train-loss: 0.010578074492514133\n",
      "[LOG 20200502-14:01:11] epoch: 2378 train-loss: 0.010578075423836708\n",
      "[LOG 20200502-14:01:12] epoch: 2379 train-loss: 0.01057807614819871\n",
      "[LOG 20200502-14:01:12] epoch: 2380 train-loss: 0.01057807645863957\n",
      "[LOG 20200502-14:01:12] epoch: 2381 train-loss: 0.010578077286481857\n",
      "[LOG 20200502-14:01:12] epoch: 2382 train-loss: 0.010578077596922716\n",
      "[LOG 20200502-14:01:12] epoch: 2383 train-loss: 0.010578078631725576\n",
      "[LOG 20200502-14:01:13] epoch: 2384 train-loss: 0.010578079356087578\n",
      "[LOG 20200502-14:01:13] epoch: 2385 train-loss: 0.010578079666528437\n",
      "[LOG 20200502-14:01:13] epoch: 2386 train-loss: 0.010578079976969294\n",
      "[LOG 20200502-14:01:13] epoch: 2387 train-loss: 0.010578080908291869\n",
      "[LOG 20200502-14:01:14] epoch: 2388 train-loss: 0.010578081632653872\n",
      "[LOG 20200502-14:01:14] epoch: 2389 train-loss: 0.010578082046575017\n",
      "[LOG 20200502-14:01:14] epoch: 2390 train-loss: 0.010578082874417305\n",
      "[LOG 20200502-14:01:14] epoch: 2391 train-loss: 0.010578083391818736\n",
      "[LOG 20200502-14:01:14] epoch: 2392 train-loss: 0.01057808432314131\n",
      "[LOG 20200502-14:01:15] epoch: 2393 train-loss: 0.010578084737062454\n",
      "[LOG 20200502-14:01:15] epoch: 2394 train-loss: 0.010578084944023026\n",
      "[LOG 20200502-14:01:15] epoch: 2395 train-loss: 0.010578085771865316\n",
      "[LOG 20200502-14:01:15] epoch: 2396 train-loss: 0.01057808618578646\n",
      "[LOG 20200502-14:01:15] epoch: 2397 train-loss: 0.010578087117109034\n",
      "[LOG 20200502-14:01:16] epoch: 2398 train-loss: 0.010578087634510465\n",
      "[LOG 20200502-14:01:16] epoch: 2399 train-loss: 0.01057808825539218\n",
      "[LOG 20200502-14:01:16] epoch: 2400 train-loss: 0.01057808856583304\n",
      "[LOG 20200502-14:01:16] epoch: 2401 train-loss: 0.010578089083234469\n",
      "[LOG 20200502-14:01:16] epoch: 2402 train-loss: 0.010578089704116186\n",
      "[LOG 20200502-14:01:17] epoch: 2403 train-loss: 0.010578090531958474\n",
      "[LOG 20200502-14:01:17] epoch: 2404 train-loss: 0.010578090738919046\n",
      "[LOG 20200502-14:01:17] epoch: 2405 train-loss: 0.010578091566761335\n",
      "[LOG 20200502-14:01:17] epoch: 2406 train-loss: 0.010578092291123338\n",
      "[LOG 20200502-14:01:17] epoch: 2407 train-loss: 0.010578093118965626\n",
      "[LOG 20200502-14:01:18] epoch: 2408 train-loss: 0.010578092912005054\n",
      "[LOG 20200502-14:01:18] epoch: 2409 train-loss: 0.010578093843327628\n",
      "[LOG 20200502-14:01:18] epoch: 2410 train-loss: 0.010578094567689631\n",
      "[LOG 20200502-14:01:18] epoch: 2411 train-loss: 0.010578095085091062\n",
      "[LOG 20200502-14:01:19] epoch: 2412 train-loss: 0.010578095705972778\n",
      "[LOG 20200502-14:01:19] epoch: 2413 train-loss: 0.010578096016413636\n",
      "[LOG 20200502-14:01:19] epoch: 2414 train-loss: 0.010578096637295352\n",
      "[LOG 20200502-14:01:19] epoch: 2415 train-loss: 0.010578097051216496\n",
      "[LOG 20200502-14:01:19] epoch: 2416 train-loss: 0.01057809798253907\n",
      "[LOG 20200502-14:01:20] epoch: 2417 train-loss: 0.010578098396460215\n",
      "[LOG 20200502-14:01:20] epoch: 2418 train-loss: 0.01057809932778279\n",
      "[LOG 20200502-14:01:20] epoch: 2419 train-loss: 0.010578099534743361\n",
      "[LOG 20200502-14:01:20] epoch: 2420 train-loss: 0.01057810036258565\n",
      "[LOG 20200502-14:01:20] epoch: 2421 train-loss: 0.01057810139738851\n",
      "[LOG 20200502-14:01:21] epoch: 2422 train-loss: 0.01057810170782937\n",
      "[LOG 20200502-14:01:21] epoch: 2423 train-loss: 0.010578102535671659\n",
      "[LOG 20200502-14:01:21] epoch: 2424 train-loss: 0.010578102639151944\n",
      "[LOG 20200502-14:01:21] epoch: 2425 train-loss: 0.010578103570474518\n",
      "[LOG 20200502-14:01:21] epoch: 2426 train-loss: 0.010578104191356234\n",
      "[LOG 20200502-14:01:22] epoch: 2427 train-loss: 0.010578105122678809\n",
      "[LOG 20200502-14:01:22] epoch: 2428 train-loss: 0.01057810564008024\n",
      "[LOG 20200502-14:01:22] epoch: 2429 train-loss: 0.01057810615748167\n",
      "[LOG 20200502-14:01:22] epoch: 2430 train-loss: 0.010578106881843673\n",
      "[LOG 20200502-14:01:22] epoch: 2431 train-loss: 0.010578107606205676\n",
      "[LOG 20200502-14:01:23] epoch: 2432 train-loss: 0.010578108951449394\n",
      "[LOG 20200502-14:01:23] epoch: 2433 train-loss: 0.010578108847969107\n",
      "[LOG 20200502-14:01:23] epoch: 2434 train-loss: 0.010578109986252256\n",
      "[LOG 20200502-14:01:23] epoch: 2435 train-loss: 0.010578110296693113\n",
      "[LOG 20200502-14:01:23] epoch: 2436 train-loss: 0.010578111538456546\n",
      "[LOG 20200502-14:01:24] epoch: 2437 train-loss: 0.010578111848897405\n",
      "[LOG 20200502-14:01:24] epoch: 2438 train-loss: 0.010578112573259406\n",
      "[LOG 20200502-14:01:24] epoch: 2439 train-loss: 0.010578113608062267\n",
      "[LOG 20200502-14:01:24] epoch: 2440 train-loss: 0.010578114125463698\n",
      "[LOG 20200502-14:01:24] epoch: 2441 train-loss: 0.0105781148498257\n",
      "[LOG 20200502-14:01:25] epoch: 2442 train-loss: 0.010578115470707417\n",
      "[LOG 20200502-14:01:25] epoch: 2443 train-loss: 0.010578116402029991\n",
      "[LOG 20200502-14:01:25] epoch: 2444 train-loss: 0.010578117229872279\n",
      "[LOG 20200502-14:01:25] epoch: 2445 train-loss: 0.010578117850753996\n",
      "[LOG 20200502-14:01:26] epoch: 2446 train-loss: 0.010578118368155427\n",
      "[LOG 20200502-14:01:26] epoch: 2447 train-loss: 0.010578119299478002\n",
      "[LOG 20200502-14:01:26] epoch: 2448 train-loss: 0.010578120334280862\n",
      "[LOG 20200502-14:01:26] epoch: 2449 train-loss: 0.010578121162123151\n",
      "[LOG 20200502-14:01:26] epoch: 2450 train-loss: 0.010578121989965439\n",
      "[LOG 20200502-14:01:27] epoch: 2451 train-loss: 0.010578123128248585\n",
      "[LOG 20200502-14:01:27] epoch: 2452 train-loss: 0.010578123335209157\n",
      "[LOG 20200502-14:01:27] epoch: 2453 train-loss: 0.01057812405957116\n",
      "[LOG 20200502-14:01:27] epoch: 2454 train-loss: 0.010578124680452876\n",
      "[LOG 20200502-14:01:27] epoch: 2455 train-loss: 0.010578125818736024\n",
      "[LOG 20200502-14:01:28] epoch: 2456 train-loss: 0.010578126336137453\n",
      "[LOG 20200502-14:01:28] epoch: 2457 train-loss: 0.0105781274744206\n",
      "[LOG 20200502-14:01:28] epoch: 2458 train-loss: 0.01057812881966432\n",
      "[LOG 20200502-14:01:28] epoch: 2459 train-loss: 0.010578129440546036\n",
      "[LOG 20200502-14:01:29] epoch: 2460 train-loss: 0.010578130164908038\n",
      "[LOG 20200502-14:01:29] epoch: 2461 train-loss: 0.010578131303191185\n",
      "[LOG 20200502-14:01:29] epoch: 2462 train-loss: 0.0105781319240729\n",
      "[LOG 20200502-14:01:29] epoch: 2463 train-loss: 0.010578133165836334\n",
      "[LOG 20200502-14:01:29] epoch: 2464 train-loss: 0.01057813378671805\n",
      "[LOG 20200502-14:01:30] epoch: 2465 train-loss: 0.01057813513196177\n",
      "[LOG 20200502-14:01:30] epoch: 2466 train-loss: 0.010578135442402627\n",
      "[LOG 20200502-14:01:30] epoch: 2467 train-loss: 0.010578136891126633\n",
      "[LOG 20200502-14:01:30] epoch: 2468 train-loss: 0.010578137822449207\n",
      "[LOG 20200502-14:01:30] epoch: 2469 train-loss: 0.010578138857252069\n",
      "[LOG 20200502-14:01:31] epoch: 2470 train-loss: 0.010578139892054928\n",
      "[LOG 20200502-14:01:31] epoch: 2471 train-loss: 0.01057814092685779\n",
      "[LOG 20200502-14:01:31] epoch: 2472 train-loss: 0.010578142168621222\n",
      "[LOG 20200502-14:01:31] epoch: 2473 train-loss: 0.010578143306904368\n",
      "[LOG 20200502-14:01:31] epoch: 2474 train-loss: 0.01057814434170723\n",
      "[LOG 20200502-14:01:32] epoch: 2475 train-loss: 0.010578145066069232\n",
      "[LOG 20200502-14:01:32] epoch: 2476 train-loss: 0.01057814641131295\n",
      "[LOG 20200502-14:01:32] epoch: 2477 train-loss: 0.010578147549596097\n",
      "[LOG 20200502-14:01:32] epoch: 2478 train-loss: 0.01057814879135953\n",
      "[LOG 20200502-14:01:32] epoch: 2479 train-loss: 0.01057814982616239\n",
      "[LOG 20200502-14:01:33] epoch: 2480 train-loss: 0.010578150860965252\n",
      "[LOG 20200502-14:01:33] epoch: 2481 train-loss: 0.010578151999248398\n",
      "[LOG 20200502-14:01:33] epoch: 2482 train-loss: 0.010578153241011832\n",
      "[LOG 20200502-14:01:33] epoch: 2483 train-loss: 0.010578154275814692\n",
      "[LOG 20200502-14:01:34] epoch: 2484 train-loss: 0.010578155517578125\n",
      "[LOG 20200502-14:01:34] epoch: 2485 train-loss: 0.01057815696630213\n",
      "[LOG 20200502-14:01:34] epoch: 2486 train-loss: 0.010578158725466993\n",
      "[LOG 20200502-14:01:34] epoch: 2487 train-loss: 0.010578159656789567\n",
      "[LOG 20200502-14:01:34] epoch: 2488 train-loss: 0.010578161105513573\n",
      "[LOG 20200502-14:01:35] epoch: 2489 train-loss: 0.010578161829875575\n",
      "[LOG 20200502-14:01:35] epoch: 2490 train-loss: 0.010578163899481297\n",
      "[LOG 20200502-14:01:35] epoch: 2491 train-loss: 0.0105781646238433\n",
      "[LOG 20200502-14:01:35] epoch: 2492 train-loss: 0.010578166589968734\n",
      "[LOG 20200502-14:01:35] epoch: 2493 train-loss: 0.010578167728251882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:01:36] epoch: 2494 train-loss: 0.010578169176975885\n",
      "[LOG 20200502-14:01:36] epoch: 2495 train-loss: 0.01057817093614075\n",
      "[LOG 20200502-14:01:36] epoch: 2496 train-loss: 0.010578172384864755\n",
      "[LOG 20200502-14:01:36] epoch: 2497 train-loss: 0.010578173212707043\n",
      "[LOG 20200502-14:01:37] epoch: 2498 train-loss: 0.010578174971871905\n",
      "[LOG 20200502-14:01:37] epoch: 2499 train-loss: 0.010578176627556482\n",
      "[LOG 20200502-14:01:37] epoch: 2500 train-loss: 0.010578178076280488\n",
      "[LOG 20200502-14:01:37] epoch: 2501 train-loss: 0.010578179525004493\n",
      "[LOG 20200502-14:01:37] epoch: 2502 train-loss: 0.01057818118068907\n",
      "[LOG 20200502-14:01:38] epoch: 2503 train-loss: 0.01057818273289336\n",
      "[LOG 20200502-14:01:38] epoch: 2504 train-loss: 0.010578184388577938\n",
      "[LOG 20200502-14:01:38] epoch: 2505 train-loss: 0.0105781861477428\n",
      "[LOG 20200502-14:01:38] epoch: 2506 train-loss: 0.010578187389506234\n",
      "[LOG 20200502-14:01:38] epoch: 2507 train-loss: 0.010578189459111955\n",
      "[LOG 20200502-14:01:39] epoch: 2508 train-loss: 0.010578191011316247\n",
      "[LOG 20200502-14:01:39] epoch: 2509 train-loss: 0.010578192253079679\n",
      "[LOG 20200502-14:01:39] epoch: 2510 train-loss: 0.010578194426165687\n",
      "[LOG 20200502-14:01:39] epoch: 2511 train-loss: 0.010578195978369977\n",
      "[LOG 20200502-14:01:39] epoch: 2512 train-loss: 0.010578198047975699\n",
      "[LOG 20200502-14:01:40] epoch: 2513 train-loss: 0.010578199496699704\n",
      "[LOG 20200502-14:01:40] epoch: 2514 train-loss: 0.010578201048903994\n",
      "[LOG 20200502-14:01:40] epoch: 2515 train-loss: 0.010578203428950574\n",
      "[LOG 20200502-14:01:40] epoch: 2516 train-loss: 0.010578205291595724\n",
      "[LOG 20200502-14:01:41] epoch: 2517 train-loss: 0.010578206843800016\n",
      "[LOG 20200502-14:01:41] epoch: 2518 train-loss: 0.010578208706445165\n",
      "[LOG 20200502-14:01:41] epoch: 2519 train-loss: 0.010578210569090314\n",
      "[LOG 20200502-14:01:41] epoch: 2520 train-loss: 0.010578212431735463\n",
      "[LOG 20200502-14:01:41] epoch: 2521 train-loss: 0.010578214811782042\n",
      "[LOG 20200502-14:01:42] epoch: 2522 train-loss: 0.010578216157025762\n",
      "[LOG 20200502-14:01:42] epoch: 2523 train-loss: 0.010578218330111768\n",
      "[LOG 20200502-14:01:42] epoch: 2524 train-loss: 0.010578220503197776\n",
      "[LOG 20200502-14:01:42] epoch: 2525 train-loss: 0.010578222365842925\n",
      "[LOG 20200502-14:01:42] epoch: 2526 train-loss: 0.01057822433196836\n",
      "[LOG 20200502-14:01:43] epoch: 2527 train-loss: 0.010578226815495227\n",
      "[LOG 20200502-14:01:43] epoch: 2528 train-loss: 0.010578228367699517\n",
      "[LOG 20200502-14:01:43] epoch: 2529 train-loss: 0.010578230747746097\n",
      "[LOG 20200502-14:01:43] epoch: 2530 train-loss: 0.010578233231272962\n",
      "[LOG 20200502-14:01:43] epoch: 2531 train-loss: 0.010578235093918111\n",
      "[LOG 20200502-14:01:44] epoch: 2532 train-loss: 0.010578236853082975\n",
      "[LOG 20200502-14:01:44] epoch: 2533 train-loss: 0.010578239440090127\n",
      "[LOG 20200502-14:01:44] epoch: 2534 train-loss: 0.010578241613176134\n",
      "[LOG 20200502-14:01:44] epoch: 2535 train-loss: 0.010578243786262142\n",
      "[LOG 20200502-14:01:45] epoch: 2536 train-loss: 0.01057824564890729\n",
      "[LOG 20200502-14:01:45] epoch: 2537 train-loss: 0.010578248442875015\n",
      "[LOG 20200502-14:01:45] epoch: 2538 train-loss: 0.010578250719441308\n",
      "[LOG 20200502-14:01:45] epoch: 2539 train-loss: 0.010578253099487888\n",
      "[LOG 20200502-14:01:45] epoch: 2540 train-loss: 0.010578255272573896\n",
      "[LOG 20200502-14:01:46] epoch: 2541 train-loss: 0.010578257652620474\n",
      "[LOG 20200502-14:01:46] epoch: 2542 train-loss: 0.010578259825706482\n",
      "[LOG 20200502-14:01:46] epoch: 2543 train-loss: 0.010578261895312203\n",
      "[LOG 20200502-14:01:46] epoch: 2544 train-loss: 0.010578264585799642\n",
      "[LOG 20200502-14:01:46] epoch: 2545 train-loss: 0.010578266241484217\n",
      "[LOG 20200502-14:01:47] epoch: 2546 train-loss: 0.010578268621530797\n",
      "[LOG 20200502-14:01:47] epoch: 2547 train-loss: 0.010578270794616805\n",
      "[LOG 20200502-14:01:47] epoch: 2548 train-loss: 0.01057827358858453\n",
      "[LOG 20200502-14:01:47] epoch: 2549 train-loss: 0.010578275865150822\n",
      "[LOG 20200502-14:01:47] epoch: 2550 train-loss: 0.010578278245197402\n",
      "[LOG 20200502-14:01:48] epoch: 2551 train-loss: 0.010578280728724267\n",
      "[LOG 20200502-14:01:48] epoch: 2552 train-loss: 0.01057828300529056\n",
      "[LOG 20200502-14:01:48] epoch: 2553 train-loss: 0.01057828538533714\n",
      "[LOG 20200502-14:01:48] epoch: 2554 train-loss: 0.010578287868864007\n",
      "[LOG 20200502-14:01:48] epoch: 2555 train-loss: 0.010578291076752875\n",
      "[LOG 20200502-14:01:49] epoch: 2556 train-loss: 0.010578292939398024\n",
      "[LOG 20200502-14:01:49] epoch: 2557 train-loss: 0.01057829511248403\n",
      "[LOG 20200502-14:01:49] epoch: 2558 train-loss: 0.010578298216892613\n",
      "[LOG 20200502-14:01:49] epoch: 2559 train-loss: 0.010578300700419478\n",
      "[LOG 20200502-14:01:49] epoch: 2560 train-loss: 0.010578303183946345\n",
      "[LOG 20200502-14:01:50] epoch: 2561 train-loss: 0.010578305874433782\n",
      "[LOG 20200502-14:01:50] epoch: 2562 train-loss: 0.010578308254480362\n",
      "[LOG 20200502-14:01:50] epoch: 2563 train-loss: 0.010578310738007227\n",
      "[LOG 20200502-14:01:50] epoch: 2564 train-loss: 0.010578313325014379\n",
      "[LOG 20200502-14:01:50] epoch: 2565 train-loss: 0.010578316015501818\n",
      "[LOG 20200502-14:01:51] epoch: 2566 train-loss: 0.010578318705989255\n",
      "[LOG 20200502-14:01:51] epoch: 2567 train-loss: 0.010578321396476693\n",
      "[LOG 20200502-14:01:51] epoch: 2568 train-loss: 0.010578323880003558\n",
      "[LOG 20200502-14:01:51] epoch: 2569 train-loss: 0.010578326673971282\n",
      "[LOG 20200502-14:01:51] epoch: 2570 train-loss: 0.01057832988186015\n",
      "[LOG 20200502-14:01:52] epoch: 2571 train-loss: 0.01057833226190673\n",
      "[LOG 20200502-14:01:52] epoch: 2572 train-loss: 0.010578335469795598\n",
      "[LOG 20200502-14:01:52] epoch: 2573 train-loss: 0.010578338160283036\n",
      "[LOG 20200502-14:01:52] epoch: 2574 train-loss: 0.01057834043684933\n",
      "[LOG 20200502-14:01:53] epoch: 2575 train-loss: 0.01057834354125791\n",
      "[LOG 20200502-14:01:53] epoch: 2576 train-loss: 0.010578345817824205\n",
      "[LOG 20200502-14:01:53] epoch: 2577 train-loss: 0.010578348508311642\n",
      "[LOG 20200502-14:01:53] epoch: 2578 train-loss: 0.010578352026641369\n",
      "[LOG 20200502-14:01:53] epoch: 2579 train-loss: 0.010578354820609093\n",
      "[LOG 20200502-14:01:54] epoch: 2580 train-loss: 0.010578357614576817\n",
      "[LOG 20200502-14:01:54] epoch: 2581 train-loss: 0.010578360615505112\n",
      "[LOG 20200502-14:01:54] epoch: 2582 train-loss: 0.010578363202512264\n",
      "[LOG 20200502-14:01:54] epoch: 2583 train-loss: 0.010578366306920847\n",
      "[LOG 20200502-14:01:54] epoch: 2584 train-loss: 0.010578369307849143\n",
      "[LOG 20200502-14:01:55] epoch: 2585 train-loss: 0.010578372101816867\n",
      "[LOG 20200502-14:01:55] epoch: 2586 train-loss: 0.01057837437838316\n",
      "[LOG 20200502-14:01:55] epoch: 2587 train-loss: 0.010578377896712886\n",
      "[LOG 20200502-14:01:55] epoch: 2588 train-loss: 0.010578381311562326\n",
      "[LOG 20200502-14:01:55] epoch: 2589 train-loss: 0.010578384312490622\n",
      "[LOG 20200502-14:01:56] epoch: 2590 train-loss: 0.01057838700297806\n",
      "[LOG 20200502-14:01:56] epoch: 2591 train-loss: 0.01057838990042607\n",
      "[LOG 20200502-14:01:56] epoch: 2592 train-loss: 0.010578393211795224\n",
      "[LOG 20200502-14:01:56] epoch: 2593 train-loss: 0.010578395798802376\n",
      "[LOG 20200502-14:01:57] epoch: 2594 train-loss: 0.010578399213651815\n",
      "[LOG 20200502-14:01:57] epoch: 2595 train-loss: 0.01057840200761954\n",
      "[LOG 20200502-14:01:57] epoch: 2596 train-loss: 0.010578405215508409\n",
      "[LOG 20200502-14:01:57] epoch: 2597 train-loss: 0.01057840831991699\n",
      "[LOG 20200502-14:01:57] epoch: 2598 train-loss: 0.010578411527805857\n",
      "[LOG 20200502-14:01:58] epoch: 2599 train-loss: 0.01057841463221444\n",
      "[LOG 20200502-14:01:58] epoch: 2600 train-loss: 0.010578416908780733\n",
      "[LOG 20200502-14:01:58] epoch: 2601 train-loss: 0.010578420323630175\n",
      "[LOG 20200502-14:01:58] epoch: 2602 train-loss: 0.010578423841959901\n",
      "[LOG 20200502-14:01:58] epoch: 2603 train-loss: 0.010578426532447338\n",
      "[LOG 20200502-14:01:59] epoch: 2604 train-loss: 0.01057842963685592\n",
      "[LOG 20200502-14:01:59] epoch: 2605 train-loss: 0.01057843305170536\n",
      "[LOG 20200502-14:01:59] epoch: 2606 train-loss: 0.010578436052633656\n",
      "[LOG 20200502-14:01:59] epoch: 2607 train-loss: 0.010578439157042239\n",
      "[LOG 20200502-14:01:59] epoch: 2608 train-loss: 0.010578442468411393\n",
      "[LOG 20200502-14:02:00] epoch: 2609 train-loss: 0.010578445779780546\n",
      "[LOG 20200502-14:02:00] epoch: 2610 train-loss: 0.010578448884189129\n",
      "[LOG 20200502-14:02:00] epoch: 2611 train-loss: 0.010578452609479427\n",
      "[LOG 20200502-14:02:00] epoch: 2612 train-loss: 0.01057845519648658\n",
      "[LOG 20200502-14:02:00] epoch: 2613 train-loss: 0.010578458611336019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:02:01] epoch: 2614 train-loss: 0.010578462129665745\n",
      "[LOG 20200502-14:02:01] epoch: 2615 train-loss: 0.010578465130594041\n",
      "[LOG 20200502-14:02:01] epoch: 2616 train-loss: 0.010578468235002624\n",
      "[LOG 20200502-14:02:01] epoch: 2617 train-loss: 0.010578471442891492\n",
      "[LOG 20200502-14:02:02] epoch: 2618 train-loss: 0.01057847465078036\n",
      "[LOG 20200502-14:02:02] epoch: 2619 train-loss: 0.0105784780656298\n",
      "[LOG 20200502-14:02:02] epoch: 2620 train-loss: 0.010578481273518668\n",
      "[LOG 20200502-14:02:02] epoch: 2621 train-loss: 0.010578484274446964\n",
      "[LOG 20200502-14:02:02] epoch: 2622 train-loss: 0.010578487585816119\n",
      "[LOG 20200502-14:02:03] epoch: 2623 train-loss: 0.010578490793704987\n",
      "[LOG 20200502-14:02:03] epoch: 2624 train-loss: 0.010578494105074141\n",
      "[LOG 20200502-14:02:03] epoch: 2625 train-loss: 0.010578497726884153\n",
      "[LOG 20200502-14:02:03] epoch: 2626 train-loss: 0.010578500831292735\n",
      "[LOG 20200502-14:02:03] epoch: 2627 train-loss: 0.010578503935701318\n",
      "[LOG 20200502-14:02:04] epoch: 2628 train-loss: 0.010578507040109899\n",
      "[LOG 20200502-14:02:04] epoch: 2629 train-loss: 0.010578510247998767\n",
      "[LOG 20200502-14:02:04] epoch: 2630 train-loss: 0.010578514180249639\n",
      "[LOG 20200502-14:02:04] epoch: 2631 train-loss: 0.010578517077697648\n",
      "[LOG 20200502-14:02:05] epoch: 2632 train-loss: 0.010578520699507661\n",
      "[LOG 20200502-14:02:05] epoch: 2633 train-loss: 0.010578523493475385\n",
      "[LOG 20200502-14:02:05] epoch: 2634 train-loss: 0.01057852649440368\n",
      "[LOG 20200502-14:02:05] epoch: 2635 train-loss: 0.010578529598812262\n",
      "[LOG 20200502-14:02:05] epoch: 2636 train-loss: 0.010578533220622275\n",
      "[LOG 20200502-14:02:06] epoch: 2637 train-loss: 0.010578535911109712\n",
      "[LOG 20200502-14:02:06] epoch: 2638 train-loss: 0.010578539015518295\n",
      "[LOG 20200502-14:02:06] epoch: 2639 train-loss: 0.010578541706005732\n",
      "[LOG 20200502-14:02:06] epoch: 2640 train-loss: 0.010578545120855173\n",
      "[LOG 20200502-14:02:06] epoch: 2641 train-loss: 0.010578548121783469\n",
      "[LOG 20200502-14:02:07] epoch: 2642 train-loss: 0.010578551950554052\n",
      "[LOG 20200502-14:02:07] epoch: 2643 train-loss: 0.01057855443408092\n",
      "[LOG 20200502-14:02:07] epoch: 2644 train-loss: 0.010578557641969787\n",
      "[LOG 20200502-14:02:07] epoch: 2645 train-loss: 0.010578560849858655\n",
      "[LOG 20200502-14:02:07] epoch: 2646 train-loss: 0.010578564057747522\n",
      "[LOG 20200502-14:02:08] epoch: 2647 train-loss: 0.010578567162156105\n",
      "[LOG 20200502-14:02:08] epoch: 2648 train-loss: 0.010578570059604116\n",
      "[LOG 20200502-14:02:08] epoch: 2649 train-loss: 0.010578573267492983\n",
      "[LOG 20200502-14:02:08] epoch: 2650 train-loss: 0.010578576061460707\n",
      "[LOG 20200502-14:02:09] epoch: 2651 train-loss: 0.010578579165869288\n",
      "[LOG 20200502-14:02:09] epoch: 2652 train-loss: 0.010578582373758158\n",
      "[LOG 20200502-14:02:09] epoch: 2653 train-loss: 0.010578585374686453\n",
      "[LOG 20200502-14:02:09] epoch: 2654 train-loss: 0.01057858837561475\n",
      "[LOG 20200502-14:02:09] epoch: 2655 train-loss: 0.010578591480023332\n",
      "[LOG 20200502-14:02:10] epoch: 2656 train-loss: 0.010578594170510769\n",
      "[LOG 20200502-14:02:10] epoch: 2657 train-loss: 0.01057859779232078\n",
      "[LOG 20200502-14:02:10] epoch: 2658 train-loss: 0.01057860048280822\n",
      "[LOG 20200502-14:02:10] epoch: 2659 train-loss: 0.010578603483736515\n",
      "[LOG 20200502-14:02:10] epoch: 2660 train-loss: 0.010578606277704239\n",
      "[LOG 20200502-14:02:11] epoch: 2661 train-loss: 0.010578609796033965\n",
      "[LOG 20200502-14:02:11] epoch: 2662 train-loss: 0.01057861259000169\n",
      "[LOG 20200502-14:02:11] epoch: 2663 train-loss: 0.010578615590929985\n",
      "[LOG 20200502-14:02:11] epoch: 2664 train-loss: 0.01057861859185828\n",
      "[LOG 20200502-14:02:12] epoch: 2665 train-loss: 0.010578621592786577\n",
      "[LOG 20200502-14:02:12] epoch: 2666 train-loss: 0.010578624179793729\n",
      "[LOG 20200502-14:02:12] epoch: 2667 train-loss: 0.010578627180722024\n",
      "[LOG 20200502-14:02:12] epoch: 2668 train-loss: 0.01057863069905175\n",
      "[LOG 20200502-14:02:12] epoch: 2669 train-loss: 0.01057863338953919\n",
      "[LOG 20200502-14:02:13] epoch: 2670 train-loss: 0.010578636080026627\n",
      "[LOG 20200502-14:02:13] epoch: 2671 train-loss: 0.010578639391395781\n",
      "[LOG 20200502-14:02:13] epoch: 2672 train-loss: 0.010578641874922646\n",
      "[LOG 20200502-14:02:13] epoch: 2673 train-loss: 0.010578644358449511\n",
      "[LOG 20200502-14:02:13] epoch: 2674 train-loss: 0.010578647669818666\n",
      "[LOG 20200502-14:02:14] epoch: 2675 train-loss: 0.010578650360306105\n",
      "[LOG 20200502-14:02:14] epoch: 2676 train-loss: 0.010578652740352683\n",
      "[LOG 20200502-14:02:14] epoch: 2677 train-loss: 0.010578656155202124\n",
      "[LOG 20200502-14:02:14] epoch: 2678 train-loss: 0.010578658742209276\n",
      "[LOG 20200502-14:02:15] epoch: 2679 train-loss: 0.010578661846617857\n",
      "[LOG 20200502-14:02:15] epoch: 2680 train-loss: 0.010578664847546153\n",
      "[LOG 20200502-14:02:15] epoch: 2681 train-loss: 0.01057866702063216\n",
      "[LOG 20200502-14:02:15] epoch: 2682 train-loss: 0.010578670228521029\n",
      "[LOG 20200502-14:02:15] epoch: 2683 train-loss: 0.010578672608567609\n",
      "[LOG 20200502-14:02:16] epoch: 2684 train-loss: 0.010578675299055047\n",
      "[LOG 20200502-14:02:16] epoch: 2685 train-loss: 0.010578677989542484\n",
      "[LOG 20200502-14:02:16] epoch: 2686 train-loss: 0.010578680886990495\n",
      "[LOG 20200502-14:02:16] epoch: 2687 train-loss: 0.010578683577477932\n",
      "[LOG 20200502-14:02:16] epoch: 2688 train-loss: 0.010578686164485084\n",
      "[LOG 20200502-14:02:17] epoch: 2689 train-loss: 0.010578689061933093\n",
      "[LOG 20200502-14:02:17] epoch: 2690 train-loss: 0.010578691131538816\n",
      "[LOG 20200502-14:02:17] epoch: 2691 train-loss: 0.010578693822026253\n",
      "[LOG 20200502-14:02:17] epoch: 2692 train-loss: 0.01057869651251369\n",
      "[LOG 20200502-14:02:18] epoch: 2693 train-loss: 0.010578699306481414\n",
      "[LOG 20200502-14:02:18] epoch: 2694 train-loss: 0.01057870127260685\n",
      "[LOG 20200502-14:02:18] epoch: 2695 train-loss: 0.010578703756133715\n",
      "[LOG 20200502-14:02:18] epoch: 2696 train-loss: 0.010578706446621153\n",
      "[LOG 20200502-14:02:18] epoch: 2697 train-loss: 0.010578709033628305\n",
      "[LOG 20200502-14:02:19] epoch: 2698 train-loss: 0.01057871182759603\n",
      "[LOG 20200502-14:02:19] epoch: 2699 train-loss: 0.010578713586760892\n",
      "[LOG 20200502-14:02:19] epoch: 2700 train-loss: 0.010578716380728615\n",
      "[LOG 20200502-14:02:19] epoch: 2701 train-loss: 0.010578718553814623\n",
      "[LOG 20200502-14:02:19] epoch: 2702 train-loss: 0.010578720830380917\n",
      "[LOG 20200502-14:02:20] epoch: 2703 train-loss: 0.010578723520868354\n",
      "[LOG 20200502-14:02:20] epoch: 2704 train-loss: 0.010578725797434648\n",
      "[LOG 20200502-14:02:20] epoch: 2705 train-loss: 0.01057872755659951\n",
      "[LOG 20200502-14:02:20] epoch: 2706 train-loss: 0.01057873024708695\n",
      "[LOG 20200502-14:02:21] epoch: 2707 train-loss: 0.010578732109732099\n",
      "[LOG 20200502-14:02:21] epoch: 2708 train-loss: 0.01057873417933782\n",
      "[LOG 20200502-14:02:21] epoch: 2709 train-loss: 0.010578736766344972\n",
      "[LOG 20200502-14:02:21] epoch: 2710 train-loss: 0.01057873914639155\n",
      "[LOG 20200502-14:02:21] epoch: 2711 train-loss: 0.010578740905556414\n",
      "[LOG 20200502-14:02:22] epoch: 2712 train-loss: 0.010578742768201563\n",
      "[LOG 20200502-14:02:22] epoch: 2713 train-loss: 0.010578745148248143\n",
      "[LOG 20200502-14:02:22] epoch: 2714 train-loss: 0.010578746803932719\n",
      "[LOG 20200502-14:02:22] epoch: 2715 train-loss: 0.010578749080499014\n",
      "[LOG 20200502-14:02:22] epoch: 2716 train-loss: 0.010578750736183591\n",
      "[LOG 20200502-14:02:23] epoch: 2717 train-loss: 0.010578752805789312\n",
      "[LOG 20200502-14:02:23] epoch: 2718 train-loss: 0.010578754875395033\n",
      "[LOG 20200502-14:02:23] epoch: 2719 train-loss: 0.010578756841520468\n",
      "[LOG 20200502-14:02:23] epoch: 2720 train-loss: 0.010578759221567048\n",
      "[LOG 20200502-14:02:23] epoch: 2721 train-loss: 0.010578760049409337\n",
      "[LOG 20200502-14:02:24] epoch: 2722 train-loss: 0.010578762429455916\n",
      "[LOG 20200502-14:02:24] epoch: 2723 train-loss: 0.010578763981660208\n",
      "[LOG 20200502-14:02:24] epoch: 2724 train-loss: 0.010578766051265929\n",
      "[LOG 20200502-14:02:24] epoch: 2725 train-loss: 0.010578767499989934\n",
      "[LOG 20200502-14:02:25] epoch: 2726 train-loss: 0.01057876915567451\n",
      "[LOG 20200502-14:02:25] epoch: 2727 train-loss: 0.01057877050091823\n",
      "[LOG 20200502-14:02:25] epoch: 2728 train-loss: 0.010578772156602807\n",
      "[LOG 20200502-14:02:25] epoch: 2729 train-loss: 0.010578773605326811\n",
      "[LOG 20200502-14:02:25] epoch: 2730 train-loss: 0.010578775778412819\n",
      "[LOG 20200502-14:02:26] epoch: 2731 train-loss: 0.010578777227136824\n",
      "[LOG 20200502-14:02:26] epoch: 2732 train-loss: 0.010578777641057968\n",
      "[LOG 20200502-14:02:26] epoch: 2733 train-loss: 0.01057877940022283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:02:26] epoch: 2734 train-loss: 0.010578780641986264\n",
      "[LOG 20200502-14:02:26] epoch: 2735 train-loss: 0.010578781573308839\n",
      "[LOG 20200502-14:02:27] epoch: 2736 train-loss: 0.010578782815072272\n",
      "[LOG 20200502-14:02:27] epoch: 2737 train-loss: 0.01057878416031599\n",
      "[LOG 20200502-14:02:27] epoch: 2738 train-loss: 0.010578784988158278\n",
      "[LOG 20200502-14:02:27] epoch: 2739 train-loss: 0.010578786229921712\n",
      "[LOG 20200502-14:02:27] epoch: 2740 train-loss: 0.010578787161244286\n",
      "[LOG 20200502-14:02:28] epoch: 2741 train-loss: 0.010578787885606289\n",
      "[LOG 20200502-14:02:28] epoch: 2742 train-loss: 0.010578788816928864\n",
      "[LOG 20200502-14:02:28] epoch: 2743 train-loss: 0.01057878995521201\n",
      "[LOG 20200502-14:02:28] epoch: 2744 train-loss: 0.010578790886534585\n",
      "[LOG 20200502-14:02:28] epoch: 2745 train-loss: 0.010578791403936015\n",
      "[LOG 20200502-14:02:29] epoch: 2746 train-loss: 0.010578792231778303\n",
      "[LOG 20200502-14:02:29] epoch: 2747 train-loss: 0.010578792749179734\n",
      "[LOG 20200502-14:02:29] epoch: 2748 train-loss: 0.010578793577022023\n",
      "[LOG 20200502-14:02:29] epoch: 2749 train-loss: 0.010578793990943167\n",
      "[LOG 20200502-14:02:30] epoch: 2750 train-loss: 0.01057879471530517\n",
      "[LOG 20200502-14:02:30] epoch: 2751 train-loss: 0.010578795336186886\n",
      "[LOG 20200502-14:02:30] epoch: 2752 train-loss: 0.010578795543147458\n",
      "[LOG 20200502-14:02:30] epoch: 2753 train-loss: 0.010578796370989747\n",
      "[LOG 20200502-14:02:30] epoch: 2754 train-loss: 0.01057879657795032\n",
      "[LOG 20200502-14:02:31] epoch: 2755 train-loss: 0.010578796784910891\n",
      "[LOG 20200502-14:02:31] epoch: 2756 train-loss: 0.010578797302312322\n",
      "[LOG 20200502-14:02:31] epoch: 2757 train-loss: 0.010578797612753179\n",
      "[LOG 20200502-14:02:31] epoch: 2758 train-loss: 0.010578798026674323\n",
      "[LOG 20200502-14:02:32] epoch: 2759 train-loss: 0.010578798233634897\n",
      "[LOG 20200502-14:02:32] epoch: 2760 train-loss: 0.01057879813015461\n",
      "[LOG 20200502-14:02:32] epoch: 2761 train-loss: 0.010578798337115182\n",
      "[LOG 20200502-14:02:32] epoch: 2762 train-loss: 0.010578798440595468\n",
      "[LOG 20200502-14:02:32] epoch: 2763 train-loss: 0.010578798233634897\n",
      "[LOG 20200502-14:02:33] epoch: 2764 train-loss: 0.010578798854516612\n",
      "[LOG 20200502-14:02:33] epoch: 2765 train-loss: 0.01057879864755604\n",
      "[LOG 20200502-14:02:33] epoch: 2766 train-loss: 0.010578798337115182\n",
      "[LOG 20200502-14:02:33] epoch: 2767 train-loss: 0.010578798337115182\n",
      "[LOG 20200502-14:02:33] epoch: 2768 train-loss: 0.01057879864755604\n",
      "[LOG 20200502-14:02:34] epoch: 2769 train-loss: 0.010578797612753179\n",
      "[LOG 20200502-14:02:34] epoch: 2770 train-loss: 0.010578798026674323\n",
      "[LOG 20200502-14:02:34] epoch: 2771 train-loss: 0.010578797302312322\n",
      "[LOG 20200502-14:02:34] epoch: 2772 train-loss: 0.010578797716233466\n",
      "[LOG 20200502-14:02:34] epoch: 2773 train-loss: 0.010578796888391176\n",
      "[LOG 20200502-14:02:35] epoch: 2774 train-loss: 0.010578796681430604\n",
      "[LOG 20200502-14:02:35] epoch: 2775 train-loss: 0.010578795646627745\n",
      "[LOG 20200502-14:02:35] epoch: 2776 train-loss: 0.010578795853588317\n",
      "[LOG 20200502-14:02:35] epoch: 2777 train-loss: 0.010578795336186886\n",
      "[LOG 20200502-14:02:35] epoch: 2778 train-loss: 0.010578794508344598\n",
      "[LOG 20200502-14:02:36] epoch: 2779 train-loss: 0.010578794611824883\n",
      "[LOG 20200502-14:02:36] epoch: 2780 train-loss: 0.010578793266581165\n",
      "[LOG 20200502-14:02:36] epoch: 2781 train-loss: 0.01057879337006145\n",
      "[LOG 20200502-14:02:36] epoch: 2782 train-loss: 0.010578792645699449\n",
      "[LOG 20200502-14:02:37] epoch: 2783 train-loss: 0.0105787915074163\n",
      "[LOG 20200502-14:02:37] epoch: 2784 train-loss: 0.010578790886534585\n",
      "[LOG 20200502-14:02:37] epoch: 2785 train-loss: 0.010578790265652869\n",
      "[LOG 20200502-14:02:37] epoch: 2786 train-loss: 0.010578789334330294\n",
      "[LOG 20200502-14:02:37] epoch: 2787 train-loss: 0.010578788816928864\n",
      "[LOG 20200502-14:02:38] epoch: 2788 train-loss: 0.010578787678645717\n",
      "[LOG 20200502-14:02:38] epoch: 2789 train-loss: 0.010578786954283714\n",
      "[LOG 20200502-14:02:38] epoch: 2790 train-loss: 0.010578785919480853\n",
      "[LOG 20200502-14:02:38] epoch: 2791 train-loss: 0.010578785195118852\n",
      "[LOG 20200502-14:02:39] epoch: 2792 train-loss: 0.010578783746394847\n",
      "[LOG 20200502-14:02:39] epoch: 2793 train-loss: 0.010578783228993416\n",
      "[LOG 20200502-14:02:39] epoch: 2794 train-loss: 0.01057878178026941\n",
      "[LOG 20200502-14:02:39] epoch: 2795 train-loss: 0.010578780952427123\n",
      "[LOG 20200502-14:02:39] epoch: 2796 train-loss: 0.010578779193262259\n",
      "[LOG 20200502-14:02:40] epoch: 2797 train-loss: 0.010578778986301687\n",
      "[LOG 20200502-14:02:40] epoch: 2798 train-loss: 0.010578777641057968\n",
      "[LOG 20200502-14:02:40] epoch: 2799 train-loss: 0.010578776088853678\n",
      "[LOG 20200502-14:02:40] epoch: 2800 train-loss: 0.010578774950570531\n",
      "[LOG 20200502-14:02:40] epoch: 2801 train-loss: 0.010578773605326811\n",
      "[LOG 20200502-14:02:41] epoch: 2802 train-loss: 0.010578771742681662\n",
      "[LOG 20200502-14:02:41] epoch: 2803 train-loss: 0.010578770811359087\n",
      "[LOG 20200502-14:02:41] epoch: 2804 train-loss: 0.010578769259154797\n",
      "[LOG 20200502-14:02:41] epoch: 2805 train-loss: 0.010578768534792794\n",
      "[LOG 20200502-14:02:42] epoch: 2806 train-loss: 0.010578766672147645\n",
      "[LOG 20200502-14:02:42] epoch: 2807 train-loss: 0.01057876470602221\n",
      "[LOG 20200502-14:02:42] epoch: 2808 train-loss: 0.010578763671219349\n",
      "[LOG 20200502-14:02:42] epoch: 2809 train-loss: 0.010578762429455916\n",
      "[LOG 20200502-14:02:42] epoch: 2810 train-loss: 0.010578760773771338\n",
      "[LOG 20200502-14:02:43] epoch: 2811 train-loss: 0.010578759118086763\n",
      "[LOG 20200502-14:02:43] epoch: 2812 train-loss: 0.01057875756588247\n",
      "[LOG 20200502-14:02:43] epoch: 2813 train-loss: 0.010578755703237321\n",
      "[LOG 20200502-14:02:43] epoch: 2814 train-loss: 0.010578754254513316\n",
      "[LOG 20200502-14:02:43] epoch: 2815 train-loss: 0.010578752495348454\n",
      "[LOG 20200502-14:02:44] epoch: 2816 train-loss: 0.010578751357065307\n",
      "[LOG 20200502-14:02:44] epoch: 2817 train-loss: 0.010578749804861017\n",
      "[LOG 20200502-14:02:44] epoch: 2818 train-loss: 0.010578747631775009\n",
      "[LOG 20200502-14:02:44] epoch: 2819 train-loss: 0.010578745976090431\n",
      "[LOG 20200502-14:02:45] epoch: 2820 train-loss: 0.010578744009964995\n",
      "[LOG 20200502-14:02:45] epoch: 2821 train-loss: 0.010578742457760705\n",
      "[LOG 20200502-14:02:45] epoch: 2822 train-loss: 0.010578740595115555\n",
      "[LOG 20200502-14:02:45] epoch: 2823 train-loss: 0.010578739042911265\n",
      "[LOG 20200502-14:02:45] epoch: 2824 train-loss: 0.010578736662864685\n",
      "[LOG 20200502-14:02:46] epoch: 2825 train-loss: 0.01057873521414068\n",
      "[LOG 20200502-14:02:46] epoch: 2826 train-loss: 0.0105787328340941\n",
      "[LOG 20200502-14:02:46] epoch: 2827 train-loss: 0.010578731178409524\n",
      "[LOG 20200502-14:02:46] epoch: 2828 train-loss: 0.010578729212284088\n",
      "[LOG 20200502-14:02:46] epoch: 2829 train-loss: 0.010578727349638939\n",
      "[LOG 20200502-14:02:47] epoch: 2830 train-loss: 0.010578725797434648\n",
      "[LOG 20200502-14:02:47] epoch: 2831 train-loss: 0.010578723520868354\n",
      "[LOG 20200502-14:02:47] epoch: 2832 train-loss: 0.010578721865183778\n",
      "[LOG 20200502-14:02:47] epoch: 2833 train-loss: 0.010578719588617483\n",
      "[LOG 20200502-14:02:48] epoch: 2834 train-loss: 0.010578717725972334\n",
      "[LOG 20200502-14:02:48] epoch: 2835 train-loss: 0.01057871544940604\n",
      "[LOG 20200502-14:02:48] epoch: 2836 train-loss: 0.010578713483280607\n",
      "[LOG 20200502-14:02:48] epoch: 2837 train-loss: 0.010578711620635457\n",
      "[LOG 20200502-14:02:48] epoch: 2838 train-loss: 0.010578709757990308\n",
      "[LOG 20200502-14:02:49] epoch: 2839 train-loss: 0.010578707377943728\n",
      "[LOG 20200502-14:02:49] epoch: 2840 train-loss: 0.010578705618778864\n",
      "[LOG 20200502-14:02:49] epoch: 2841 train-loss: 0.010578703135251999\n",
      "[LOG 20200502-14:02:49] epoch: 2842 train-loss: 0.010578700962165991\n",
      "[LOG 20200502-14:02:49] epoch: 2843 train-loss: 0.010578698996040557\n",
      "[LOG 20200502-14:02:50] epoch: 2844 train-loss: 0.01057869702991512\n",
      "[LOG 20200502-14:02:50] epoch: 2845 train-loss: 0.010578694546388255\n",
      "[LOG 20200502-14:02:50] epoch: 2846 train-loss: 0.010578692683743106\n",
      "[LOG 20200502-14:02:50] epoch: 2847 train-loss: 0.010578690096735954\n",
      "[LOG 20200502-14:02:50] epoch: 2848 train-loss: 0.010578688544531664\n",
      "[LOG 20200502-14:02:51] epoch: 2849 train-loss: 0.010578685957524512\n",
      "[LOG 20200502-14:02:51] epoch: 2850 train-loss: 0.01057868388791879\n",
      "[LOG 20200502-14:02:51] epoch: 2851 train-loss: 0.01057868181831307\n",
      "[LOG 20200502-14:02:51] epoch: 2852 train-loss: 0.010578679541746775\n",
      "[LOG 20200502-14:02:51] epoch: 2853 train-loss: 0.01057867705821991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:02:52] epoch: 2854 train-loss: 0.010578674988614188\n",
      "[LOG 20200502-14:02:52] epoch: 2855 train-loss: 0.010578672919008467\n",
      "[LOG 20200502-14:02:52] epoch: 2856 train-loss: 0.010578669918080172\n",
      "[LOG 20200502-14:02:52] epoch: 2857 train-loss: 0.010578667641513877\n",
      "[LOG 20200502-14:02:53] epoch: 2858 train-loss: 0.010578664744065868\n",
      "[LOG 20200502-14:02:53] epoch: 2859 train-loss: 0.010578662467499575\n",
      "[LOG 20200502-14:02:53] epoch: 2860 train-loss: 0.010578660397893853\n",
      "[LOG 20200502-14:02:53] epoch: 2861 train-loss: 0.010578657914366987\n",
      "[LOG 20200502-14:02:53] epoch: 2862 train-loss: 0.010578655948241552\n",
      "[LOG 20200502-14:02:54] epoch: 2863 train-loss: 0.010578652533392111\n",
      "[LOG 20200502-14:02:54] epoch: 2864 train-loss: 0.010578650153345533\n",
      "[LOG 20200502-14:02:54] epoch: 2865 train-loss: 0.010578647359377809\n",
      "[LOG 20200502-14:02:54] epoch: 2866 train-loss: 0.010578645082811514\n",
      "[LOG 20200502-14:02:54] epoch: 2867 train-loss: 0.010578642909725508\n",
      "[LOG 20200502-14:02:55] epoch: 2868 train-loss: 0.010578639908797212\n",
      "[LOG 20200502-14:02:55] epoch: 2869 train-loss: 0.010578637114829488\n",
      "[LOG 20200502-14:02:55] epoch: 2870 train-loss: 0.010578635252184339\n",
      "[LOG 20200502-14:02:55] epoch: 2871 train-loss: 0.010578632251256041\n",
      "[LOG 20200502-14:02:55] epoch: 2872 train-loss: 0.01057863018165032\n",
      "[LOG 20200502-14:02:56] epoch: 2873 train-loss: 0.010578626870281167\n",
      "[LOG 20200502-14:02:56] epoch: 2874 train-loss: 0.0105786243867543\n",
      "[LOG 20200502-14:02:56] epoch: 2875 train-loss: 0.010578622213668294\n",
      "[LOG 20200502-14:02:56] epoch: 2876 train-loss: 0.010578620040582286\n",
      "[LOG 20200502-14:02:57] epoch: 2877 train-loss: 0.010578617143134275\n",
      "[LOG 20200502-14:02:57] epoch: 2878 train-loss: 0.01057861465960741\n",
      "[LOG 20200502-14:02:57] epoch: 2879 train-loss: 0.010578611865639687\n",
      "[LOG 20200502-14:02:57] epoch: 2880 train-loss: 0.010578609382112822\n",
      "[LOG 20200502-14:02:57] epoch: 2881 train-loss: 0.010578606277704239\n",
      "[LOG 20200502-14:02:58] epoch: 2882 train-loss: 0.010578604208098518\n",
      "[LOG 20200502-14:02:58] epoch: 2883 train-loss: 0.010578601621091366\n",
      "[LOG 20200502-14:02:58] epoch: 2884 train-loss: 0.010578598827123642\n",
      "[LOG 20200502-14:02:58] epoch: 2885 train-loss: 0.01057859675751792\n",
      "[LOG 20200502-14:02:58] epoch: 2886 train-loss: 0.010578594480951628\n",
      "[LOG 20200502-14:02:59] epoch: 2887 train-loss: 0.010578591790464189\n",
      "[LOG 20200502-14:02:59] epoch: 2888 train-loss: 0.01057858889301618\n",
      "[LOG 20200502-14:02:59] epoch: 2889 train-loss: 0.010578586306009028\n",
      "[LOG 20200502-14:02:59] epoch: 2890 train-loss: 0.010578583925962448\n",
      "[LOG 20200502-14:02:59] epoch: 2891 train-loss: 0.010578581131994724\n",
      "[LOG 20200502-14:03:00] epoch: 2892 train-loss: 0.010578579062389003\n",
      "[LOG 20200502-14:03:00] epoch: 2893 train-loss: 0.01057857626842128\n",
      "[LOG 20200502-14:03:00] epoch: 2894 train-loss: 0.010578573784894414\n",
      "[LOG 20200502-14:03:00] epoch: 2895 train-loss: 0.010578570887446404\n",
      "[LOG 20200502-14:03:00] epoch: 2896 train-loss: 0.01057856861088011\n",
      "[LOG 20200502-14:03:01] epoch: 2897 train-loss: 0.010578565920392672\n",
      "[LOG 20200502-14:03:01] epoch: 2898 train-loss: 0.010578563540346093\n",
      "[LOG 20200502-14:03:01] epoch: 2899 train-loss: 0.010578560849858655\n",
      "[LOG 20200502-14:03:01] epoch: 2900 train-loss: 0.010578558573292362\n",
      "[LOG 20200502-14:03:02] epoch: 2901 train-loss: 0.01057855598628521\n",
      "[LOG 20200502-14:03:02] epoch: 2902 train-loss: 0.01057855360623863\n",
      "[LOG 20200502-14:03:02] epoch: 2903 train-loss: 0.010578551019231478\n",
      "[LOG 20200502-14:03:02] epoch: 2904 train-loss: 0.010578548432224326\n",
      "[LOG 20200502-14:03:02] epoch: 2905 train-loss: 0.010578546052177748\n",
      "[LOG 20200502-14:03:03] epoch: 2906 train-loss: 0.010578543361690309\n",
      "[LOG 20200502-14:03:03] epoch: 2907 train-loss: 0.010578541292084588\n",
      "[LOG 20200502-14:03:03] epoch: 2908 train-loss: 0.010578538187676005\n",
      "[LOG 20200502-14:03:03] epoch: 2909 train-loss: 0.01057853622155057\n",
      "[LOG 20200502-14:03:03] epoch: 2910 train-loss: 0.010578533634543419\n",
      "[LOG 20200502-14:03:04] epoch: 2911 train-loss: 0.010578531357977126\n",
      "[LOG 20200502-14:03:04] epoch: 2912 train-loss: 0.010578528770969974\n",
      "[LOG 20200502-14:03:04] epoch: 2913 train-loss: 0.010578526287443109\n",
      "[LOG 20200502-14:03:04] epoch: 2914 train-loss: 0.010578524010876814\n",
      "[LOG 20200502-14:03:04] epoch: 2915 train-loss: 0.010578521320389377\n",
      "[LOG 20200502-14:03:05] epoch: 2916 train-loss: 0.010578518629901938\n",
      "[LOG 20200502-14:03:05] epoch: 2917 train-loss: 0.010578516146375073\n",
      "[LOG 20200502-14:03:05] epoch: 2918 train-loss: 0.010578513559367921\n",
      "[LOG 20200502-14:03:05] epoch: 2919 train-loss: 0.010578511179321341\n",
      "[LOG 20200502-14:03:06] epoch: 2920 train-loss: 0.010578508695794476\n",
      "[LOG 20200502-14:03:06] epoch: 2921 train-loss: 0.010578506522708468\n",
      "[LOG 20200502-14:03:06] epoch: 2922 train-loss: 0.010578504039181603\n",
      "[LOG 20200502-14:03:06] epoch: 2923 train-loss: 0.01057850124521388\n",
      "[LOG 20200502-14:03:06] epoch: 2924 train-loss: 0.010578499175608158\n",
      "[LOG 20200502-14:03:07] epoch: 2925 train-loss: 0.01057849700252215\n",
      "[LOG 20200502-14:03:07] epoch: 2926 train-loss: 0.010578494415514998\n",
      "[LOG 20200502-14:03:07] epoch: 2927 train-loss: 0.01057849203546842\n",
      "[LOG 20200502-14:03:07] epoch: 2928 train-loss: 0.010578489758902125\n",
      "[LOG 20200502-14:03:07] epoch: 2929 train-loss: 0.010578487068414688\n",
      "[LOG 20200502-14:03:08] epoch: 2930 train-loss: 0.010578484688368108\n",
      "[LOG 20200502-14:03:08] epoch: 2931 train-loss: 0.010578482722242674\n",
      "[LOG 20200502-14:03:08] epoch: 2932 train-loss: 0.01057847992827495\n",
      "[LOG 20200502-14:03:08] epoch: 2933 train-loss: 0.010578477755188942\n",
      "[LOG 20200502-14:03:08] epoch: 2934 train-loss: 0.010578475064701505\n",
      "[LOG 20200502-14:03:09] epoch: 2935 train-loss: 0.010578472995095782\n",
      "[LOG 20200502-14:03:09] epoch: 2936 train-loss: 0.010578470822009776\n",
      "[LOG 20200502-14:03:09] epoch: 2937 train-loss: 0.010578467924561765\n",
      "[LOG 20200502-14:03:09] epoch: 2938 train-loss: 0.010578465854956044\n",
      "[LOG 20200502-14:03:09] epoch: 2939 train-loss: 0.010578463164468607\n",
      "[LOG 20200502-14:03:10] epoch: 2940 train-loss: 0.010578461094862886\n",
      "[LOG 20200502-14:03:10] epoch: 2941 train-loss: 0.01057845881829659\n",
      "[LOG 20200502-14:03:10] epoch: 2942 train-loss: 0.010578456645210585\n",
      "[LOG 20200502-14:03:10] epoch: 2943 train-loss: 0.010578454058203433\n",
      "[LOG 20200502-14:03:10] epoch: 2944 train-loss: 0.010578451885117425\n",
      "[LOG 20200502-14:03:11] epoch: 2945 train-loss: 0.010578449815511703\n",
      "[LOG 20200502-14:03:11] epoch: 2946 train-loss: 0.010578447745905982\n",
      "[LOG 20200502-14:03:11] epoch: 2947 train-loss: 0.010578445262379117\n",
      "[LOG 20200502-14:03:11] epoch: 2948 train-loss: 0.010578442985812822\n",
      "[LOG 20200502-14:03:12] epoch: 2949 train-loss: 0.01057844070924653\n",
      "[LOG 20200502-14:03:12] epoch: 2950 train-loss: 0.010578438225719664\n",
      "[LOG 20200502-14:03:12] epoch: 2951 train-loss: 0.010578436259594228\n",
      "[LOG 20200502-14:03:12] epoch: 2952 train-loss: 0.010578433879547648\n",
      "[LOG 20200502-14:03:12] epoch: 2953 train-loss: 0.010578432016902499\n",
      "[LOG 20200502-14:03:13] epoch: 2954 train-loss: 0.010578429740336206\n",
      "[LOG 20200502-14:03:13] epoch: 2955 train-loss: 0.010578427360289626\n",
      "[LOG 20200502-14:03:13] epoch: 2956 train-loss: 0.010578424773282476\n",
      "[LOG 20200502-14:03:13] epoch: 2957 train-loss: 0.010578422600196468\n",
      "[LOG 20200502-14:03:13] epoch: 2958 train-loss: 0.01057842042711046\n",
      "[LOG 20200502-14:03:14] epoch: 2959 train-loss: 0.010578418460985025\n",
      "[LOG 20200502-14:03:14] epoch: 2960 train-loss: 0.010578416080938445\n",
      "[LOG 20200502-14:03:14] epoch: 2961 train-loss: 0.010578414011332724\n",
      "[LOG 20200502-14:03:14] epoch: 2962 train-loss: 0.010578411734766431\n",
      "[LOG 20200502-14:03:14] epoch: 2963 train-loss: 0.010578409458200136\n",
      "[LOG 20200502-14:03:15] epoch: 2964 train-loss: 0.010578408216436705\n",
      "[LOG 20200502-14:03:15] epoch: 2965 train-loss: 0.010578405318988694\n",
      "[LOG 20200502-14:03:15] epoch: 2966 train-loss: 0.010578403559823832\n",
      "[LOG 20200502-14:03:15] epoch: 2967 train-loss: 0.010578401179777252\n",
      "[LOG 20200502-14:03:16] epoch: 2968 train-loss: 0.010578399213651815\n",
      "[LOG 20200502-14:03:16] epoch: 2969 train-loss: 0.010578396626644664\n",
      "[LOG 20200502-14:03:16] epoch: 2970 train-loss: 0.010578394970960088\n",
      "[LOG 20200502-14:03:16] epoch: 2971 train-loss: 0.010578392487433221\n",
      "[LOG 20200502-14:03:16] epoch: 2972 train-loss: 0.010578390521307787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:03:17] epoch: 2973 train-loss: 0.01057838855518235\n",
      "[LOG 20200502-14:03:17] epoch: 2974 train-loss: 0.010578386382096343\n",
      "[LOG 20200502-14:03:17] epoch: 2975 train-loss: 0.01057838410553005\n",
      "[LOG 20200502-14:03:17] epoch: 2976 train-loss: 0.010578382449845472\n",
      "[LOG 20200502-14:03:17] epoch: 2977 train-loss: 0.010578380069798894\n",
      "[LOG 20200502-14:03:18] epoch: 2978 train-loss: 0.010578378414114317\n",
      "[LOG 20200502-14:03:18] epoch: 2979 train-loss: 0.010578376137548022\n",
      "[LOG 20200502-14:03:18] epoch: 2980 train-loss: 0.0105783740679423\n",
      "[LOG 20200502-14:03:18] epoch: 2981 train-loss: 0.010578372722698582\n",
      "[LOG 20200502-14:03:19] epoch: 2982 train-loss: 0.010578370032211145\n",
      "[LOG 20200502-14:03:19] epoch: 2983 train-loss: 0.01057836806608571\n",
      "[LOG 20200502-14:03:19] epoch: 2984 train-loss: 0.010578365789519416\n",
      "[LOG 20200502-14:03:19] epoch: 2985 train-loss: 0.010578364030354552\n",
      "[LOG 20200502-14:03:19] epoch: 2986 train-loss: 0.010578362064229118\n",
      "[LOG 20200502-14:03:20] epoch: 2987 train-loss: 0.010578359684182538\n",
      "[LOG 20200502-14:03:20] epoch: 2988 train-loss: 0.010578358235458532\n",
      "[LOG 20200502-14:03:20] epoch: 2989 train-loss: 0.01057835595889224\n",
      "[LOG 20200502-14:03:20] epoch: 2990 train-loss: 0.010578353889286518\n",
      "[LOG 20200502-14:03:20] epoch: 2991 train-loss: 0.010578352130121656\n",
      "[LOG 20200502-14:03:21] epoch: 2992 train-loss: 0.01057834985355536\n",
      "[LOG 20200502-14:03:21] epoch: 2993 train-loss: 0.010578347990910212\n",
      "[LOG 20200502-14:03:21] epoch: 2994 train-loss: 0.010578346335225634\n",
      "[LOG 20200502-14:03:21] epoch: 2995 train-loss: 0.010578344576060772\n",
      "[LOG 20200502-14:03:21] epoch: 2996 train-loss: 0.01057834250645505\n",
      "[LOG 20200502-14:03:22] epoch: 2997 train-loss: 0.010578340333369043\n",
      "[LOG 20200502-14:03:22] epoch: 2998 train-loss: 0.010578338470723894\n",
      "[LOG 20200502-14:03:22] epoch: 2999 train-loss: 0.010578336608078744\n",
      "[LOG 20200502-14:03:22] epoch: 3000 train-loss: 0.010578334538473023\n",
      "[LOG 20200502-14:03:23] epoch: 3001 train-loss: 0.01057833277930816\n",
      "[LOG 20200502-14:03:23] epoch: 3002 train-loss: 0.010578331123623583\n",
      "[LOG 20200502-14:03:23] epoch: 3003 train-loss: 0.010578328950537575\n",
      "[LOG 20200502-14:03:23] epoch: 3004 train-loss: 0.01057832750181357\n",
      "[LOG 20200502-14:03:23] epoch: 3005 train-loss: 0.010578325432207849\n",
      "[LOG 20200502-14:03:24] epoch: 3006 train-loss: 0.010578323362602128\n",
      "[LOG 20200502-14:03:24] epoch: 3007 train-loss: 0.01057832170691755\n",
      "[LOG 20200502-14:03:24] epoch: 3008 train-loss: 0.010578320051232973\n",
      "[LOG 20200502-14:03:24] epoch: 3009 train-loss: 0.010578318395548396\n",
      "[LOG 20200502-14:03:24] epoch: 3010 train-loss: 0.010578316118982103\n",
      "[LOG 20200502-14:03:25] epoch: 3011 train-loss: 0.010578314463297525\n",
      "[LOG 20200502-14:03:25] epoch: 3012 train-loss: 0.01057831197977066\n",
      "[LOG 20200502-14:03:25] epoch: 3013 train-loss: 0.010578310324086083\n",
      "[LOG 20200502-14:03:25] epoch: 3014 train-loss: 0.010578308254480362\n",
      "[LOG 20200502-14:03:25] epoch: 3015 train-loss: 0.010578306288354926\n",
      "[LOG 20200502-14:03:26] epoch: 3016 train-loss: 0.01057830411526892\n",
      "[LOG 20200502-14:03:26] epoch: 3017 train-loss: 0.010578302045663198\n",
      "[LOG 20200502-14:03:26] epoch: 3018 train-loss: 0.010578300389978621\n",
      "[LOG 20200502-14:03:26] epoch: 3019 train-loss: 0.010578297906451754\n",
      "[LOG 20200502-14:03:27] epoch: 3020 train-loss: 0.010578296147286892\n",
      "[LOG 20200502-14:03:27] epoch: 3021 train-loss: 0.01057829407768117\n",
      "[LOG 20200502-14:03:27] epoch: 3022 train-loss: 0.01057829200807545\n",
      "[LOG 20200502-14:03:27] epoch: 3023 train-loss: 0.010578289938469728\n",
      "[LOG 20200502-14:03:27] epoch: 3024 train-loss: 0.010578288386265436\n",
      "[LOG 20200502-14:03:28] epoch: 3025 train-loss: 0.010578286316659715\n",
      "[LOG 20200502-14:03:28] epoch: 3026 train-loss: 0.01057828435053428\n",
      "[LOG 20200502-14:03:28] epoch: 3027 train-loss: 0.0105782819704877\n",
      "[LOG 20200502-14:03:28] epoch: 3028 train-loss: 0.010578280314803123\n",
      "[LOG 20200502-14:03:28] epoch: 3029 train-loss: 0.010578278969559405\n",
      "[LOG 20200502-14:03:29] epoch: 3030 train-loss: 0.010578277003433969\n",
      "[LOG 20200502-14:03:29] epoch: 3031 train-loss: 0.010578274933828248\n",
      "[LOG 20200502-14:03:29] epoch: 3032 train-loss: 0.010578273174663385\n",
      "[LOG 20200502-14:03:29] epoch: 3033 train-loss: 0.010578271105057664\n",
      "[LOG 20200502-14:03:29] epoch: 3034 train-loss: 0.010578269035451941\n",
      "[LOG 20200502-14:03:30] epoch: 3035 train-loss: 0.010578267276287079\n",
      "[LOG 20200502-14:03:30] epoch: 3036 train-loss: 0.010578265206681358\n",
      "[LOG 20200502-14:03:30] epoch: 3037 train-loss: 0.01057826386143764\n",
      "[LOG 20200502-14:03:30] epoch: 3038 train-loss: 0.010578261791831918\n",
      "[LOG 20200502-14:03:30] epoch: 3039 train-loss: 0.010578259929186769\n",
      "[LOG 20200502-14:03:31] epoch: 3040 train-loss: 0.010578257652620474\n",
      "[LOG 20200502-14:03:31] epoch: 3041 train-loss: 0.010578256307376755\n",
      "[LOG 20200502-14:03:31] epoch: 3042 train-loss: 0.010578254030810462\n",
      "[LOG 20200502-14:03:31] epoch: 3043 train-loss: 0.010578252685566744\n",
      "[LOG 20200502-14:03:32] epoch: 3044 train-loss: 0.010578250615961023\n",
      "[LOG 20200502-14:03:32] epoch: 3045 train-loss: 0.010578248753315873\n",
      "[LOG 20200502-14:03:32] epoch: 3046 train-loss: 0.010578247304591868\n",
      "[LOG 20200502-14:03:32] epoch: 3047 train-loss: 0.010578245752387576\n",
      "[LOG 20200502-14:03:32] epoch: 3048 train-loss: 0.010578244096703\n",
      "[LOG 20200502-14:03:33] epoch: 3049 train-loss: 0.010578242027097277\n",
      "[LOG 20200502-14:03:33] epoch: 3050 train-loss: 0.010578240267932415\n",
      "[LOG 20200502-14:03:33] epoch: 3051 train-loss: 0.010578238301806979\n",
      "[LOG 20200502-14:03:33] epoch: 3052 train-loss: 0.010578236749602688\n",
      "[LOG 20200502-14:03:33] epoch: 3053 train-loss: 0.010578234990437826\n",
      "[LOG 20200502-14:03:34] epoch: 3054 train-loss: 0.010578233127792677\n",
      "[LOG 20200502-14:03:34] epoch: 3055 train-loss: 0.010578231679068672\n",
      "[LOG 20200502-14:03:34] epoch: 3056 train-loss: 0.010578229919903807\n",
      "[LOG 20200502-14:03:34] epoch: 3057 train-loss: 0.010578228057258658\n",
      "[LOG 20200502-14:03:34] epoch: 3058 train-loss: 0.010578226608534655\n",
      "[LOG 20200502-14:03:35] epoch: 3059 train-loss: 0.010578224952850077\n",
      "[LOG 20200502-14:03:35] epoch: 3060 train-loss: 0.01057822277976407\n",
      "[LOG 20200502-14:03:35] epoch: 3061 train-loss: 0.01057822143452035\n",
      "[LOG 20200502-14:03:35] epoch: 3062 train-loss: 0.010578219571875202\n",
      "[LOG 20200502-14:03:36] epoch: 3063 train-loss: 0.010578217398789194\n",
      "[LOG 20200502-14:03:36] epoch: 3064 train-loss: 0.01057821667442719\n",
      "[LOG 20200502-14:03:36] epoch: 3065 train-loss: 0.01057821460482147\n",
      "[LOG 20200502-14:03:36] epoch: 3066 train-loss: 0.01057821305261718\n",
      "[LOG 20200502-14:03:36] epoch: 3067 train-loss: 0.010578211293452315\n",
      "[LOG 20200502-14:03:37] epoch: 3068 train-loss: 0.010578209534287453\n",
      "[LOG 20200502-14:03:37] epoch: 3069 train-loss: 0.010578208189043734\n",
      "[LOG 20200502-14:03:37] epoch: 3070 train-loss: 0.010578206843800016\n",
      "[LOG 20200502-14:03:37] epoch: 3071 train-loss: 0.01057820487767458\n",
      "[LOG 20200502-14:03:37] epoch: 3072 train-loss: 0.010578203842871718\n",
      "[LOG 20200502-14:03:38] epoch: 3073 train-loss: 0.010578201773265997\n",
      "[LOG 20200502-14:03:38] epoch: 3074 train-loss: 0.010578200221061707\n",
      "[LOG 20200502-14:03:38] epoch: 3075 train-loss: 0.01057819856537713\n",
      "[LOG 20200502-14:03:38] epoch: 3076 train-loss: 0.010578197737534841\n",
      "[LOG 20200502-14:03:38] epoch: 3077 train-loss: 0.010578195978369977\n",
      "[LOG 20200502-14:03:39] epoch: 3078 train-loss: 0.010578194219205115\n",
      "[LOG 20200502-14:03:39] epoch: 3079 train-loss: 0.010578192253079679\n",
      "[LOG 20200502-14:03:39] epoch: 3080 train-loss: 0.010578191011316247\n",
      "[LOG 20200502-14:03:39] epoch: 3081 train-loss: 0.010578189769552814\n",
      "[LOG 20200502-14:03:39] epoch: 3082 train-loss: 0.010578187906907665\n",
      "[LOG 20200502-14:03:40] epoch: 3083 train-loss: 0.010578186354703374\n",
      "[LOG 20200502-14:03:40] epoch: 3084 train-loss: 0.01057818511293994\n",
      "[LOG 20200502-14:03:40] epoch: 3085 train-loss: 0.01057818356073565\n",
      "[LOG 20200502-14:03:40] epoch: 3086 train-loss: 0.010578182318972217\n",
      "[LOG 20200502-14:03:41] epoch: 3087 train-loss: 0.010578180559807353\n",
      "[LOG 20200502-14:03:41] epoch: 3088 train-loss: 0.010578179421524206\n",
      "[LOG 20200502-14:03:41] epoch: 3089 train-loss: 0.010578177558879057\n",
      "[LOG 20200502-14:03:41] epoch: 3090 train-loss: 0.010578176317115625\n",
      "[LOG 20200502-14:03:41] epoch: 3091 train-loss: 0.010578174557950761\n",
      "[LOG 20200502-14:03:42] epoch: 3092 train-loss: 0.010578173419667615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:03:42] epoch: 3093 train-loss: 0.010578171763983037\n",
      "[LOG 20200502-14:03:42] epoch: 3094 train-loss: 0.01057817062569989\n",
      "[LOG 20200502-14:03:42] epoch: 3095 train-loss: 0.010578168970015314\n",
      "[LOG 20200502-14:03:42] epoch: 3096 train-loss: 0.010578167935212454\n",
      "[LOG 20200502-14:03:43] epoch: 3097 train-loss: 0.010578166486488448\n",
      "[LOG 20200502-14:03:43] epoch: 3098 train-loss: 0.010578164727323584\n",
      "[LOG 20200502-14:03:43] epoch: 3099 train-loss: 0.010578163589040438\n",
      "[LOG 20200502-14:03:43] epoch: 3100 train-loss: 0.01057816224379672\n",
      "[LOG 20200502-14:03:43] epoch: 3101 train-loss: 0.010578160691592429\n",
      "[LOG 20200502-14:03:44] epoch: 3102 train-loss: 0.01057815934634871\n",
      "[LOG 20200502-14:03:44] epoch: 3103 train-loss: 0.010578157587183846\n",
      "[LOG 20200502-14:03:44] epoch: 3104 train-loss: 0.010578156862821844\n",
      "[LOG 20200502-14:03:44] epoch: 3105 train-loss: 0.010578155310617553\n",
      "[LOG 20200502-14:03:44] epoch: 3106 train-loss: 0.010578153758413263\n",
      "[LOG 20200502-14:03:45] epoch: 3107 train-loss: 0.010578152620130114\n",
      "[LOG 20200502-14:03:45] epoch: 3108 train-loss: 0.010578151585327255\n",
      "[LOG 20200502-14:03:45] epoch: 3109 train-loss: 0.01057815013660325\n",
      "[LOG 20200502-14:03:45] epoch: 3110 train-loss: 0.010578148377438387\n",
      "[LOG 20200502-14:03:45] epoch: 3111 train-loss: 0.010578147549596097\n",
      "[LOG 20200502-14:03:46] epoch: 3112 train-loss: 0.01057814641131295\n",
      "[LOG 20200502-14:03:46] epoch: 3113 train-loss: 0.010578144652148088\n",
      "[LOG 20200502-14:03:46] epoch: 3114 train-loss: 0.010578143410384655\n",
      "[LOG 20200502-14:03:46] epoch: 3115 train-loss: 0.01057814247906208\n",
      "[LOG 20200502-14:03:47] epoch: 3116 train-loss: 0.010578141340778934\n",
      "[LOG 20200502-14:03:47] epoch: 3117 train-loss: 0.010578139995535215\n",
      "[LOG 20200502-14:03:47] epoch: 3118 train-loss: 0.010578138443330923\n",
      "[LOG 20200502-14:03:47] epoch: 3119 train-loss: 0.01057813771896892\n",
      "[LOG 20200502-14:03:47] epoch: 3120 train-loss: 0.010578136580685774\n",
      "[LOG 20200502-14:03:48] epoch: 3121 train-loss: 0.010578135028481483\n",
      "[LOG 20200502-14:03:48] epoch: 3122 train-loss: 0.010578133579757478\n",
      "[LOG 20200502-14:03:48] epoch: 3123 train-loss: 0.010578132544954618\n",
      "[LOG 20200502-14:03:48] epoch: 3124 train-loss: 0.010578131199710898\n",
      "[LOG 20200502-14:03:49] epoch: 3125 train-loss: 0.010578130164908038\n",
      "[LOG 20200502-14:03:49] epoch: 3126 train-loss: 0.010578129233585464\n",
      "[LOG 20200502-14:03:49] epoch: 3127 train-loss: 0.010578128198782602\n",
      "[LOG 20200502-14:03:49] epoch: 3128 train-loss: 0.01057812643961774\n",
      "[LOG 20200502-14:03:49] epoch: 3129 train-loss: 0.010578126025696596\n",
      "[LOG 20200502-14:03:50] epoch: 3130 train-loss: 0.010578124473492304\n",
      "[LOG 20200502-14:03:50] epoch: 3131 train-loss: 0.010578123128248585\n",
      "[LOG 20200502-14:03:50] epoch: 3132 train-loss: 0.01057812219692601\n",
      "[LOG 20200502-14:03:50] epoch: 3133 train-loss: 0.010578121369083723\n",
      "[LOG 20200502-14:03:50] epoch: 3134 train-loss: 0.010578119713399146\n",
      "[LOG 20200502-14:03:51] epoch: 3135 train-loss: 0.010578118885556856\n",
      "[LOG 20200502-14:03:51] epoch: 3136 train-loss: 0.010578117643793425\n",
      "[LOG 20200502-14:03:51] epoch: 3137 train-loss: 0.01057811671247085\n",
      "[LOG 20200502-14:03:51] epoch: 3138 train-loss: 0.01057811536722713\n",
      "[LOG 20200502-14:03:52] epoch: 3139 train-loss: 0.010578114539384842\n",
      "[LOG 20200502-14:03:52] epoch: 3140 train-loss: 0.010578113194141123\n",
      "[LOG 20200502-14:03:52] epoch: 3141 train-loss: 0.010578112262818549\n",
      "[LOG 20200502-14:03:52] epoch: 3142 train-loss: 0.010578111124535402\n",
      "[LOG 20200502-14:03:52] epoch: 3143 train-loss: 0.010578110193212828\n",
      "[LOG 20200502-14:03:53] epoch: 3144 train-loss: 0.010578109365370538\n",
      "[LOG 20200502-14:03:53] epoch: 3145 train-loss: 0.010578108330567678\n",
      "[LOG 20200502-14:03:53] epoch: 3146 train-loss: 0.01057810770968596\n",
      "[LOG 20200502-14:03:53] epoch: 3147 train-loss: 0.01057810646792253\n",
      "[LOG 20200502-14:03:54] epoch: 3148 train-loss: 0.010578104708757665\n",
      "[LOG 20200502-14:03:54] epoch: 3149 train-loss: 0.01057810326003366\n",
      "[LOG 20200502-14:03:54] epoch: 3150 train-loss: 0.010578102535671659\n",
      "[LOG 20200502-14:03:54] epoch: 3151 train-loss: 0.010578101293908225\n",
      "[LOG 20200502-14:03:54] epoch: 3152 train-loss: 0.01057809984518422\n",
      "[LOG 20200502-14:03:55] epoch: 3153 train-loss: 0.010578098499940502\n",
      "[LOG 20200502-14:03:55] epoch: 3154 train-loss: 0.010578097361657355\n",
      "[LOG 20200502-14:03:55] epoch: 3155 train-loss: 0.010578096326854493\n",
      "[LOG 20200502-14:03:55] epoch: 3156 train-loss: 0.010578095085091062\n",
      "[LOG 20200502-14:03:55] epoch: 3157 train-loss: 0.010578093636367056\n",
      "[LOG 20200502-14:03:56] epoch: 3158 train-loss: 0.010578092394603623\n",
      "[LOG 20200502-14:03:56] epoch: 3159 train-loss: 0.010578091256320477\n",
      "[LOG 20200502-14:03:56] epoch: 3160 train-loss: 0.010578090014557043\n",
      "[LOG 20200502-14:03:56] epoch: 3161 train-loss: 0.010578088979754183\n",
      "[LOG 20200502-14:03:56] epoch: 3162 train-loss: 0.010578087531030178\n",
      "[LOG 20200502-14:03:57] epoch: 3163 train-loss: 0.010578086910148462\n",
      "[LOG 20200502-14:03:57] epoch: 3164 train-loss: 0.010578085254463885\n",
      "[LOG 20200502-14:03:57] epoch: 3165 train-loss: 0.01057808432314131\n",
      "[LOG 20200502-14:03:57] epoch: 3166 train-loss: 0.010578083184858164\n",
      "[LOG 20200502-14:03:57] epoch: 3167 train-loss: 0.010578082150055302\n",
      "[LOG 20200502-14:03:58] epoch: 3168 train-loss: 0.010578080804811584\n",
      "[LOG 20200502-14:03:58] epoch: 3169 train-loss: 0.010578079666528437\n",
      "[LOG 20200502-14:03:58] epoch: 3170 train-loss: 0.010578078321284719\n",
      "[LOG 20200502-14:03:58] epoch: 3171 train-loss: 0.010578077700403001\n",
      "[LOG 20200502-14:03:59] epoch: 3172 train-loss: 0.010578076665600141\n",
      "[LOG 20200502-14:03:59] epoch: 3173 train-loss: 0.010578075527316995\n",
      "[LOG 20200502-14:03:59] epoch: 3174 train-loss: 0.010578074285553562\n",
      "[LOG 20200502-14:03:59] epoch: 3175 train-loss: 0.010578073147270415\n",
      "[LOG 20200502-14:03:59] epoch: 3176 train-loss: 0.01057807221594784\n",
      "[LOG 20200502-14:04:00] epoch: 3177 train-loss: 0.01057807138810555\n",
      "[LOG 20200502-14:04:00] epoch: 3178 train-loss: 0.01057807014634212\n",
      "[LOG 20200502-14:04:00] epoch: 3179 train-loss: 0.010578069215019545\n",
      "[LOG 20200502-14:04:00] epoch: 3180 train-loss: 0.01057806828369697\n",
      "[LOG 20200502-14:04:00] epoch: 3181 train-loss: 0.010578067352374395\n",
      "[LOG 20200502-14:04:01] epoch: 3182 train-loss: 0.01057806642105182\n",
      "[LOG 20200502-14:04:01] epoch: 3183 train-loss: 0.010578064661886957\n",
      "[LOG 20200502-14:04:01] epoch: 3184 train-loss: 0.010578064558406671\n",
      "[LOG 20200502-14:04:01] epoch: 3185 train-loss: 0.01057806352360381\n",
      "[LOG 20200502-14:04:01] epoch: 3186 train-loss: 0.010578062385320663\n",
      "[LOG 20200502-14:04:02] epoch: 3187 train-loss: 0.01057806114355723\n",
      "[LOG 20200502-14:04:02] epoch: 3188 train-loss: 0.010578060626155801\n",
      "[LOG 20200502-14:04:02] epoch: 3189 train-loss: 0.010578059694833226\n",
      "[LOG 20200502-14:04:02] epoch: 3190 train-loss: 0.010578058660030365\n",
      "[LOG 20200502-14:04:02] epoch: 3191 train-loss: 0.01057805803914865\n",
      "[LOG 20200502-14:04:03] epoch: 3192 train-loss: 0.010578057107826075\n",
      "[LOG 20200502-14:04:03] epoch: 3193 train-loss: 0.010578056279983785\n",
      "[LOG 20200502-14:04:03] epoch: 3194 train-loss: 0.010578055555621782\n",
      "[LOG 20200502-14:04:03] epoch: 3195 train-loss: 0.010578054520818923\n",
      "[LOG 20200502-14:04:04] epoch: 3196 train-loss: 0.010578053692976633\n",
      "[LOG 20200502-14:04:04] epoch: 3197 train-loss: 0.010578053072094917\n",
      "[LOG 20200502-14:04:04] epoch: 3198 train-loss: 0.01057805224425263\n",
      "[LOG 20200502-14:04:04] epoch: 3199 train-loss: 0.01057805141641034\n",
      "[LOG 20200502-14:04:04] epoch: 3200 train-loss: 0.01057805089900891\n",
      "[LOG 20200502-14:04:05] epoch: 3201 train-loss: 0.01057804955376519\n",
      "[LOG 20200502-14:04:05] epoch: 3202 train-loss: 0.010578048829403188\n",
      "[LOG 20200502-14:04:05] epoch: 3203 train-loss: 0.010578048518962331\n",
      "[LOG 20200502-14:04:05] epoch: 3204 train-loss: 0.010578047587639756\n",
      "[LOG 20200502-14:04:05] epoch: 3205 train-loss: 0.010578046656317182\n",
      "[LOG 20200502-14:04:06] epoch: 3206 train-loss: 0.010578046345876323\n",
      "[LOG 20200502-14:04:06] epoch: 3207 train-loss: 0.010578045414553748\n",
      "[LOG 20200502-14:04:06] epoch: 3208 train-loss: 0.010578044793672033\n",
      "[LOG 20200502-14:04:06] epoch: 3209 train-loss: 0.010578044586711459\n",
      "[LOG 20200502-14:04:07] epoch: 3210 train-loss: 0.010578043448428312\n",
      "[LOG 20200502-14:04:07] epoch: 3211 train-loss: 0.01057804272406631\n",
      "[LOG 20200502-14:04:07] epoch: 3212 train-loss: 0.010578042413625453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:04:07] epoch: 3213 train-loss: 0.010578041482302878\n",
      "[LOG 20200502-14:04:07] epoch: 3214 train-loss: 0.010578041378822591\n",
      "[LOG 20200502-14:04:08] epoch: 3215 train-loss: 0.010578040550980303\n",
      "[LOG 20200502-14:04:08] epoch: 3216 train-loss: 0.010578040033578873\n",
      "[LOG 20200502-14:04:08] epoch: 3217 train-loss: 0.010578039412697157\n",
      "[LOG 20200502-14:04:08] epoch: 3218 train-loss: 0.010578039205736585\n",
      "[LOG 20200502-14:04:09] epoch: 3219 train-loss: 0.010578038067453437\n",
      "[LOG 20200502-14:04:09] epoch: 3220 train-loss: 0.010578037860492865\n",
      "[LOG 20200502-14:04:09] epoch: 3221 train-loss: 0.010578037032650577\n",
      "[LOG 20200502-14:04:09] epoch: 3222 train-loss: 0.010578036825690005\n",
      "[LOG 20200502-14:04:10] epoch: 3223 train-loss: 0.010578036515249146\n",
      "[LOG 20200502-14:04:10] epoch: 3224 train-loss: 0.010578035480446286\n",
      "[LOG 20200502-14:04:10] epoch: 3225 train-loss: 0.010578035170005428\n",
      "[LOG 20200502-14:04:10] epoch: 3226 train-loss: 0.010578034445643425\n",
      "[LOG 20200502-14:04:11] epoch: 3227 train-loss: 0.010578034445643425\n",
      "[LOG 20200502-14:04:11] epoch: 3228 train-loss: 0.010578033721281422\n",
      "[LOG 20200502-14:04:11] epoch: 3229 train-loss: 0.01057803351432085\n",
      "[LOG 20200502-14:04:11] epoch: 3230 train-loss: 0.01057803351432085\n",
      "[LOG 20200502-14:04:12] epoch: 3231 train-loss: 0.010578032789958848\n",
      "[LOG 20200502-14:04:12] epoch: 3232 train-loss: 0.010578031858636273\n",
      "[LOG 20200502-14:04:12] epoch: 3233 train-loss: 0.01057803196211656\n",
      "[LOG 20200502-14:04:12] epoch: 3234 train-loss: 0.01057803196211656\n",
      "[LOG 20200502-14:04:13] epoch: 3235 train-loss: 0.01057803113427427\n",
      "[LOG 20200502-14:04:13] epoch: 3236 train-loss: 0.010578031030793985\n",
      "[LOG 20200502-14:04:13] epoch: 3237 train-loss: 0.010578030823833413\n",
      "[LOG 20200502-14:04:14] epoch: 3238 train-loss: 0.010578030202951696\n",
      "[LOG 20200502-14:04:14] epoch: 3239 train-loss: 0.010578030306431983\n",
      "[LOG 20200502-14:04:14] epoch: 3240 train-loss: 0.010578029892510839\n",
      "[LOG 20200502-14:04:14] epoch: 3241 train-loss: 0.010578029685550265\n",
      "[LOG 20200502-14:04:15] epoch: 3242 train-loss: 0.010578029271629121\n",
      "[LOG 20200502-14:04:15] epoch: 3243 train-loss: 0.010578028961188264\n",
      "[LOG 20200502-14:04:15] epoch: 3244 train-loss: 0.010578028443786833\n",
      "[LOG 20200502-14:04:15] epoch: 3245 train-loss: 0.010578028650747405\n",
      "[LOG 20200502-14:04:15] epoch: 3246 train-loss: 0.010578028547267119\n",
      "[LOG 20200502-14:04:16] epoch: 3247 train-loss: 0.010578028443786833\n",
      "[LOG 20200502-14:04:16] epoch: 3248 train-loss: 0.010578028133345975\n",
      "[LOG 20200502-14:04:16] epoch: 3249 train-loss: 0.010578027822905116\n",
      "[LOG 20200502-14:04:17] epoch: 3250 train-loss: 0.010578027926385403\n",
      "[LOG 20200502-14:04:17] epoch: 3251 train-loss: 0.010578027822905116\n",
      "[LOG 20200502-14:04:17] epoch: 3252 train-loss: 0.010578027512464259\n",
      "[LOG 20200502-14:04:17] epoch: 3253 train-loss: 0.010578027615944544\n",
      "[LOG 20200502-14:04:18] epoch: 3254 train-loss: 0.010578027305503687\n",
      "[LOG 20200502-14:04:18] epoch: 3255 train-loss: 0.01057802771942483\n",
      "[LOG 20200502-14:04:18] epoch: 3256 train-loss: 0.010578027305503687\n",
      "[LOG 20200502-14:04:18] epoch: 3257 train-loss: 0.010578027512464259\n",
      "[LOG 20200502-14:04:18] epoch: 3258 train-loss: 0.010578027512464259\n",
      "[LOG 20200502-14:04:19] epoch: 3259 train-loss: 0.010578027926385403\n",
      "[LOG 20200502-14:04:19] epoch: 3260 train-loss: 0.010578027305503687\n",
      "[LOG 20200502-14:04:19] epoch: 3261 train-loss: 0.010578027408983972\n",
      "[LOG 20200502-14:04:19] epoch: 3262 train-loss: 0.010578027408983972\n",
      "[LOG 20200502-14:04:20] epoch: 3263 train-loss: 0.010578027408983972\n",
      "[LOG 20200502-14:04:20] epoch: 3264 train-loss: 0.010578026477661397\n",
      "[LOG 20200502-14:04:20] epoch: 3265 train-loss: 0.010578026477661397\n",
      "[LOG 20200502-14:04:20] epoch: 3266 train-loss: 0.010578026891582541\n",
      "[LOG 20200502-14:04:21] epoch: 3267 train-loss: 0.010578026270700825\n",
      "[LOG 20200502-14:04:21] epoch: 3268 train-loss: 0.010578025856779682\n",
      "[LOG 20200502-14:04:21] epoch: 3269 train-loss: 0.010578025753299395\n",
      "[LOG 20200502-14:04:22] epoch: 3270 train-loss: 0.010578026063740253\n",
      "[LOG 20200502-14:04:22] epoch: 3271 train-loss: 0.010578025546338823\n",
      "[LOG 20200502-14:04:22] epoch: 3272 train-loss: 0.01057802564981911\n",
      "[LOG 20200502-14:04:22] epoch: 3273 train-loss: 0.010578025132417679\n",
      "[LOG 20200502-14:04:22] epoch: 3274 train-loss: 0.01057802533937825\n",
      "[LOG 20200502-14:04:23] epoch: 3275 train-loss: 0.010578024718496535\n",
      "[LOG 20200502-14:04:23] epoch: 3276 train-loss: 0.01057802482197682\n",
      "[LOG 20200502-14:04:23] epoch: 3277 train-loss: 0.010578024925457107\n",
      "[LOG 20200502-14:04:24] epoch: 3278 train-loss: 0.010578025028937392\n",
      "[LOG 20200502-14:04:24] epoch: 3279 train-loss: 0.010578024615016248\n",
      "[LOG 20200502-14:04:24] epoch: 3280 train-loss: 0.010578024925457107\n",
      "[LOG 20200502-14:04:24] epoch: 3281 train-loss: 0.010578024925457107\n",
      "[LOG 20200502-14:04:25] epoch: 3282 train-loss: 0.010578024615016248\n",
      "[LOG 20200502-14:04:25] epoch: 3283 train-loss: 0.010578025132417679\n",
      "[LOG 20200502-14:04:25] epoch: 3284 train-loss: 0.010578024925457107\n",
      "[LOG 20200502-14:04:25] epoch: 3285 train-loss: 0.01057802482197682\n",
      "[LOG 20200502-14:04:26] epoch: 3286 train-loss: 0.01057802482197682\n",
      "[LOG 20200502-14:04:26] epoch: 3287 train-loss: 0.010578025028937392\n",
      "[LOG 20200502-14:04:26] epoch: 3288 train-loss: 0.010578025235897966\n",
      "[LOG 20200502-14:04:26] epoch: 3289 train-loss: 0.010578025442858538\n",
      "[LOG 20200502-14:04:26] epoch: 3290 train-loss: 0.010578025132417679\n",
      "[LOG 20200502-14:04:27] epoch: 3291 train-loss: 0.01057802533937825\n",
      "[LOG 20200502-14:04:27] epoch: 3292 train-loss: 0.010578025442858538\n",
      "[LOG 20200502-14:04:27] epoch: 3293 train-loss: 0.01057802564981911\n",
      "[LOG 20200502-14:04:27] epoch: 3294 train-loss: 0.01057802533937825\n",
      "[LOG 20200502-14:04:28] epoch: 3295 train-loss: 0.010578026270700825\n",
      "[LOG 20200502-14:04:28] epoch: 3296 train-loss: 0.010578025546338823\n",
      "[LOG 20200502-14:04:28] epoch: 3297 train-loss: 0.010578026063740253\n",
      "[LOG 20200502-14:04:28] epoch: 3298 train-loss: 0.010578026270700825\n",
      "[LOG 20200502-14:04:29] epoch: 3299 train-loss: 0.010578026581141684\n",
      "[LOG 20200502-14:04:29] epoch: 3300 train-loss: 0.010578026891582541\n",
      "[LOG 20200502-14:04:29] epoch: 3301 train-loss: 0.0105780272020234\n",
      "[LOG 20200502-14:04:29] epoch: 3302 train-loss: 0.010578027098543115\n",
      "[LOG 20200502-14:04:29] epoch: 3303 train-loss: 0.010578027512464259\n",
      "[LOG 20200502-14:04:30] epoch: 3304 train-loss: 0.010578027926385403\n",
      "[LOG 20200502-14:04:30] epoch: 3305 train-loss: 0.010578028340306547\n",
      "[LOG 20200502-14:04:30] epoch: 3306 train-loss: 0.010578028650747405\n",
      "[LOG 20200502-14:04:30] epoch: 3307 train-loss: 0.010578028547267119\n",
      "[LOG 20200502-14:04:31] epoch: 3308 train-loss: 0.010578029375109408\n",
      "[LOG 20200502-14:04:31] epoch: 3309 train-loss: 0.010578029478589693\n",
      "[LOG 20200502-14:04:31] epoch: 3310 train-loss: 0.010578029995991124\n",
      "[LOG 20200502-14:04:31] epoch: 3311 train-loss: 0.010578029789030552\n",
      "[LOG 20200502-14:04:31] epoch: 3312 train-loss: 0.010578030202951696\n",
      "[LOG 20200502-14:04:32] epoch: 3313 train-loss: 0.010578030823833413\n",
      "[LOG 20200502-14:04:32] epoch: 3314 train-loss: 0.010578030823833413\n",
      "[LOG 20200502-14:04:32] epoch: 3315 train-loss: 0.010578031548195414\n",
      "[LOG 20200502-14:04:32] epoch: 3316 train-loss: 0.01057803196211656\n",
      "[LOG 20200502-14:04:33] epoch: 3317 train-loss: 0.010578032169077132\n",
      "[LOG 20200502-14:04:33] epoch: 3318 train-loss: 0.010578033100399706\n",
      "[LOG 20200502-14:04:33] epoch: 3319 train-loss: 0.010578033617801137\n",
      "[LOG 20200502-14:04:33] epoch: 3320 train-loss: 0.010578033721281422\n",
      "[LOG 20200502-14:04:33] epoch: 3321 train-loss: 0.010578034342163138\n",
      "[LOG 20200502-14:04:34] epoch: 3322 train-loss: 0.010578034549123712\n",
      "[LOG 20200502-14:04:34] epoch: 3323 train-loss: 0.01057803506652514\n",
      "[LOG 20200502-14:04:34] epoch: 3324 train-loss: 0.010578035376966\n",
      "[LOG 20200502-14:04:34] epoch: 3325 train-loss: 0.010578036308288574\n",
      "[LOG 20200502-14:04:35] epoch: 3326 train-loss: 0.010578036722209718\n",
      "[LOG 20200502-14:04:35] epoch: 3327 train-loss: 0.010578037239611149\n",
      "[LOG 20200502-14:04:35] epoch: 3328 train-loss: 0.010578037963973152\n",
      "[LOG 20200502-14:04:35] epoch: 3329 train-loss: 0.01057803827441401\n",
      "[LOG 20200502-14:04:35] epoch: 3330 train-loss: 0.01057803879181544\n",
      "[LOG 20200502-14:04:36] epoch: 3331 train-loss: 0.010578039930098586\n",
      "[LOG 20200502-14:04:36] epoch: 3332 train-loss: 0.010578040344019731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:04:36] epoch: 3333 train-loss: 0.010578040964901447\n",
      "[LOG 20200502-14:04:36] epoch: 3334 train-loss: 0.01057804117186202\n",
      "[LOG 20200502-14:04:37] epoch: 3335 train-loss: 0.010578041792743735\n",
      "[LOG 20200502-14:04:37] epoch: 3336 train-loss: 0.010578042620586025\n",
      "[LOG 20200502-14:04:37] epoch: 3337 train-loss: 0.010578043034507168\n",
      "[LOG 20200502-14:04:37] epoch: 3338 train-loss: 0.010578043965829743\n",
      "[LOG 20200502-14:04:38] epoch: 3339 train-loss: 0.010578044793672033\n",
      "[LOG 20200502-14:04:38] epoch: 3340 train-loss: 0.010578045518034034\n",
      "[LOG 20200502-14:04:38] epoch: 3341 train-loss: 0.010578046449356608\n",
      "[LOG 20200502-14:04:38] epoch: 3342 train-loss: 0.010578046552836895\n",
      "[LOG 20200502-14:04:38] epoch: 3343 train-loss: 0.010578047587639756\n",
      "[LOG 20200502-14:04:39] epoch: 3344 train-loss: 0.010578048725922903\n",
      "[LOG 20200502-14:04:39] epoch: 3345 train-loss: 0.010578048932883475\n",
      "[LOG 20200502-14:04:39] epoch: 3346 train-loss: 0.01057804955376519\n",
      "[LOG 20200502-14:04:39] epoch: 3347 train-loss: 0.010578050692048337\n",
      "[LOG 20200502-14:04:40] epoch: 3348 train-loss: 0.010578051519890627\n",
      "[LOG 20200502-14:04:40] epoch: 3349 train-loss: 0.010578052140772343\n",
      "[LOG 20200502-14:04:40] epoch: 3350 train-loss: 0.01057805327905549\n",
      "[LOG 20200502-14:04:40] epoch: 3351 train-loss: 0.010578053382535776\n",
      "[LOG 20200502-14:04:40] epoch: 3352 train-loss: 0.01057805483125978\n",
      "[LOG 20200502-14:04:41] epoch: 3353 train-loss: 0.01057805565910207\n",
      "[LOG 20200502-14:04:41] epoch: 3354 train-loss: 0.010578056797385216\n",
      "[LOG 20200502-14:04:41] epoch: 3355 train-loss: 0.010578057625227504\n",
      "[LOG 20200502-14:04:41] epoch: 3356 train-loss: 0.010578058453069793\n",
      "[LOG 20200502-14:04:42] epoch: 3357 train-loss: 0.010578058763510652\n",
      "[LOG 20200502-14:04:42] epoch: 3358 train-loss: 0.01057806010875437\n",
      "[LOG 20200502-14:04:42] epoch: 3359 train-loss: 0.010578061040076945\n",
      "[LOG 20200502-14:04:42] epoch: 3360 train-loss: 0.010578062074879805\n",
      "[LOG 20200502-14:04:42] epoch: 3361 train-loss: 0.010578062799241807\n",
      "[LOG 20200502-14:04:43] epoch: 3362 train-loss: 0.010578063213162951\n",
      "[LOG 20200502-14:04:43] epoch: 3363 train-loss: 0.01057806352360381\n",
      "[LOG 20200502-14:04:43] epoch: 3364 train-loss: 0.010578064454926385\n",
      "[LOG 20200502-14:04:43] epoch: 3365 train-loss: 0.010578064765367243\n",
      "[LOG 20200502-14:04:44] epoch: 3366 train-loss: 0.010578066007130675\n",
      "[LOG 20200502-14:04:44] epoch: 3367 train-loss: 0.010578066110610962\n",
      "[LOG 20200502-14:04:44] epoch: 3368 train-loss: 0.010578067145413823\n",
      "[LOG 20200502-14:04:44] epoch: 3369 train-loss: 0.010578067973256111\n",
      "[LOG 20200502-14:04:44] epoch: 3370 train-loss: 0.010578068490657542\n",
      "[LOG 20200502-14:04:45] epoch: 3371 train-loss: 0.010578069111539258\n",
      "[LOG 20200502-14:04:45] epoch: 3372 train-loss: 0.010578070042861832\n",
      "[LOG 20200502-14:04:45] epoch: 3373 train-loss: 0.010578070663743548\n",
      "[LOG 20200502-14:04:45] epoch: 3374 train-loss: 0.01057807138810555\n",
      "[LOG 20200502-14:04:46] epoch: 3375 train-loss: 0.010578072008987268\n",
      "[LOG 20200502-14:04:46] epoch: 3376 train-loss: 0.010578072733349271\n",
      "[LOG 20200502-14:04:46] epoch: 3377 train-loss: 0.0105780732507507\n",
      "[LOG 20200502-14:04:46] epoch: 3378 train-loss: 0.01057807459599442\n",
      "[LOG 20200502-14:04:46] epoch: 3379 train-loss: 0.010578075009915564\n",
      "[LOG 20200502-14:04:47] epoch: 3380 train-loss: 0.010578076044718424\n",
      "[LOG 20200502-14:04:47] epoch: 3381 train-loss: 0.010578076562119855\n",
      "[LOG 20200502-14:04:47] epoch: 3382 train-loss: 0.01057807749344243\n",
      "[LOG 20200502-14:04:47] epoch: 3383 train-loss: 0.010578078321284719\n",
      "[LOG 20200502-14:04:47] epoch: 3384 train-loss: 0.01057807956304815\n",
      "[LOG 20200502-14:04:48] epoch: 3385 train-loss: 0.010578080494370725\n",
      "[LOG 20200502-14:04:48] epoch: 3386 train-loss: 0.010578081322213015\n",
      "[LOG 20200502-14:04:48] epoch: 3387 train-loss: 0.0105780814256933\n",
      "[LOG 20200502-14:04:48] epoch: 3388 train-loss: 0.010578082977897592\n",
      "[LOG 20200502-14:04:49] epoch: 3389 train-loss: 0.010578083909220166\n",
      "[LOG 20200502-14:04:49] epoch: 3390 train-loss: 0.010578084633582167\n",
      "[LOG 20200502-14:04:49] epoch: 3391 train-loss: 0.010578085771865316\n",
      "[LOG 20200502-14:04:49] epoch: 3392 train-loss: 0.01057808670318789\n",
      "[LOG 20200502-14:04:50] epoch: 3393 train-loss: 0.01057808773799075\n",
      "[LOG 20200502-14:04:50] epoch: 3394 train-loss: 0.010578088151911894\n",
      "[LOG 20200502-14:04:50] epoch: 3395 train-loss: 0.010578089704116186\n",
      "[LOG 20200502-14:04:50] epoch: 3396 train-loss: 0.010578090738919046\n",
      "[LOG 20200502-14:04:51] epoch: 3397 train-loss: 0.010578091463281048\n",
      "[LOG 20200502-14:04:51] epoch: 3398 train-loss: 0.010578092291123338\n",
      "[LOG 20200502-14:04:51] epoch: 3399 train-loss: 0.010578093325926198\n",
      "[LOG 20200502-14:04:51] epoch: 3400 train-loss: 0.010578094671169916\n",
      "[LOG 20200502-14:04:51] epoch: 3401 train-loss: 0.010578095705972778\n",
      "[LOG 20200502-14:04:52] epoch: 3402 train-loss: 0.010578096844255924\n",
      "[LOG 20200502-14:04:52] epoch: 3403 train-loss: 0.010578098396460215\n",
      "[LOG 20200502-14:04:52] epoch: 3404 train-loss: 0.010578098396460215\n",
      "[LOG 20200502-14:04:53] epoch: 3405 train-loss: 0.010578100155625079\n",
      "[LOG 20200502-14:04:53] epoch: 3406 train-loss: 0.010578101190427939\n",
      "[LOG 20200502-14:04:53] epoch: 3407 train-loss: 0.010578101604349084\n",
      "[LOG 20200502-14:04:53] epoch: 3408 train-loss: 0.010578103156553375\n",
      "[LOG 20200502-14:04:54] epoch: 3409 train-loss: 0.010578104398316808\n",
      "[LOG 20200502-14:04:54] epoch: 3410 train-loss: 0.010578105433119668\n",
      "[LOG 20200502-14:04:54] epoch: 3411 train-loss: 0.010578106260961957\n",
      "[LOG 20200502-14:04:54] epoch: 3412 train-loss: 0.01057810770968596\n",
      "[LOG 20200502-14:04:55] epoch: 3413 train-loss: 0.010578108744488822\n",
      "[LOG 20200502-14:04:55] epoch: 3414 train-loss: 0.010578110193212828\n",
      "[LOG 20200502-14:04:55] epoch: 3415 train-loss: 0.010578111124535402\n",
      "[LOG 20200502-14:04:55] epoch: 3416 train-loss: 0.010578112055857977\n",
      "[LOG 20200502-14:04:55] epoch: 3417 train-loss: 0.010578113918503126\n",
      "[LOG 20200502-14:04:56] epoch: 3418 train-loss: 0.010578114642865129\n",
      "[LOG 20200502-14:04:56] epoch: 3419 train-loss: 0.010578115470707417\n",
      "[LOG 20200502-14:04:56] epoch: 3420 train-loss: 0.010578117126391994\n",
      "[LOG 20200502-14:04:56] epoch: 3421 train-loss: 0.010578118057714568\n",
      "[LOG 20200502-14:04:57] epoch: 3422 train-loss: 0.010578119713399146\n",
      "[LOG 20200502-14:04:57] epoch: 3423 train-loss: 0.01057812064472172\n",
      "[LOG 20200502-14:04:57] epoch: 3424 train-loss: 0.010578121886485152\n",
      "[LOG 20200502-14:04:57] epoch: 3425 train-loss: 0.010578123645650016\n",
      "[LOG 20200502-14:04:57] epoch: 3426 train-loss: 0.010578125197854307\n",
      "[LOG 20200502-14:04:58] epoch: 3427 train-loss: 0.010578126025696596\n",
      "[LOG 20200502-14:04:58] epoch: 3428 train-loss: 0.010578127784861458\n",
      "[LOG 20200502-14:04:58] epoch: 3429 train-loss: 0.010578129026624892\n",
      "[LOG 20200502-14:04:58] epoch: 3430 train-loss: 0.010578130164908038\n",
      "[LOG 20200502-14:04:58] epoch: 3431 train-loss: 0.010578131717112329\n",
      "[LOG 20200502-14:04:59] epoch: 3432 train-loss: 0.010578132544954618\n",
      "[LOG 20200502-14:04:59] epoch: 3433 train-loss: 0.010578133993678622\n",
      "[LOG 20200502-14:04:59] epoch: 3434 train-loss: 0.010578135959804058\n",
      "[LOG 20200502-14:04:59] epoch: 3435 train-loss: 0.010578136477205489\n",
      "[LOG 20200502-14:05:00] epoch: 3436 train-loss: 0.01057813854681121\n",
      "[LOG 20200502-14:05:00] epoch: 3437 train-loss: 0.010578139788574643\n",
      "[LOG 20200502-14:05:00] epoch: 3438 train-loss: 0.010578141237298647\n",
      "[LOG 20200502-14:05:00] epoch: 3439 train-loss: 0.010578142582542367\n",
      "[LOG 20200502-14:05:00] epoch: 3440 train-loss: 0.010578143824305799\n",
      "[LOG 20200502-14:05:01] epoch: 3441 train-loss: 0.010578145686950948\n",
      "[LOG 20200502-14:05:01] epoch: 3442 train-loss: 0.01057814723915524\n",
      "[LOG 20200502-14:05:01] epoch: 3443 train-loss: 0.010578148480918672\n",
      "[LOG 20200502-14:05:01] epoch: 3444 train-loss: 0.01057814982616239\n",
      "[LOG 20200502-14:05:02] epoch: 3445 train-loss: 0.010578151792287827\n",
      "[LOG 20200502-14:05:02] epoch: 3446 train-loss: 0.01057815303405126\n",
      "[LOG 20200502-14:05:02] epoch: 3447 train-loss: 0.010578155000176694\n",
      "[LOG 20200502-14:05:02] epoch: 3448 train-loss: 0.010578155724538697\n",
      "[LOG 20200502-14:05:02] epoch: 3449 train-loss: 0.01057815696630213\n",
      "[LOG 20200502-14:05:03] epoch: 3450 train-loss: 0.010578158208065562\n",
      "[LOG 20200502-14:05:03] epoch: 3451 train-loss: 0.010578159967230426\n",
      "[LOG 20200502-14:05:03] epoch: 3452 train-loss: 0.010578160691592429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:05:03] epoch: 3453 train-loss: 0.01057816193335586\n",
      "[LOG 20200502-14:05:03] epoch: 3454 train-loss: 0.010578162968158722\n",
      "[LOG 20200502-14:05:04] epoch: 3455 train-loss: 0.010578164209922155\n",
      "[LOG 20200502-14:05:04] epoch: 3456 train-loss: 0.010578165658646159\n",
      "[LOG 20200502-14:05:04] epoch: 3457 train-loss: 0.010578166900409592\n",
      "[LOG 20200502-14:05:04] epoch: 3458 train-loss: 0.010578168142173026\n",
      "[LOG 20200502-14:05:05] epoch: 3459 train-loss: 0.010578169590897031\n",
      "[LOG 20200502-14:05:05] epoch: 3460 train-loss: 0.010578170729180178\n",
      "[LOG 20200502-14:05:05] epoch: 3461 train-loss: 0.010578172695305612\n",
      "[LOG 20200502-14:05:05] epoch: 3462 train-loss: 0.010578173626628187\n",
      "[LOG 20200502-14:05:05] epoch: 3463 train-loss: 0.010578174557950761\n",
      "[LOG 20200502-14:05:06] epoch: 3464 train-loss: 0.010578176006674767\n",
      "[LOG 20200502-14:05:06] epoch: 3465 train-loss: 0.010578177869319916\n",
      "[LOG 20200502-14:05:06] epoch: 3466 train-loss: 0.010578178697162204\n",
      "[LOG 20200502-14:05:06] epoch: 3467 train-loss: 0.010578179835445352\n",
      "[LOG 20200502-14:05:06] epoch: 3468 train-loss: 0.010578181491129927\n",
      "[LOG 20200502-14:05:07] epoch: 3469 train-loss: 0.010578183146814505\n",
      "[LOG 20200502-14:05:07] epoch: 3470 train-loss: 0.010578184285097651\n",
      "[LOG 20200502-14:05:07] epoch: 3471 train-loss: 0.010578185837301943\n",
      "[LOG 20200502-14:05:07] epoch: 3472 train-loss: 0.010578187596466806\n",
      "[LOG 20200502-14:05:08] epoch: 3473 train-loss: 0.010578188734749952\n",
      "[LOG 20200502-14:05:08] epoch: 3474 train-loss: 0.010578190183473958\n",
      "[LOG 20200502-14:05:08] epoch: 3475 train-loss: 0.010578191735678248\n",
      "[LOG 20200502-14:05:08] epoch: 3476 train-loss: 0.01057819328788254\n",
      "[LOG 20200502-14:05:08] epoch: 3477 train-loss: 0.010578194633126259\n",
      "[LOG 20200502-14:05:09] epoch: 3478 train-loss: 0.010578196495771408\n",
      "[LOG 20200502-14:05:09] epoch: 3479 train-loss: 0.010578197737534841\n",
      "[LOG 20200502-14:05:09] epoch: 3480 train-loss: 0.010578199186258845\n",
      "[LOG 20200502-14:05:09] epoch: 3481 train-loss: 0.010578200428022278\n",
      "[LOG 20200502-14:05:09] epoch: 3482 train-loss: 0.010578202083706856\n",
      "[LOG 20200502-14:05:10] epoch: 3483 train-loss: 0.010578203842871718\n",
      "[LOG 20200502-14:05:10] epoch: 3484 train-loss: 0.010578205188115438\n",
      "[LOG 20200502-14:05:10] epoch: 3485 train-loss: 0.010578206843800016\n",
      "[LOG 20200502-14:05:10] epoch: 3486 train-loss: 0.010578208396004306\n",
      "[LOG 20200502-14:05:10] epoch: 3487 train-loss: 0.010578209844728311\n",
      "[LOG 20200502-14:05:11] epoch: 3488 train-loss: 0.010578211500412889\n",
      "[LOG 20200502-14:05:11] epoch: 3489 train-loss: 0.010578213363058038\n",
      "[LOG 20200502-14:05:11] epoch: 3490 train-loss: 0.010578214708301757\n",
      "[LOG 20200502-14:05:11] epoch: 3491 train-loss: 0.010578216157025762\n",
      "[LOG 20200502-14:05:12] epoch: 3492 train-loss: 0.010578217916190624\n",
      "[LOG 20200502-14:05:12] epoch: 3493 train-loss: 0.010578219985796345\n",
      "[LOG 20200502-14:05:12] epoch: 3494 train-loss: 0.01057822143452035\n",
      "[LOG 20200502-14:05:12] epoch: 3495 train-loss: 0.010578222986724641\n",
      "[LOG 20200502-14:05:12] epoch: 3496 train-loss: 0.010578224435448647\n",
      "[LOG 20200502-14:05:13] epoch: 3497 train-loss: 0.010578225780692365\n",
      "[LOG 20200502-14:05:13] epoch: 3498 train-loss: 0.010578228471179804\n",
      "[LOG 20200502-14:05:13] epoch: 3499 train-loss: 0.010578229712943235\n",
      "[LOG 20200502-14:05:13] epoch: 3500 train-loss: 0.0105782314721081\n",
      "[LOG 20200502-14:05:13] epoch: 3501 train-loss: 0.010578232920832105\n",
      "[LOG 20200502-14:05:14] epoch: 3502 train-loss: 0.010578235093918111\n",
      "[LOG 20200502-14:05:14] epoch: 3503 train-loss: 0.010578236646122403\n",
      "[LOG 20200502-14:05:14] epoch: 3504 train-loss: 0.010578238301806979\n",
      "[LOG 20200502-14:05:14] epoch: 3505 train-loss: 0.010578240267932415\n",
      "[LOG 20200502-14:05:14] epoch: 3506 train-loss: 0.010578242130577564\n",
      "[LOG 20200502-14:05:15] epoch: 3507 train-loss: 0.010578243786262142\n",
      "[LOG 20200502-14:05:15] epoch: 3508 train-loss: 0.01057824564890729\n",
      "[LOG 20200502-14:05:15] epoch: 3509 train-loss: 0.010578247304591868\n",
      "[LOG 20200502-14:05:15] epoch: 3510 train-loss: 0.010578249167237017\n",
      "[LOG 20200502-14:05:16] epoch: 3511 train-loss: 0.010578250719441308\n",
      "[LOG 20200502-14:05:16] epoch: 3512 train-loss: 0.010578252271645598\n",
      "[LOG 20200502-14:05:16] epoch: 3513 train-loss: 0.010578254755172465\n",
      "[LOG 20200502-14:05:16] epoch: 3514 train-loss: 0.010578256617817614\n",
      "[LOG 20200502-14:05:17] epoch: 3515 train-loss: 0.01057825806654162\n",
      "[LOG 20200502-14:05:17] epoch: 3516 train-loss: 0.010578259929186769\n",
      "[LOG 20200502-14:05:17] epoch: 3517 train-loss: 0.010578262102272775\n",
      "[LOG 20200502-14:05:17] epoch: 3518 train-loss: 0.010578263757957352\n",
      "[LOG 20200502-14:05:18] epoch: 3519 train-loss: 0.010578265827563074\n",
      "[LOG 20200502-14:05:18] epoch: 3520 train-loss: 0.01057826748324765\n",
      "[LOG 20200502-14:05:18] epoch: 3521 train-loss: 0.010578269035451941\n",
      "[LOG 20200502-14:05:18] epoch: 3522 train-loss: 0.010578271001577377\n",
      "[LOG 20200502-14:05:18] epoch: 3523 train-loss: 0.010578273692064814\n",
      "[LOG 20200502-14:05:19] epoch: 3524 train-loss: 0.01057827514078882\n",
      "[LOG 20200502-14:05:19] epoch: 3525 train-loss: 0.010578277106914256\n",
      "[LOG 20200502-14:05:19] epoch: 3526 train-loss: 0.010578278866079118\n",
      "[LOG 20200502-14:05:19] epoch: 3527 train-loss: 0.010578280521763695\n",
      "[LOG 20200502-14:05:19] epoch: 3528 train-loss: 0.010578282487889131\n",
      "[LOG 20200502-14:05:20] epoch: 3529 train-loss: 0.010578284143573709\n",
      "[LOG 20200502-14:05:20] epoch: 3530 train-loss: 0.01057828538533714\n",
      "[LOG 20200502-14:05:20] epoch: 3531 train-loss: 0.01057828724798229\n",
      "[LOG 20200502-14:05:20] epoch: 3532 train-loss: 0.010578288696706295\n",
      "[LOG 20200502-14:05:21] epoch: 3533 train-loss: 0.010578290455871157\n",
      "[LOG 20200502-14:05:21] epoch: 3534 train-loss: 0.010578292111555735\n",
      "[LOG 20200502-14:05:21] epoch: 3535 train-loss: 0.010578293767240312\n",
      "[LOG 20200502-14:05:21] epoch: 3536 train-loss: 0.01057829542292489\n",
      "[LOG 20200502-14:05:21] epoch: 3537 train-loss: 0.01057829645772775\n",
      "[LOG 20200502-14:05:22] epoch: 3538 train-loss: 0.010578298734294044\n",
      "[LOG 20200502-14:05:22] epoch: 3539 train-loss: 0.010578300389978621\n",
      "[LOG 20200502-14:05:22] epoch: 3540 train-loss: 0.010578302149143484\n",
      "[LOG 20200502-14:05:22] epoch: 3541 train-loss: 0.010578303494387202\n",
      "[LOG 20200502-14:05:22] epoch: 3542 train-loss: 0.010578305563992925\n",
      "[LOG 20200502-14:05:23] epoch: 3543 train-loss: 0.010578306805756357\n",
      "[LOG 20200502-14:05:23] epoch: 3544 train-loss: 0.010578308978842365\n",
      "[LOG 20200502-14:05:23] epoch: 3545 train-loss: 0.010578310531046655\n",
      "[LOG 20200502-14:05:23] epoch: 3546 train-loss: 0.010578312807612948\n",
      "[LOG 20200502-14:05:23] epoch: 3547 train-loss: 0.010578314256336954\n",
      "[LOG 20200502-14:05:24] epoch: 3548 train-loss: 0.010578316015501818\n",
      "[LOG 20200502-14:05:24] epoch: 3549 train-loss: 0.010578317671186395\n",
      "[LOG 20200502-14:05:24] epoch: 3550 train-loss: 0.010578319844272401\n",
      "[LOG 20200502-14:05:24] epoch: 3551 train-loss: 0.01057832118951612\n",
      "[LOG 20200502-14:05:25] epoch: 3552 train-loss: 0.0105783235695627\n",
      "[LOG 20200502-14:05:25] epoch: 3553 train-loss: 0.010578324604365561\n",
      "[LOG 20200502-14:05:25] epoch: 3554 train-loss: 0.010578326673971282\n",
      "[LOG 20200502-14:05:25] epoch: 3555 train-loss: 0.010578328640096717\n",
      "[LOG 20200502-14:05:25] epoch: 3556 train-loss: 0.010578330916663011\n",
      "[LOG 20200502-14:05:26] epoch: 3557 train-loss: 0.010578332675827874\n",
      "[LOG 20200502-14:05:26] epoch: 3558 train-loss: 0.010578334745433595\n",
      "[LOG 20200502-14:05:26] epoch: 3559 train-loss: 0.010578336297637887\n",
      "[LOG 20200502-14:05:26] epoch: 3560 train-loss: 0.01057833805680275\n",
      "[LOG 20200502-14:05:27] epoch: 3561 train-loss: 0.010578339919447899\n",
      "[LOG 20200502-14:05:27] epoch: 3562 train-loss: 0.01057834198905362\n",
      "[LOG 20200502-14:05:27] epoch: 3563 train-loss: 0.010578343955179056\n",
      "[LOG 20200502-14:05:27] epoch: 3564 train-loss: 0.010578346335225634\n",
      "[LOG 20200502-14:05:28] epoch: 3565 train-loss: 0.010578347887429927\n",
      "[LOG 20200502-14:05:28] epoch: 3566 train-loss: 0.010578349750075076\n",
      "[LOG 20200502-14:05:28] epoch: 3567 train-loss: 0.010578351612720225\n",
      "[LOG 20200502-14:05:28] epoch: 3568 train-loss: 0.010578353785806231\n",
      "[LOG 20200502-14:05:28] epoch: 3569 train-loss: 0.010578355544971095\n",
      "[LOG 20200502-14:05:29] epoch: 3570 train-loss: 0.01057835751109653\n",
      "[LOG 20200502-14:05:29] epoch: 3571 train-loss: 0.010578359684182538\n",
      "[LOG 20200502-14:05:29] epoch: 3572 train-loss: 0.01057836196074883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:05:29] epoch: 3573 train-loss: 0.010578363512953123\n",
      "[LOG 20200502-14:05:29] epoch: 3574 train-loss: 0.010578365272117985\n",
      "[LOG 20200502-14:05:30] epoch: 3575 train-loss: 0.010578367652164565\n",
      "[LOG 20200502-14:05:30] epoch: 3576 train-loss: 0.010578369514809715\n",
      "[LOG 20200502-14:05:30] epoch: 3577 train-loss: 0.010578371791376008\n",
      "[LOG 20200502-14:05:30] epoch: 3578 train-loss: 0.010578373757501444\n",
      "[LOG 20200502-14:05:30] epoch: 3579 train-loss: 0.010578375827107165\n",
      "[LOG 20200502-14:05:31] epoch: 3580 train-loss: 0.01057837831063403\n",
      "[LOG 20200502-14:05:31] epoch: 3581 train-loss: 0.010578379759358035\n",
      "[LOG 20200502-14:05:31] epoch: 3582 train-loss: 0.0105783822428849\n",
      "[LOG 20200502-14:05:31] epoch: 3583 train-loss: 0.01057838410553005\n",
      "[LOG 20200502-14:05:31] epoch: 3584 train-loss: 0.010578386278616058\n",
      "[LOG 20200502-14:05:32] epoch: 3585 train-loss: 0.01057838803778092\n",
      "[LOG 20200502-14:05:32] epoch: 3586 train-loss: 0.010578390314347215\n",
      "[LOG 20200502-14:05:32] epoch: 3587 train-loss: 0.01057839228047265\n",
      "[LOG 20200502-14:05:32] epoch: 3588 train-loss: 0.010578394867479801\n",
      "[LOG 20200502-14:05:33] epoch: 3589 train-loss: 0.010578396833605237\n",
      "[LOG 20200502-14:05:33] epoch: 3590 train-loss: 0.010578398799730672\n",
      "[LOG 20200502-14:05:33] epoch: 3591 train-loss: 0.010578401386737823\n",
      "[LOG 20200502-14:05:33] epoch: 3592 train-loss: 0.010578403249382973\n",
      "[LOG 20200502-14:05:33] epoch: 3593 train-loss: 0.01057840542246898\n",
      "[LOG 20200502-14:05:34] epoch: 3594 train-loss: 0.010578407595554987\n",
      "[LOG 20200502-14:05:34] epoch: 3595 train-loss: 0.010578409872121282\n",
      "[LOG 20200502-14:05:34] epoch: 3596 train-loss: 0.010578411734766431\n",
      "[LOG 20200502-14:05:34] epoch: 3597 train-loss: 0.010578413804372152\n",
      "[LOG 20200502-14:05:35] epoch: 3598 train-loss: 0.010578415253096156\n",
      "[LOG 20200502-14:05:35] epoch: 3599 train-loss: 0.010578417115741305\n",
      "[LOG 20200502-14:05:35] epoch: 3600 train-loss: 0.01057841887490617\n",
      "[LOG 20200502-14:05:35] epoch: 3601 train-loss: 0.010578420737551318\n",
      "[LOG 20200502-14:05:35] epoch: 3602 train-loss: 0.01057842249671618\n",
      "[LOG 20200502-14:05:36] epoch: 3603 train-loss: 0.01057842435936133\n",
      "[LOG 20200502-14:05:36] epoch: 3604 train-loss: 0.010578425911565622\n",
      "[LOG 20200502-14:05:36] epoch: 3605 train-loss: 0.010578427981171343\n",
      "[LOG 20200502-14:05:36] epoch: 3606 train-loss: 0.01057843015425735\n",
      "[LOG 20200502-14:05:36] epoch: 3607 train-loss: 0.010578431809941927\n",
      "[LOG 20200502-14:05:37] epoch: 3608 train-loss: 0.01057843336214622\n",
      "[LOG 20200502-14:05:37] epoch: 3609 train-loss: 0.010578435638712512\n",
      "[LOG 20200502-14:05:37] epoch: 3610 train-loss: 0.010578437708318233\n",
      "[LOG 20200502-14:05:37] epoch: 3611 train-loss: 0.010578439053561952\n",
      "[LOG 20200502-14:05:38] epoch: 3612 train-loss: 0.010578441433608532\n",
      "[LOG 20200502-14:05:38] epoch: 3613 train-loss: 0.010578442985812822\n",
      "[LOG 20200502-14:05:38] epoch: 3614 train-loss: 0.0105784446414974\n",
      "[LOG 20200502-14:05:38] epoch: 3615 train-loss: 0.010578446193701692\n",
      "[LOG 20200502-14:05:38] epoch: 3616 train-loss: 0.010578448677228557\n",
      "[LOG 20200502-14:05:39] epoch: 3617 train-loss: 0.010578450746834278\n",
      "[LOG 20200502-14:05:39] epoch: 3618 train-loss: 0.010578452402518855\n",
      "[LOG 20200502-14:05:39] epoch: 3619 train-loss: 0.01057845488604572\n",
      "[LOG 20200502-14:05:39] epoch: 3620 train-loss: 0.010578456541730298\n",
      "[LOG 20200502-14:05:39] epoch: 3621 train-loss: 0.010578458197414875\n",
      "[LOG 20200502-14:05:40] epoch: 3622 train-loss: 0.010578460060060024\n",
      "[LOG 20200502-14:05:40] epoch: 3623 train-loss: 0.010578461922705173\n",
      "[LOG 20200502-14:05:40] epoch: 3624 train-loss: 0.010578463992310895\n",
      "[LOG 20200502-14:05:40] epoch: 3625 train-loss: 0.010578466268877188\n",
      "[LOG 20200502-14:05:40] epoch: 3626 train-loss: 0.010578468028042052\n",
      "[LOG 20200502-14:05:41] epoch: 3627 train-loss: 0.010578470304608345\n",
      "[LOG 20200502-14:05:41] epoch: 3628 train-loss: 0.010578471856812635\n",
      "[LOG 20200502-14:05:41] epoch: 3629 train-loss: 0.010578473822938072\n",
      "[LOG 20200502-14:05:41] epoch: 3630 train-loss: 0.010578476306464937\n",
      "[LOG 20200502-14:05:41] epoch: 3631 train-loss: 0.010578477755188942\n",
      "[LOG 20200502-14:05:42] epoch: 3632 train-loss: 0.010578479824794663\n",
      "[LOG 20200502-14:05:42] epoch: 3633 train-loss: 0.010578481894400384\n",
      "[LOG 20200502-14:05:42] epoch: 3634 train-loss: 0.010578484067486392\n",
      "[LOG 20200502-14:05:42] epoch: 3635 train-loss: 0.010578485826651255\n",
      "[LOG 20200502-14:05:43] epoch: 3636 train-loss: 0.010578487896256976\n",
      "[LOG 20200502-14:05:43] epoch: 3637 train-loss: 0.010578489862382412\n",
      "[LOG 20200502-14:05:43] epoch: 3638 train-loss: 0.010578491621547274\n",
      "[LOG 20200502-14:05:43] epoch: 3639 train-loss: 0.01057849389811357\n",
      "[LOG 20200502-14:05:43] epoch: 3640 train-loss: 0.01057849596771929\n",
      "[LOG 20200502-14:05:44] epoch: 3641 train-loss: 0.010578497623403868\n",
      "[LOG 20200502-14:05:44] epoch: 3642 train-loss: 0.010578500003450446\n",
      "[LOG 20200502-14:05:44] epoch: 3643 train-loss: 0.01057850176261531\n",
      "[LOG 20200502-14:05:44] epoch: 3644 train-loss: 0.010578504039181603\n",
      "[LOG 20200502-14:05:45] epoch: 3645 train-loss: 0.010578506522708468\n",
      "[LOG 20200502-14:05:45] epoch: 3646 train-loss: 0.010578508178393045\n",
      "[LOG 20200502-14:05:45] epoch: 3647 train-loss: 0.010578509730597338\n",
      "[LOG 20200502-14:05:45] epoch: 3648 train-loss: 0.010578511903683344\n",
      "[LOG 20200502-14:05:45] epoch: 3649 train-loss: 0.01057851386980878\n",
      "[LOG 20200502-14:05:46] epoch: 3650 train-loss: 0.010578516146375073\n",
      "[LOG 20200502-14:05:46] epoch: 3651 train-loss: 0.010578518319461081\n",
      "[LOG 20200502-14:05:46] epoch: 3652 train-loss: 0.010578519871665372\n",
      "[LOG 20200502-14:05:46] epoch: 3653 train-loss: 0.010578522251711952\n",
      "[LOG 20200502-14:05:46] epoch: 3654 train-loss: 0.01057852442479796\n",
      "[LOG 20200502-14:05:47] epoch: 3655 train-loss: 0.01057852597700225\n",
      "[LOG 20200502-14:05:47] epoch: 3656 train-loss: 0.010578528046607971\n",
      "[LOG 20200502-14:05:47] epoch: 3657 train-loss: 0.010578530633615123\n",
      "[LOG 20200502-14:05:47] epoch: 3658 train-loss: 0.010578532185819414\n",
      "[LOG 20200502-14:05:48] epoch: 3659 train-loss: 0.010578534048464563\n",
      "[LOG 20200502-14:05:48] epoch: 3660 train-loss: 0.010578535393708281\n",
      "[LOG 20200502-14:05:48] epoch: 3661 train-loss: 0.010578536945912573\n",
      "[LOG 20200502-14:05:48] epoch: 3662 train-loss: 0.010578538912038008\n",
      "[LOG 20200502-14:05:48] epoch: 3663 train-loss: 0.010578540257281728\n",
      "[LOG 20200502-14:05:49] epoch: 3664 train-loss: 0.010578541912966304\n",
      "[LOG 20200502-14:05:49] epoch: 3665 train-loss: 0.010578543672131168\n",
      "[LOG 20200502-14:05:49] epoch: 3666 train-loss: 0.010578545224335458\n",
      "[LOG 20200502-14:05:49] epoch: 3667 train-loss: 0.010578546983500322\n",
      "[LOG 20200502-14:05:49] epoch: 3668 train-loss: 0.010578548432224326\n",
      "[LOG 20200502-14:05:50] epoch: 3669 train-loss: 0.010578550294869475\n",
      "[LOG 20200502-14:05:50] epoch: 3670 train-loss: 0.010578551122711765\n",
      "[LOG 20200502-14:05:50] epoch: 3671 train-loss: 0.010578553088837199\n",
      "[LOG 20200502-14:05:50] epoch: 3672 train-loss: 0.010578554744521776\n",
      "[LOG 20200502-14:05:50] epoch: 3673 train-loss: 0.010578556089765496\n",
      "[LOG 20200502-14:05:51] epoch: 3674 train-loss: 0.01057855836633179\n",
      "[LOG 20200502-14:05:51] epoch: 3675 train-loss: 0.010578559815055795\n",
      "[LOG 20200502-14:05:51] epoch: 3676 train-loss: 0.01057856147074037\n",
      "[LOG 20200502-14:05:51] epoch: 3677 train-loss: 0.010578563229905235\n",
      "[LOG 20200502-14:05:52] epoch: 3678 train-loss: 0.010578564264708094\n",
      "[LOG 20200502-14:05:52] epoch: 3679 train-loss: 0.010578566334313817\n",
      "[LOG 20200502-14:05:52] epoch: 3680 train-loss: 0.010578567783037821\n",
      "[LOG 20200502-14:05:52] epoch: 3681 train-loss: 0.01057856964568297\n",
      "[LOG 20200502-14:05:52] epoch: 3682 train-loss: 0.01057857099092669\n",
      "[LOG 20200502-14:05:53] epoch: 3683 train-loss: 0.010578572646611266\n",
      "[LOG 20200502-14:05:53] epoch: 3684 train-loss: 0.010578574509256415\n",
      "[LOG 20200502-14:05:53] epoch: 3685 train-loss: 0.01057857626842128\n",
      "[LOG 20200502-14:05:53] epoch: 3686 train-loss: 0.010578577510184713\n",
      "[LOG 20200502-14:05:53] epoch: 3687 train-loss: 0.010578579269349575\n",
      "[LOG 20200502-14:05:54] epoch: 3688 train-loss: 0.01057858071807358\n",
      "[LOG 20200502-14:05:54] epoch: 3689 train-loss: 0.010578582166797586\n",
      "[LOG 20200502-14:05:54] epoch: 3690 train-loss: 0.010578583822482161\n",
      "[LOG 20200502-14:05:54] epoch: 3691 train-loss: 0.010578585478166739\n",
      "[LOG 20200502-14:05:54] epoch: 3692 train-loss: 0.010578587133851316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:05:55] epoch: 3693 train-loss: 0.010578588582575321\n",
      "[LOG 20200502-14:05:55] epoch: 3694 train-loss: 0.010578590031299327\n",
      "[LOG 20200502-14:05:55] epoch: 3695 train-loss: 0.010578591790464189\n",
      "[LOG 20200502-14:05:55] epoch: 3696 train-loss: 0.010578593239188194\n",
      "[LOG 20200502-14:05:56] epoch: 3697 train-loss: 0.01057859520531363\n",
      "[LOG 20200502-14:05:56] epoch: 3698 train-loss: 0.010578596136636205\n",
      "[LOG 20200502-14:05:56] epoch: 3699 train-loss: 0.01057859810276164\n",
      "[LOG 20200502-14:05:56] epoch: 3700 train-loss: 0.010578599448005358\n",
      "[LOG 20200502-14:05:56] epoch: 3701 train-loss: 0.010578601103689935\n",
      "[LOG 20200502-14:05:57] epoch: 3702 train-loss: 0.010578602241973082\n",
      "[LOG 20200502-14:05:57] epoch: 3703 train-loss: 0.010578604001137946\n",
      "[LOG 20200502-14:05:57] epoch: 3704 train-loss: 0.010578605449861951\n",
      "[LOG 20200502-14:05:57] epoch: 3705 train-loss: 0.010578607105546527\n",
      "[LOG 20200502-14:05:58] epoch: 3706 train-loss: 0.010578608968191676\n",
      "[LOG 20200502-14:05:58] epoch: 3707 train-loss: 0.01057860989951425\n",
      "[LOG 20200502-14:05:58] epoch: 3708 train-loss: 0.010578611451718543\n",
      "[LOG 20200502-14:05:58] epoch: 3709 train-loss: 0.01057861310740312\n",
      "[LOG 20200502-14:05:58] epoch: 3710 train-loss: 0.01057861465960741\n",
      "[LOG 20200502-14:05:59] epoch: 3711 train-loss: 0.010578616004851129\n",
      "[LOG 20200502-14:05:59] epoch: 3712 train-loss: 0.010578617350094847\n",
      "[LOG 20200502-14:05:59] epoch: 3713 train-loss: 0.010578619005779425\n",
      "[LOG 20200502-14:05:59] epoch: 3714 train-loss: 0.01057862045450343\n",
      "[LOG 20200502-14:05:59] epoch: 3715 train-loss: 0.010578621696266863\n",
      "[LOG 20200502-14:06:00] epoch: 3716 train-loss: 0.01057862283455001\n",
      "[LOG 20200502-14:06:00] epoch: 3717 train-loss: 0.010578623765872585\n",
      "[LOG 20200502-14:06:00] epoch: 3718 train-loss: 0.01057862469719516\n",
      "[LOG 20200502-14:06:00] epoch: 3719 train-loss: 0.010578625628517734\n",
      "[LOG 20200502-14:06:01] epoch: 3720 train-loss: 0.010578626352879737\n",
      "[LOG 20200502-14:06:01] epoch: 3721 train-loss: 0.010578627387682596\n",
      "[LOG 20200502-14:06:01] epoch: 3722 train-loss: 0.010578628422485458\n",
      "[LOG 20200502-14:06:01] epoch: 3723 train-loss: 0.010578629457288317\n",
      "[LOG 20200502-14:06:02] epoch: 3724 train-loss: 0.010578630802532038\n",
      "[LOG 20200502-14:06:02] epoch: 3725 train-loss: 0.010578631112972895\n",
      "[LOG 20200502-14:06:02] epoch: 3726 train-loss: 0.010578632458216615\n",
      "[LOG 20200502-14:06:02] epoch: 3727 train-loss: 0.010578633596499762\n",
      "[LOG 20200502-14:06:03] epoch: 3728 train-loss: 0.010578634631302621\n",
      "[LOG 20200502-14:06:03] epoch: 3729 train-loss: 0.010578634838263193\n",
      "[LOG 20200502-14:06:03] epoch: 3730 train-loss: 0.010578636080026627\n",
      "[LOG 20200502-14:06:03] epoch: 3731 train-loss: 0.010578637218309773\n",
      "[LOG 20200502-14:06:04] epoch: 3732 train-loss: 0.010578637839191489\n",
      "[LOG 20200502-14:06:04] epoch: 3733 train-loss: 0.01057863835659292\n",
      "[LOG 20200502-14:06:04] epoch: 3734 train-loss: 0.01057863918443521\n",
      "[LOG 20200502-14:06:04] epoch: 3735 train-loss: 0.010578640115757784\n",
      "[LOG 20200502-14:06:04] epoch: 3736 train-loss: 0.010578641357521215\n",
      "[LOG 20200502-14:06:05] epoch: 3737 train-loss: 0.010578641874922646\n",
      "[LOG 20200502-14:06:05] epoch: 3738 train-loss: 0.010578642392324077\n",
      "[LOG 20200502-14:06:05] epoch: 3739 train-loss: 0.01057864363408751\n",
      "[LOG 20200502-14:06:05] epoch: 3740 train-loss: 0.010578644048008654\n",
      "[LOG 20200502-14:06:06] epoch: 3741 train-loss: 0.010578644461929798\n",
      "[LOG 20200502-14:06:06] epoch: 3742 train-loss: 0.010578645393252373\n",
      "[LOG 20200502-14:06:06] epoch: 3743 train-loss: 0.010578646428055234\n",
      "[LOG 20200502-14:06:06] epoch: 3744 train-loss: 0.010578646738496091\n",
      "[LOG 20200502-14:06:06] epoch: 3745 train-loss: 0.01057864756633838\n",
      "[LOG 20200502-14:06:07] epoch: 3746 train-loss: 0.010578648394180669\n",
      "[LOG 20200502-14:06:07] epoch: 3747 train-loss: 0.010578649015062384\n",
      "[LOG 20200502-14:06:07] epoch: 3748 train-loss: 0.010578649222022958\n",
      "[LOG 20200502-14:06:07] epoch: 3749 train-loss: 0.010578649842904674\n",
      "[LOG 20200502-14:06:08] epoch: 3750 train-loss: 0.01057865098118782\n",
      "[LOG 20200502-14:06:08] epoch: 3751 train-loss: 0.010578651602069536\n",
      "[LOG 20200502-14:06:08] epoch: 3752 train-loss: 0.010578651912510395\n",
      "[LOG 20200502-14:06:08] epoch: 3753 train-loss: 0.010578652429911826\n",
      "[LOG 20200502-14:06:08] epoch: 3754 train-loss: 0.010578653154273828\n",
      "[LOG 20200502-14:06:09] epoch: 3755 train-loss: 0.010578653568194972\n",
      "[LOG 20200502-14:06:09] epoch: 3756 train-loss: 0.010578654292556975\n",
      "[LOG 20200502-14:06:09] epoch: 3757 train-loss: 0.010578654602997832\n",
      "[LOG 20200502-14:06:09] epoch: 3758 train-loss: 0.01057865491343869\n",
      "[LOG 20200502-14:06:09] epoch: 3759 train-loss: 0.010578655430840122\n",
      "[LOG 20200502-14:06:10] epoch: 3760 train-loss: 0.010578655948241552\n",
      "[LOG 20200502-14:06:10] epoch: 3761 train-loss: 0.010578656362162696\n",
      "[LOG 20200502-14:06:10] epoch: 3762 train-loss: 0.01057865677608384\n",
      "[LOG 20200502-14:06:10] epoch: 3763 train-loss: 0.010578657086524699\n",
      "[LOG 20200502-14:06:11] epoch: 3764 train-loss: 0.01057865760392613\n",
      "[LOG 20200502-14:06:11] epoch: 3765 train-loss: 0.010578658121327559\n",
      "[LOG 20200502-14:06:11] epoch: 3766 train-loss: 0.010578658431768417\n",
      "[LOG 20200502-14:06:11] epoch: 3767 train-loss: 0.010578658431768417\n",
      "[LOG 20200502-14:06:11] epoch: 3768 train-loss: 0.010578658224807845\n",
      "[LOG 20200502-14:06:12] epoch: 3769 train-loss: 0.010578657810886702\n",
      "[LOG 20200502-14:06:12] epoch: 3770 train-loss: 0.010578658017847273\n",
      "[LOG 20200502-14:06:12] epoch: 3771 train-loss: 0.010578657914366987\n",
      "[LOG 20200502-14:06:12] epoch: 3772 train-loss: 0.010578657707406415\n",
      "[LOG 20200502-14:06:12] epoch: 3773 train-loss: 0.010578657086524699\n",
      "[LOG 20200502-14:06:13] epoch: 3774 train-loss: 0.010578657396965556\n",
      "[LOG 20200502-14:06:13] epoch: 3775 train-loss: 0.010578657086524699\n",
      "[LOG 20200502-14:06:13] epoch: 3776 train-loss: 0.010578657190004984\n",
      "[LOG 20200502-14:06:13] epoch: 3777 train-loss: 0.010578656362162696\n",
      "[LOG 20200502-14:06:13] epoch: 3778 train-loss: 0.010578655844761265\n",
      "[LOG 20200502-14:06:14] epoch: 3779 train-loss: 0.010578655016918978\n",
      "[LOG 20200502-14:06:14] epoch: 3780 train-loss: 0.010578655016918978\n",
      "[LOG 20200502-14:06:14] epoch: 3781 train-loss: 0.010578654809958406\n",
      "[LOG 20200502-14:06:14] epoch: 3782 train-loss: 0.010578654189076688\n",
      "[LOG 20200502-14:06:15] epoch: 3783 train-loss: 0.010578654292556975\n",
      "[LOG 20200502-14:06:15] epoch: 3784 train-loss: 0.010578653671675257\n",
      "[LOG 20200502-14:06:15] epoch: 3785 train-loss: 0.0105786533612344\n",
      "[LOG 20200502-14:06:15] epoch: 3786 train-loss: 0.010578652429911826\n",
      "[LOG 20200502-14:06:15] epoch: 3787 train-loss: 0.010578652222951254\n",
      "[LOG 20200502-14:06:16] epoch: 3788 train-loss: 0.010578652015990682\n",
      "[LOG 20200502-14:06:16] epoch: 3789 train-loss: 0.010578651084668107\n",
      "[LOG 20200502-14:06:16] epoch: 3790 train-loss: 0.010578650670746962\n",
      "[LOG 20200502-14:06:16] epoch: 3791 train-loss: 0.010578650360306105\n",
      "[LOG 20200502-14:06:17] epoch: 3792 train-loss: 0.010578649325503243\n",
      "[LOG 20200502-14:06:17] epoch: 3793 train-loss: 0.0105786489115821\n",
      "[LOG 20200502-14:06:17] epoch: 3794 train-loss: 0.010578648497660955\n",
      "[LOG 20200502-14:06:17] epoch: 3795 train-loss: 0.01057864756633838\n",
      "[LOG 20200502-14:06:18] epoch: 3796 train-loss: 0.010578647255897522\n",
      "[LOG 20200502-14:06:18] epoch: 3797 train-loss: 0.01057864653153552\n",
      "[LOG 20200502-14:06:18] epoch: 3798 train-loss: 0.010578645910653803\n",
      "[LOG 20200502-14:06:18] epoch: 3799 train-loss: 0.010578644358449511\n",
      "[LOG 20200502-14:06:19] epoch: 3800 train-loss: 0.010578644358449511\n",
      "[LOG 20200502-14:06:19] epoch: 3801 train-loss: 0.010578643220166365\n",
      "[LOG 20200502-14:06:19] epoch: 3802 train-loss: 0.010578643013205793\n",
      "[LOG 20200502-14:06:19] epoch: 3803 train-loss: 0.010578641874922646\n",
      "[LOG 20200502-14:06:20] epoch: 3804 train-loss: 0.010578641150560644\n",
      "[LOG 20200502-14:06:20] epoch: 3805 train-loss: 0.010578640529678928\n",
      "[LOG 20200502-14:06:20] epoch: 3806 train-loss: 0.010578639598356353\n",
      "[LOG 20200502-14:06:20] epoch: 3807 train-loss: 0.010578638770514064\n",
      "[LOG 20200502-14:06:21] epoch: 3808 train-loss: 0.010578638046152063\n",
      "[LOG 20200502-14:06:21] epoch: 3809 train-loss: 0.010578637114829488\n",
      "[LOG 20200502-14:06:21] epoch: 3810 train-loss: 0.010578636183506913\n",
      "[LOG 20200502-14:06:21] epoch: 3811 train-loss: 0.010578635148704052\n",
      "[LOG 20200502-14:06:21] epoch: 3812 train-loss: 0.010578634527822336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:06:22] epoch: 3813 train-loss: 0.01057863307909833\n",
      "[LOG 20200502-14:06:22] epoch: 3814 train-loss: 0.010578632147775756\n",
      "[LOG 20200502-14:06:22] epoch: 3815 train-loss: 0.01057863100949261\n",
      "[LOG 20200502-14:06:22] epoch: 3816 train-loss: 0.010578629871209463\n",
      "[LOG 20200502-14:06:23] epoch: 3817 train-loss: 0.010578628008564314\n",
      "[LOG 20200502-14:06:23] epoch: 3818 train-loss: 0.01057862707724174\n",
      "[LOG 20200502-14:06:23] epoch: 3819 train-loss: 0.010578625318076875\n",
      "[LOG 20200502-14:06:23] epoch: 3820 train-loss: 0.010578623662392298\n",
      "[LOG 20200502-14:06:24] epoch: 3821 train-loss: 0.010578622524109151\n",
      "[LOG 20200502-14:06:24] epoch: 3822 train-loss: 0.010578621075385146\n",
      "[LOG 20200502-14:06:24] epoch: 3823 train-loss: 0.010578619212739997\n",
      "[LOG 20200502-14:06:24] epoch: 3824 train-loss: 0.010578617246614562\n",
      "[LOG 20200502-14:06:24] epoch: 3825 train-loss: 0.010578615590929985\n",
      "[LOG 20200502-14:06:25] epoch: 3826 train-loss: 0.010578614038725695\n",
      "[LOG 20200502-14:06:25] epoch: 3827 train-loss: 0.010578612486521402\n",
      "[LOG 20200502-14:06:25] epoch: 3828 train-loss: 0.010578610623876253\n",
      "[LOG 20200502-14:06:25] epoch: 3829 train-loss: 0.010578609382112822\n",
      "[LOG 20200502-14:06:25] epoch: 3830 train-loss: 0.010578607415987385\n",
      "[LOG 20200502-14:06:26] epoch: 3831 train-loss: 0.010578606070743667\n",
      "[LOG 20200502-14:06:26] epoch: 3832 train-loss: 0.010578604104618231\n",
      "[LOG 20200502-14:06:26] epoch: 3833 train-loss: 0.010578602448933654\n",
      "[LOG 20200502-14:06:26] epoch: 3834 train-loss: 0.010578600379327932\n",
      "[LOG 20200502-14:06:27] epoch: 3835 train-loss: 0.0105785991375645\n",
      "[LOG 20200502-14:06:27] epoch: 3836 train-loss: 0.01057859675751792\n",
      "[LOG 20200502-14:06:27] epoch: 3837 train-loss: 0.010578595515754487\n",
      "[LOG 20200502-14:06:27] epoch: 3838 train-loss: 0.010578593653109338\n",
      "[LOG 20200502-14:06:28] epoch: 3839 train-loss: 0.010578591686983904\n",
      "[LOG 20200502-14:06:28] epoch: 3840 train-loss: 0.01057858992781904\n",
      "[LOG 20200502-14:06:28] epoch: 3841 train-loss: 0.010578588272134462\n",
      "[LOG 20200502-14:06:28] epoch: 3842 train-loss: 0.010578586202528741\n",
      "[LOG 20200502-14:06:28] epoch: 3843 train-loss: 0.01057858413292302\n",
      "[LOG 20200502-14:06:29] epoch: 3844 train-loss: 0.01057858258071873\n",
      "[LOG 20200502-14:06:29] epoch: 3845 train-loss: 0.01057858020067215\n",
      "[LOG 20200502-14:06:29] epoch: 3846 train-loss: 0.010578578441507287\n",
      "[LOG 20200502-14:06:29] epoch: 3847 train-loss: 0.010578576682342423\n",
      "[LOG 20200502-14:06:29] epoch: 3848 train-loss: 0.010578574612736702\n",
      "[LOG 20200502-14:06:30] epoch: 3849 train-loss: 0.01057857285357184\n",
      "[LOG 20200502-14:06:30] epoch: 3850 train-loss: 0.010578571197887262\n",
      "[LOG 20200502-14:06:30] epoch: 3851 train-loss: 0.010578569024801254\n",
      "[LOG 20200502-14:06:30] epoch: 3852 train-loss: 0.010578567058675818\n",
      "[LOG 20200502-14:06:30] epoch: 3853 train-loss: 0.010578565402991243\n",
      "[LOG 20200502-14:06:31] epoch: 3854 train-loss: 0.010578563436865807\n",
      "[LOG 20200502-14:06:31] epoch: 3855 train-loss: 0.01057856147074037\n",
      "[LOG 20200502-14:06:31] epoch: 3856 train-loss: 0.01057855940113465\n",
      "[LOG 20200502-14:06:31] epoch: 3857 train-loss: 0.010578557435009215\n",
      "[LOG 20200502-14:06:32] epoch: 3858 train-loss: 0.010578555365403494\n",
      "[LOG 20200502-14:06:32] epoch: 3859 train-loss: 0.010578553709718917\n",
      "[LOG 20200502-14:06:32] epoch: 3860 train-loss: 0.010578551329672337\n",
      "[LOG 20200502-14:06:32] epoch: 3861 train-loss: 0.010578548432224326\n",
      "[LOG 20200502-14:06:32] epoch: 3862 train-loss: 0.010578546362618605\n",
      "[LOG 20200502-14:06:33] epoch: 3863 train-loss: 0.010578543982572026\n",
      "[LOG 20200502-14:06:33] epoch: 3864 train-loss: 0.010578541602525447\n",
      "[LOG 20200502-14:06:33] epoch: 3865 train-loss: 0.010578539429439439\n",
      "[LOG 20200502-14:06:33] epoch: 3866 train-loss: 0.010578536428511143\n",
      "[LOG 20200502-14:06:34] epoch: 3867 train-loss: 0.010578534462385707\n",
      "[LOG 20200502-14:06:34] epoch: 3868 train-loss: 0.010578531461457411\n",
      "[LOG 20200502-14:06:34] epoch: 3869 train-loss: 0.010578529081410833\n",
      "[LOG 20200502-14:06:34] epoch: 3870 train-loss: 0.01057852649440368\n",
      "[LOG 20200502-14:06:35] epoch: 3871 train-loss: 0.010578524321317673\n",
      "[LOG 20200502-14:06:35] epoch: 3872 train-loss: 0.010578521423869662\n",
      "[LOG 20200502-14:06:35] epoch: 3873 train-loss: 0.010578519147303369\n",
      "[LOG 20200502-14:06:35] epoch: 3874 train-loss: 0.010578516560296217\n",
      "[LOG 20200502-14:06:36] epoch: 3875 train-loss: 0.01057851386980878\n",
      "[LOG 20200502-14:06:36] epoch: 3876 train-loss: 0.010578511903683344\n",
      "[LOG 20200502-14:06:36] epoch: 3877 train-loss: 0.010578508695794476\n",
      "[LOG 20200502-14:06:36] epoch: 3878 train-loss: 0.010578506626188755\n",
      "[LOG 20200502-14:06:36] epoch: 3879 train-loss: 0.010578504039181603\n",
      "[LOG 20200502-14:06:37] epoch: 3880 train-loss: 0.010578501969575882\n",
      "[LOG 20200502-14:06:37] epoch: 3881 train-loss: 0.010578498968647586\n",
      "[LOG 20200502-14:06:37] epoch: 3882 train-loss: 0.010578496174679862\n",
      "[LOG 20200502-14:06:37] epoch: 3883 train-loss: 0.01057849389811357\n",
      "[LOG 20200502-14:06:38] epoch: 3884 train-loss: 0.010578491621547274\n",
      "[LOG 20200502-14:06:38] epoch: 3885 train-loss: 0.010578488517138693\n",
      "[LOG 20200502-14:06:38] epoch: 3886 train-loss: 0.010578486344052685\n",
      "[LOG 20200502-14:06:38] epoch: 3887 train-loss: 0.010578483653565248\n",
      "[LOG 20200502-14:06:38] epoch: 3888 train-loss: 0.01057848096307781\n",
      "[LOG 20200502-14:06:39] epoch: 3889 train-loss: 0.010578478686511517\n",
      "[LOG 20200502-14:06:39] epoch: 3890 train-loss: 0.010578475892543793\n",
      "[LOG 20200502-14:06:39] epoch: 3891 train-loss: 0.010578473926418357\n",
      "[LOG 20200502-14:06:39] epoch: 3892 train-loss: 0.010578471132450633\n",
      "[LOG 20200502-14:06:40] epoch: 3893 train-loss: 0.010578468648923768\n",
      "[LOG 20200502-14:06:40] epoch: 3894 train-loss: 0.010578465544515185\n",
      "[LOG 20200502-14:06:40] epoch: 3895 train-loss: 0.010578462957508035\n",
      "[LOG 20200502-14:06:40] epoch: 3896 train-loss: 0.010578460370500883\n",
      "[LOG 20200502-14:06:40] epoch: 3897 train-loss: 0.010578458714816306\n",
      "[LOG 20200502-14:06:41] epoch: 3898 train-loss: 0.010578455506927438\n",
      "[LOG 20200502-14:06:41] epoch: 3899 train-loss: 0.010578453437321715\n",
      "[LOG 20200502-14:06:41] epoch: 3900 train-loss: 0.010578450332913134\n",
      "[LOG 20200502-14:06:41] epoch: 3901 train-loss: 0.010578448263307413\n",
      "[LOG 20200502-14:06:42] epoch: 3902 train-loss: 0.010578445055418544\n",
      "[LOG 20200502-14:06:42] epoch: 3903 train-loss: 0.010578442054490248\n",
      "[LOG 20200502-14:06:42] epoch: 3904 train-loss: 0.010578438950081667\n",
      "[LOG 20200502-14:06:42] epoch: 3905 train-loss: 0.010578436259594228\n",
      "[LOG 20200502-14:06:42] epoch: 3906 train-loss: 0.010578432948225074\n",
      "[LOG 20200502-14:06:43] epoch: 3907 train-loss: 0.010578430464698209\n",
      "[LOG 20200502-14:06:43] epoch: 3908 train-loss: 0.010578426946368482\n",
      "[LOG 20200502-14:06:43] epoch: 3909 train-loss: 0.01057842435936133\n",
      "[LOG 20200502-14:06:43] epoch: 3910 train-loss: 0.010578421358433034\n",
      "[LOG 20200502-14:06:43] epoch: 3911 train-loss: 0.010578418254024453\n",
      "[LOG 20200502-14:06:44] epoch: 3912 train-loss: 0.010578414942655299\n",
      "[LOG 20200502-14:06:44] epoch: 3913 train-loss: 0.010578411734766431\n",
      "[LOG 20200502-14:06:44] epoch: 3914 train-loss: 0.01057840914775928\n",
      "[LOG 20200502-14:06:44] epoch: 3915 train-loss: 0.01057840645727184\n",
      "[LOG 20200502-14:06:45] epoch: 3916 train-loss: 0.010578403249382973\n",
      "[LOG 20200502-14:06:45] epoch: 3917 train-loss: 0.01057840066237582\n",
      "[LOG 20200502-14:06:45] epoch: 3918 train-loss: 0.01057839704056581\n",
      "[LOG 20200502-14:06:46] epoch: 3919 train-loss: 0.010578394143117799\n",
      "[LOG 20200502-14:06:46] epoch: 3920 train-loss: 0.010578391349150075\n",
      "[LOG 20200502-14:06:46] epoch: 3921 train-loss: 0.01057838855518235\n",
      "[LOG 20200502-14:06:46] epoch: 3922 train-loss: 0.010578385450773768\n",
      "[LOG 20200502-14:06:47] epoch: 3923 train-loss: 0.01057838255332576\n",
      "[LOG 20200502-14:06:47] epoch: 3924 train-loss: 0.010578379241956605\n",
      "[LOG 20200502-14:06:47] epoch: 3925 train-loss: 0.01057837644798888\n",
      "[LOG 20200502-14:06:47] epoch: 3926 train-loss: 0.010578373757501444\n",
      "[LOG 20200502-14:06:48] epoch: 3927 train-loss: 0.010578370653092861\n",
      "[LOG 20200502-14:06:48] epoch: 3928 train-loss: 0.01057836806608571\n",
      "[LOG 20200502-14:06:48] epoch: 3929 train-loss: 0.010578364858196842\n",
      "[LOG 20200502-14:06:48] epoch: 3930 train-loss: 0.010578361753788259\n",
      "[LOG 20200502-14:06:49] epoch: 3931 train-loss: 0.010578358545899391\n",
      "[LOG 20200502-14:06:49] epoch: 3932 train-loss: 0.010578356579773955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:06:49] epoch: 3933 train-loss: 0.010578352751003372\n",
      "[LOG 20200502-14:06:49] epoch: 3934 train-loss: 0.010578350060515933\n",
      "[LOG 20200502-14:06:49] epoch: 3935 train-loss: 0.010578347370028496\n",
      "[LOG 20200502-14:06:50] epoch: 3936 train-loss: 0.010578344265619913\n",
      "[LOG 20200502-14:06:50] epoch: 3937 train-loss: 0.010578341368171904\n",
      "[LOG 20200502-14:06:50] epoch: 3938 train-loss: 0.010578338160283036\n",
      "[LOG 20200502-14:06:50] epoch: 3939 train-loss: 0.010578335159354739\n",
      "[LOG 20200502-14:06:51] epoch: 3940 train-loss: 0.010578333089749018\n",
      "[LOG 20200502-14:06:51] epoch: 3941 train-loss: 0.010578329778379865\n",
      "[LOG 20200502-14:06:51] epoch: 3942 train-loss: 0.010578326053089566\n",
      "[LOG 20200502-14:06:51] epoch: 3943 train-loss: 0.010578322741720412\n",
      "[LOG 20200502-14:06:51] epoch: 3944 train-loss: 0.01057831932687097\n",
      "[LOG 20200502-14:06:52] epoch: 3945 train-loss: 0.010578315601580672\n",
      "[LOG 20200502-14:06:52] epoch: 3946 train-loss: 0.010578312393691804\n",
      "[LOG 20200502-14:06:52] epoch: 3947 train-loss: 0.010578309496243795\n",
      "[LOG 20200502-14:06:52] epoch: 3948 train-loss: 0.010578305770953497\n",
      "[LOG 20200502-14:06:53] epoch: 3949 train-loss: 0.010578302666544914\n",
      "[LOG 20200502-14:06:53] epoch: 3950 train-loss: 0.010578299251695475\n",
      "[LOG 20200502-14:06:53] epoch: 3951 train-loss: 0.010578295836846033\n",
      "[LOG 20200502-14:06:53] epoch: 3952 train-loss: 0.010578292215036022\n",
      "[LOG 20200502-14:06:53] epoch: 3953 train-loss: 0.010578288903666867\n",
      "[LOG 20200502-14:06:54] epoch: 3954 train-loss: 0.010578285799258284\n",
      "[LOG 20200502-14:06:54] epoch: 3955 train-loss: 0.010578282487889131\n",
      "[LOG 20200502-14:06:54] epoch: 3956 train-loss: 0.010578279176519977\n",
      "[LOG 20200502-14:06:54] epoch: 3957 train-loss: 0.010578275554709964\n",
      "[LOG 20200502-14:06:55] epoch: 3958 train-loss: 0.010578272450301383\n",
      "[LOG 20200502-14:06:55] epoch: 3959 train-loss: 0.010578268931971656\n",
      "[LOG 20200502-14:06:55] epoch: 3960 train-loss: 0.010578265724082788\n",
      "[LOG 20200502-14:06:55] epoch: 3961 train-loss: 0.010578261688351631\n",
      "[LOG 20200502-14:06:56] epoch: 3962 train-loss: 0.010578258894383907\n",
      "[LOG 20200502-14:06:56] epoch: 3963 train-loss: 0.010578255272573896\n",
      "[LOG 20200502-14:06:56] epoch: 3964 train-loss: 0.010578252271645598\n",
      "[LOG 20200502-14:06:56] epoch: 3965 train-loss: 0.010578249270717302\n",
      "[LOG 20200502-14:06:57] epoch: 3966 train-loss: 0.010578245545427004\n",
      "[LOG 20200502-14:06:57] epoch: 3967 train-loss: 0.010578242441018423\n",
      "[LOG 20200502-14:06:57] epoch: 3968 train-loss: 0.010578239129649268\n",
      "[LOG 20200502-14:06:57] epoch: 3969 train-loss: 0.010578235818280114\n",
      "[LOG 20200502-14:06:57] epoch: 3970 train-loss: 0.010578232196470102\n",
      "[LOG 20200502-14:06:58] epoch: 3971 train-loss: 0.01057822909206152\n",
      "[LOG 20200502-14:06:58] epoch: 3972 train-loss: 0.010578225780692365\n",
      "[LOG 20200502-14:06:58] epoch: 3973 train-loss: 0.01057822277976407\n",
      "[LOG 20200502-14:06:58] epoch: 3974 train-loss: 0.010578219261434343\n",
      "[LOG 20200502-14:06:59] epoch: 3975 train-loss: 0.010578215950065188\n",
      "[LOG 20200502-14:06:59] epoch: 3976 train-loss: 0.010578212949136892\n",
      "[LOG 20200502-14:06:59] epoch: 3977 train-loss: 0.010578208913405737\n",
      "[LOG 20200502-14:06:59] epoch: 3978 train-loss: 0.010578206015957726\n",
      "[LOG 20200502-14:07:00] epoch: 3979 train-loss: 0.010578201566305425\n",
      "[LOG 20200502-14:07:00] epoch: 3980 train-loss: 0.010578198047975699\n",
      "[LOG 20200502-14:07:00] epoch: 3981 train-loss: 0.010578194219205115\n",
      "[LOG 20200502-14:07:00] epoch: 3982 train-loss: 0.010578190183473958\n",
      "[LOG 20200502-14:07:00] epoch: 3983 train-loss: 0.010578186872104803\n",
      "[LOG 20200502-14:07:01] epoch: 3984 train-loss: 0.01057818304333422\n",
      "[LOG 20200502-14:07:01] epoch: 3985 train-loss: 0.010578179214563634\n",
      "[LOG 20200502-14:07:01] epoch: 3986 train-loss: 0.01057817538579305\n",
      "[LOG 20200502-14:07:01] epoch: 3987 train-loss: 0.010578171660502752\n",
      "[LOG 20200502-14:07:02] epoch: 3988 train-loss: 0.010578167728251882\n",
      "[LOG 20200502-14:07:02] epoch: 3989 train-loss: 0.010578164106441868\n",
      "[LOG 20200502-14:07:02] epoch: 3990 train-loss: 0.010578160588112142\n",
      "[LOG 20200502-14:07:02] epoch: 3991 train-loss: 0.010578157069782415\n",
      "[LOG 20200502-14:07:03] epoch: 3992 train-loss: 0.01057815303405126\n",
      "[LOG 20200502-14:07:03] epoch: 3993 train-loss: 0.010578149205280675\n",
      "[LOG 20200502-14:07:03] epoch: 3994 train-loss: 0.010578145066069232\n",
      "[LOG 20200502-14:07:03] epoch: 3995 train-loss: 0.010578141547739506\n",
      "[LOG 20200502-14:07:03] epoch: 3996 train-loss: 0.01057813771896892\n",
      "[LOG 20200502-14:07:04] epoch: 3997 train-loss: 0.010578134407599768\n",
      "[LOG 20200502-14:07:04] epoch: 3998 train-loss: 0.010578130785789754\n",
      "[LOG 20200502-14:07:04] epoch: 3999 train-loss: 0.010578126750058599\n",
      "[LOG 20200502-14:07:04] epoch: 4000 train-loss: 0.010578122714327442\n",
      "[LOG 20200502-14:07:05] epoch: 4001 train-loss: 0.010578119402958287\n",
      "[LOG 20200502-14:07:05] epoch: 4002 train-loss: 0.010578115160266558\n",
      "[LOG 20200502-14:07:05] epoch: 4003 train-loss: 0.010578111641936831\n",
      "[LOG 20200502-14:07:05] epoch: 4004 train-loss: 0.01057810802012682\n",
      "[LOG 20200502-14:07:06] epoch: 4005 train-loss: 0.010578104294836521\n",
      "[LOG 20200502-14:07:06] epoch: 4006 train-loss: 0.010578100259105364\n",
      "[LOG 20200502-14:07:06] epoch: 4007 train-loss: 0.010578097258177068\n",
      "[LOG 20200502-14:07:06] epoch: 4008 train-loss: 0.010578093118965626\n",
      "[LOG 20200502-14:07:07] epoch: 4009 train-loss: 0.0105780896006359\n",
      "[LOG 20200502-14:07:07] epoch: 4010 train-loss: 0.010578085668385029\n",
      "[LOG 20200502-14:07:07] epoch: 4011 train-loss: 0.010578081736134158\n",
      "[LOG 20200502-14:07:08] epoch: 4012 train-loss: 0.01057807801084386\n",
      "[LOG 20200502-14:07:08] epoch: 4013 train-loss: 0.01057807376815213\n",
      "[LOG 20200502-14:07:08] epoch: 4014 train-loss: 0.010578069628940688\n",
      "[LOG 20200502-14:07:08] epoch: 4015 train-loss: 0.0105780650758081\n",
      "[LOG 20200502-14:07:09] epoch: 4016 train-loss: 0.01057806114355723\n",
      "[LOG 20200502-14:07:09] epoch: 4017 train-loss: 0.010578056797385216\n",
      "[LOG 20200502-14:07:09] epoch: 4018 train-loss: 0.010578052451213201\n",
      "[LOG 20200502-14:07:09] epoch: 4019 train-loss: 0.0105780480015609\n",
      "[LOG 20200502-14:07:10] epoch: 4020 train-loss: 0.010578043965829743\n",
      "[LOG 20200502-14:07:10] epoch: 4021 train-loss: 0.0105780398266183\n",
      "[LOG 20200502-14:07:10] epoch: 4022 train-loss: 0.010578035790887143\n",
      "[LOG 20200502-14:07:10] epoch: 4023 train-loss: 0.01057803144471513\n",
      "[LOG 20200502-14:07:10] epoch: 4024 train-loss: 0.0105780272020234\n",
      "[LOG 20200502-14:07:11] epoch: 4025 train-loss: 0.010578023062811958\n",
      "[LOG 20200502-14:07:11] epoch: 4026 train-loss: 0.01057801850967937\n",
      "[LOG 20200502-14:07:11] epoch: 4027 train-loss: 0.010578014473948214\n",
      "[LOG 20200502-14:07:11] epoch: 4028 train-loss: 0.010578010127776198\n",
      "[LOG 20200502-14:07:11] epoch: 4029 train-loss: 0.010578005988564756\n",
      "[LOG 20200502-14:07:12] epoch: 4030 train-loss: 0.010578001642392742\n",
      "[LOG 20200502-14:07:12] epoch: 4031 train-loss: 0.010577997813622156\n",
      "[LOG 20200502-14:07:12] epoch: 4032 train-loss: 0.010577993363969855\n",
      "[LOG 20200502-14:07:12] epoch: 4033 train-loss: 0.010577989121278128\n",
      "[LOG 20200502-14:07:13] epoch: 4034 train-loss: 0.01057798508554697\n",
      "[LOG 20200502-14:07:13] epoch: 4035 train-loss: 0.010577980739374956\n",
      "[LOG 20200502-14:07:13] epoch: 4036 train-loss: 0.010577976082762083\n",
      "[LOG 20200502-14:07:13] epoch: 4037 train-loss: 0.010577972150511213\n",
      "[LOG 20200502-14:07:13] epoch: 4038 train-loss: 0.01057796749389834\n",
      "[LOG 20200502-14:07:14] epoch: 4039 train-loss: 0.010577963975568613\n",
      "[LOG 20200502-14:07:14] epoch: 4040 train-loss: 0.010577959215475453\n",
      "[LOG 20200502-14:07:14] epoch: 4041 train-loss: 0.010577955179744296\n",
      "[LOG 20200502-14:07:14] epoch: 4042 train-loss: 0.010577951040532853\n",
      "[LOG 20200502-14:07:15] epoch: 4043 train-loss: 0.010577946590880552\n",
      "[LOG 20200502-14:07:15] epoch: 4044 train-loss: 0.010577941727307107\n",
      "[LOG 20200502-14:07:15] epoch: 4045 train-loss: 0.01057793696721395\n",
      "[LOG 20200502-14:07:15] epoch: 4046 train-loss: 0.010577932310601076\n",
      "[LOG 20200502-14:07:15] epoch: 4047 train-loss: 0.010577927136586772\n",
      "[LOG 20200502-14:07:16] epoch: 4048 train-loss: 0.01057792216953304\n",
      "[LOG 20200502-14:07:16] epoch: 4049 train-loss: 0.01057791823728217\n",
      "[LOG 20200502-14:07:16] epoch: 4050 train-loss: 0.010577913373708725\n",
      "[LOG 20200502-14:07:16] epoch: 4051 train-loss: 0.01057790799273385\n",
      "[LOG 20200502-14:07:17] epoch: 4052 train-loss: 0.010577903543081548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:07:17] epoch: 4053 train-loss: 0.010577898989948962\n",
      "[LOG 20200502-14:07:17] epoch: 4054 train-loss: 0.010577894747257233\n",
      "[LOG 20200502-14:07:17] epoch: 4055 train-loss: 0.010577889469762644\n",
      "[LOG 20200502-14:07:17] epoch: 4056 train-loss: 0.010577884606189199\n",
      "[LOG 20200502-14:07:18] epoch: 4057 train-loss: 0.01057788005305661\n",
      "[LOG 20200502-14:07:18] epoch: 4058 train-loss: 0.01057787560340431\n",
      "[LOG 20200502-14:07:18] epoch: 4059 train-loss: 0.01057787032590972\n",
      "[LOG 20200502-14:07:18] epoch: 4060 train-loss: 0.010577866083217992\n",
      "[LOG 20200502-14:07:18] epoch: 4061 train-loss: 0.010577860805723403\n",
      "[LOG 20200502-14:07:19] epoch: 4062 train-loss: 0.010577856563031673\n",
      "[LOG 20200502-14:07:19] epoch: 4063 train-loss: 0.010577851699458228\n",
      "[LOG 20200502-14:07:19] epoch: 4064 train-loss: 0.010577846525443925\n",
      "[LOG 20200502-14:07:19] epoch: 4065 train-loss: 0.010577842075791623\n",
      "[LOG 20200502-14:07:20] epoch: 4066 train-loss: 0.010577837212218178\n",
      "[LOG 20200502-14:07:20] epoch: 4067 train-loss: 0.010577832348644733\n",
      "[LOG 20200502-14:07:20] epoch: 4068 train-loss: 0.01057782769203186\n",
      "[LOG 20200502-14:07:20] epoch: 4069 train-loss: 0.0105778229319387\n",
      "[LOG 20200502-14:07:21] epoch: 4070 train-loss: 0.010577818585766686\n",
      "[LOG 20200502-14:07:21] epoch: 4071 train-loss: 0.01057781320479181\n",
      "[LOG 20200502-14:07:21] epoch: 4072 train-loss: 0.01057780823773808\n",
      "[LOG 20200502-14:07:21] epoch: 4073 train-loss: 0.01057780347764492\n",
      "[LOG 20200502-14:07:21] epoch: 4074 train-loss: 0.010577797889709473\n",
      "[LOG 20200502-14:07:22] epoch: 4075 train-loss: 0.010577792612214884\n",
      "[LOG 20200502-14:07:22] epoch: 4076 train-loss: 0.010577787541680865\n",
      "[LOG 20200502-14:07:22] epoch: 4077 train-loss: 0.010577782367666563\n",
      "[LOG 20200502-14:07:22] epoch: 4078 train-loss: 0.0105777768832114\n",
      "[LOG 20200502-14:07:22] epoch: 4079 train-loss: 0.010577771812677383\n",
      "[LOG 20200502-14:07:23] epoch: 4080 train-loss: 0.010577766224741936\n",
      "[LOG 20200502-14:07:23] epoch: 4081 train-loss: 0.010577761154207919\n",
      "[LOG 20200502-14:07:23] epoch: 4082 train-loss: 0.010577755566272471\n",
      "[LOG 20200502-14:07:23] epoch: 4083 train-loss: 0.01057775059921874\n",
      "[LOG 20200502-14:07:24] epoch: 4084 train-loss: 0.010577745218243863\n",
      "[LOG 20200502-14:07:24] epoch: 4085 train-loss: 0.010577740354670418\n",
      "[LOG 20200502-14:07:24] epoch: 4086 train-loss: 0.01057773476673497\n",
      "[LOG 20200502-14:07:24] epoch: 4087 train-loss: 0.010577729489240382\n",
      "[LOG 20200502-14:07:24] epoch: 4088 train-loss: 0.010577724315226078\n",
      "[LOG 20200502-14:07:25] epoch: 4089 train-loss: 0.010577718623810344\n",
      "[LOG 20200502-14:07:25] epoch: 4090 train-loss: 0.010577713553276327\n",
      "[LOG 20200502-14:07:25] epoch: 4091 train-loss: 0.010577707965340879\n",
      "[LOG 20200502-14:07:25] epoch: 4092 train-loss: 0.010577702791326575\n",
      "[LOG 20200502-14:07:25] epoch: 4093 train-loss: 0.010577697617312273\n",
      "[LOG 20200502-14:07:26] epoch: 4094 train-loss: 0.010577692546778254\n",
      "[LOG 20200502-14:07:26] epoch: 4095 train-loss: 0.010577687269283665\n",
      "[LOG 20200502-14:07:26] epoch: 4096 train-loss: 0.010577681370907359\n",
      "[LOG 20200502-14:07:26] epoch: 4097 train-loss: 0.010577676507333914\n",
      "[LOG 20200502-14:07:27] epoch: 4098 train-loss: 0.010577671436799897\n",
      "[LOG 20200502-14:07:27] epoch: 4099 train-loss: 0.010577666262785593\n",
      "[LOG 20200502-14:07:27] epoch: 4100 train-loss: 0.010577660467889573\n",
      "[LOG 20200502-14:07:27] epoch: 4101 train-loss: 0.010577654466032982\n",
      "[LOG 20200502-14:07:27] epoch: 4102 train-loss: 0.010577648567656675\n",
      "[LOG 20200502-14:07:28] epoch: 4103 train-loss: 0.01057764266928037\n",
      "[LOG 20200502-14:07:28] epoch: 4104 train-loss: 0.010577636460463205\n",
      "[LOG 20200502-14:07:28] epoch: 4105 train-loss: 0.010577630458606614\n",
      "[LOG 20200502-14:07:28] epoch: 4106 train-loss: 0.010577624767190881\n",
      "[LOG 20200502-14:07:28] epoch: 4107 train-loss: 0.010577619075775146\n",
      "[LOG 20200502-14:07:29] epoch: 4108 train-loss: 0.010577613591319986\n",
      "[LOG 20200502-14:07:29] epoch: 4109 train-loss: 0.010577607175542248\n",
      "[LOG 20200502-14:07:29] epoch: 4110 train-loss: 0.010577601484126516\n",
      "[LOG 20200502-14:07:29] epoch: 4111 train-loss: 0.010577595378789637\n",
      "[LOG 20200502-14:07:30] epoch: 4112 train-loss: 0.010577589583893618\n",
      "[LOG 20200502-14:07:30] epoch: 4113 train-loss: 0.010577583788997598\n",
      "[LOG 20200502-14:07:30] epoch: 4114 train-loss: 0.010577577787141005\n",
      "[LOG 20200502-14:07:30] epoch: 4115 train-loss: 0.01057757137136327\n",
      "[LOG 20200502-14:07:31] epoch: 4116 train-loss: 0.010577565990388393\n",
      "[LOG 20200502-14:07:31] epoch: 4117 train-loss: 0.010577559574610658\n",
      "[LOG 20200502-14:07:31] epoch: 4118 train-loss: 0.010577553572754065\n",
      "[LOG 20200502-14:07:31] epoch: 4119 train-loss: 0.010577548398739763\n",
      "[LOG 20200502-14:07:31] epoch: 4120 train-loss: 0.010577541982962025\n",
      "[LOG 20200502-14:07:32] epoch: 4121 train-loss: 0.010577535774144862\n",
      "[LOG 20200502-14:07:32] epoch: 4122 train-loss: 0.010577529875768555\n",
      "[LOG 20200502-14:07:32] epoch: 4123 train-loss: 0.010577523873911964\n",
      "[LOG 20200502-14:07:32] epoch: 4124 train-loss: 0.010577517561614513\n",
      "[LOG 20200502-14:07:32] epoch: 4125 train-loss: 0.010577511973679066\n",
      "[LOG 20200502-14:07:33] epoch: 4126 train-loss: 0.0105775050404999\n",
      "[LOG 20200502-14:07:33] epoch: 4127 train-loss: 0.010577498831682734\n",
      "[LOG 20200502-14:07:33] epoch: 4128 train-loss: 0.010577492001983855\n",
      "[LOG 20200502-14:07:33] epoch: 4129 train-loss: 0.010577485482725833\n",
      "[LOG 20200502-14:07:33] epoch: 4130 train-loss: 0.010577478756507238\n",
      "[LOG 20200502-14:07:34] epoch: 4131 train-loss: 0.010577472030288644\n",
      "[LOG 20200502-14:07:34] epoch: 4132 train-loss: 0.010577465717991194\n",
      "[LOG 20200502-14:07:34] epoch: 4133 train-loss: 0.010577459302213457\n",
      "[LOG 20200502-14:07:34] epoch: 4134 train-loss: 0.010577452472514577\n",
      "[LOG 20200502-14:07:35] epoch: 4135 train-loss: 0.010577445953256555\n",
      "[LOG 20200502-14:07:35] epoch: 4136 train-loss: 0.010577438916597102\n",
      "[LOG 20200502-14:07:35] epoch: 4137 train-loss: 0.010577432707779937\n",
      "[LOG 20200502-14:07:35] epoch: 4138 train-loss: 0.010577425567640198\n",
      "[LOG 20200502-14:07:35] epoch: 4139 train-loss: 0.010577418944901891\n",
      "[LOG 20200502-14:07:36] epoch: 4140 train-loss: 0.010577412529124154\n",
      "[LOG 20200502-14:07:36] epoch: 4141 train-loss: 0.010577406113346418\n",
      "[LOG 20200502-14:07:36] epoch: 4142 train-loss: 0.010577398973206678\n",
      "[LOG 20200502-14:07:36] epoch: 4143 train-loss: 0.010577392453948656\n",
      "[LOG 20200502-14:07:36] epoch: 4144 train-loss: 0.01057738552076949\n",
      "[LOG 20200502-14:07:37] epoch: 4145 train-loss: 0.010577378898031183\n",
      "[LOG 20200502-14:07:37] epoch: 4146 train-loss: 0.010577372068332301\n",
      "[LOG 20200502-14:07:37] epoch: 4147 train-loss: 0.010577364928192563\n",
      "[LOG 20200502-14:07:37] epoch: 4148 train-loss: 0.010577358719375398\n",
      "[LOG 20200502-14:07:38] epoch: 4149 train-loss: 0.010577351993156804\n",
      "[LOG 20200502-14:07:38] epoch: 4150 train-loss: 0.010577344439095922\n",
      "[LOG 20200502-14:07:38] epoch: 4151 train-loss: 0.010577336988515325\n",
      "[LOG 20200502-14:07:38] epoch: 4152 train-loss: 0.0105773297448953\n",
      "[LOG 20200502-14:07:38] epoch: 4153 train-loss: 0.010577322397794988\n",
      "[LOG 20200502-14:07:39] epoch: 4154 train-loss: 0.010577315050694678\n",
      "[LOG 20200502-14:07:39] epoch: 4155 train-loss: 0.010577307807074653\n",
      "[LOG 20200502-14:07:39] epoch: 4156 train-loss: 0.010577300563454628\n",
      "[LOG 20200502-14:07:39] epoch: 4157 train-loss: 0.010577293112874031\n",
      "[LOG 20200502-14:07:39] epoch: 4158 train-loss: 0.010577285455332862\n",
      "[LOG 20200502-14:07:40] epoch: 4159 train-loss: 0.01057727810823255\n",
      "[LOG 20200502-14:07:40] epoch: 4160 train-loss: 0.010577270657651953\n",
      "[LOG 20200502-14:07:40] epoch: 4161 train-loss: 0.010577263207071357\n",
      "[LOG 20200502-14:07:40] epoch: 4162 train-loss: 0.010577255859971046\n",
      "[LOG 20200502-14:07:40] epoch: 4163 train-loss: 0.010577248719831308\n",
      "[LOG 20200502-14:07:41] epoch: 4164 train-loss: 0.010577240648368994\n",
      "[LOG 20200502-14:07:41] epoch: 4165 train-loss: 0.010577233094308112\n",
      "[LOG 20200502-14:07:41] epoch: 4166 train-loss: 0.010577225643727515\n",
      "[LOG 20200502-14:07:41] epoch: 4167 train-loss: 0.010577217779225774\n",
      "[LOG 20200502-14:07:42] epoch: 4168 train-loss: 0.010577210535605749\n",
      "[LOG 20200502-14:07:42] epoch: 4169 train-loss: 0.01057720236066315\n",
      "[LOG 20200502-14:07:42] epoch: 4170 train-loss: 0.010577195324003696\n",
      "[LOG 20200502-14:07:42] epoch: 4171 train-loss: 0.010577188080383671\n",
      "[LOG 20200502-14:07:43] epoch: 4172 train-loss: 0.010577180319362216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:07:43] epoch: 4173 train-loss: 0.010577172144419618\n",
      "[LOG 20200502-14:07:43] epoch: 4174 train-loss: 0.010577164072957303\n",
      "[LOG 20200502-14:07:43] epoch: 4175 train-loss: 0.010577155691054132\n",
      "[LOG 20200502-14:07:44] epoch: 4176 train-loss: 0.010577147412631247\n",
      "[LOG 20200502-14:07:44] epoch: 4177 train-loss: 0.010577139341168933\n",
      "[LOG 20200502-14:07:44] epoch: 4178 train-loss: 0.010577130855785476\n",
      "[LOG 20200502-14:07:44] epoch: 4179 train-loss: 0.010577122991283735\n",
      "[LOG 20200502-14:07:45] epoch: 4180 train-loss: 0.010577114609380564\n",
      "[LOG 20200502-14:07:45] epoch: 4181 train-loss: 0.010577106744878821\n",
      "[LOG 20200502-14:07:45] epoch: 4182 train-loss: 0.010577098259495364\n",
      "[LOG 20200502-14:07:45] epoch: 4183 train-loss: 0.010577089981072478\n",
      "[LOG 20200502-14:07:46] epoch: 4184 train-loss: 0.010577081702649593\n",
      "[LOG 20200502-14:07:46] epoch: 4185 train-loss: 0.010577073424226709\n",
      "[LOG 20200502-14:07:46] epoch: 4186 train-loss: 0.010577064835362963\n",
      "[LOG 20200502-14:07:46] epoch: 4187 train-loss: 0.010577056970861223\n",
      "[LOG 20200502-14:07:46] epoch: 4188 train-loss: 0.010577048278517194\n",
      "[LOG 20200502-14:07:47] epoch: 4189 train-loss: 0.010577040000094308\n",
      "[LOG 20200502-14:07:47] epoch: 4190 train-loss: 0.010577031721671423\n",
      "[LOG 20200502-14:07:47] epoch: 4191 train-loss: 0.01057702313280768\n",
      "[LOG 20200502-14:07:47] epoch: 4192 train-loss: 0.010577014854384793\n",
      "[LOG 20200502-14:07:48] epoch: 4193 train-loss: 0.010577006472481621\n",
      "[LOG 20200502-14:07:48] epoch: 4194 train-loss: 0.010576997159255875\n",
      "[LOG 20200502-14:07:48] epoch: 4195 train-loss: 0.010576988984313276\n",
      "[LOG 20200502-14:07:48] epoch: 4196 train-loss: 0.010576979878048101\n",
      "[LOG 20200502-14:07:48] epoch: 4197 train-loss: 0.010576970978743501\n",
      "[LOG 20200502-14:07:49] epoch: 4198 train-loss: 0.010576962182919184\n",
      "[LOG 20200502-14:07:49] epoch: 4199 train-loss: 0.01057695255925258\n",
      "[LOG 20200502-14:07:49] epoch: 4200 train-loss: 0.010576943556467691\n",
      "[LOG 20200502-14:07:49] epoch: 4201 train-loss: 0.010576934346722232\n",
      "[LOG 20200502-14:07:50] epoch: 4202 train-loss: 0.010576925343937345\n",
      "[LOG 20200502-14:07:50] epoch: 4203 train-loss: 0.010576916444632743\n",
      "[LOG 20200502-14:07:50] epoch: 4204 train-loss: 0.010576907338367568\n",
      "[LOG 20200502-14:07:50] epoch: 4205 train-loss: 0.01057689781818125\n",
      "[LOG 20200502-14:07:51] epoch: 4206 train-loss: 0.010576888918876648\n",
      "[LOG 20200502-14:07:51] epoch: 4207 train-loss: 0.010576879709131189\n",
      "[LOG 20200502-14:07:51] epoch: 4208 train-loss: 0.010576871120267443\n",
      "[LOG 20200502-14:07:51] epoch: 4209 train-loss: 0.010576862014002271\n",
      "[LOG 20200502-14:07:51] epoch: 4210 train-loss: 0.010576853114697669\n",
      "[LOG 20200502-14:07:52] epoch: 4211 train-loss: 0.010576842973629633\n",
      "[LOG 20200502-14:07:52] epoch: 4212 train-loss: 0.010576834177805318\n",
      "[LOG 20200502-14:07:52] epoch: 4213 train-loss: 0.010576825278500715\n",
      "[LOG 20200502-14:07:52] epoch: 4214 train-loss: 0.010576815861794684\n",
      "[LOG 20200502-14:07:53] epoch: 4215 train-loss: 0.010576806134647794\n",
      "[LOG 20200502-14:07:53] epoch: 4216 train-loss: 0.01057679620054033\n",
      "[LOG 20200502-14:07:53] epoch: 4217 train-loss: 0.010576786369913153\n",
      "[LOG 20200502-14:07:53] epoch: 4218 train-loss: 0.010576776435805691\n",
      "[LOG 20200502-14:07:53] epoch: 4219 train-loss: 0.010576766915619373\n",
      "[LOG 20200502-14:07:54] epoch: 4220 train-loss: 0.010576756878031624\n",
      "[LOG 20200502-14:07:54] epoch: 4221 train-loss: 0.01057674725436502\n",
      "[LOG 20200502-14:07:54] epoch: 4222 train-loss: 0.010576737423737844\n",
      "[LOG 20200502-14:07:54] epoch: 4223 train-loss: 0.010576727386150096\n",
      "[LOG 20200502-14:07:55] epoch: 4224 train-loss: 0.01057671776248349\n",
      "[LOG 20200502-14:07:55] epoch: 4225 train-loss: 0.0105767080353366\n",
      "[LOG 20200502-14:07:55] epoch: 4226 train-loss: 0.010576697997748852\n",
      "[LOG 20200502-14:07:55] epoch: 4227 train-loss: 0.010576687856680818\n",
      "[LOG 20200502-14:07:55] epoch: 4228 train-loss: 0.010576677922573354\n",
      "[LOG 20200502-14:07:56] epoch: 4229 train-loss: 0.010576667574544748\n",
      "[LOG 20200502-14:07:56] epoch: 4230 train-loss: 0.010576657847397856\n",
      "[LOG 20200502-14:07:56] epoch: 4231 train-loss: 0.010576648120250966\n",
      "[LOG 20200502-14:07:56] epoch: 4232 train-loss: 0.010576637979182932\n",
      "[LOG 20200502-14:07:57] epoch: 4233 train-loss: 0.010576627941595184\n",
      "[LOG 20200502-14:07:57] epoch: 4234 train-loss: 0.010576617386606004\n",
      "[LOG 20200502-14:07:57] epoch: 4235 train-loss: 0.010576606624656253\n",
      "[LOG 20200502-14:07:57] epoch: 4236 train-loss: 0.010576595966186788\n",
      "[LOG 20200502-14:07:57] epoch: 4237 train-loss: 0.010576585618158182\n",
      "[LOG 20200502-14:07:58] epoch: 4238 train-loss: 0.01057657516664929\n",
      "[LOG 20200502-14:07:58] epoch: 4239 train-loss: 0.01057656461166011\n",
      "[LOG 20200502-14:07:58] epoch: 4240 train-loss: 0.010576553953190645\n",
      "[LOG 20200502-14:07:58] epoch: 4241 train-loss: 0.010576543398201466\n",
      "[LOG 20200502-14:07:58] epoch: 4242 train-loss: 0.010576532429291142\n",
      "[LOG 20200502-14:07:59] epoch: 4243 train-loss: 0.010576522184742821\n",
      "[LOG 20200502-14:07:59] epoch: 4244 train-loss: 0.010576511319312785\n",
      "[LOG 20200502-14:07:59] epoch: 4245 train-loss: 0.010576500971284177\n",
      "[LOG 20200502-14:07:59] epoch: 4246 train-loss: 0.010576490002373854\n",
      "[LOG 20200502-14:08:00] epoch: 4247 train-loss: 0.010576478929983245\n",
      "[LOG 20200502-14:08:00] epoch: 4248 train-loss: 0.010576468374994066\n",
      "[LOG 20200502-14:08:00] epoch: 4249 train-loss: 0.010576457613044314\n",
      "[LOG 20200502-14:08:00] epoch: 4250 train-loss: 0.010576446540653706\n",
      "[LOG 20200502-14:08:00] epoch: 4251 train-loss: 0.010576436399585672\n",
      "[LOG 20200502-14:08:01] epoch: 4252 train-loss: 0.010576425430675348\n",
      "[LOG 20200502-14:08:01] epoch: 4253 train-loss: 0.010576413737403022\n",
      "[LOG 20200502-14:08:01] epoch: 4254 train-loss: 0.01057640245805184\n",
      "[LOG 20200502-14:08:01] epoch: 4255 train-loss: 0.010576391489141516\n",
      "[LOG 20200502-14:08:02] epoch: 4256 train-loss: 0.010576379899349477\n",
      "[LOG 20200502-14:08:02] epoch: 4257 train-loss: 0.010576368723478582\n",
      "[LOG 20200502-14:08:02] epoch: 4258 train-loss: 0.010576357133686543\n",
      "[LOG 20200502-14:08:02] epoch: 4259 train-loss: 0.01057634585433536\n",
      "[LOG 20200502-14:08:02] epoch: 4260 train-loss: 0.010576334368023608\n",
      "[LOG 20200502-14:08:03] epoch: 4261 train-loss: 0.010576323295632998\n",
      "[LOG 20200502-14:08:03] epoch: 4262 train-loss: 0.010576312016281817\n",
      "[LOG 20200502-14:08:03] epoch: 4263 train-loss: 0.010576300116048919\n",
      "[LOG 20200502-14:08:03] epoch: 4264 train-loss: 0.010576288836697737\n",
      "[LOG 20200502-14:08:03] epoch: 4265 train-loss: 0.010576277557346556\n",
      "[LOG 20200502-14:08:04] epoch: 4266 train-loss: 0.010576266071034802\n",
      "[LOG 20200502-14:08:04] epoch: 4267 train-loss: 0.010576254688203335\n",
      "[LOG 20200502-14:08:04] epoch: 4268 train-loss: 0.010576242891450724\n",
      "[LOG 20200502-14:08:04] epoch: 4269 train-loss: 0.010576231508619256\n",
      "[LOG 20200502-14:08:04] epoch: 4270 train-loss: 0.01057621981534693\n",
      "[LOG 20200502-14:08:05] epoch: 4271 train-loss: 0.010576207604673173\n",
      "[LOG 20200502-14:08:05] epoch: 4272 train-loss: 0.010576195290519131\n",
      "[LOG 20200502-14:08:05] epoch: 4273 train-loss: 0.010576183390286233\n",
      "[LOG 20200502-14:08:05] epoch: 4274 train-loss: 0.010576171490053335\n",
      "[LOG 20200502-14:08:05] epoch: 4275 train-loss: 0.010576159486340152\n",
      "[LOG 20200502-14:08:06] epoch: 4276 train-loss: 0.010576147379146682\n",
      "[LOG 20200502-14:08:06] epoch: 4277 train-loss: 0.01057613558239407\n",
      "[LOG 20200502-14:08:06] epoch: 4278 train-loss: 0.0105761234752006\n",
      "[LOG 20200502-14:08:06] epoch: 4279 train-loss: 0.010576110954085985\n",
      "[LOG 20200502-14:08:07] epoch: 4280 train-loss: 0.010576098950372802\n",
      "[LOG 20200502-14:08:07] epoch: 4281 train-loss: 0.01057608715362019\n",
      "[LOG 20200502-14:08:07] epoch: 4282 train-loss: 0.010576075149907006\n",
      "[LOG 20200502-14:08:07] epoch: 4283 train-loss: 0.010576062525312105\n",
      "[LOG 20200502-14:08:07] epoch: 4284 train-loss: 0.01057605031463835\n",
      "[LOG 20200502-14:08:08] epoch: 4285 train-loss: 0.010576038517885737\n",
      "[LOG 20200502-14:08:08] epoch: 4286 train-loss: 0.010576025996771123\n",
      "[LOG 20200502-14:08:08] epoch: 4287 train-loss: 0.010576013579136796\n",
      "[LOG 20200502-14:08:08] epoch: 4288 train-loss: 0.010576000437140465\n",
      "[LOG 20200502-14:08:09] epoch: 4289 train-loss: 0.010575988122986423\n",
      "[LOG 20200502-14:08:09] epoch: 4290 train-loss: 0.010575975705352094\n",
      "[LOG 20200502-14:08:09] epoch: 4291 train-loss: 0.010575962356395192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:08:09] epoch: 4292 train-loss: 0.010575949835280577\n",
      "[LOG 20200502-14:08:09] epoch: 4293 train-loss: 0.010575937003725104\n",
      "[LOG 20200502-14:08:10] epoch: 4294 train-loss: 0.010575924379130205\n",
      "[LOG 20200502-14:08:10] epoch: 4295 train-loss: 0.010575911133653589\n",
      "[LOG 20200502-14:08:10] epoch: 4296 train-loss: 0.010575898922979832\n",
      "[LOG 20200502-14:08:10] epoch: 4297 train-loss: 0.010575886194904646\n",
      "[LOG 20200502-14:08:11] epoch: 4298 train-loss: 0.010575873363349173\n",
      "[LOG 20200502-14:08:11] epoch: 4299 train-loss: 0.010575860428313414\n",
      "[LOG 20200502-14:08:11] epoch: 4300 train-loss: 0.010575847803718515\n",
      "[LOG 20200502-14:08:11] epoch: 4301 train-loss: 0.010575835075643327\n",
      "[LOG 20200502-14:08:12] epoch: 4302 train-loss: 0.010575822244087854\n",
      "[LOG 20200502-14:08:12] epoch: 4303 train-loss: 0.01057580868817038\n",
      "[LOG 20200502-14:08:12] epoch: 4304 train-loss: 0.010575795442693763\n",
      "[LOG 20200502-14:08:12] epoch: 4305 train-loss: 0.010575781990256574\n",
      "[LOG 20200502-14:08:12] epoch: 4306 train-loss: 0.010575769055220816\n",
      "[LOG 20200502-14:08:13] epoch: 4307 train-loss: 0.010575755188862482\n",
      "[LOG 20200502-14:08:13] epoch: 4308 train-loss: 0.010575742253826724\n",
      "[LOG 20200502-14:08:13] epoch: 4309 train-loss: 0.010575728697909249\n",
      "[LOG 20200502-14:08:13] epoch: 4310 train-loss: 0.010575715141991774\n",
      "[LOG 20200502-14:08:14] epoch: 4311 train-loss: 0.010575701793034872\n",
      "[LOG 20200502-14:08:14] epoch: 4312 train-loss: 0.01057568761623568\n",
      "[LOG 20200502-14:08:14] epoch: 4313 train-loss: 0.010575674888160493\n",
      "[LOG 20200502-14:08:14] epoch: 4314 train-loss: 0.010575661849644449\n",
      "[LOG 20200502-14:08:15] epoch: 4315 train-loss: 0.01057564839720726\n",
      "[LOG 20200502-14:08:15] epoch: 4316 train-loss: 0.010575634530848928\n",
      "[LOG 20200502-14:08:15] epoch: 4317 train-loss: 0.010575621492332883\n",
      "[LOG 20200502-14:08:15] epoch: 4318 train-loss: 0.01057560731553369\n",
      "[LOG 20200502-14:08:15] epoch: 4319 train-loss: 0.01057559365613593\n",
      "[LOG 20200502-14:08:16] epoch: 4320 train-loss: 0.010575580100218454\n",
      "[LOG 20200502-14:08:16] epoch: 4321 train-loss: 0.010575565923419263\n",
      "[LOG 20200502-14:08:16] epoch: 4322 train-loss: 0.0105755515396595\n",
      "[LOG 20200502-14:08:16] epoch: 4323 train-loss: 0.010575537673301168\n",
      "[LOG 20200502-14:08:16] epoch: 4324 train-loss: 0.010575524324344264\n",
      "[LOG 20200502-14:08:17] epoch: 4325 train-loss: 0.010575510044064786\n",
      "[LOG 20200502-14:08:17] epoch: 4326 train-loss: 0.010575495556824736\n",
      "[LOG 20200502-14:08:17] epoch: 4327 train-loss: 0.010575481483505832\n",
      "[LOG 20200502-14:08:17] epoch: 4328 train-loss: 0.010575467824108072\n",
      "[LOG 20200502-14:08:17] epoch: 4329 train-loss: 0.01057545416471031\n",
      "[LOG 20200502-14:08:18] epoch: 4330 train-loss: 0.010575440505312549\n",
      "[LOG 20200502-14:08:18] epoch: 4331 train-loss: 0.010575425914592214\n",
      "[LOG 20200502-14:08:18] epoch: 4332 train-loss: 0.01057541204823388\n",
      "[LOG 20200502-14:08:18] epoch: 4333 train-loss: 0.010575397457513545\n",
      "[LOG 20200502-14:08:19] epoch: 4334 train-loss: 0.010575383384194639\n",
      "[LOG 20200502-14:08:19] epoch: 4335 train-loss: 0.010575369000434875\n",
      "[LOG 20200502-14:08:19] epoch: 4336 train-loss: 0.010575354720155397\n",
      "[LOG 20200502-14:08:19] epoch: 4337 train-loss: 0.010575339818994204\n",
      "[LOG 20200502-14:08:19] epoch: 4338 train-loss: 0.010575325538714727\n",
      "[LOG 20200502-14:08:20] epoch: 4339 train-loss: 0.010575310844514105\n",
      "[LOG 20200502-14:08:20] epoch: 4340 train-loss: 0.010575296253793769\n",
      "[LOG 20200502-14:08:20] epoch: 4341 train-loss: 0.01057528176655372\n",
      "[LOG 20200502-14:08:20] epoch: 4342 train-loss: 0.010575267072353099\n",
      "[LOG 20200502-14:08:20] epoch: 4343 train-loss: 0.010575252688593335\n",
      "[LOG 20200502-14:08:21] epoch: 4344 train-loss: 0.010575237787432142\n",
      "[LOG 20200502-14:08:21] epoch: 4345 train-loss: 0.010575223714113235\n",
      "[LOG 20200502-14:08:21] epoch: 4346 train-loss: 0.010575208709471755\n",
      "[LOG 20200502-14:08:21] epoch: 4347 train-loss: 0.010575194532672564\n",
      "[LOG 20200502-14:08:22] epoch: 4348 train-loss: 0.01057517963151137\n",
      "[LOG 20200502-14:08:22] epoch: 4349 train-loss: 0.01057516410946846\n",
      "[LOG 20200502-14:08:22] epoch: 4350 train-loss: 0.01057514962222841\n",
      "[LOG 20200502-14:08:22] epoch: 4351 train-loss: 0.010575134617586931\n",
      "[LOG 20200502-14:08:22] epoch: 4352 train-loss: 0.010575119509465165\n",
      "[LOG 20200502-14:08:23] epoch: 4353 train-loss: 0.010575103987422254\n",
      "[LOG 20200502-14:08:23] epoch: 4354 train-loss: 0.010575089189741347\n",
      "[LOG 20200502-14:08:23] epoch: 4355 train-loss: 0.010575074081619581\n",
      "[LOG 20200502-14:08:23] epoch: 4356 train-loss: 0.010575059283938672\n",
      "[LOG 20200502-14:08:24] epoch: 4357 train-loss: 0.010575044072336622\n",
      "[LOG 20200502-14:08:24] epoch: 4358 train-loss: 0.01057502906769514\n",
      "[LOG 20200502-14:08:24] epoch: 4359 train-loss: 0.010575014270014234\n",
      "[LOG 20200502-14:08:24] epoch: 4360 train-loss: 0.010574999265372753\n",
      "[LOG 20200502-14:08:24] epoch: 4361 train-loss: 0.010574983639849557\n",
      "[LOG 20200502-14:08:25] epoch: 4362 train-loss: 0.01057496853172779\n",
      "[LOG 20200502-14:08:25] epoch: 4363 train-loss: 0.01057495249228345\n",
      "[LOG 20200502-14:08:25] epoch: 4364 train-loss: 0.010574937280681398\n",
      "[LOG 20200502-14:08:25] epoch: 4365 train-loss: 0.01057492196559906\n",
      "[LOG 20200502-14:08:26] epoch: 4366 train-loss: 0.01057490592615472\n",
      "[LOG 20200502-14:08:26] epoch: 4367 train-loss: 0.010574891024993526\n",
      "[LOG 20200502-14:08:26] epoch: 4368 train-loss: 0.010574875709911188\n",
      "[LOG 20200502-14:08:26] epoch: 4369 train-loss: 0.01057485987742742\n",
      "[LOG 20200502-14:08:26] epoch: 4370 train-loss: 0.01057484456234508\n",
      "[LOG 20200502-14:08:27] epoch: 4371 train-loss: 0.010574828936821885\n",
      "[LOG 20200502-14:08:27] epoch: 4372 train-loss: 0.010574813621739546\n",
      "[LOG 20200502-14:08:27] epoch: 4373 train-loss: 0.010574798099696636\n",
      "[LOG 20200502-14:08:27] epoch: 4374 train-loss: 0.010574782681134012\n",
      "[LOG 20200502-14:08:28] epoch: 4375 train-loss: 0.010574766745169958\n",
      "[LOG 20200502-14:08:28] epoch: 4376 train-loss: 0.01057475060224533\n",
      "[LOG 20200502-14:08:28] epoch: 4377 train-loss: 0.010574734769761562\n",
      "[LOG 20200502-14:08:28] epoch: 4378 train-loss: 0.010574718730317222\n",
      "[LOG 20200502-14:08:28] epoch: 4379 train-loss: 0.010574703311754597\n",
      "[LOG 20200502-14:08:29] epoch: 4380 train-loss: 0.010574687065349685\n",
      "[LOG 20200502-14:08:29] epoch: 4381 train-loss: 0.010574671439826488\n",
      "[LOG 20200502-14:08:29] epoch: 4382 train-loss: 0.01057465508994129\n",
      "[LOG 20200502-14:08:29] epoch: 4383 train-loss: 0.010574639464418093\n",
      "[LOG 20200502-14:08:30] epoch: 4384 train-loss: 0.010574623321493467\n",
      "[LOG 20200502-14:08:30] epoch: 4385 train-loss: 0.010574608006411128\n",
      "[LOG 20200502-14:08:30] epoch: 4386 train-loss: 0.010574591760006215\n",
      "[LOG 20200502-14:08:30] epoch: 4387 train-loss: 0.010574575720561875\n",
      "[LOG 20200502-14:08:30] epoch: 4388 train-loss: 0.01057455926719639\n",
      "[LOG 20200502-14:08:31] epoch: 4389 train-loss: 0.010574542917311192\n",
      "[LOG 20200502-14:08:31] epoch: 4390 train-loss: 0.010574527084827423\n",
      "[LOG 20200502-14:08:31] epoch: 4391 train-loss: 0.010574510114060508\n",
      "[LOG 20200502-14:08:31] epoch: 4392 train-loss: 0.010574494074616168\n",
      "[LOG 20200502-14:08:32] epoch: 4393 train-loss: 0.010574478449092971\n",
      "[LOG 20200502-14:08:32] epoch: 4394 train-loss: 0.010574461685286628\n",
      "[LOG 20200502-14:08:32] epoch: 4395 train-loss: 0.010574445645842288\n",
      "[LOG 20200502-14:08:32] epoch: 4396 train-loss: 0.01057442950291766\n",
      "[LOG 20200502-14:08:32] epoch: 4397 train-loss: 0.010574413359993033\n",
      "[LOG 20200502-14:08:33] epoch: 4398 train-loss: 0.010574397010107836\n",
      "[LOG 20200502-14:08:33] epoch: 4399 train-loss: 0.010574380453262065\n",
      "[LOG 20200502-14:08:33] epoch: 4400 train-loss: 0.01057436348249515\n",
      "[LOG 20200502-14:08:33] epoch: 4401 train-loss: 0.010574347339570522\n",
      "[LOG 20200502-14:08:34] epoch: 4402 train-loss: 0.010574330989685323\n",
      "[LOG 20200502-14:08:34] epoch: 4403 train-loss: 0.010574314122398695\n",
      "[LOG 20200502-14:08:34] epoch: 4404 train-loss: 0.010574297462072637\n",
      "[LOG 20200502-14:08:34] epoch: 4405 train-loss: 0.010574281112187438\n",
      "[LOG 20200502-14:08:34] epoch: 4406 train-loss: 0.010574264658821953\n",
      "[LOG 20200502-14:08:35] epoch: 4407 train-loss: 0.01057424820545647\n",
      "[LOG 20200502-14:08:35] epoch: 4408 train-loss: 0.01057423133816984\n",
      "[LOG 20200502-14:08:35] epoch: 4409 train-loss: 0.01057421498828464\n",
      "[LOG 20200502-14:08:35] epoch: 4410 train-loss: 0.01057419843143887\n",
      "[LOG 20200502-14:08:35] epoch: 4411 train-loss: 0.010574181150231097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:08:36] epoch: 4412 train-loss: 0.010574164489905039\n",
      "[LOG 20200502-14:08:36] epoch: 4413 train-loss: 0.01057414762261841\n",
      "[LOG 20200502-14:08:36] epoch: 4414 train-loss: 0.010574130651851496\n",
      "[LOG 20200502-14:08:36] epoch: 4415 train-loss: 0.01057411419848601\n",
      "[LOG 20200502-14:08:37] epoch: 4416 train-loss: 0.01057409712423881\n",
      "[LOG 20200502-14:08:37] epoch: 4417 train-loss: 0.010574080049991608\n",
      "[LOG 20200502-14:08:37] epoch: 4418 train-loss: 0.010574063182704978\n",
      "[LOG 20200502-14:08:37] epoch: 4419 train-loss: 0.010574046418898635\n",
      "[LOG 20200502-14:08:37] epoch: 4420 train-loss: 0.010574030172493722\n",
      "[LOG 20200502-14:08:38] epoch: 4421 train-loss: 0.010574012580845091\n",
      "[LOG 20200502-14:08:38] epoch: 4422 train-loss: 0.01057399519615703\n",
      "[LOG 20200502-14:08:38] epoch: 4423 train-loss: 0.010573978225390116\n",
      "[LOG 20200502-14:08:38] epoch: 4424 train-loss: 0.010573961047662629\n",
      "[LOG 20200502-14:08:39] epoch: 4425 train-loss: 0.010573944180376\n",
      "[LOG 20200502-14:08:39] epoch: 4426 train-loss: 0.010573926485247083\n",
      "[LOG 20200502-14:08:39] epoch: 4427 train-loss: 0.010573909928401312\n",
      "[LOG 20200502-14:08:39] epoch: 4428 train-loss: 0.010573892750673823\n",
      "[LOG 20200502-14:08:39] epoch: 4429 train-loss: 0.010573875572946336\n",
      "[LOG 20200502-14:08:40] epoch: 4430 train-loss: 0.010573858809139993\n",
      "[LOG 20200502-14:08:40] epoch: 4431 train-loss: 0.01057384152793222\n",
      "[LOG 20200502-14:08:40] epoch: 4432 train-loss: 0.01057382414324416\n",
      "[LOG 20200502-14:08:40] epoch: 4433 train-loss: 0.010573806758556101\n",
      "[LOG 20200502-14:08:41] epoch: 4434 train-loss: 0.010573789580828614\n",
      "[LOG 20200502-14:08:41] epoch: 4435 train-loss: 0.010573772403101126\n",
      "[LOG 20200502-14:08:41] epoch: 4436 train-loss: 0.010573754604491923\n",
      "[LOG 20200502-14:08:41] epoch: 4437 train-loss: 0.01057373701284329\n",
      "[LOG 20200502-14:08:41] epoch: 4438 train-loss: 0.010573720352517234\n",
      "[LOG 20200502-14:08:42] epoch: 4439 train-loss: 0.01057370255390803\n",
      "[LOG 20200502-14:08:42] epoch: 4440 train-loss: 0.01057368516921997\n",
      "[LOG 20200502-14:08:42] epoch: 4441 train-loss: 0.010573667888012197\n",
      "[LOG 20200502-14:08:42] epoch: 4442 train-loss: 0.010573650503324138\n",
      "[LOG 20200502-14:08:43] epoch: 4443 train-loss: 0.01057363280819522\n",
      "[LOG 20200502-14:08:43] epoch: 4444 train-loss: 0.010573615113066303\n",
      "[LOG 20200502-14:08:43] epoch: 4445 train-loss: 0.010573597417937385\n",
      "[LOG 20200502-14:08:43] epoch: 4446 train-loss: 0.010573579308887323\n",
      "[LOG 20200502-14:08:43] epoch: 4447 train-loss: 0.010573561924199263\n",
      "[LOG 20200502-14:08:44] epoch: 4448 train-loss: 0.010573544746471776\n",
      "[LOG 20200502-14:08:44] epoch: 4449 train-loss: 0.010573526947862573\n",
      "[LOG 20200502-14:08:44] epoch: 4450 train-loss: 0.010573509459694227\n",
      "[LOG 20200502-14:08:44] epoch: 4451 train-loss: 0.010573492075006167\n",
      "[LOG 20200502-14:08:44] epoch: 4452 train-loss: 0.01057347386247582\n",
      "[LOG 20200502-14:08:45] epoch: 4453 train-loss: 0.010573456063866615\n",
      "[LOG 20200502-14:08:45] epoch: 4454 train-loss: 0.010573438368737698\n",
      "[LOG 20200502-14:08:45] epoch: 4455 train-loss: 0.01057342015620735\n",
      "[LOG 20200502-14:08:45] epoch: 4456 train-loss: 0.010573402150637574\n",
      "[LOG 20200502-14:08:46] epoch: 4457 train-loss: 0.0105733848694298\n",
      "[LOG 20200502-14:08:46] epoch: 4458 train-loss: 0.010573366553419165\n",
      "[LOG 20200502-14:08:46] epoch: 4459 train-loss: 0.010573348754809963\n",
      "[LOG 20200502-14:08:46] epoch: 4460 train-loss: 0.010573331059681045\n",
      "[LOG 20200502-14:08:46] epoch: 4461 train-loss: 0.010573313468032412\n",
      "[LOG 20200502-14:08:47] epoch: 4462 train-loss: 0.01057329566942321\n",
      "[LOG 20200502-14:08:47] epoch: 4463 train-loss: 0.010573277249932289\n",
      "[LOG 20200502-14:08:47] epoch: 4464 train-loss: 0.010573258623480797\n",
      "[LOG 20200502-14:08:47] epoch: 4465 train-loss: 0.010573240824871592\n",
      "[LOG 20200502-14:08:48] epoch: 4466 train-loss: 0.010573222301900387\n",
      "[LOG 20200502-14:08:48] epoch: 4467 train-loss: 0.010573204399810897\n",
      "[LOG 20200502-14:08:48] epoch: 4468 train-loss: 0.010573186290760836\n",
      "[LOG 20200502-14:08:48] epoch: 4469 train-loss: 0.010573168492151631\n",
      "[LOG 20200502-14:08:48] epoch: 4470 train-loss: 0.010573151003983285\n",
      "[LOG 20200502-14:08:49] epoch: 4471 train-loss: 0.010573132170571221\n",
      "[LOG 20200502-14:08:49] epoch: 4472 train-loss: 0.010573113958040873\n",
      "[LOG 20200502-14:08:49] epoch: 4473 train-loss: 0.010573095021148523\n",
      "[LOG 20200502-14:08:49] epoch: 4474 train-loss: 0.01057307691209846\n",
      "[LOG 20200502-14:08:50] epoch: 4475 train-loss: 0.010573058492607541\n",
      "[LOG 20200502-14:08:50] epoch: 4476 train-loss: 0.010573039866156049\n",
      "[LOG 20200502-14:08:50] epoch: 4477 train-loss: 0.0105730216536257\n",
      "[LOG 20200502-14:08:50] epoch: 4478 train-loss: 0.010573003337615065\n",
      "[LOG 20200502-14:08:50] epoch: 4479 train-loss: 0.010572985228565004\n",
      "[LOG 20200502-14:08:51] epoch: 4480 train-loss: 0.010572966602113511\n",
      "[LOG 20200502-14:08:51] epoch: 4481 train-loss: 0.010572947975662019\n",
      "[LOG 20200502-14:08:51] epoch: 4482 train-loss: 0.010572929659651386\n",
      "[LOG 20200502-14:08:51] epoch: 4483 train-loss: 0.010572910101877319\n",
      "[LOG 20200502-14:08:52] epoch: 4484 train-loss: 0.010572892303268114\n",
      "[LOG 20200502-14:08:52] epoch: 4485 train-loss: 0.010572873883777194\n",
      "[LOG 20200502-14:08:52] epoch: 4486 train-loss: 0.010572854946884844\n",
      "[LOG 20200502-14:08:52] epoch: 4487 train-loss: 0.010572836216953065\n",
      "[LOG 20200502-14:08:52] epoch: 4488 train-loss: 0.010572818314863576\n",
      "[LOG 20200502-14:08:53] epoch: 4489 train-loss: 0.010572799171010653\n",
      "[LOG 20200502-14:08:53] epoch: 4490 train-loss: 0.01057278054455916\n",
      "[LOG 20200502-14:08:53] epoch: 4491 train-loss: 0.010572761400706239\n",
      "[LOG 20200502-14:08:53] epoch: 4492 train-loss: 0.010572742774254747\n",
      "[LOG 20200502-14:08:53] epoch: 4493 train-loss: 0.01057272394084268\n",
      "[LOG 20200502-14:08:54] epoch: 4494 train-loss: 0.010572705003950331\n",
      "[LOG 20200502-14:08:54] epoch: 4495 train-loss: 0.010572686687939696\n",
      "[LOG 20200502-14:08:54] epoch: 4496 train-loss: 0.010572667854527632\n",
      "[LOG 20200502-14:08:54] epoch: 4497 train-loss: 0.010572648607194424\n",
      "[LOG 20200502-14:08:55] epoch: 4498 train-loss: 0.01057262977378236\n",
      "[LOG 20200502-14:08:55] epoch: 4499 train-loss: 0.01057261104385058\n",
      "[LOG 20200502-14:08:55] epoch: 4500 train-loss: 0.010572591382596228\n",
      "[LOG 20200502-14:08:55] epoch: 4501 train-loss: 0.010572572963105308\n",
      "[LOG 20200502-14:08:56] epoch: 4502 train-loss: 0.0105725537157721\n",
      "[LOG 20200502-14:08:56] epoch: 4503 train-loss: 0.01057253498584032\n",
      "[LOG 20200502-14:08:56] epoch: 4504 train-loss: 0.010572515428066254\n",
      "[LOG 20200502-14:08:56] epoch: 4505 train-loss: 0.010572496905095048\n",
      "[LOG 20200502-14:08:56] epoch: 4506 train-loss: 0.010572477554281553\n",
      "[LOG 20200502-14:08:57] epoch: 4507 train-loss: 0.010572458099987771\n",
      "[LOG 20200502-14:08:57] epoch: 4508 train-loss: 0.010572438749174276\n",
      "[LOG 20200502-14:08:57] epoch: 4509 train-loss: 0.010572419501841068\n",
      "[LOG 20200502-14:08:57] epoch: 4510 train-loss: 0.010572400875389576\n",
      "[LOG 20200502-14:08:57] epoch: 4511 train-loss: 0.01057238100717465\n",
      "[LOG 20200502-14:08:58] epoch: 4512 train-loss: 0.010572362380723158\n",
      "[LOG 20200502-14:08:58] epoch: 4513 train-loss: 0.01057234313338995\n",
      "[LOG 20200502-14:08:58] epoch: 4514 train-loss: 0.010572323265175024\n",
      "[LOG 20200502-14:08:58] epoch: 4515 train-loss: 0.010572304121322103\n",
      "[LOG 20200502-14:08:58] epoch: 4516 train-loss: 0.010572284460067749\n",
      "[LOG 20200502-14:08:59] epoch: 4517 train-loss: 0.010572265109254254\n",
      "[LOG 20200502-14:08:59] epoch: 4518 train-loss: 0.010572246379322477\n",
      "[LOG 20200502-14:08:59] epoch: 4519 train-loss: 0.010572226407627264\n",
      "[LOG 20200502-14:08:59] epoch: 4520 train-loss: 0.010572207367254628\n",
      "[LOG 20200502-14:08:59] epoch: 4521 train-loss: 0.010572187912960848\n",
      "[LOG 20200502-14:09:00] epoch: 4522 train-loss: 0.010572168355186781\n",
      "[LOG 20200502-14:09:00] epoch: 4523 train-loss: 0.010572148590452142\n",
      "[LOG 20200502-14:09:00] epoch: 4524 train-loss: 0.010572128722237216\n",
      "[LOG 20200502-14:09:00] epoch: 4525 train-loss: 0.01057210885402229\n",
      "[LOG 20200502-14:09:01] epoch: 4526 train-loss: 0.010572089606689082\n",
      "[LOG 20200502-14:09:01] epoch: 4527 train-loss: 0.010572070255875587\n",
      "[LOG 20200502-14:09:01] epoch: 4528 train-loss: 0.010572050387660662\n",
      "[LOG 20200502-14:09:01] epoch: 4529 train-loss: 0.010572030312485166\n",
      "[LOG 20200502-14:09:01] epoch: 4530 train-loss: 0.010572010547750525\n",
      "[LOG 20200502-14:09:02] epoch: 4531 train-loss: 0.010571990783015886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:09:02] epoch: 4532 train-loss: 0.010571970914800962\n",
      "[LOG 20200502-14:09:02] epoch: 4533 train-loss: 0.010571951563987467\n",
      "[LOG 20200502-14:09:02] epoch: 4534 train-loss: 0.010571931385331683\n",
      "[LOG 20200502-14:09:02] epoch: 4535 train-loss: 0.010571911827557616\n",
      "[LOG 20200502-14:09:03] epoch: 4536 train-loss: 0.010571891441941261\n",
      "[LOG 20200502-14:09:03] epoch: 4537 train-loss: 0.010571871573726336\n",
      "[LOG 20200502-14:09:03] epoch: 4538 train-loss: 0.010571851395070553\n",
      "[LOG 20200502-14:09:03] epoch: 4539 train-loss: 0.010571831526855627\n",
      "[LOG 20200502-14:09:03] epoch: 4540 train-loss: 0.010571811141239272\n",
      "[LOG 20200502-14:09:04] epoch: 4541 train-loss: 0.01057179168694549\n",
      "[LOG 20200502-14:09:04] epoch: 4542 train-loss: 0.010571771611769995\n",
      "[LOG 20200502-14:09:04] epoch: 4543 train-loss: 0.01057175143311421\n",
      "[LOG 20200502-14:09:04] epoch: 4544 train-loss: 0.010571731254458427\n",
      "[LOG 20200502-14:09:05] epoch: 4545 train-loss: 0.010571710454920927\n",
      "[LOG 20200502-14:09:05] epoch: 4546 train-loss: 0.010571690483225716\n",
      "[LOG 20200502-14:09:05] epoch: 4547 train-loss: 0.010571670511530505\n",
      "[LOG 20200502-14:09:05] epoch: 4548 train-loss: 0.010571650332874723\n",
      "[LOG 20200502-14:09:05] epoch: 4549 train-loss: 0.01057163036117951\n",
      "[LOG 20200502-14:09:06] epoch: 4550 train-loss: 0.010571609872082869\n",
      "[LOG 20200502-14:09:06] epoch: 4551 train-loss: 0.01057158907254537\n",
      "[LOG 20200502-14:09:06] epoch: 4552 train-loss: 0.010571568997369872\n",
      "[LOG 20200502-14:09:06] epoch: 4553 train-loss: 0.01057154850827323\n",
      "[LOG 20200502-14:09:06] epoch: 4554 train-loss: 0.01057152853657802\n",
      "[LOG 20200502-14:09:07] epoch: 4555 train-loss: 0.010571508668363094\n",
      "[LOG 20200502-14:09:07] epoch: 4556 train-loss: 0.010571487661865022\n",
      "[LOG 20200502-14:09:07] epoch: 4557 train-loss: 0.010571466551886665\n",
      "[LOG 20200502-14:09:07] epoch: 4558 train-loss: 0.010571446580191454\n",
      "[LOG 20200502-14:09:07] epoch: 4559 train-loss: 0.010571425677173667\n",
      "[LOG 20200502-14:09:08] epoch: 4560 train-loss: 0.010571405188077025\n",
      "[LOG 20200502-14:09:08] epoch: 4561 train-loss: 0.01057138511290153\n",
      "[LOG 20200502-14:09:08] epoch: 4562 train-loss: 0.010571364313364029\n",
      "[LOG 20200502-14:09:08] epoch: 4563 train-loss: 0.010571343203385672\n",
      "[LOG 20200502-14:09:09] epoch: 4564 train-loss: 0.01057132271428903\n",
      "[LOG 20200502-14:09:09] epoch: 4565 train-loss: 0.010571301811271243\n",
      "[LOG 20200502-14:09:09] epoch: 4566 train-loss: 0.010571281218694316\n",
      "[LOG 20200502-14:09:09] epoch: 4567 train-loss: 0.010571261040038533\n",
      "[LOG 20200502-14:09:09] epoch: 4568 train-loss: 0.010571240137020746\n",
      "[LOG 20200502-14:09:10] epoch: 4569 train-loss: 0.010571218820081817\n",
      "[LOG 20200502-14:09:10] epoch: 4570 train-loss: 0.010571197917064032\n",
      "[LOG 20200502-14:09:10] epoch: 4571 train-loss: 0.010571177221006818\n",
      "[LOG 20200502-14:09:10] epoch: 4572 train-loss: 0.010571156214508746\n",
      "[LOG 20200502-14:09:10] epoch: 4573 train-loss: 0.01057113531149096\n",
      "[LOG 20200502-14:09:11] epoch: 4574 train-loss: 0.010571114615433745\n",
      "[LOG 20200502-14:09:11] epoch: 4575 train-loss: 0.010571093608935675\n",
      "[LOG 20200502-14:09:11] epoch: 4576 train-loss: 0.010571072085036172\n",
      "[LOG 20200502-14:09:11] epoch: 4577 train-loss: 0.010571051492459245\n",
      "[LOG 20200502-14:09:12] epoch: 4578 train-loss: 0.010571030485961173\n",
      "[LOG 20200502-14:09:12] epoch: 4579 train-loss: 0.010571010203825103\n",
      "[LOG 20200502-14:09:12] epoch: 4580 train-loss: 0.010570988059043884\n",
      "[LOG 20200502-14:09:12] epoch: 4581 train-loss: 0.010570967259506384\n",
      "[LOG 20200502-14:09:12] epoch: 4582 train-loss: 0.010570946459968885\n",
      "[LOG 20200502-14:09:13] epoch: 4583 train-loss: 0.010570925143029954\n",
      "[LOG 20200502-14:09:13] epoch: 4584 train-loss: 0.010570904033051597\n",
      "[LOG 20200502-14:09:13] epoch: 4585 train-loss: 0.010570882819592953\n",
      "[LOG 20200502-14:09:13] epoch: 4586 train-loss: 0.010570861088732878\n",
      "[LOG 20200502-14:09:13] epoch: 4587 train-loss: 0.010570839875274234\n",
      "[LOG 20200502-14:09:14] epoch: 4588 train-loss: 0.010570818661815591\n",
      "[LOG 20200502-14:09:14] epoch: 4589 train-loss: 0.01057079734487666\n",
      "[LOG 20200502-14:09:14] epoch: 4590 train-loss: 0.010570776234898303\n",
      "[LOG 20200502-14:09:14] epoch: 4591 train-loss: 0.010570754607518515\n",
      "[LOG 20200502-14:09:15] epoch: 4592 train-loss: 0.010570733083619012\n",
      "[LOG 20200502-14:09:15] epoch: 4593 train-loss: 0.010570711870160367\n",
      "[LOG 20200502-14:09:15] epoch: 4594 train-loss: 0.010570690139300294\n",
      "[LOG 20200502-14:09:15] epoch: 4595 train-loss: 0.010570668718881078\n",
      "[LOG 20200502-14:09:16] epoch: 4596 train-loss: 0.010570647194981575\n",
      "[LOG 20200502-14:09:16] epoch: 4597 train-loss: 0.010570625671082072\n",
      "[LOG 20200502-14:09:16] epoch: 4598 train-loss: 0.010570604043702284\n",
      "[LOG 20200502-14:09:16] epoch: 4599 train-loss: 0.010570582002401352\n",
      "[LOG 20200502-14:09:16] epoch: 4600 train-loss: 0.010570560892422995\n",
      "[LOG 20200502-14:09:17] epoch: 4601 train-loss: 0.010570539265043206\n",
      "[LOG 20200502-14:09:17] epoch: 4602 train-loss: 0.010570517430702845\n",
      "[LOG 20200502-14:09:17] epoch: 4603 train-loss: 0.01057049518244134\n",
      "[LOG 20200502-14:09:17] epoch: 4604 train-loss: 0.010570473762022125\n",
      "[LOG 20200502-14:09:18] epoch: 4605 train-loss: 0.010570451306800047\n",
      "[LOG 20200502-14:09:18] epoch: 4606 train-loss: 0.010570430610742833\n",
      "[LOG 20200502-14:09:18] epoch: 4607 train-loss: 0.010570408155520758\n",
      "[LOG 20200502-14:09:18] epoch: 4608 train-loss: 0.010570386321180396\n",
      "[LOG 20200502-14:09:18] epoch: 4609 train-loss: 0.010570364590320323\n",
      "[LOG 20200502-14:09:19] epoch: 4610 train-loss: 0.010570342549019389\n",
      "[LOG 20200502-14:09:19] epoch: 4611 train-loss: 0.0105703209216396\n",
      "[LOG 20200502-14:09:19] epoch: 4612 train-loss: 0.010570298776858382\n",
      "[LOG 20200502-14:09:19] epoch: 4613 train-loss: 0.010570276425116591\n",
      "[LOG 20200502-14:09:20] epoch: 4614 train-loss: 0.010570254590776231\n",
      "[LOG 20200502-14:09:20] epoch: 4615 train-loss: 0.010570232342514727\n",
      "[LOG 20200502-14:09:20] epoch: 4616 train-loss: 0.010570210611654652\n",
      "[LOG 20200502-14:09:20] epoch: 4617 train-loss: 0.010570188673834005\n",
      "[LOG 20200502-14:09:21] epoch: 4618 train-loss: 0.010570166011651358\n",
      "[LOG 20200502-14:09:21] epoch: 4619 train-loss: 0.01057014407383071\n",
      "[LOG 20200502-14:09:21] epoch: 4620 train-loss: 0.010570121825569205\n",
      "[LOG 20200502-14:09:21] epoch: 4621 train-loss: 0.010570099577307701\n",
      "[LOG 20200502-14:09:22] epoch: 4622 train-loss: 0.01057007722556591\n",
      "[LOG 20200502-14:09:22] epoch: 4623 train-loss: 0.010570054356422689\n",
      "[LOG 20200502-14:09:22] epoch: 4624 train-loss: 0.010570032108161185\n",
      "[LOG 20200502-14:09:22] epoch: 4625 train-loss: 0.010570010170340538\n",
      "[LOG 20200502-14:09:23] epoch: 4626 train-loss: 0.010569987922079034\n",
      "[LOG 20200502-14:09:23] epoch: 4627 train-loss: 0.010569965673817528\n",
      "[LOG 20200502-14:09:23] epoch: 4628 train-loss: 0.010569942701194022\n",
      "[LOG 20200502-14:09:23] epoch: 4629 train-loss: 0.010569920245971944\n",
      "[LOG 20200502-14:09:23] epoch: 4630 train-loss: 0.010569897790749868\n",
      "[LOG 20200502-14:09:24] epoch: 4631 train-loss: 0.010569875542488363\n",
      "[LOG 20200502-14:09:24] epoch: 4632 train-loss: 0.010569852776825428\n",
      "[LOG 20200502-14:09:24] epoch: 4633 train-loss: 0.010569830011162493\n",
      "[LOG 20200502-14:09:25] epoch: 4634 train-loss: 0.010569807762900988\n",
      "[LOG 20200502-14:09:25] epoch: 4635 train-loss: 0.01056978510071834\n",
      "[LOG 20200502-14:09:25] epoch: 4636 train-loss: 0.010569762128094832\n",
      "[LOG 20200502-14:09:25] epoch: 4637 train-loss: 0.010569739258951612\n",
      "[LOG 20200502-14:09:26] epoch: 4638 train-loss: 0.010569716907209821\n",
      "[LOG 20200502-14:09:26] epoch: 4639 train-loss: 0.010569694141546885\n",
      "[LOG 20200502-14:09:26] epoch: 4640 train-loss: 0.010569671272403665\n",
      "[LOG 20200502-14:09:26] epoch: 4641 train-loss: 0.010569648092819585\n",
      "[LOG 20200502-14:09:26] epoch: 4642 train-loss: 0.01056962584455808\n",
      "[LOG 20200502-14:09:27] epoch: 4643 train-loss: 0.010569602871934572\n",
      "[LOG 20200502-14:09:27] epoch: 4644 train-loss: 0.010569579588870207\n",
      "[LOG 20200502-14:09:27] epoch: 4645 train-loss: 0.0105695566162467\n",
      "[LOG 20200502-14:09:27] epoch: 4646 train-loss: 0.010569534057544338\n",
      "[LOG 20200502-14:09:28] epoch: 4647 train-loss: 0.010569511395361688\n",
      "[LOG 20200502-14:09:28] epoch: 4648 train-loss: 0.010569488008817038\n",
      "[LOG 20200502-14:09:28] epoch: 4649 train-loss: 0.010569464829232957\n",
      "[LOG 20200502-14:09:28] epoch: 4650 train-loss: 0.010569441442688307\n",
      "[LOG 20200502-14:09:29] epoch: 4651 train-loss: 0.010569419401387373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:09:29] epoch: 4652 train-loss: 0.01056939580788215\n",
      "[LOG 20200502-14:09:29] epoch: 4653 train-loss: 0.010569372421337498\n",
      "[LOG 20200502-14:09:29] epoch: 4654 train-loss: 0.010569349345233705\n",
      "[LOG 20200502-14:09:30] epoch: 4655 train-loss: 0.010569326165649626\n",
      "[LOG 20200502-14:09:30] epoch: 4656 train-loss: 0.010569302675624689\n",
      "[LOG 20200502-14:09:30] epoch: 4657 train-loss: 0.010569279392560324\n",
      "[LOG 20200502-14:09:30] epoch: 4658 train-loss: 0.010569256006015671\n",
      "[LOG 20200502-14:09:31] epoch: 4659 train-loss: 0.010569232412510447\n",
      "[LOG 20200502-14:09:31] epoch: 4660 train-loss: 0.010569209543367227\n",
      "[LOG 20200502-14:09:31] epoch: 4661 train-loss: 0.010569185742901431\n",
      "[LOG 20200502-14:09:31] epoch: 4662 train-loss: 0.010569162459837066\n",
      "[LOG 20200502-14:09:32] epoch: 4663 train-loss: 0.01056913948721356\n",
      "[LOG 20200502-14:09:32] epoch: 4664 train-loss: 0.010569115376306904\n",
      "[LOG 20200502-14:09:32] epoch: 4665 train-loss: 0.010569091679321395\n",
      "[LOG 20200502-14:09:32] epoch: 4666 train-loss: 0.010569068292776743\n",
      "[LOG 20200502-14:09:33] epoch: 4667 train-loss: 0.01056904469927152\n",
      "[LOG 20200502-14:09:33] epoch: 4668 train-loss: 0.010569021312726868\n",
      "[LOG 20200502-14:09:33] epoch: 4669 train-loss: 0.0105689973053005\n",
      "[LOG 20200502-14:09:33] epoch: 4670 train-loss: 0.010568973608314991\n",
      "[LOG 20200502-14:09:34] epoch: 4671 train-loss: 0.010568949911329482\n",
      "[LOG 20200502-14:09:34] epoch: 4672 train-loss: 0.010568926007383399\n",
      "[LOG 20200502-14:09:34] epoch: 4673 train-loss: 0.010568902103437318\n",
      "[LOG 20200502-14:09:34] epoch: 4674 train-loss: 0.01056887861341238\n",
      "[LOG 20200502-14:09:35] epoch: 4675 train-loss: 0.010568854295545153\n",
      "[LOG 20200502-14:09:35] epoch: 4676 train-loss: 0.010568830391599072\n",
      "[LOG 20200502-14:09:35] epoch: 4677 train-loss: 0.010568806901574135\n",
      "[LOG 20200502-14:09:35] epoch: 4678 train-loss: 0.010568782790667482\n",
      "[LOG 20200502-14:09:36] epoch: 4679 train-loss: 0.010568758576280542\n",
      "[LOG 20200502-14:09:36] epoch: 4680 train-loss: 0.010568734568854174\n",
      "[LOG 20200502-14:09:36] epoch: 4681 train-loss: 0.010568711182309521\n",
      "[LOG 20200502-14:09:36] epoch: 4682 train-loss: 0.010568686657481723\n",
      "[LOG 20200502-14:09:37] epoch: 4683 train-loss: 0.01056866254657507\n",
      "[LOG 20200502-14:09:37] epoch: 4684 train-loss: 0.010568638435668416\n",
      "[LOG 20200502-14:09:37] epoch: 4685 train-loss: 0.010568614324761761\n",
      "[LOG 20200502-14:09:37] epoch: 4686 train-loss: 0.010568590110374821\n",
      "[LOG 20200502-14:09:37] epoch: 4687 train-loss: 0.010568565895987881\n",
      "[LOG 20200502-14:09:38] epoch: 4688 train-loss: 0.010568541888561513\n",
      "[LOG 20200502-14:09:38] epoch: 4689 train-loss: 0.010568516846332286\n",
      "[LOG 20200502-14:09:38] epoch: 4690 train-loss: 0.010568492838905917\n",
      "[LOG 20200502-14:09:38] epoch: 4691 train-loss: 0.010568469038440121\n",
      "[LOG 20200502-14:09:39] epoch: 4692 train-loss: 0.010568444203171466\n",
      "[LOG 20200502-14:09:39] epoch: 4693 train-loss: 0.010568419781823954\n",
      "[LOG 20200502-14:09:39] epoch: 4694 train-loss: 0.010568395360476442\n",
      "[LOG 20200502-14:09:39] epoch: 4695 train-loss: 0.010568370628688071\n",
      "[LOG 20200502-14:09:40] epoch: 4696 train-loss: 0.010568346724741988\n",
      "[LOG 20200502-14:09:40] epoch: 4697 train-loss: 0.010568322303394476\n",
      "[LOG 20200502-14:09:40] epoch: 4698 train-loss: 0.01056829746812582\n",
      "[LOG 20200502-14:09:40] epoch: 4699 train-loss: 0.010568272322416306\n",
      "[LOG 20200502-14:09:40] epoch: 4700 train-loss: 0.010568248728911081\n",
      "[LOG 20200502-14:09:41] epoch: 4701 train-loss: 0.010568223686681854\n",
      "[LOG 20200502-14:09:41] epoch: 4702 train-loss: 0.010568198644452624\n",
      "[LOG 20200502-14:09:41] epoch: 4703 train-loss: 0.010568174430065684\n",
      "[LOG 20200502-14:09:41] epoch: 4704 train-loss: 0.010568148973915312\n",
      "[LOG 20200502-14:09:42] epoch: 4705 train-loss: 0.01056812506996923\n",
      "[LOG 20200502-14:09:42] epoch: 4706 train-loss: 0.01056809982077943\n",
      "[LOG 20200502-14:09:42] epoch: 4707 train-loss: 0.0105680747785502\n",
      "[LOG 20200502-14:09:42] epoch: 4708 train-loss: 0.010568049943281544\n",
      "[LOG 20200502-14:09:43] epoch: 4709 train-loss: 0.010568025108012889\n",
      "[LOG 20200502-14:09:43] epoch: 4710 train-loss: 0.01056800058318509\n",
      "[LOG 20200502-14:09:43] epoch: 4711 train-loss: 0.010567975540955862\n",
      "[LOG 20200502-14:09:43] epoch: 4712 train-loss: 0.010567950498726632\n",
      "[LOG 20200502-14:09:43] epoch: 4713 train-loss: 0.01056792555997769\n",
      "[LOG 20200502-14:09:44] epoch: 4714 train-loss: 0.010567900000347031\n",
      "[LOG 20200502-14:09:44] epoch: 4715 train-loss: 0.01056787557899952\n",
      "[LOG 20200502-14:09:44] epoch: 4716 train-loss: 0.010567850640250577\n",
      "[LOG 20200502-14:09:44] epoch: 4717 train-loss: 0.010567824977139631\n",
      "[LOG 20200502-14:09:45] epoch: 4718 train-loss: 0.010567799934910404\n",
      "[LOG 20200502-14:09:45] epoch: 4719 train-loss: 0.010567774892681174\n",
      "[LOG 20200502-14:09:45] epoch: 4720 train-loss: 0.010567749643491374\n",
      "[LOG 20200502-14:09:45] epoch: 4721 train-loss: 0.010567724704742432\n",
      "[LOG 20200502-14:09:45] epoch: 4722 train-loss: 0.010567699145111773\n",
      "[LOG 20200502-14:09:46] epoch: 4723 train-loss: 0.010567674102882544\n",
      "[LOG 20200502-14:09:46] epoch: 4724 train-loss: 0.010567648543251885\n",
      "[LOG 20200502-14:09:46] epoch: 4725 train-loss: 0.010567623190581799\n",
      "[LOG 20200502-14:09:46] epoch: 4726 train-loss: 0.010567598044872284\n",
      "[LOG 20200502-14:09:47] epoch: 4727 train-loss: 0.010567572278281053\n",
      "[LOG 20200502-14:09:47] epoch: 4728 train-loss: 0.010567547029091252\n",
      "[LOG 20200502-14:09:47] epoch: 4729 train-loss: 0.010567521572940879\n",
      "[LOG 20200502-14:09:47] epoch: 4730 train-loss: 0.01056749601331022\n",
      "[LOG 20200502-14:09:48] epoch: 4731 train-loss: 0.010567470557159848\n",
      "[LOG 20200502-14:09:48] epoch: 4732 train-loss: 0.010567445101009475\n",
      "[LOG 20200502-14:09:48] epoch: 4733 train-loss: 0.010567419541378817\n",
      "[LOG 20200502-14:09:48] epoch: 4734 train-loss: 0.01056739418870873\n",
      "[LOG 20200502-14:09:48] epoch: 4735 train-loss: 0.01056736862907807\n",
      "[LOG 20200502-14:09:49] epoch: 4736 train-loss: 0.010567343172927698\n",
      "[LOG 20200502-14:09:49] epoch: 4737 train-loss: 0.010567317406336466\n",
      "[LOG 20200502-14:09:49] epoch: 4738 train-loss: 0.010567291225824092\n",
      "[LOG 20200502-14:09:49] epoch: 4739 train-loss: 0.010567266183594862\n",
      "[LOG 20200502-14:09:50] epoch: 4740 train-loss: 0.010567240313523345\n",
      "[LOG 20200502-14:09:50] epoch: 4741 train-loss: 0.010567214339971542\n",
      "[LOG 20200502-14:09:50] epoch: 4742 train-loss: 0.010567188780340884\n",
      "[LOG 20200502-14:09:50] epoch: 4743 train-loss: 0.010567163013749652\n",
      "[LOG 20200502-14:09:51] epoch: 4744 train-loss: 0.010567137247158421\n",
      "[LOG 20200502-14:09:51] epoch: 4745 train-loss: 0.010567111273606619\n",
      "[LOG 20200502-14:09:51] epoch: 4746 train-loss: 0.010567085610495673\n",
      "[LOG 20200502-14:09:51] epoch: 4747 train-loss: 0.010567059947384728\n",
      "[LOG 20200502-14:09:51] epoch: 4748 train-loss: 0.010567033456431495\n",
      "[LOG 20200502-14:09:52] epoch: 4749 train-loss: 0.01056700779332055\n",
      "[LOG 20200502-14:09:52] epoch: 4750 train-loss: 0.01056698171628846\n",
      "[LOG 20200502-14:09:52] epoch: 4751 train-loss: 0.01056695646709866\n",
      "[LOG 20200502-14:09:52] epoch: 4752 train-loss: 0.010566929976145426\n",
      "[LOG 20200502-14:09:53] epoch: 4753 train-loss: 0.010566904209554195\n",
      "[LOG 20200502-14:09:53] epoch: 4754 train-loss: 0.010566878442962965\n",
      "[LOG 20200502-14:09:53] epoch: 4755 train-loss: 0.010566852055490017\n",
      "[LOG 20200502-14:09:53] epoch: 4756 train-loss: 0.010566826495859358\n",
      "[LOG 20200502-14:09:53] epoch: 4757 train-loss: 0.010566800211866697\n",
      "[LOG 20200502-14:09:54] epoch: 4758 train-loss: 0.01056677434179518\n",
      "[LOG 20200502-14:09:54] epoch: 4759 train-loss: 0.010566748368243376\n",
      "[LOG 20200502-14:09:54] epoch: 4760 train-loss: 0.010566722084250715\n",
      "[LOG 20200502-14:09:54] epoch: 4761 train-loss: 0.01056669590373834\n",
      "[LOG 20200502-14:09:54] epoch: 4762 train-loss: 0.010566669930186536\n",
      "[LOG 20200502-14:09:55] epoch: 4763 train-loss: 0.010566643646193875\n",
      "[LOG 20200502-14:09:55] epoch: 4764 train-loss: 0.010566617569161786\n",
      "[LOG 20200502-14:09:55] epoch: 4765 train-loss: 0.010566591802570555\n",
      "[LOG 20200502-14:09:55] epoch: 4766 train-loss: 0.01056656562205818\n",
      "[LOG 20200502-14:09:56] epoch: 4767 train-loss: 0.010566539441545805\n",
      "[LOG 20200502-14:09:56] epoch: 4768 train-loss: 0.010566513364513716\n",
      "[LOG 20200502-14:09:56] epoch: 4769 train-loss: 0.010566487287481626\n",
      "[LOG 20200502-14:09:56] epoch: 4770 train-loss: 0.010566461313929822\n",
      "[LOG 20200502-14:09:56] epoch: 4771 train-loss: 0.010566434719496302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:09:57] epoch: 4772 train-loss: 0.0105664087459445\n",
      "[LOG 20200502-14:09:57] epoch: 4773 train-loss: 0.010566382565432124\n",
      "[LOG 20200502-14:09:57] epoch: 4774 train-loss: 0.01056635638491975\n",
      "[LOG 20200502-14:09:57] epoch: 4775 train-loss: 0.010566329997446802\n",
      "[LOG 20200502-14:09:58] epoch: 4776 train-loss: 0.01056630371345414\n",
      "[LOG 20200502-14:09:58] epoch: 4777 train-loss: 0.010566277636422051\n",
      "[LOG 20200502-14:09:58] epoch: 4778 train-loss: 0.01056625135242939\n",
      "[LOG 20200502-14:09:58] epoch: 4779 train-loss: 0.010566225068436729\n",
      "[LOG 20200502-14:09:59] epoch: 4780 train-loss: 0.010566199198365211\n",
      "[LOG 20200502-14:09:59] epoch: 4781 train-loss: 0.010566172603931692\n",
      "[LOG 20200502-14:09:59] epoch: 4782 train-loss: 0.010566147044301033\n",
      "[LOG 20200502-14:09:59] epoch: 4783 train-loss: 0.010566120242906941\n",
      "[LOG 20200502-14:10:00] epoch: 4784 train-loss: 0.010566094062394567\n",
      "[LOG 20200502-14:10:00] epoch: 4785 train-loss: 0.010566068088842763\n",
      "[LOG 20200502-14:10:00] epoch: 4786 train-loss: 0.010566042011810673\n",
      "[LOG 20200502-14:10:00] epoch: 4787 train-loss: 0.010566015831298299\n",
      "[LOG 20200502-14:10:01] epoch: 4788 train-loss: 0.010565990168187354\n",
      "[LOG 20200502-14:10:01] epoch: 4789 train-loss: 0.01056596315983269\n",
      "[LOG 20200502-14:10:01] epoch: 4790 train-loss: 0.010565937289761173\n",
      "[LOG 20200502-14:10:01] epoch: 4791 train-loss: 0.010565910902288225\n",
      "[LOG 20200502-14:10:02] epoch: 4792 train-loss: 0.010565885135696994\n",
      "[LOG 20200502-14:10:02] epoch: 4793 train-loss: 0.010565858851704333\n",
      "[LOG 20200502-14:10:02] epoch: 4794 train-loss: 0.0105658330851131\n",
      "[LOG 20200502-14:10:02] epoch: 4795 train-loss: 0.01056580680112044\n",
      "[LOG 20200502-14:10:03] epoch: 4796 train-loss: 0.010565780517127778\n",
      "[LOG 20200502-14:10:03] epoch: 4797 train-loss: 0.010565754233135117\n",
      "[LOG 20200502-14:10:03] epoch: 4798 train-loss: 0.010565727949142456\n",
      "[LOG 20200502-14:10:03] epoch: 4799 train-loss: 0.010565702182551226\n",
      "[LOG 20200502-14:10:04] epoch: 4800 train-loss: 0.010565675795078278\n",
      "[LOG 20200502-14:10:04] epoch: 4801 train-loss: 0.010565650338927904\n",
      "[LOG 20200502-14:10:04] epoch: 4802 train-loss: 0.010565623744494386\n",
      "[LOG 20200502-14:10:04] epoch: 4803 train-loss: 0.010565598288344013\n",
      "[LOG 20200502-14:10:04] epoch: 4804 train-loss: 0.010565572211311923\n",
      "[LOG 20200502-14:10:05] epoch: 4805 train-loss: 0.010565545927319262\n",
      "[LOG 20200502-14:10:05] epoch: 4806 train-loss: 0.010565519953767458\n",
      "[LOG 20200502-14:10:05] epoch: 4807 train-loss: 0.010565494497617086\n",
      "[LOG 20200502-14:10:05] epoch: 4808 train-loss: 0.010565468420584997\n",
      "[LOG 20200502-14:10:06] epoch: 4809 train-loss: 0.010565442447033193\n",
      "[LOG 20200502-14:10:06] epoch: 4810 train-loss: 0.010565416266520819\n",
      "[LOG 20200502-14:10:06] epoch: 4811 train-loss: 0.010565390499929586\n",
      "[LOG 20200502-14:10:06] epoch: 4812 train-loss: 0.010565364940298928\n",
      "[LOG 20200502-14:10:06] epoch: 4813 train-loss: 0.010565338966747126\n",
      "[LOG 20200502-14:10:07] epoch: 4814 train-loss: 0.010565313510596752\n",
      "[LOG 20200502-14:10:07] epoch: 4815 train-loss: 0.010565287744005522\n",
      "[LOG 20200502-14:10:07] epoch: 4816 train-loss: 0.010565262287855148\n",
      "[LOG 20200502-14:10:07] epoch: 4817 train-loss: 0.01056523672822449\n",
      "[LOG 20200502-14:10:08] epoch: 4818 train-loss: 0.010565210858152973\n",
      "[LOG 20200502-14:10:08] epoch: 4819 train-loss: 0.010565185505482886\n",
      "[LOG 20200502-14:10:08] epoch: 4820 train-loss: 0.010565160049332513\n",
      "[LOG 20200502-14:10:08] epoch: 4821 train-loss: 0.010565134903622998\n",
      "[LOG 20200502-14:10:08] epoch: 4822 train-loss: 0.01056510903355148\n",
      "[LOG 20200502-14:10:09] epoch: 4823 train-loss: 0.010565083887841966\n",
      "[LOG 20200502-14:10:09] epoch: 4824 train-loss: 0.01056505853517188\n",
      "[LOG 20200502-14:10:09] epoch: 4825 train-loss: 0.010565032975541221\n",
      "[LOG 20200502-14:10:09] epoch: 4826 train-loss: 0.010565007829831706\n",
      "[LOG 20200502-14:10:10] epoch: 4827 train-loss: 0.010564982787602477\n",
      "[LOG 20200502-14:10:10] epoch: 4828 train-loss: 0.010564957538412677\n",
      "[LOG 20200502-14:10:10] epoch: 4829 train-loss: 0.01056493218574259\n",
      "[LOG 20200502-14:10:10] epoch: 4830 train-loss: 0.010564907143513361\n",
      "[LOG 20200502-14:10:11] epoch: 4831 train-loss: 0.010564881894323561\n",
      "[LOG 20200502-14:10:11] epoch: 4832 train-loss: 0.01056485716253519\n",
      "[LOG 20200502-14:10:11] epoch: 4833 train-loss: 0.010564832637707392\n",
      "[LOG 20200502-14:10:11] epoch: 4834 train-loss: 0.010564807491997877\n",
      "[LOG 20200502-14:10:11] epoch: 4835 train-loss: 0.010564783070650365\n",
      "[LOG 20200502-14:10:12] epoch: 4836 train-loss: 0.010564757614499994\n",
      "[LOG 20200502-14:10:12] epoch: 4837 train-loss: 0.010564733089672195\n",
      "[LOG 20200502-14:10:12] epoch: 4838 train-loss: 0.010564708668324683\n",
      "[LOG 20200502-14:10:12] epoch: 4839 train-loss: 0.010564684246977171\n",
      "[LOG 20200502-14:10:13] epoch: 4840 train-loss: 0.010564659722149372\n",
      "[LOG 20200502-14:10:13] epoch: 4841 train-loss: 0.010564635197321573\n",
      "[LOG 20200502-14:10:13] epoch: 4842 train-loss: 0.010564610879454348\n",
      "[LOG 20200502-14:10:13] epoch: 4843 train-loss: 0.01056458635462655\n",
      "[LOG 20200502-14:10:13] epoch: 4844 train-loss: 0.010564562761121325\n",
      "[LOG 20200502-14:10:14] epoch: 4845 train-loss: 0.010564538132813241\n",
      "[LOG 20200502-14:10:14] epoch: 4846 train-loss: 0.010564514435827732\n",
      "[LOG 20200502-14:10:14] epoch: 4847 train-loss: 0.010564489910999933\n",
      "[LOG 20200502-14:10:14] epoch: 4848 train-loss: 0.010564466214014424\n",
      "[LOG 20200502-14:10:15] epoch: 4849 train-loss: 0.010564442723989487\n",
      "[LOG 20200502-14:10:15] epoch: 4850 train-loss: 0.01056441892352369\n",
      "[LOG 20200502-14:10:15] epoch: 4851 train-loss: 0.010564395226538181\n",
      "[LOG 20200502-14:10:15] epoch: 4852 train-loss: 0.010564371839993529\n",
      "[LOG 20200502-14:10:16] epoch: 4853 train-loss: 0.010564348246488307\n",
      "[LOG 20200502-14:10:16] epoch: 4854 train-loss: 0.010564324652983082\n",
      "[LOG 20200502-14:10:16] epoch: 4855 train-loss: 0.010564301369918717\n",
      "[LOG 20200502-14:10:16] epoch: 4856 train-loss: 0.010564278293814924\n",
      "[LOG 20200502-14:10:16] epoch: 4857 train-loss: 0.01056425521771113\n",
      "[LOG 20200502-14:10:17] epoch: 4858 train-loss: 0.010564232555528482\n",
      "[LOG 20200502-14:10:17] epoch: 4859 train-loss: 0.010564209272464117\n",
      "[LOG 20200502-14:10:17] epoch: 4860 train-loss: 0.01056418650680118\n",
      "[LOG 20200502-14:10:17] epoch: 4861 train-loss: 0.010564163844618533\n",
      "[LOG 20200502-14:10:17] epoch: 4862 train-loss: 0.010564141182435883\n",
      "[LOG 20200502-14:10:18] epoch: 4863 train-loss: 0.010564118313292662\n",
      "[LOG 20200502-14:10:18] epoch: 4864 train-loss: 0.010564096375472017\n",
      "[LOG 20200502-14:10:18] epoch: 4865 train-loss: 0.010564074127210511\n",
      "[LOG 20200502-14:10:18] epoch: 4866 train-loss: 0.010564052189389864\n",
      "[LOG 20200502-14:10:19] epoch: 4867 train-loss: 0.010564029837648073\n",
      "[LOG 20200502-14:10:19] epoch: 4868 train-loss: 0.010564007692866854\n",
      "[LOG 20200502-14:10:19] epoch: 4869 train-loss: 0.010563985858526494\n",
      "[LOG 20200502-14:10:19] epoch: 4870 train-loss: 0.010563964127666421\n",
      "[LOG 20200502-14:10:20] epoch: 4871 train-loss: 0.01056394281072749\n",
      "[LOG 20200502-14:10:20] epoch: 4872 train-loss: 0.010563921079867415\n",
      "[LOG 20200502-14:10:20] epoch: 4873 train-loss: 0.010563899866408773\n",
      "[LOG 20200502-14:10:20] epoch: 4874 train-loss: 0.010563878239028983\n",
      "[LOG 20200502-14:10:20] epoch: 4875 train-loss: 0.010563857232530912\n",
      "[LOG 20200502-14:10:21] epoch: 4876 train-loss: 0.010563836329513125\n",
      "[LOG 20200502-14:10:21] epoch: 4877 train-loss: 0.010563815529975627\n",
      "[LOG 20200502-14:10:21] epoch: 4878 train-loss: 0.010563794730438126\n",
      "[LOG 20200502-14:10:21] epoch: 4879 train-loss: 0.0105637741378612\n",
      "[LOG 20200502-14:10:22] epoch: 4880 train-loss: 0.010563753752244843\n",
      "[LOG 20200502-14:10:22] epoch: 4881 train-loss: 0.010563733470108774\n",
      "[LOG 20200502-14:10:22] epoch: 4882 train-loss: 0.010563713394933276\n",
      "[LOG 20200502-14:10:22] epoch: 4883 train-loss: 0.010563693526718352\n",
      "[LOG 20200502-14:10:23] epoch: 4884 train-loss: 0.010563673451542854\n",
      "[LOG 20200502-14:10:23] epoch: 4885 train-loss: 0.010563653583327929\n",
      "[LOG 20200502-14:10:23] epoch: 4886 train-loss: 0.01056363433599472\n",
      "[LOG 20200502-14:10:23] epoch: 4887 train-loss: 0.010563614674740367\n",
      "[LOG 20200502-14:10:24] epoch: 4888 train-loss: 0.010563595013486015\n",
      "[LOG 20200502-14:10:24] epoch: 4889 train-loss: 0.010563576076593664\n",
      "[LOG 20200502-14:10:24] epoch: 4890 train-loss: 0.010563557139701314\n",
      "[LOG 20200502-14:10:24] epoch: 4891 train-loss: 0.010563538409769535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:10:25] epoch: 4892 train-loss: 0.01056351988679833\n",
      "[LOG 20200502-14:10:25] epoch: 4893 train-loss: 0.010563501570787694\n",
      "[LOG 20200502-14:10:25] epoch: 4894 train-loss: 0.010563482633895345\n",
      "[LOG 20200502-14:10:25] epoch: 4895 train-loss: 0.010563464731805854\n",
      "[LOG 20200502-14:10:25] epoch: 4896 train-loss: 0.010563446312314935\n",
      "[LOG 20200502-14:10:26] epoch: 4897 train-loss: 0.010563428720666302\n",
      "[LOG 20200502-14:10:26] epoch: 4898 train-loss: 0.010563411025537385\n",
      "[LOG 20200502-14:10:26] epoch: 4899 train-loss: 0.010563393330408467\n",
      "[LOG 20200502-14:10:26] epoch: 4900 train-loss: 0.010563375842240121\n",
      "[LOG 20200502-14:10:27] epoch: 4901 train-loss: 0.010563358664512634\n",
      "[LOG 20200502-14:10:27] epoch: 4902 train-loss: 0.010563342004186578\n",
      "[LOG 20200502-14:10:27] epoch: 4903 train-loss: 0.01056332534386052\n",
      "[LOG 20200502-14:10:27] epoch: 4904 train-loss: 0.010563308269613318\n",
      "[LOG 20200502-14:10:27] epoch: 4905 train-loss: 0.01056329191972812\n",
      "[LOG 20200502-14:10:28] epoch: 4906 train-loss: 0.010563275673323207\n",
      "[LOG 20200502-14:10:28] epoch: 4907 train-loss: 0.010563259426918294\n",
      "[LOG 20200502-14:10:28] epoch: 4908 train-loss: 0.010563243180513382\n",
      "[LOG 20200502-14:10:28] epoch: 4909 train-loss: 0.010563227761950757\n",
      "[LOG 20200502-14:10:29] epoch: 4910 train-loss: 0.010563212343388133\n",
      "[LOG 20200502-14:10:29] epoch: 4911 train-loss: 0.010563197131786082\n",
      "[LOG 20200502-14:10:29] epoch: 4912 train-loss: 0.010563182127144601\n",
      "[LOG 20200502-14:10:29] epoch: 4913 train-loss: 0.010563166501621405\n",
      "[LOG 20200502-14:10:29] epoch: 4914 train-loss: 0.010563151807420783\n",
      "[LOG 20200502-14:10:30] epoch: 4915 train-loss: 0.010563137216700448\n",
      "[LOG 20200502-14:10:30] epoch: 4916 train-loss: 0.010563123246861829\n",
      "[LOG 20200502-14:10:30] epoch: 4917 train-loss: 0.010563108345700635\n",
      "[LOG 20200502-14:10:30] epoch: 4918 train-loss: 0.01056309478978316\n",
      "[LOG 20200502-14:10:30] epoch: 4919 train-loss: 0.01056308133734597\n",
      "[LOG 20200502-14:10:31] epoch: 4920 train-loss: 0.010563067367507352\n",
      "[LOG 20200502-14:10:31] epoch: 4921 train-loss: 0.010563053811589876\n",
      "[LOG 20200502-14:10:31] epoch: 4922 train-loss: 0.01056304056611326\n",
      "[LOG 20200502-14:10:31] epoch: 4923 train-loss: 0.010563027631077502\n",
      "[LOG 20200502-14:10:32] epoch: 4924 train-loss: 0.010563015006482601\n",
      "[LOG 20200502-14:10:32] epoch: 4925 train-loss: 0.010563002278407415\n",
      "[LOG 20200502-14:10:32] epoch: 4926 train-loss: 0.010562990067733658\n",
      "[LOG 20200502-14:10:32] epoch: 4927 train-loss: 0.010562977960540188\n",
      "[LOG 20200502-14:10:32] epoch: 4928 train-loss: 0.010562966163787577\n",
      "[LOG 20200502-14:10:33] epoch: 4929 train-loss: 0.010562954367034964\n",
      "[LOG 20200502-14:10:33] epoch: 4930 train-loss: 0.010562942880723212\n",
      "[LOG 20200502-14:10:33] epoch: 4931 train-loss: 0.010562931497891745\n",
      "[LOG 20200502-14:10:33] epoch: 4932 train-loss: 0.01056292032202085\n",
      "[LOG 20200502-14:10:34] epoch: 4933 train-loss: 0.010562909560071098\n",
      "[LOG 20200502-14:10:34] epoch: 4934 train-loss: 0.010562899005081918\n",
      "[LOG 20200502-14:10:34] epoch: 4935 train-loss: 0.01056288845009274\n",
      "[LOG 20200502-14:10:34] epoch: 4936 train-loss: 0.01056287789510356\n",
      "[LOG 20200502-14:10:34] epoch: 4937 train-loss: 0.01056286816795667\n",
      "[LOG 20200502-14:10:35] epoch: 4938 train-loss: 0.010562858440809779\n",
      "[LOG 20200502-14:10:35] epoch: 4939 train-loss: 0.010562848713662889\n",
      "[LOG 20200502-14:10:35] epoch: 4940 train-loss: 0.010562839607397715\n",
      "[LOG 20200502-14:10:35] epoch: 4941 train-loss: 0.01056283050113254\n",
      "[LOG 20200502-14:10:36] epoch: 4942 train-loss: 0.01056282108442651\n",
      "[LOG 20200502-14:10:36] epoch: 4943 train-loss: 0.010562812702523338\n",
      "[LOG 20200502-14:10:36] epoch: 4944 train-loss: 0.01056280473454131\n",
      "[LOG 20200502-14:10:36] epoch: 4945 train-loss: 0.010562795628276136\n",
      "[LOG 20200502-14:10:36] epoch: 4946 train-loss: 0.01056278838465611\n",
      "[LOG 20200502-14:10:37] epoch: 4947 train-loss: 0.010562780209713511\n",
      "[LOG 20200502-14:10:37] epoch: 4948 train-loss: 0.010562772759132914\n",
      "[LOG 20200502-14:10:37] epoch: 4949 train-loss: 0.010562765412032604\n",
      "[LOG 20200502-14:10:37] epoch: 4950 train-loss: 0.010562758064932294\n",
      "[LOG 20200502-14:10:37] epoch: 4951 train-loss: 0.010562751131753126\n",
      "[LOG 20200502-14:10:38] epoch: 4952 train-loss: 0.010562744922935963\n",
      "[LOG 20200502-14:10:38] epoch: 4953 train-loss: 0.010562738196717368\n",
      "[LOG 20200502-14:10:38] epoch: 4954 train-loss: 0.010562731987900205\n",
      "[LOG 20200502-14:10:38] epoch: 4955 train-loss: 0.010562725882563326\n",
      "[LOG 20200502-14:10:38] epoch: 4956 train-loss: 0.010562720605068736\n",
      "[LOG 20200502-14:10:39] epoch: 4957 train-loss: 0.010562714913653003\n",
      "[LOG 20200502-14:10:39] epoch: 4958 train-loss: 0.010562709532678127\n",
      "[LOG 20200502-14:10:39] epoch: 4959 train-loss: 0.010562704358663823\n",
      "[LOG 20200502-14:10:39] epoch: 4960 train-loss: 0.010562699495090378\n",
      "[LOG 20200502-14:10:40] epoch: 4961 train-loss: 0.010562694838477505\n",
      "[LOG 20200502-14:10:40] epoch: 4962 train-loss: 0.010562690492305491\n",
      "[LOG 20200502-14:10:40] epoch: 4963 train-loss: 0.010562686456574334\n",
      "[LOG 20200502-14:10:40] epoch: 4964 train-loss: 0.010562682317362891\n",
      "[LOG 20200502-14:10:40] epoch: 4965 train-loss: 0.010562678488592306\n",
      "[LOG 20200502-14:10:41] epoch: 4966 train-loss: 0.010562675177223153\n",
      "[LOG 20200502-14:10:41] epoch: 4967 train-loss: 0.01056267155541314\n",
      "[LOG 20200502-14:10:41] epoch: 4968 train-loss: 0.010562668657965131\n",
      "[LOG 20200502-14:10:41] epoch: 4969 train-loss: 0.010562665346595976\n",
      "[LOG 20200502-14:10:41] epoch: 4970 train-loss: 0.010562662966549397\n",
      "[LOG 20200502-14:10:42] epoch: 4971 train-loss: 0.010562660483022531\n",
      "[LOG 20200502-14:10:42] epoch: 4972 train-loss: 0.010562657999495665\n",
      "[LOG 20200502-14:10:42] epoch: 4973 train-loss: 0.010562656447291374\n",
      "[LOG 20200502-14:10:42] epoch: 4974 train-loss: 0.010562654791606797\n",
      "[LOG 20200502-14:10:42] epoch: 4975 train-loss: 0.010562652618520789\n",
      "[LOG 20200502-14:10:43] epoch: 4976 train-loss: 0.01056265127327707\n",
      "[LOG 20200502-14:10:43] epoch: 4977 train-loss: 0.010562650548915068\n",
      "[LOG 20200502-14:10:43] epoch: 4978 train-loss: 0.010562649100191064\n",
      "[LOG 20200502-14:10:43] epoch: 4979 train-loss: 0.010562648789750205\n",
      "[LOG 20200502-14:10:44] epoch: 4980 train-loss: 0.010562647651467059\n",
      "[LOG 20200502-14:10:44] epoch: 4981 train-loss: 0.01056264785842763\n",
      "[LOG 20200502-14:10:44] epoch: 4982 train-loss: 0.010562647444506487\n",
      "[LOG 20200502-14:10:44] epoch: 4983 train-loss: 0.0105626473410262\n",
      "[LOG 20200502-14:10:44] epoch: 4984 train-loss: 0.010562647237545915\n",
      "[LOG 20200502-14:10:45] epoch: 4985 train-loss: 0.010562648065388203\n",
      "[LOG 20200502-14:10:45] epoch: 4986 train-loss: 0.010562648582789633\n",
      "[LOG 20200502-14:10:45] epoch: 4987 train-loss: 0.01056264920367135\n",
      "[LOG 20200502-14:10:45] epoch: 4988 train-loss: 0.010562650548915068\n",
      "[LOG 20200502-14:10:45] epoch: 4989 train-loss: 0.010562651687198214\n",
      "[LOG 20200502-14:10:46] epoch: 4990 train-loss: 0.010562653446363078\n",
      "[LOG 20200502-14:10:46] epoch: 4991 train-loss: 0.010562654791606797\n",
      "[LOG 20200502-14:10:46] epoch: 4992 train-loss: 0.010562656447291374\n",
      "[LOG 20200502-14:10:46] epoch: 4993 train-loss: 0.010562658827337954\n",
      "[LOG 20200502-14:10:47] epoch: 4994 train-loss: 0.010562660586502817\n",
      "[LOG 20200502-14:10:47] epoch: 4995 train-loss: 0.010562662656108538\n",
      "[LOG 20200502-14:10:47] epoch: 4996 train-loss: 0.010562665450076262\n",
      "[LOG 20200502-14:10:47] epoch: 4997 train-loss: 0.0105626681405637\n",
      "[LOG 20200502-14:10:47] epoch: 4998 train-loss: 0.01056267103801171\n",
      "[LOG 20200502-14:10:48] epoch: 4999 train-loss: 0.010562674142420292\n",
      "[LOG 20200502-14:10:48] epoch: 5000 train-loss: 0.010562677660750018\n",
      "[LOG 20200502-14:10:48] epoch: 5001 train-loss: 0.010562680454717742\n",
      "[LOG 20200502-14:10:48] epoch: 5002 train-loss: 0.010562684386968613\n",
      "[LOG 20200502-14:10:49] epoch: 5003 train-loss: 0.01056268790529834\n",
      "[LOG 20200502-14:10:49] epoch: 5004 train-loss: 0.010562691734068923\n",
      "[LOG 20200502-14:10:49] epoch: 5005 train-loss: 0.010562695562839508\n",
      "[LOG 20200502-14:10:49] epoch: 5006 train-loss: 0.010562699391610093\n",
      "[LOG 20200502-14:10:49] epoch: 5007 train-loss: 0.010562704048222966\n",
      "[LOG 20200502-14:10:50] epoch: 5008 train-loss: 0.010562708497875266\n",
      "[LOG 20200502-14:10:50] epoch: 5009 train-loss: 0.010562712947527567\n",
      "[LOG 20200502-14:10:50] epoch: 5010 train-loss: 0.010562717707620727\n",
      "[LOG 20200502-14:10:50] epoch: 5011 train-loss: 0.0105627223642336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:10:50] epoch: 5012 train-loss: 0.010562727331287332\n",
      "[LOG 20200502-14:10:51] epoch: 5013 train-loss: 0.01056273260878192\n",
      "[LOG 20200502-14:10:51] epoch: 5014 train-loss: 0.01056273788627651\n",
      "[LOG 20200502-14:10:51] epoch: 5015 train-loss: 0.010562742853330242\n",
      "[LOG 20200502-14:10:51] epoch: 5016 train-loss: 0.010562748855186833\n",
      "[LOG 20200502-14:10:52] epoch: 5017 train-loss: 0.010562754236161709\n",
      "[LOG 20200502-14:10:52] epoch: 5018 train-loss: 0.010562760031057728\n",
      "[LOG 20200502-14:10:52] epoch: 5019 train-loss: 0.010562765825953748\n",
      "[LOG 20200502-14:10:52] epoch: 5020 train-loss: 0.010562771517369483\n",
      "[LOG 20200502-14:10:52] epoch: 5021 train-loss: 0.010562777726186646\n",
      "[LOG 20200502-14:10:53] epoch: 5022 train-loss: 0.010562783831523525\n",
      "[LOG 20200502-14:10:53] epoch: 5023 train-loss: 0.010562789833380116\n",
      "[LOG 20200502-14:10:53] epoch: 5024 train-loss: 0.010562795731756423\n",
      "[LOG 20200502-14:10:53] epoch: 5025 train-loss: 0.010562802251014445\n",
      "[LOG 20200502-14:10:53] epoch: 5026 train-loss: 0.01056280866679218\n",
      "[LOG 20200502-14:10:54] epoch: 5027 train-loss: 0.01056281528953049\n",
      "[LOG 20200502-14:10:54] epoch: 5028 train-loss: 0.010562822015749084\n",
      "[LOG 20200502-14:10:54] epoch: 5029 train-loss: 0.010562828638487391\n",
      "[LOG 20200502-14:10:54] epoch: 5030 train-loss: 0.010562835054265128\n",
      "[LOG 20200502-14:10:54] epoch: 5031 train-loss: 0.010562841987444295\n",
      "[LOG 20200502-14:10:55] epoch: 5032 train-loss: 0.010562848713662889\n",
      "[LOG 20200502-14:10:55] epoch: 5033 train-loss: 0.010562855957282914\n",
      "[LOG 20200502-14:10:55] epoch: 5034 train-loss: 0.010562862580021223\n",
      "[LOG 20200502-14:10:55] epoch: 5035 train-loss: 0.010562869927121533\n",
      "[LOG 20200502-14:10:56] epoch: 5036 train-loss: 0.010562876963780986\n",
      "[LOG 20200502-14:10:56] epoch: 5037 train-loss: 0.010562884103920724\n",
      "[LOG 20200502-14:10:56] epoch: 5038 train-loss: 0.010562890726659033\n",
      "[LOG 20200502-14:10:56] epoch: 5039 train-loss: 0.01056289817723963\n",
      "[LOG 20200502-14:10:56] epoch: 5040 train-loss: 0.01056290552433994\n",
      "[LOG 20200502-14:10:57] epoch: 5041 train-loss: 0.010562912560999393\n",
      "[LOG 20200502-14:10:57] epoch: 5042 train-loss: 0.010562919390698275\n",
      "[LOG 20200502-14:10:57] epoch: 5043 train-loss: 0.010562926737798585\n",
      "[LOG 20200502-14:10:57] epoch: 5044 train-loss: 0.010562934188379182\n",
      "[LOG 20200502-14:10:57] epoch: 5045 train-loss: 0.010562941018078063\n",
      "[LOG 20200502-14:10:58] epoch: 5046 train-loss: 0.01056294846865866\n",
      "[LOG 20200502-14:10:58] epoch: 5047 train-loss: 0.010562955194877254\n",
      "[LOG 20200502-14:10:58] epoch: 5048 train-loss: 0.010562962541977564\n",
      "[LOG 20200502-14:10:58] epoch: 5049 train-loss: 0.010562969889077876\n",
      "[LOG 20200502-14:10:58] epoch: 5050 train-loss: 0.010562977029217614\n",
      "[LOG 20200502-14:10:59] epoch: 5051 train-loss: 0.010562983651955923\n",
      "[LOG 20200502-14:10:59] epoch: 5052 train-loss: 0.010562991206016805\n",
      "[LOG 20200502-14:10:59] epoch: 5053 train-loss: 0.010562998035715686\n",
      "[LOG 20200502-14:10:59] epoch: 5054 train-loss: 0.010563004968894852\n",
      "[LOG 20200502-14:11:00] epoch: 5055 train-loss: 0.010563011695113447\n",
      "[LOG 20200502-14:11:00] epoch: 5056 train-loss: 0.01056301842133204\n",
      "[LOG 20200502-14:11:00] epoch: 5057 train-loss: 0.010563025354511209\n",
      "[LOG 20200502-14:11:00] epoch: 5058 train-loss: 0.010563032080729803\n",
      "[LOG 20200502-14:11:00] epoch: 5059 train-loss: 0.01056303870346811\n",
      "[LOG 20200502-14:11:01] epoch: 5060 train-loss: 0.010563044808804989\n",
      "[LOG 20200502-14:11:01] epoch: 5061 train-loss: 0.010563051431543298\n",
      "[LOG 20200502-14:11:01] epoch: 5062 train-loss: 0.010563057847321033\n",
      "[LOG 20200502-14:11:01] epoch: 5063 train-loss: 0.010563064159618484\n",
      "[LOG 20200502-14:11:01] epoch: 5064 train-loss: 0.010563070264955362\n",
      "[LOG 20200502-14:11:02] epoch: 5065 train-loss: 0.010563076163331667\n",
      "[LOG 20200502-14:11:02] epoch: 5066 train-loss: 0.010563082372148832\n",
      "[LOG 20200502-14:11:02] epoch: 5067 train-loss: 0.010563087546163134\n",
      "[LOG 20200502-14:11:02] epoch: 5068 train-loss: 0.010563093134098582\n",
      "[LOG 20200502-14:11:03] epoch: 5069 train-loss: 0.010563098825514317\n",
      "[LOG 20200502-14:11:03] epoch: 5070 train-loss: 0.010563104516930051\n",
      "[LOG 20200502-14:11:03] epoch: 5071 train-loss: 0.010563109070062637\n",
      "[LOG 20200502-14:11:03] epoch: 5072 train-loss: 0.010563113933636082\n",
      "[LOG 20200502-14:11:03] epoch: 5073 train-loss: 0.010563118693729242\n",
      "[LOG 20200502-14:11:04] epoch: 5074 train-loss: 0.010563123350342115\n",
      "[LOG 20200502-14:11:04] epoch: 5075 train-loss: 0.010563128420876132\n",
      "[LOG 20200502-14:11:04] epoch: 5076 train-loss: 0.010563132767048147\n",
      "[LOG 20200502-14:11:04] epoch: 5077 train-loss: 0.010563136699299017\n",
      "[LOG 20200502-14:11:05] epoch: 5078 train-loss: 0.010563140424589315\n",
      "[LOG 20200502-14:11:05] epoch: 5079 train-loss: 0.010563144667281045\n",
      "[LOG 20200502-14:11:05] epoch: 5080 train-loss: 0.010563148082130484\n",
      "[LOG 20200502-14:11:05] epoch: 5081 train-loss: 0.010563151393499639\n",
      "[LOG 20200502-14:11:05] epoch: 5082 train-loss: 0.01056315429094765\n",
      "[LOG 20200502-14:11:06] epoch: 5083 train-loss: 0.010563157084915373\n",
      "[LOG 20200502-14:11:06] epoch: 5084 train-loss: 0.010563159982363382\n",
      "[LOG 20200502-14:11:06] epoch: 5085 train-loss: 0.010563162879811393\n",
      "[LOG 20200502-14:11:06] epoch: 5086 train-loss: 0.010563164328535398\n",
      "[LOG 20200502-14:11:06] epoch: 5087 train-loss: 0.010563166812062263\n",
      "[LOG 20200502-14:11:07] epoch: 5088 train-loss: 0.010563168260786269\n",
      "[LOG 20200502-14:11:07] epoch: 5089 train-loss: 0.010563169295589129\n",
      "[LOG 20200502-14:11:07] epoch: 5090 train-loss: 0.01056317084779342\n",
      "[LOG 20200502-14:11:07] epoch: 5091 train-loss: 0.010563171054753993\n",
      "[LOG 20200502-14:11:07] epoch: 5092 train-loss: 0.010563172296517424\n",
      "[LOG 20200502-14:11:08] epoch: 5093 train-loss: 0.010563172503477998\n",
      "[LOG 20200502-14:11:08] epoch: 5094 train-loss: 0.010563172399997711\n",
      "[LOG 20200502-14:11:08] epoch: 5095 train-loss: 0.010563171986076567\n",
      "[LOG 20200502-14:11:08] epoch: 5096 train-loss: 0.01056317084779342\n",
      "[LOG 20200502-14:11:09] epoch: 5097 train-loss: 0.01056317033039199\n",
      "[LOG 20200502-14:11:09] epoch: 5098 train-loss: 0.01056316846774684\n",
      "[LOG 20200502-14:11:09] epoch: 5099 train-loss: 0.010563167743384838\n",
      "[LOG 20200502-14:11:09] epoch: 5100 train-loss: 0.010563165363338258\n",
      "[LOG 20200502-14:11:09] epoch: 5101 train-loss: 0.010563162879811393\n",
      "[LOG 20200502-14:11:10] epoch: 5102 train-loss: 0.010563161017166244\n",
      "[LOG 20200502-14:11:10] epoch: 5103 train-loss: 0.010563157602316804\n",
      "[LOG 20200502-14:11:10] epoch: 5104 train-loss: 0.01056315398050679\n",
      "[LOG 20200502-14:11:10] epoch: 5105 train-loss: 0.010563150669137636\n",
      "[LOG 20200502-14:11:11] epoch: 5106 train-loss: 0.010563146322965622\n",
      "[LOG 20200502-14:11:11] epoch: 5107 train-loss: 0.01056314187331332\n",
      "[LOG 20200502-14:11:11] epoch: 5108 train-loss: 0.010563136492338445\n",
      "[LOG 20200502-14:11:11] epoch: 5109 train-loss: 0.010563131628765\n",
      "[LOG 20200502-14:11:11] epoch: 5110 train-loss: 0.01056312614430984\n",
      "[LOG 20200502-14:11:12] epoch: 5111 train-loss: 0.010563120142453246\n",
      "[LOG 20200502-14:11:12] epoch: 5112 train-loss: 0.01056311403711637\n",
      "[LOG 20200502-14:11:12] epoch: 5113 train-loss: 0.010563106690016057\n",
      "[LOG 20200502-14:11:12] epoch: 5114 train-loss: 0.010563099653356604\n",
      "[LOG 20200502-14:11:13] epoch: 5115 train-loss: 0.010563092202776007\n",
      "[LOG 20200502-14:11:13] epoch: 5116 train-loss: 0.010563084131313695\n",
      "[LOG 20200502-14:11:13] epoch: 5117 train-loss: 0.01056307533548938\n",
      "[LOG 20200502-14:11:13] epoch: 5118 train-loss: 0.010563066332704492\n",
      "[LOG 20200502-14:11:13] epoch: 5119 train-loss: 0.01056305712295903\n",
      "[LOG 20200502-14:11:14] epoch: 5120 train-loss: 0.010563047085371282\n",
      "[LOG 20200502-14:11:14] epoch: 5121 train-loss: 0.010563037047783533\n",
      "[LOG 20200502-14:11:14] epoch: 5122 train-loss: 0.01056302659627464\n",
      "[LOG 20200502-14:11:14] epoch: 5123 train-loss: 0.010563015523884032\n",
      "[LOG 20200502-14:11:15] epoch: 5124 train-loss: 0.010563003934091993\n",
      "[LOG 20200502-14:11:15] epoch: 5125 train-loss: 0.010562991930378808\n",
      "[LOG 20200502-14:11:15] epoch: 5126 train-loss: 0.010562979512744479\n",
      "[LOG 20200502-14:11:15] epoch: 5127 train-loss: 0.010562966991629865\n",
      "[LOG 20200502-14:11:15] epoch: 5128 train-loss: 0.010562953539192677\n",
      "[LOG 20200502-14:11:16] epoch: 5129 train-loss: 0.01056293977631463\n",
      "[LOG 20200502-14:11:16] epoch: 5130 train-loss: 0.010562926116916869\n",
      "[LOG 20200502-14:11:16] epoch: 5131 train-loss: 0.010562910905314816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:11:16] epoch: 5132 train-loss: 0.01056289579719305\n",
      "[LOG 20200502-14:11:16] epoch: 5133 train-loss: 0.01056288027515014\n",
      "[LOG 20200502-14:11:17] epoch: 5134 train-loss: 0.010562863925264942\n",
      "[LOG 20200502-14:11:17] epoch: 5135 train-loss: 0.01056284736841917\n",
      "[LOG 20200502-14:11:17] epoch: 5136 train-loss: 0.010562830087211397\n",
      "[LOG 20200502-14:11:17] epoch: 5137 train-loss: 0.010562812702523338\n",
      "[LOG 20200502-14:11:18] epoch: 5138 train-loss: 0.010562795110874705\n",
      "[LOG 20200502-14:11:18] epoch: 5139 train-loss: 0.010562776898344358\n",
      "[LOG 20200502-14:11:18] epoch: 5140 train-loss: 0.010562757961452007\n",
      "[LOG 20200502-14:11:18] epoch: 5141 train-loss: 0.010562738507158227\n",
      "[LOG 20200502-14:11:18] epoch: 5142 train-loss: 0.010562718535463015\n",
      "[LOG 20200502-14:11:19] epoch: 5143 train-loss: 0.010562698460287519\n",
      "[LOG 20200502-14:11:19] epoch: 5144 train-loss: 0.010562678074671162\n",
      "[LOG 20200502-14:11:19] epoch: 5145 train-loss: 0.010562657275133662\n",
      "[LOG 20200502-14:11:19] epoch: 5146 train-loss: 0.010562635440793302\n",
      "[LOG 20200502-14:11:19] epoch: 5147 train-loss: 0.010562613813413514\n",
      "[LOG 20200502-14:11:20] epoch: 5148 train-loss: 0.010562591358191438\n",
      "[LOG 20200502-14:11:20] epoch: 5149 train-loss: 0.010562569006449647\n",
      "[LOG 20200502-14:11:20] epoch: 5150 train-loss: 0.010562545205983851\n",
      "[LOG 20200502-14:11:20] epoch: 5151 train-loss: 0.010562521922919486\n",
      "[LOG 20200502-14:11:20] epoch: 5152 train-loss: 0.010562498018973403\n",
      "[LOG 20200502-14:11:21] epoch: 5153 train-loss: 0.010562473494145606\n",
      "[LOG 20200502-14:11:21] epoch: 5154 train-loss: 0.010562448658876948\n",
      "[LOG 20200502-14:11:21] epoch: 5155 train-loss: 0.010562423306206862\n",
      "[LOG 20200502-14:11:21] epoch: 5156 train-loss: 0.010562397953536775\n",
      "[LOG 20200502-14:11:21] epoch: 5157 train-loss: 0.01056237229042583\n",
      "[LOG 20200502-14:11:22] epoch: 5158 train-loss: 0.010562346006433168\n",
      "[LOG 20200502-14:11:22] epoch: 5159 train-loss: 0.010562319722440507\n",
      "[LOG 20200502-14:11:22] epoch: 5160 train-loss: 0.0105622923001647\n",
      "[LOG 20200502-14:11:22] epoch: 5161 train-loss: 0.010562265602250894\n",
      "[LOG 20200502-14:11:23] epoch: 5162 train-loss: 0.010562237145172225\n",
      "[LOG 20200502-14:11:23] epoch: 5163 train-loss: 0.010562209826376703\n",
      "[LOG 20200502-14:11:23] epoch: 5164 train-loss: 0.010562181265817748\n",
      "[LOG 20200502-14:11:23] epoch: 5165 train-loss: 0.01056215280873908\n",
      "[LOG 20200502-14:11:23] epoch: 5166 train-loss: 0.010562124144699838\n",
      "[LOG 20200502-14:11:24] epoch: 5167 train-loss: 0.010562094859778881\n",
      "[LOG 20200502-14:11:24] epoch: 5168 train-loss: 0.01056206516093678\n",
      "[LOG 20200502-14:11:24] epoch: 5169 train-loss: 0.010562035565574964\n",
      "[LOG 20200502-14:11:24] epoch: 5170 train-loss: 0.010562005866732862\n",
      "[LOG 20200502-14:11:24] epoch: 5171 train-loss: 0.010561975340048472\n",
      "[LOG 20200502-14:11:25] epoch: 5172 train-loss: 0.01056194512380494\n",
      "[LOG 20200502-14:11:25] epoch: 5173 train-loss: 0.010561914493640264\n",
      "[LOG 20200502-14:11:25] epoch: 5174 train-loss: 0.010561883863475587\n",
      "[LOG 20200502-14:11:25] epoch: 5175 train-loss: 0.010561852301988337\n",
      "[LOG 20200502-14:11:25] epoch: 5176 train-loss: 0.010561821257902516\n",
      "[LOG 20200502-14:11:26] epoch: 5177 train-loss: 0.01056178959293498\n",
      "[LOG 20200502-14:11:26] epoch: 5178 train-loss: 0.010561758238408301\n",
      "[LOG 20200502-14:11:26] epoch: 5179 train-loss: 0.010561726366480192\n",
      "[LOG 20200502-14:11:26] epoch: 5180 train-loss: 0.010561694391071796\n",
      "[LOG 20200502-14:11:27] epoch: 5181 train-loss: 0.010561662933064831\n",
      "[LOG 20200502-14:11:27] epoch: 5182 train-loss: 0.010561629922853576\n",
      "[LOG 20200502-14:11:27] epoch: 5183 train-loss: 0.010561598154405752\n",
      "[LOG 20200502-14:11:27] epoch: 5184 train-loss: 0.010561565972036786\n",
      "[LOG 20200502-14:11:28] epoch: 5185 train-loss: 0.010561532754864957\n",
      "[LOG 20200502-14:11:28] epoch: 5186 train-loss: 0.01056150057249599\n",
      "[LOG 20200502-14:11:28] epoch: 5187 train-loss: 0.010561467769245306\n",
      "[LOG 20200502-14:11:28] epoch: 5188 train-loss: 0.010561435690356625\n",
      "[LOG 20200502-14:11:28] epoch: 5189 train-loss: 0.010561402576665083\n",
      "[LOG 20200502-14:11:29] epoch: 5190 train-loss: 0.010561369256012969\n",
      "[LOG 20200502-14:11:29] epoch: 5191 train-loss: 0.010561337177124288\n",
      "[LOG 20200502-14:11:29] epoch: 5192 train-loss: 0.010561303649511602\n",
      "[LOG 20200502-14:11:29] epoch: 5193 train-loss: 0.010561270432339774\n",
      "[LOG 20200502-14:11:30] epoch: 5194 train-loss: 0.010561237422128519\n",
      "[LOG 20200502-14:11:30] epoch: 5195 train-loss: 0.010561204929318693\n",
      "[LOG 20200502-14:11:30] epoch: 5196 train-loss: 0.01056117181562715\n",
      "[LOG 20200502-14:11:30] epoch: 5197 train-loss: 0.010561138908896182\n",
      "[LOG 20200502-14:11:31] epoch: 5198 train-loss: 0.010561106002165211\n",
      "[LOG 20200502-14:11:31] epoch: 5199 train-loss: 0.010561073095434241\n",
      "[LOG 20200502-14:11:31] epoch: 5200 train-loss: 0.0105610399817427\n",
      "[LOG 20200502-14:11:31] epoch: 5201 train-loss: 0.010561007385452589\n",
      "[LOG 20200502-14:11:32] epoch: 5202 train-loss: 0.010560974271761047\n",
      "[LOG 20200502-14:11:32] epoch: 5203 train-loss: 0.010560941261549791\n",
      "[LOG 20200502-14:11:32] epoch: 5204 train-loss: 0.010560909079180824\n",
      "[LOG 20200502-14:11:32] epoch: 5205 train-loss: 0.010560876689851284\n",
      "[LOG 20200502-14:11:33] epoch: 5206 train-loss: 0.010560843990080886\n",
      "[LOG 20200502-14:11:33] epoch: 5207 train-loss: 0.010560811600751348\n",
      "[LOG 20200502-14:11:33] epoch: 5208 train-loss: 0.01056077941838238\n",
      "[LOG 20200502-14:11:33] epoch: 5209 train-loss: 0.010560746925572554\n",
      "[LOG 20200502-14:11:34] epoch: 5210 train-loss: 0.010560715260605017\n",
      "[LOG 20200502-14:11:34] epoch: 5211 train-loss: 0.010560682767795192\n",
      "[LOG 20200502-14:11:34] epoch: 5212 train-loss: 0.010560651309788227\n",
      "[LOG 20200502-14:11:34] epoch: 5213 train-loss: 0.010560619437860118\n",
      "[LOG 20200502-14:11:34] epoch: 5214 train-loss: 0.01056058808333344\n",
      "[LOG 20200502-14:11:35] epoch: 5215 train-loss: 0.010560556314885616\n",
      "[LOG 20200502-14:11:35] epoch: 5216 train-loss: 0.010560524546437793\n",
      "[LOG 20200502-14:11:35] epoch: 5217 train-loss: 0.010560493605832258\n",
      "[LOG 20200502-14:11:35] epoch: 5218 train-loss: 0.010560462458266152\n",
      "[LOG 20200502-14:11:36] epoch: 5219 train-loss: 0.010560431207219759\n",
      "[LOG 20200502-14:11:36] epoch: 5220 train-loss: 0.010560400577055084\n",
      "[LOG 20200502-14:11:36] epoch: 5221 train-loss: 0.010560369843410121\n",
      "[LOG 20200502-14:11:36] epoch: 5222 train-loss: 0.010560339109765159\n",
      "[LOG 20200502-14:11:36] epoch: 5223 train-loss: 0.010560309203962484\n",
      "[LOG 20200502-14:11:37] epoch: 5224 train-loss: 0.010560279194679525\n",
      "[LOG 20200502-14:11:37] epoch: 5225 train-loss: 0.010560249495837424\n",
      "[LOG 20200502-14:11:37] epoch: 5226 train-loss: 0.010560219072633319\n",
      "[LOG 20200502-14:11:37] epoch: 5227 train-loss: 0.010560189891192649\n",
      "[LOG 20200502-14:11:38] epoch: 5228 train-loss: 0.010560160606271692\n",
      "[LOG 20200502-14:11:38] epoch: 5229 train-loss: 0.010560131321350733\n",
      "[LOG 20200502-14:11:38] epoch: 5230 train-loss: 0.010560102139910063\n",
      "[LOG 20200502-14:11:38] epoch: 5231 train-loss: 0.010560073165429963\n",
      "[LOG 20200502-14:11:39] epoch: 5232 train-loss: 0.010560044190949865\n",
      "[LOG 20200502-14:11:39] epoch: 5233 train-loss: 0.010560015423430337\n",
      "[LOG 20200502-14:11:39] epoch: 5234 train-loss: 0.010559986862871382\n",
      "[LOG 20200502-14:11:39] epoch: 5235 train-loss: 0.010559959440595575\n",
      "[LOG 20200502-14:11:40] epoch: 5236 train-loss: 0.010559931190477477\n",
      "[LOG 20200502-14:11:40] epoch: 5237 train-loss: 0.010559903354280524\n",
      "[LOG 20200502-14:11:40] epoch: 5238 train-loss: 0.010559876035485003\n",
      "[LOG 20200502-14:11:40] epoch: 5239 train-loss: 0.010559848613209195\n",
      "[LOG 20200502-14:11:40] epoch: 5240 train-loss: 0.010559821190933386\n",
      "[LOG 20200502-14:11:41] epoch: 5241 train-loss: 0.01055979449301958\n",
      "[LOG 20200502-14:11:41] epoch: 5242 train-loss: 0.010559767588145204\n",
      "[LOG 20200502-14:11:41] epoch: 5243 train-loss: 0.010559741200672256\n",
      "[LOG 20200502-14:11:41] epoch: 5244 train-loss: 0.010559714606238736\n",
      "[LOG 20200502-14:11:41] epoch: 5245 train-loss: 0.010559688736167219\n",
      "[LOG 20200502-14:11:42] epoch: 5246 train-loss: 0.010559662969575988\n",
      "[LOG 20200502-14:11:42] epoch: 5247 train-loss: 0.010559637202984758\n",
      "[LOG 20200502-14:11:42] epoch: 5248 train-loss: 0.01055961133291324\n",
      "[LOG 20200502-14:11:42] epoch: 5249 train-loss: 0.010559585876762867\n",
      "[LOG 20200502-14:11:43] epoch: 5250 train-loss: 0.010559561041494211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:11:43] epoch: 5251 train-loss: 0.010559536206225554\n",
      "[LOG 20200502-14:11:43] epoch: 5252 train-loss: 0.010559511370956898\n",
      "[LOG 20200502-14:11:43] epoch: 5253 train-loss: 0.010559486432207955\n",
      "[LOG 20200502-14:11:43] epoch: 5254 train-loss: 0.010559462217821015\n",
      "[LOG 20200502-14:11:44] epoch: 5255 train-loss: 0.010559438313874934\n",
      "[LOG 20200502-14:11:44] epoch: 5256 train-loss: 0.010559414306448566\n",
      "[LOG 20200502-14:11:44] epoch: 5257 train-loss: 0.010559390712943342\n",
      "[LOG 20200502-14:11:44] epoch: 5258 train-loss: 0.010559366912477545\n",
      "[LOG 20200502-14:11:45] epoch: 5259 train-loss: 0.01055934362941318\n",
      "[LOG 20200502-14:11:45] epoch: 5260 train-loss: 0.010559320967230532\n",
      "[LOG 20200502-14:11:45] epoch: 5261 train-loss: 0.01055929758068588\n",
      "[LOG 20200502-14:11:45] epoch: 5262 train-loss: 0.010559274815022945\n",
      "[LOG 20200502-14:11:45] epoch: 5263 train-loss: 0.010559252670241727\n",
      "[LOG 20200502-14:11:46] epoch: 5264 train-loss: 0.010559230525460508\n",
      "[LOG 20200502-14:11:46] epoch: 5265 train-loss: 0.010559208380679289\n",
      "[LOG 20200502-14:11:46] epoch: 5266 train-loss: 0.010559186339378357\n",
      "[LOG 20200502-14:11:46] epoch: 5267 train-loss: 0.010559164711998569\n",
      "[LOG 20200502-14:11:47] epoch: 5268 train-loss: 0.010559143188099066\n",
      "[LOG 20200502-14:11:47] epoch: 5269 train-loss: 0.010559121457238993\n",
      "[LOG 20200502-14:11:47] epoch: 5270 train-loss: 0.010559100554221205\n",
      "[LOG 20200502-14:11:47] epoch: 5271 train-loss: 0.01055907965120342\n",
      "[LOG 20200502-14:11:47] epoch: 5272 train-loss: 0.010559059472547637\n",
      "[LOG 20200502-14:11:48] epoch: 5273 train-loss: 0.010559038466049565\n",
      "[LOG 20200502-14:11:48] epoch: 5274 train-loss: 0.010559018390874067\n",
      "[LOG 20200502-14:11:48] epoch: 5275 train-loss: 0.010558998108737998\n",
      "[LOG 20200502-14:11:48] epoch: 5276 train-loss: 0.010558977930082215\n",
      "[LOG 20200502-14:11:49] epoch: 5277 train-loss: 0.010558958372308148\n",
      "[LOG 20200502-14:11:49] epoch: 5278 train-loss: 0.010558939021494653\n",
      "[LOG 20200502-14:11:49] epoch: 5279 train-loss: 0.010558919256760014\n",
      "[LOG 20200502-14:11:49] epoch: 5280 train-loss: 0.010558900319867663\n",
      "[LOG 20200502-14:11:49] epoch: 5280 new best train-loss: 0.010558900319867663 found\n",
      "[LOG 20200502-14:11:50] epoch: 5281 train-loss: 0.010558881382975314\n",
      "[LOG 20200502-14:11:50] epoch: 5282 train-loss: 0.010558862653043535\n",
      "[LOG 20200502-14:11:50] epoch: 5283 train-loss: 0.010558843612670898\n",
      "[LOG 20200502-14:11:50] epoch: 5284 train-loss: 0.010558825089699693\n",
      "[LOG 20200502-14:11:51] epoch: 5285 train-loss: 0.010558807601531347\n",
      "[LOG 20200502-14:11:51] epoch: 5286 train-loss: 0.010558788871599568\n",
      "[LOG 20200502-14:11:51] epoch: 5287 train-loss: 0.010558771590391794\n",
      "[LOG 20200502-14:11:51] epoch: 5288 train-loss: 0.010558753481341733\n",
      "[LOG 20200502-14:11:52] epoch: 5289 train-loss: 0.010558735889693102\n",
      "[LOG 20200502-14:11:52] epoch: 5290 train-loss: 0.01055871829804447\n",
      "[LOG 20200502-14:11:52] epoch: 5290 new best train-loss: 0.01055871829804447 found\n",
      "[LOG 20200502-14:11:52] epoch: 5291 train-loss: 0.010558701534238126\n",
      "[LOG 20200502-14:11:52] epoch: 5292 train-loss: 0.010558684770431783\n",
      "[LOG 20200502-14:11:52] epoch: 5293 train-loss: 0.01055866748922401\n",
      "[LOG 20200502-14:11:53] epoch: 5294 train-loss: 0.01055865093237824\n",
      "[LOG 20200502-14:11:53] epoch: 5295 train-loss: 0.010558634789453613\n",
      "[LOG 20200502-14:11:53] epoch: 5296 train-loss: 0.010558618439568413\n",
      "[LOG 20200502-14:11:53] epoch: 5297 train-loss: 0.010558602296643786\n",
      "[LOG 20200502-14:11:54] epoch: 5298 train-loss: 0.01055858615371916\n",
      "[LOG 20200502-14:11:54] epoch: 5299 train-loss: 0.010558570528195964\n",
      "[LOG 20200502-14:11:54] epoch: 5300 train-loss: 0.010558554592231909\n",
      "[LOG 20200502-14:11:54] epoch: 5300 new best train-loss: 0.010558554592231909 found\n",
      "[LOG 20200502-14:11:54] epoch: 5301 train-loss: 0.01055853958759043\n",
      "[LOG 20200502-14:11:55] epoch: 5302 train-loss: 0.010558524479468664\n",
      "[LOG 20200502-14:11:55] epoch: 5303 train-loss: 0.01055850906090604\n",
      "[LOG 20200502-14:11:55] epoch: 5304 train-loss: 0.010558494056264559\n",
      "[LOG 20200502-14:11:55] epoch: 5305 train-loss: 0.010558478844662508\n",
      "[LOG 20200502-14:11:56] epoch: 5306 train-loss: 0.010558464357422458\n",
      "[LOG 20200502-14:11:56] epoch: 5307 train-loss: 0.010558449766702123\n",
      "[LOG 20200502-14:11:56] epoch: 5308 train-loss: 0.010558435486422645\n",
      "[LOG 20200502-14:11:56] epoch: 5309 train-loss: 0.010558421723544598\n",
      "[LOG 20200502-14:11:57] epoch: 5310 train-loss: 0.010558407132824263\n",
      "[LOG 20200502-14:11:57] epoch: 5310 new best train-loss: 0.010558407132824263 found\n",
      "[LOG 20200502-14:11:57] epoch: 5311 train-loss: 0.010558393266465928\n",
      "[LOG 20200502-14:11:57] epoch: 5312 train-loss: 0.010558379607068168\n",
      "[LOG 20200502-14:11:57] epoch: 5313 train-loss: 0.010558366568552123\n",
      "[LOG 20200502-14:11:58] epoch: 5314 train-loss: 0.010558352391752932\n",
      "[LOG 20200502-14:11:58] epoch: 5315 train-loss: 0.010558339767158031\n",
      "[LOG 20200502-14:11:58] epoch: 5316 train-loss: 0.010558326418201128\n",
      "[LOG 20200502-14:11:58] epoch: 5317 train-loss: 0.010558313793606229\n",
      "[LOG 20200502-14:11:58] epoch: 5318 train-loss: 0.01055830054812961\n",
      "[LOG 20200502-14:11:59] epoch: 5319 train-loss: 0.010558287923534712\n",
      "[LOG 20200502-14:11:59] epoch: 5320 train-loss: 0.01055827560938067\n",
      "[LOG 20200502-14:11:59] epoch: 5320 new best train-loss: 0.01055827560938067 found\n",
      "[LOG 20200502-14:11:59] epoch: 5321 train-loss: 0.010558263295226626\n",
      "[LOG 20200502-14:11:59] epoch: 5322 train-loss: 0.010558250981072584\n",
      "[LOG 20200502-14:12:00] epoch: 5323 train-loss: 0.010558238770398829\n",
      "[LOG 20200502-14:12:00] epoch: 5324 train-loss: 0.010558226559725072\n",
      "[LOG 20200502-14:12:00] epoch: 5325 train-loss: 0.010558214659492174\n",
      "[LOG 20200502-14:12:00] epoch: 5326 train-loss: 0.010558203276660707\n",
      "[LOG 20200502-14:12:00] epoch: 5327 train-loss: 0.010558191376427809\n",
      "[LOG 20200502-14:12:01] epoch: 5328 train-loss: 0.010558179579675198\n",
      "[LOG 20200502-14:12:01] epoch: 5329 train-loss: 0.010558168610764874\n",
      "[LOG 20200502-14:12:01] epoch: 5330 train-loss: 0.010558157331413694\n",
      "[LOG 20200502-14:12:01] epoch: 5330 new best train-loss: 0.010558157331413694 found\n",
      "[LOG 20200502-14:12:01] epoch: 5331 train-loss: 0.01055814636250337\n",
      "[LOG 20200502-14:12:02] epoch: 5332 train-loss: 0.010558135393593047\n",
      "[LOG 20200502-14:12:02] epoch: 5333 train-loss: 0.010558124424682723\n",
      "[LOG 20200502-14:12:02] epoch: 5334 train-loss: 0.010558113869693544\n",
      "[LOG 20200502-14:12:02] epoch: 5335 train-loss: 0.010558103211224079\n",
      "[LOG 20200502-14:12:02] epoch: 5336 train-loss: 0.010558092759715186\n",
      "[LOG 20200502-14:12:03] epoch: 5337 train-loss: 0.010558082308206294\n",
      "[LOG 20200502-14:12:03] epoch: 5338 train-loss: 0.010558072270618545\n",
      "[LOG 20200502-14:12:03] epoch: 5339 train-loss: 0.010558062233030796\n",
      "[LOG 20200502-14:12:03] epoch: 5340 train-loss: 0.010558051988482475\n",
      "[LOG 20200502-14:12:03] epoch: 5340 new best train-loss: 0.010558051988482475 found\n",
      "[LOG 20200502-14:12:04] epoch: 5341 train-loss: 0.010558042468296157\n",
      "[LOG 20200502-14:12:04] epoch: 5342 train-loss: 0.01055803263766898\n",
      "[LOG 20200502-14:12:04] epoch: 5343 train-loss: 0.010558022807041803\n",
      "[LOG 20200502-14:12:04] epoch: 5344 train-loss: 0.010558014011217488\n",
      "[LOG 20200502-14:12:05] epoch: 5345 train-loss: 0.01055800397362974\n",
      "[LOG 20200502-14:12:05] epoch: 5346 train-loss: 0.01055799528128571\n",
      "[LOG 20200502-14:12:05] epoch: 5347 train-loss: 0.010557986278500821\n",
      "[LOG 20200502-14:12:05] epoch: 5348 train-loss: 0.010557976551353931\n",
      "[LOG 20200502-14:12:05] epoch: 5349 train-loss: 0.01055796765204933\n",
      "[LOG 20200502-14:12:06] epoch: 5350 train-loss: 0.010557959063185586\n",
      "[LOG 20200502-14:12:06] epoch: 5350 new best train-loss: 0.010557959063185586 found\n",
      "[LOG 20200502-14:12:06] epoch: 5351 train-loss: 0.010557950163880983\n",
      "[LOG 20200502-14:12:06] epoch: 5352 train-loss: 0.010557941471536955\n",
      "[LOG 20200502-14:12:06] epoch: 5353 train-loss: 0.010557933296594355\n",
      "[LOG 20200502-14:12:07] epoch: 5354 train-loss: 0.010557924604250325\n",
      "[LOG 20200502-14:12:07] epoch: 5355 train-loss: 0.010557916636268297\n",
      "[LOG 20200502-14:12:07] epoch: 5356 train-loss: 0.010557907943924269\n",
      "[LOG 20200502-14:12:07] epoch: 5357 train-loss: 0.010557899665501382\n",
      "[LOG 20200502-14:12:07] epoch: 5358 train-loss: 0.010557891697519355\n",
      "[LOG 20200502-14:12:08] epoch: 5359 train-loss: 0.010557883833017614\n",
      "[LOG 20200502-14:12:08] epoch: 5360 train-loss: 0.010557876278956732\n",
      "[LOG 20200502-14:12:08] epoch: 5360 new best train-loss: 0.010557876278956732 found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:12:08] epoch: 5361 train-loss: 0.010557868104014132\n",
      "[LOG 20200502-14:12:08] epoch: 5362 train-loss: 0.010557860653433535\n",
      "[LOG 20200502-14:12:09] epoch: 5363 train-loss: 0.010557853099372651\n",
      "[LOG 20200502-14:12:09] epoch: 5364 train-loss: 0.010557845338351197\n",
      "[LOG 20200502-14:12:09] epoch: 5365 train-loss: 0.01055783840517203\n",
      "[LOG 20200502-14:12:09] epoch: 5366 train-loss: 0.010557830954591433\n",
      "[LOG 20200502-14:12:09] epoch: 5367 train-loss: 0.01055782340053055\n",
      "[LOG 20200502-14:12:10] epoch: 5368 train-loss: 0.010557816363871098\n",
      "[LOG 20200502-14:12:10] epoch: 5369 train-loss: 0.01055780922373136\n",
      "[LOG 20200502-14:12:10] epoch: 5370 train-loss: 0.010557802497512765\n",
      "[LOG 20200502-14:12:10] epoch: 5370 new best train-loss: 0.010557802497512765 found\n",
      "[LOG 20200502-14:12:10] epoch: 5371 train-loss: 0.010557795460853312\n",
      "[LOG 20200502-14:12:11] epoch: 5372 train-loss: 0.010557788424193859\n",
      "[LOG 20200502-14:12:11] epoch: 5373 train-loss: 0.010557781904935837\n",
      "[LOG 20200502-14:12:11] epoch: 5374 train-loss: 0.010557774557835527\n",
      "[LOG 20200502-14:12:11] epoch: 5375 train-loss: 0.010557768452498648\n",
      "[LOG 20200502-14:12:12] epoch: 5376 train-loss: 0.01055776234716177\n",
      "[LOG 20200502-14:12:12] epoch: 5377 train-loss: 0.010557756138344606\n",
      "[LOG 20200502-14:12:12] epoch: 5378 train-loss: 0.010557749205165438\n",
      "[LOG 20200502-14:12:12] epoch: 5379 train-loss: 0.010557743410269419\n",
      "[LOG 20200502-14:12:12] epoch: 5380 train-loss: 0.010557736994491683\n",
      "[LOG 20200502-14:12:12] epoch: 5380 new best train-loss: 0.010557736994491683 found\n",
      "[LOG 20200502-14:12:13] epoch: 5381 train-loss: 0.010557730992635092\n",
      "[LOG 20200502-14:12:13] epoch: 5382 train-loss: 0.010557724266416497\n",
      "[LOG 20200502-14:12:13] epoch: 5383 train-loss: 0.01055771867848105\n",
      "[LOG 20200502-14:12:13] epoch: 5384 train-loss: 0.010557712573144171\n",
      "[LOG 20200502-14:12:14] epoch: 5385 train-loss: 0.010557706881728437\n",
      "[LOG 20200502-14:12:14] epoch: 5386 train-loss: 0.010557700983352132\n",
      "[LOG 20200502-14:12:14] epoch: 5387 train-loss: 0.01055769518845611\n",
      "[LOG 20200502-14:12:14] epoch: 5388 train-loss: 0.01055768970400095\n",
      "[LOG 20200502-14:12:15] epoch: 5389 train-loss: 0.010557683805624643\n",
      "[LOG 20200502-14:12:15] epoch: 5390 train-loss: 0.010557678321169483\n",
      "[LOG 20200502-14:12:15] epoch: 5390 new best train-loss: 0.010557678321169483 found\n",
      "[LOG 20200502-14:12:15] epoch: 5391 train-loss: 0.010557672629753748\n",
      "[LOG 20200502-14:12:15] epoch: 5392 train-loss: 0.0105576670418183\n",
      "[LOG 20200502-14:12:15] epoch: 5393 train-loss: 0.01055766207476457\n",
      "[LOG 20200502-14:12:16] epoch: 5394 train-loss: 0.010557656176388264\n",
      "[LOG 20200502-14:12:16] epoch: 5395 train-loss: 0.01055765100237396\n",
      "[LOG 20200502-14:12:16] epoch: 5396 train-loss: 0.01055764603532023\n",
      "[LOG 20200502-14:12:16] epoch: 5397 train-loss: 0.010557640550865067\n",
      "[LOG 20200502-14:12:17] epoch: 5398 train-loss: 0.01055763548033105\n",
      "[LOG 20200502-14:12:17] epoch: 5399 train-loss: 0.010557630306316746\n",
      "[LOG 20200502-14:12:17] epoch: 5400 train-loss: 0.010557625546223588\n",
      "[LOG 20200502-14:12:17] epoch: 5400 new best train-loss: 0.010557625546223588 found\n",
      "[LOG 20200502-14:12:17] epoch: 5401 train-loss: 0.010557620372209284\n",
      "[LOG 20200502-14:12:17] epoch: 5402 train-loss: 0.010557615405155553\n",
      "[LOG 20200502-14:12:18] epoch: 5403 train-loss: 0.010557610541582108\n",
      "[LOG 20200502-14:12:18] epoch: 5404 train-loss: 0.010557605678008663\n",
      "[LOG 20200502-14:12:18] epoch: 5405 train-loss: 0.010557600814435218\n",
      "[LOG 20200502-14:12:18] epoch: 5406 train-loss: 0.010557596157822344\n",
      "[LOG 20200502-14:12:19] epoch: 5407 train-loss: 0.010557590776847469\n",
      "[LOG 20200502-14:12:19] epoch: 5408 train-loss: 0.010557586948076883\n",
      "[LOG 20200502-14:12:19] epoch: 5409 train-loss: 0.010557581567102008\n",
      "[LOG 20200502-14:12:19] epoch: 5410 train-loss: 0.010557577117449708\n",
      "[LOG 20200502-14:12:19] epoch: 5410 new best train-loss: 0.010557577117449708 found\n",
      "[LOG 20200502-14:12:19] epoch: 5411 train-loss: 0.010557572357356548\n",
      "[LOG 20200502-14:12:20] epoch: 5412 train-loss: 0.010557567493783103\n",
      "[LOG 20200502-14:12:20] epoch: 5413 train-loss: 0.010557563458051946\n",
      "[LOG 20200502-14:12:20] epoch: 5414 train-loss: 0.010557559008399645\n",
      "[LOG 20200502-14:12:20] epoch: 5415 train-loss: 0.0105575541448262\n",
      "[LOG 20200502-14:12:21] epoch: 5416 train-loss: 0.010557549695173899\n",
      "[LOG 20200502-14:12:21] epoch: 5417 train-loss: 0.010557545142041313\n",
      "[LOG 20200502-14:12:21] epoch: 5418 train-loss: 0.010557540692389011\n",
      "[LOG 20200502-14:12:21] epoch: 5419 train-loss: 0.01055753624273671\n",
      "[LOG 20200502-14:12:22] epoch: 5420 train-loss: 0.010557532413966127\n",
      "[LOG 20200502-14:12:22] epoch: 5420 new best train-loss: 0.010557532413966127 found\n",
      "[LOG 20200502-14:12:22] epoch: 5421 train-loss: 0.010557527860833539\n",
      "[LOG 20200502-14:12:22] epoch: 5422 train-loss: 0.010557523204220666\n",
      "[LOG 20200502-14:12:22] epoch: 5423 train-loss: 0.010557519065009223\n",
      "[LOG 20200502-14:12:23] epoch: 5424 train-loss: 0.010557515132758353\n",
      "[LOG 20200502-14:12:23] epoch: 5425 train-loss: 0.010557510890066624\n",
      "[LOG 20200502-14:12:23] epoch: 5426 train-loss: 0.010557506336934037\n",
      "[LOG 20200502-14:12:23] epoch: 5427 train-loss: 0.01055750230120288\n",
      "[LOG 20200502-14:12:24] epoch: 5428 train-loss: 0.010557498058511151\n",
      "[LOG 20200502-14:12:24] epoch: 5429 train-loss: 0.010557493401898278\n",
      "[LOG 20200502-14:12:24] epoch: 5430 train-loss: 0.01055748967660798\n",
      "[LOG 20200502-14:12:24] epoch: 5430 new best train-loss: 0.01055748967660798 found\n",
      "[LOG 20200502-14:12:25] epoch: 5431 train-loss: 0.010557485744357109\n",
      "[LOG 20200502-14:12:25] epoch: 5432 train-loss: 0.010557481398185095\n",
      "[LOG 20200502-14:12:25] epoch: 5433 train-loss: 0.010557477052013079\n",
      "[LOG 20200502-14:12:25] epoch: 5434 train-loss: 0.010557472912801636\n",
      "[LOG 20200502-14:12:25] epoch: 5435 train-loss: 0.010557468773590194\n",
      "[LOG 20200502-14:12:26] epoch: 5436 train-loss: 0.01055746442741818\n",
      "[LOG 20200502-14:12:26] epoch: 5437 train-loss: 0.010557460288206736\n",
      "[LOG 20200502-14:12:26] epoch: 5438 train-loss: 0.010557456562916437\n",
      "[LOG 20200502-14:12:26] epoch: 5439 train-loss: 0.010557452009783851\n",
      "[LOG 20200502-14:12:27] epoch: 5440 train-loss: 0.010557448284493553\n",
      "[LOG 20200502-14:12:27] epoch: 5440 new best train-loss: 0.010557448284493553 found\n",
      "[LOG 20200502-14:12:27] epoch: 5441 train-loss: 0.010557443731360965\n",
      "[LOG 20200502-14:12:27] epoch: 5442 train-loss: 0.010557439695629809\n",
      "[LOG 20200502-14:12:27] epoch: 5443 train-loss: 0.010557435659898652\n",
      "[LOG 20200502-14:12:28] epoch: 5444 train-loss: 0.010557431624167495\n",
      "[LOG 20200502-14:12:28] epoch: 5445 train-loss: 0.01055742727799548\n",
      "[LOG 20200502-14:12:28] epoch: 5446 train-loss: 0.010557423035303751\n",
      "[LOG 20200502-14:12:29] epoch: 5447 train-loss: 0.010557419620454311\n",
      "[LOG 20200502-14:12:29] epoch: 5448 train-loss: 0.010557415067321725\n",
      "[LOG 20200502-14:12:29] epoch: 5449 train-loss: 0.010557411135070853\n",
      "[LOG 20200502-14:12:29] epoch: 5450 train-loss: 0.010557406788898839\n",
      "[LOG 20200502-14:12:29] epoch: 5450 new best train-loss: 0.010557406788898839 found\n",
      "[LOG 20200502-14:12:30] epoch: 5451 train-loss: 0.01055740254620711\n",
      "[LOG 20200502-14:12:30] epoch: 5452 train-loss: 0.010557398096554808\n",
      "[LOG 20200502-14:12:30] epoch: 5453 train-loss: 0.010557394164303938\n",
      "[LOG 20200502-14:12:30] epoch: 5454 train-loss: 0.01055738992161221\n",
      "[LOG 20200502-14:12:31] epoch: 5455 train-loss: 0.010557385575440194\n",
      "[LOG 20200502-14:12:31] epoch: 5456 train-loss: 0.01055738122926818\n",
      "[LOG 20200502-14:12:31] epoch: 5457 train-loss: 0.010557377090056738\n",
      "[LOG 20200502-14:12:31] epoch: 5458 train-loss: 0.010557372743884722\n",
      "[LOG 20200502-14:12:32] epoch: 5459 train-loss: 0.010557368397712708\n",
      "[LOG 20200502-14:12:32] epoch: 5460 train-loss: 0.010557364155020978\n",
      "[LOG 20200502-14:12:32] epoch: 5460 new best train-loss: 0.010557364155020978 found\n",
      "[LOG 20200502-14:12:32] epoch: 5461 train-loss: 0.010557360015809536\n",
      "[LOG 20200502-14:12:32] epoch: 5462 train-loss: 0.01055735546267695\n",
      "[LOG 20200502-14:12:33] epoch: 5463 train-loss: 0.010557351426945792\n",
      "[LOG 20200502-14:12:33] epoch: 5464 train-loss: 0.010557346563372347\n",
      "[LOG 20200502-14:12:33] epoch: 5465 train-loss: 0.010557342217200331\n",
      "[LOG 20200502-14:12:33] epoch: 5466 train-loss: 0.010557337974508604\n",
      "[LOG 20200502-14:12:33] epoch: 5467 train-loss: 0.010557333110935159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:12:34] epoch: 5468 train-loss: 0.01055732886824343\n",
      "[LOG 20200502-14:12:34] epoch: 5469 train-loss: 0.010557323797709413\n",
      "[LOG 20200502-14:12:34] epoch: 5470 train-loss: 0.010557319348057112\n",
      "[LOG 20200502-14:12:34] epoch: 5470 new best train-loss: 0.010557319348057112 found\n",
      "[LOG 20200502-14:12:35] epoch: 5471 train-loss: 0.010557314587963952\n",
      "[LOG 20200502-14:12:35] epoch: 5472 train-loss: 0.010557310241791937\n",
      "[LOG 20200502-14:12:35] epoch: 5473 train-loss: 0.01055730568865935\n",
      "[LOG 20200502-14:12:35] epoch: 5474 train-loss: 0.010557300825085904\n",
      "[LOG 20200502-14:12:36] epoch: 5475 train-loss: 0.010557296168473031\n",
      "[LOG 20200502-14:12:36] epoch: 5476 train-loss: 0.010557291201419301\n",
      "[LOG 20200502-14:12:36] epoch: 5477 train-loss: 0.010557286441326141\n",
      "[LOG 20200502-14:12:37] epoch: 5478 train-loss: 0.010557281370792124\n",
      "[LOG 20200502-14:12:37] epoch: 5479 train-loss: 0.010557276093297534\n",
      "[LOG 20200502-14:12:37] epoch: 5480 train-loss: 0.010557271333204376\n",
      "[LOG 20200502-14:12:37] epoch: 5480 new best train-loss: 0.010557271333204376 found\n",
      "[LOG 20200502-14:12:38] epoch: 5481 train-loss: 0.010557266366150644\n",
      "[LOG 20200502-14:12:38] epoch: 5482 train-loss: 0.010557261088656055\n",
      "[LOG 20200502-14:12:38] epoch: 5483 train-loss: 0.010557256121602323\n",
      "[LOG 20200502-14:12:38] epoch: 5484 train-loss: 0.010557251051068306\n",
      "[LOG 20200502-14:12:39] epoch: 5485 train-loss: 0.010557245773573717\n",
      "[LOG 20200502-14:12:39] epoch: 5486 train-loss: 0.010557240392598841\n",
      "[LOG 20200502-14:12:39] epoch: 5487 train-loss: 0.01055723542554511\n",
      "[LOG 20200502-14:12:39] epoch: 5488 train-loss: 0.010557229941089949\n",
      "[LOG 20200502-14:12:40] epoch: 5489 train-loss: 0.010557224663595358\n",
      "[LOG 20200502-14:12:40] epoch: 5490 train-loss: 0.010557218868699338\n",
      "[LOG 20200502-14:12:40] epoch: 5490 new best train-loss: 0.010557218868699338 found\n",
      "[LOG 20200502-14:12:40] epoch: 5491 train-loss: 0.010557213798165321\n",
      "[LOG 20200502-14:12:40] epoch: 5492 train-loss: 0.01055720831371016\n",
      "[LOG 20200502-14:12:41] epoch: 5493 train-loss: 0.010557202518814139\n",
      "[LOG 20200502-14:12:41] epoch: 5494 train-loss: 0.010557196620437834\n",
      "[LOG 20200502-14:12:41] epoch: 5495 train-loss: 0.010557190825541815\n",
      "[LOG 20200502-14:12:41] epoch: 5496 train-loss: 0.010557184720204936\n",
      "[LOG 20200502-14:12:42] epoch: 5497 train-loss: 0.010557179235749774\n",
      "[LOG 20200502-14:12:42] epoch: 5498 train-loss: 0.010557173647814326\n",
      "[LOG 20200502-14:12:42] epoch: 5499 train-loss: 0.010557166921595732\n",
      "[LOG 20200502-14:12:42] epoch: 5500 train-loss: 0.010557161333660284\n",
      "[LOG 20200502-14:12:42] epoch: 5500 new best train-loss: 0.010557161333660284 found\n",
      "[LOG 20200502-14:12:43] epoch: 5501 train-loss: 0.01055715512484312\n",
      "[LOG 20200502-14:12:43] epoch: 5502 train-loss: 0.010557149122986529\n",
      "[LOG 20200502-14:12:43] epoch: 5503 train-loss: 0.010557142396767935\n",
      "[LOG 20200502-14:12:43] epoch: 5504 train-loss: 0.010557136498391628\n",
      "[LOG 20200502-14:12:44] epoch: 5505 train-loss: 0.010557130082613893\n",
      "[LOG 20200502-14:12:44] epoch: 5506 train-loss: 0.010557123770316442\n",
      "[LOG 20200502-14:12:44] epoch: 5507 train-loss: 0.010557116940617561\n",
      "[LOG 20200502-14:12:44] epoch: 5508 train-loss: 0.01055711011091868\n",
      "[LOG 20200502-14:12:45] epoch: 5509 train-loss: 0.010557104005581804\n",
      "[LOG 20200502-14:12:45] epoch: 5510 train-loss: 0.01055709696892235\n",
      "[LOG 20200502-14:12:45] epoch: 5510 new best train-loss: 0.01055709696892235 found\n",
      "[LOG 20200502-14:12:45] epoch: 5511 train-loss: 0.01055709013922347\n",
      "[LOG 20200502-14:12:45] epoch: 5512 train-loss: 0.010557082999083731\n",
      "[LOG 20200502-14:12:46] epoch: 5513 train-loss: 0.010557075962424278\n",
      "[LOG 20200502-14:12:46] epoch: 5514 train-loss: 0.01055706882228454\n",
      "[LOG 20200502-14:12:46] epoch: 5515 train-loss: 0.010557062096065946\n",
      "[LOG 20200502-14:12:46] epoch: 5516 train-loss: 0.010557054645485349\n",
      "[LOG 20200502-14:12:47] epoch: 5517 train-loss: 0.010557047401865324\n",
      "[LOG 20200502-14:12:47] epoch: 5518 train-loss: 0.010557040261725584\n",
      "[LOG 20200502-14:12:47] epoch: 5519 train-loss: 0.01055703250070413\n",
      "[LOG 20200502-14:12:47] epoch: 5520 train-loss: 0.010557024946643246\n",
      "[LOG 20200502-14:12:47] epoch: 5520 new best train-loss: 0.010557024946643246 found\n",
      "[LOG 20200502-14:12:48] epoch: 5521 train-loss: 0.010557017806503508\n",
      "[LOG 20200502-14:12:48] epoch: 5522 train-loss: 0.010557009942001767\n",
      "[LOG 20200502-14:12:48] epoch: 5523 train-loss: 0.010557002077500025\n",
      "[LOG 20200502-14:12:48] epoch: 5524 train-loss: 0.010556994523439143\n",
      "[LOG 20200502-14:12:49] epoch: 5525 train-loss: 0.010556986348496543\n",
      "[LOG 20200502-14:12:49] epoch: 5526 train-loss: 0.010556978070073657\n",
      "[LOG 20200502-14:12:49] epoch: 5527 train-loss: 0.010556970516012775\n",
      "[LOG 20200502-14:12:49] epoch: 5528 train-loss: 0.010556962030629316\n",
      "[LOG 20200502-14:12:49] epoch: 5529 train-loss: 0.010556953648726145\n",
      "[LOG 20200502-14:12:50] epoch: 5530 train-loss: 0.010556945266822973\n",
      "[LOG 20200502-14:12:50] epoch: 5530 new best train-loss: 0.010556945266822973 found\n",
      "[LOG 20200502-14:12:50] epoch: 5531 train-loss: 0.010556936574478945\n",
      "[LOG 20200502-14:12:50] epoch: 5532 train-loss: 0.010556928606496917\n",
      "[LOG 20200502-14:12:50] epoch: 5533 train-loss: 0.010556919293271171\n",
      "[LOG 20200502-14:12:51] epoch: 5534 train-loss: 0.010556910704407427\n",
      "[LOG 20200502-14:12:51] epoch: 5535 train-loss: 0.010556901598142253\n",
      "[LOG 20200502-14:12:51] epoch: 5536 train-loss: 0.010556893112758795\n",
      "[LOG 20200502-14:12:51] epoch: 5537 train-loss: 0.010556884109973907\n",
      "[LOG 20200502-14:12:52] epoch: 5538 train-loss: 0.010556874693267874\n",
      "[LOG 20200502-14:12:52] epoch: 5539 train-loss: 0.010556865897443559\n",
      "[LOG 20200502-14:12:52] epoch: 5540 train-loss: 0.010556856584217813\n",
      "[LOG 20200502-14:12:52] epoch: 5540 new best train-loss: 0.010556856584217813 found\n",
      "[LOG 20200502-14:12:52] epoch: 5541 train-loss: 0.010556846857070923\n",
      "[LOG 20200502-14:12:53] epoch: 5542 train-loss: 0.010556837233404318\n",
      "[LOG 20200502-14:12:53] epoch: 5543 train-loss: 0.010556827920178572\n",
      "[LOG 20200502-14:12:53] epoch: 5544 train-loss: 0.010556818089551397\n",
      "[LOG 20200502-14:12:53] epoch: 5545 train-loss: 0.010556808051963648\n",
      "[LOG 20200502-14:12:54] epoch: 5546 train-loss: 0.010556798221336471\n",
      "[LOG 20200502-14:12:54] epoch: 5547 train-loss: 0.010556788183748722\n",
      "[LOG 20200502-14:12:54] epoch: 5548 train-loss: 0.010556778146160973\n",
      "[LOG 20200502-14:12:54] epoch: 5549 train-loss: 0.01055676800509294\n",
      "[LOG 20200502-14:12:55] epoch: 5550 train-loss: 0.010556757864024904\n",
      "[LOG 20200502-14:12:55] epoch: 5550 new best train-loss: 0.010556757864024904 found\n",
      "[LOG 20200502-14:12:55] epoch: 5551 train-loss: 0.01055674689511458\n",
      "[LOG 20200502-14:12:55] epoch: 5552 train-loss: 0.010556736547085974\n",
      "[LOG 20200502-14:12:56] epoch: 5553 train-loss: 0.010556725681655936\n",
      "[LOG 20200502-14:12:56] epoch: 5554 train-loss: 0.010556714919706186\n",
      "[LOG 20200502-14:12:56] epoch: 5555 train-loss: 0.010556704468197294\n",
      "[LOG 20200502-14:12:56] epoch: 5556 train-loss: 0.010556693292326398\n",
      "[LOG 20200502-14:12:57] epoch: 5557 train-loss: 0.010556682323416075\n",
      "[LOG 20200502-14:12:57] epoch: 5558 train-loss: 0.010556671251025464\n",
      "[LOG 20200502-14:12:57] epoch: 5559 train-loss: 0.010556659454272853\n",
      "[LOG 20200502-14:12:57] epoch: 5560 train-loss: 0.010556648692323102\n",
      "[LOG 20200502-14:12:57] epoch: 5560 new best train-loss: 0.010556648692323102 found\n",
      "[LOG 20200502-14:12:58] epoch: 5561 train-loss: 0.010556636999050776\n",
      "[LOG 20200502-14:12:58] epoch: 5562 train-loss: 0.010556625098817878\n",
      "[LOG 20200502-14:12:58] epoch: 5563 train-loss: 0.010556613612506125\n",
      "[LOG 20200502-14:12:58] epoch: 5564 train-loss: 0.010556601712273227\n",
      "[LOG 20200502-14:12:59] epoch: 5565 train-loss: 0.010556590122481188\n",
      "[LOG 20200502-14:12:59] epoch: 5566 train-loss: 0.010556577911807431\n",
      "[LOG 20200502-14:12:59] epoch: 5567 train-loss: 0.010556565390692817\n",
      "[LOG 20200502-14:12:59] epoch: 5568 train-loss: 0.010556552973058488\n",
      "[LOG 20200502-14:13:00] epoch: 5569 train-loss: 0.010556541176305877\n",
      "[LOG 20200502-14:13:00] epoch: 5570 train-loss: 0.010556528034309546\n",
      "[LOG 20200502-14:13:00] epoch: 5570 new best train-loss: 0.010556528034309546 found\n",
      "[LOG 20200502-14:13:00] epoch: 5571 train-loss: 0.010556515720155504\n",
      "[LOG 20200502-14:13:00] epoch: 5572 train-loss: 0.01055650319904089\n",
      "[LOG 20200502-14:13:01] epoch: 5573 train-loss: 0.010556490470965704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:13:01] epoch: 5574 train-loss: 0.010556477535929944\n",
      "[LOG 20200502-14:13:01] epoch: 5575 train-loss: 0.010556464600894187\n",
      "[LOG 20200502-14:13:02] epoch: 5576 train-loss: 0.010556451458897855\n",
      "[LOG 20200502-14:13:02] epoch: 5577 train-loss: 0.010556438316901525\n",
      "[LOG 20200502-14:13:02] epoch: 5578 train-loss: 0.01055642476098405\n",
      "[LOG 20200502-14:13:02] epoch: 5579 train-loss: 0.010556411308546862\n",
      "[LOG 20200502-14:13:03] epoch: 5580 train-loss: 0.010556397752629386\n",
      "[LOG 20200502-14:13:03] epoch: 5580 new best train-loss: 0.010556397752629386 found\n",
      "[LOG 20200502-14:13:03] epoch: 5581 train-loss: 0.010556384714113342\n",
      "[LOG 20200502-14:13:03] epoch: 5582 train-loss: 0.010556370330353579\n",
      "[LOG 20200502-14:13:03] epoch: 5583 train-loss: 0.010556357084876962\n",
      "[LOG 20200502-14:13:03] epoch: 5584 train-loss: 0.010556343011558056\n",
      "[LOG 20200502-14:13:04] epoch: 5585 train-loss: 0.010556328524318006\n",
      "[LOG 20200502-14:13:04] epoch: 5586 train-loss: 0.010556314657959674\n",
      "[LOG 20200502-14:13:04] epoch: 5587 train-loss: 0.010556299549837908\n",
      "[LOG 20200502-14:13:04] epoch: 5588 train-loss: 0.010556285890440146\n",
      "[LOG 20200502-14:13:05] epoch: 5589 train-loss: 0.010556271092759239\n",
      "[LOG 20200502-14:13:05] epoch: 5590 train-loss: 0.010556256708999475\n",
      "[LOG 20200502-14:13:05] epoch: 5590 new best train-loss: 0.010556256708999475 found\n",
      "[LOG 20200502-14:13:05] epoch: 5591 train-loss: 0.010556241911318567\n",
      "[LOG 20200502-14:13:05] epoch: 5592 train-loss: 0.01055622763103909\n",
      "[LOG 20200502-14:13:06] epoch: 5593 train-loss: 0.010556212212476466\n",
      "[LOG 20200502-14:13:06] epoch: 5594 train-loss: 0.010556197518275844\n",
      "[LOG 20200502-14:13:06] epoch: 5595 train-loss: 0.010556182203193506\n",
      "[LOG 20200502-14:13:06] epoch: 5596 train-loss: 0.010556167405512597\n",
      "[LOG 20200502-14:13:07] epoch: 5597 train-loss: 0.010556152193910547\n",
      "[LOG 20200502-14:13:07] epoch: 5598 train-loss: 0.010556136878828207\n",
      "[LOG 20200502-14:13:07] epoch: 5599 train-loss: 0.01055612125330501\n",
      "[LOG 20200502-14:13:07] epoch: 5600 train-loss: 0.01055610604170296\n",
      "[LOG 20200502-14:13:07] epoch: 5600 new best train-loss: 0.01055610604170296 found\n",
      "[LOG 20200502-14:13:08] epoch: 5601 train-loss: 0.010556089898778332\n",
      "[LOG 20200502-14:13:08] epoch: 5602 train-loss: 0.010556074583695995\n",
      "[LOG 20200502-14:13:08] epoch: 5603 train-loss: 0.010556059268613657\n",
      "[LOG 20200502-14:13:09] epoch: 5604 train-loss: 0.010556043436129888\n",
      "[LOG 20200502-14:13:09] epoch: 5605 train-loss: 0.01055602708624469\n",
      "[LOG 20200502-14:13:09] epoch: 5606 train-loss: 0.01055601156420178\n",
      "[LOG 20200502-14:13:09] epoch: 5607 train-loss: 0.01055599521431658\n",
      "[LOG 20200502-14:13:09] epoch: 5608 train-loss: 0.010555978864431381\n",
      "[LOG 20200502-14:13:10] epoch: 5609 train-loss: 0.010555962514546182\n",
      "[LOG 20200502-14:13:10] epoch: 5610 train-loss: 0.010555946371621557\n",
      "[LOG 20200502-14:13:10] epoch: 5610 new best train-loss: 0.010555946371621557 found\n",
      "[LOG 20200502-14:13:10] epoch: 5611 train-loss: 0.010555930021736357\n",
      "[LOG 20200502-14:13:10] epoch: 5612 train-loss: 0.010555913257930014\n",
      "[LOG 20200502-14:13:11] epoch: 5613 train-loss: 0.010555897218485674\n",
      "[LOG 20200502-14:13:11] epoch: 5614 train-loss: 0.010555880351199044\n",
      "[LOG 20200502-14:13:11] epoch: 5615 train-loss: 0.010555863690872988\n",
      "[LOG 20200502-14:13:11] epoch: 5616 train-loss: 0.01055584703054693\n",
      "[LOG 20200502-14:13:12] epoch: 5617 train-loss: 0.01055582995629973\n",
      "[LOG 20200502-14:13:12] epoch: 5618 train-loss: 0.010555813813375102\n",
      "[LOG 20200502-14:13:12] epoch: 5619 train-loss: 0.0105557967391279\n",
      "[LOG 20200502-14:13:12] epoch: 5620 train-loss: 0.010555779871841272\n",
      "[LOG 20200502-14:13:12] epoch: 5620 new best train-loss: 0.010555779871841272 found\n",
      "[LOG 20200502-14:13:12] epoch: 5621 train-loss: 0.010555762694113784\n",
      "[LOG 20200502-14:13:13] epoch: 5622 train-loss: 0.01055574593030744\n",
      "[LOG 20200502-14:13:13] epoch: 5623 train-loss: 0.01055572833865881\n",
      "[LOG 20200502-14:13:13] epoch: 5624 train-loss: 0.010555711367891895\n",
      "[LOG 20200502-14:13:13] epoch: 5625 train-loss: 0.010555694500605265\n",
      "[LOG 20200502-14:13:14] epoch: 5626 train-loss: 0.010555677115917206\n",
      "[LOG 20200502-14:13:14] epoch: 5627 train-loss: 0.010555659938189719\n",
      "[LOG 20200502-14:13:14] epoch: 5628 train-loss: 0.010555642863942517\n",
      "[LOG 20200502-14:13:14] epoch: 5629 train-loss: 0.01055562620361646\n",
      "[LOG 20200502-14:13:15] epoch: 5630 train-loss: 0.010555608508487543\n",
      "[LOG 20200502-14:13:15] epoch: 5630 new best train-loss: 0.010555608508487543 found\n",
      "[LOG 20200502-14:13:15] epoch: 5631 train-loss: 0.010555590813358625\n",
      "[LOG 20200502-14:13:15] epoch: 5632 train-loss: 0.01055557384259171\n",
      "[LOG 20200502-14:13:15] epoch: 5633 train-loss: 0.010555556250943078\n",
      "[LOG 20200502-14:13:15] epoch: 5634 train-loss: 0.01055553907321559\n",
      "[LOG 20200502-14:13:16] epoch: 5635 train-loss: 0.010555521792007817\n",
      "[LOG 20200502-14:13:16] epoch: 5636 train-loss: 0.0105555040968789\n",
      "[LOG 20200502-14:13:16] epoch: 5637 train-loss: 0.010555486608710554\n",
      "[LOG 20200502-14:13:16] epoch: 5638 train-loss: 0.010555469430983067\n",
      "[LOG 20200502-14:13:17] epoch: 5639 train-loss: 0.010555451528893577\n",
      "[LOG 20200502-14:13:17] epoch: 5640 train-loss: 0.010555435179008378\n",
      "[LOG 20200502-14:13:17] epoch: 5640 new best train-loss: 0.010555435179008378 found\n",
      "[LOG 20200502-14:13:17] epoch: 5641 train-loss: 0.01055541800128089\n",
      "[LOG 20200502-14:13:17] epoch: 5642 train-loss: 0.010555399685270257\n",
      "[LOG 20200502-14:13:17] epoch: 5643 train-loss: 0.010555382611023055\n",
      "[LOG 20200502-14:13:18] epoch: 5644 train-loss: 0.010555365950696997\n",
      "[LOG 20200502-14:13:18] epoch: 5645 train-loss: 0.01055534825556808\n",
      "[LOG 20200502-14:13:18] epoch: 5646 train-loss: 0.010555331388281452\n",
      "[LOG 20200502-14:13:18] epoch: 5647 train-loss: 0.010555313900113106\n",
      "[LOG 20200502-14:13:19] epoch: 5648 train-loss: 0.010555297343267335\n",
      "[LOG 20200502-14:13:19] epoch: 5649 train-loss: 0.01055528037250042\n",
      "[LOG 20200502-14:13:19] epoch: 5650 train-loss: 0.010555262677371502\n",
      "[LOG 20200502-14:13:19] epoch: 5650 new best train-loss: 0.010555262677371502 found\n",
      "[LOG 20200502-14:13:19] epoch: 5651 train-loss: 0.010555245913565159\n",
      "[LOG 20200502-14:13:20] epoch: 5652 train-loss: 0.010555228735837672\n",
      "[LOG 20200502-14:13:20] epoch: 5653 train-loss: 0.01055521248943276\n",
      "[LOG 20200502-14:13:20] epoch: 5654 train-loss: 0.01055519531170527\n",
      "[LOG 20200502-14:13:20] epoch: 5655 train-loss: 0.010555178547898928\n",
      "[LOG 20200502-14:13:21] epoch: 5656 train-loss: 0.010555162301494015\n",
      "[LOG 20200502-14:13:21] epoch: 5657 train-loss: 0.010555145227246814\n",
      "[LOG 20200502-14:13:21] epoch: 5658 train-loss: 0.01055512877388133\n",
      "[LOG 20200502-14:13:21] epoch: 5659 train-loss: 0.010555112113555273\n",
      "[LOG 20200502-14:13:21] epoch: 5660 train-loss: 0.010555095867150359\n",
      "[LOG 20200502-14:13:21] epoch: 5660 new best train-loss: 0.010555095867150359 found\n",
      "[LOG 20200502-14:13:22] epoch: 5661 train-loss: 0.010555079206824303\n",
      "[LOG 20200502-14:13:22] epoch: 5662 train-loss: 0.010555063167379962\n",
      "[LOG 20200502-14:13:22] epoch: 5663 train-loss: 0.010555047127935622\n",
      "[LOG 20200502-14:13:22] epoch: 5664 train-loss: 0.01055503160589271\n",
      "[LOG 20200502-14:13:22] epoch: 5665 train-loss: 0.010555015773408942\n",
      "[LOG 20200502-14:13:23] epoch: 5666 train-loss: 0.010554999630484316\n",
      "[LOG 20200502-14:13:23] epoch: 5667 train-loss: 0.010554984315401979\n",
      "[LOG 20200502-14:13:23] epoch: 5668 train-loss: 0.01055496900031964\n",
      "[LOG 20200502-14:13:23] epoch: 5669 train-loss: 0.01055495347827673\n",
      "[LOG 20200502-14:13:24] epoch: 5670 train-loss: 0.010554938266674677\n",
      "[LOG 20200502-14:13:24] epoch: 5670 new best train-loss: 0.010554938266674677 found\n",
      "[LOG 20200502-14:13:24] epoch: 5671 train-loss: 0.010554922744631767\n",
      "[LOG 20200502-14:13:24] epoch: 5672 train-loss: 0.01055490794695086\n",
      "[LOG 20200502-14:13:24] epoch: 5673 train-loss: 0.010554893356230523\n",
      "[LOG 20200502-14:13:25] epoch: 5674 train-loss: 0.01055487845506933\n",
      "[LOG 20200502-14:13:25] epoch: 5675 train-loss: 0.010554864174789853\n",
      "[LOG 20200502-14:13:25] epoch: 5676 train-loss: 0.010554849377108945\n",
      "[LOG 20200502-14:13:25] epoch: 5677 train-loss: 0.01055483530379004\n",
      "[LOG 20200502-14:13:25] epoch: 5678 train-loss: 0.010554821023510562\n",
      "[LOG 20200502-14:13:26] epoch: 5679 train-loss: 0.010554807364112802\n",
      "[LOG 20200502-14:13:26] epoch: 5680 train-loss: 0.010554792980353037\n",
      "[LOG 20200502-14:13:26] epoch: 5680 new best train-loss: 0.010554792980353037 found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:13:26] epoch: 5681 train-loss: 0.010554779631396135\n",
      "[LOG 20200502-14:13:26] epoch: 5682 train-loss: 0.010554766489399804\n",
      "[LOG 20200502-14:13:27] epoch: 5683 train-loss: 0.010554752933482328\n",
      "[LOG 20200502-14:13:27] epoch: 5684 train-loss: 0.01055473999844657\n",
      "[LOG 20200502-14:13:27] epoch: 5685 train-loss: 0.010554726546009382\n",
      "[LOG 20200502-14:13:27] epoch: 5686 train-loss: 0.01055471423185534\n",
      "[LOG 20200502-14:13:28] epoch: 5687 train-loss: 0.01055470160726044\n",
      "[LOG 20200502-14:13:28] epoch: 5688 train-loss: 0.010554688982665539\n",
      "[LOG 20200502-14:13:28] epoch: 5689 train-loss: 0.010554676668511497\n",
      "[LOG 20200502-14:13:28] epoch: 5690 train-loss: 0.01055466445783774\n",
      "[LOG 20200502-14:13:28] epoch: 5690 new best train-loss: 0.01055466445783774 found\n",
      "[LOG 20200502-14:13:28] epoch: 5691 train-loss: 0.01055465317848656\n",
      "[LOG 20200502-14:13:29] epoch: 5692 train-loss: 0.010554640864332518\n",
      "[LOG 20200502-14:13:29] epoch: 5693 train-loss: 0.01055462999890248\n",
      "[LOG 20200502-14:13:29] epoch: 5694 train-loss: 0.01055461892651187\n",
      "[LOG 20200502-14:13:29] epoch: 5695 train-loss: 0.010554607543680403\n",
      "[LOG 20200502-14:13:30] epoch: 5696 train-loss: 0.010554596885210939\n",
      "[LOG 20200502-14:13:30] epoch: 5697 train-loss: 0.010554586123261187\n",
      "[LOG 20200502-14:13:30] epoch: 5698 train-loss: 0.010554575568272008\n",
      "[LOG 20200502-14:13:30] epoch: 5699 train-loss: 0.010554565013282828\n",
      "[LOG 20200502-14:13:31] epoch: 5700 train-loss: 0.01055455549309651\n",
      "[LOG 20200502-14:13:31] epoch: 5700 new best train-loss: 0.01055455549309651 found\n",
      "[LOG 20200502-14:13:31] epoch: 5701 train-loss: 0.010554545455508761\n",
      "[LOG 20200502-14:13:31] epoch: 5702 train-loss: 0.01055453603880273\n",
      "[LOG 20200502-14:13:31] epoch: 5703 train-loss: 0.010554526518616412\n",
      "[LOG 20200502-14:13:32] epoch: 5704 train-loss: 0.01055451761931181\n",
      "[LOG 20200502-14:13:32] epoch: 5705 train-loss: 0.01055450840956635\n",
      "[LOG 20200502-14:13:32] epoch: 5706 train-loss: 0.01055449971722232\n",
      "[LOG 20200502-14:13:32] epoch: 5707 train-loss: 0.01055449102487829\n",
      "[LOG 20200502-14:13:33] epoch: 5708 train-loss: 0.010554482332534261\n",
      "[LOG 20200502-14:13:33] epoch: 5709 train-loss: 0.010554474054111375\n",
      "[LOG 20200502-14:13:33] epoch: 5710 train-loss: 0.010554466396570206\n",
      "[LOG 20200502-14:13:33] epoch: 5710 new best train-loss: 0.010554466396570206 found\n",
      "[LOG 20200502-14:13:33] epoch: 5711 train-loss: 0.01055445863554875\n",
      "[LOG 20200502-14:13:33] epoch: 5712 train-loss: 0.010554451184968153\n",
      "[LOG 20200502-14:13:34] epoch: 5713 train-loss: 0.010554443630907271\n",
      "[LOG 20200502-14:13:34] epoch: 5714 train-loss: 0.010554437111649249\n",
      "[LOG 20200502-14:13:34] epoch: 5715 train-loss: 0.010554430385430654\n",
      "[LOG 20200502-14:13:34] epoch: 5716 train-loss: 0.010554423245290915\n",
      "[LOG 20200502-14:13:35] epoch: 5717 train-loss: 0.010554417036473751\n",
      "[LOG 20200502-14:13:35] epoch: 5718 train-loss: 0.0105544107241763\n",
      "[LOG 20200502-14:13:35] epoch: 5719 train-loss: 0.010554405032760568\n",
      "[LOG 20200502-14:13:35] epoch: 5720 train-loss: 0.010554399030903975\n",
      "[LOG 20200502-14:13:35] epoch: 5720 new best train-loss: 0.010554399030903975 found\n",
      "[LOG 20200502-14:13:36] epoch: 5721 train-loss: 0.010554393029047383\n",
      "[LOG 20200502-14:13:36] epoch: 5722 train-loss: 0.010554387648072507\n",
      "[LOG 20200502-14:13:36] epoch: 5723 train-loss: 0.010554382681018777\n",
      "[LOG 20200502-14:13:36] epoch: 5724 train-loss: 0.010554377507004473\n",
      "[LOG 20200502-14:13:36] epoch: 5725 train-loss: 0.0105543728503916\n",
      "[LOG 20200502-14:13:37] epoch: 5726 train-loss: 0.010554368193778727\n",
      "[LOG 20200502-14:13:37] epoch: 5727 train-loss: 0.01055436364064614\n",
      "[LOG 20200502-14:13:37] epoch: 5728 train-loss: 0.010554359708395269\n",
      "[LOG 20200502-14:13:37] epoch: 5729 train-loss: 0.01055435546570354\n",
      "[LOG 20200502-14:13:38] epoch: 5730 train-loss: 0.010554352464775244\n",
      "[LOG 20200502-14:13:38] epoch: 5730 new best train-loss: 0.010554352464775244 found\n",
      "[LOG 20200502-14:13:38] epoch: 5731 train-loss: 0.010554348739484945\n",
      "[LOG 20200502-14:13:38] epoch: 5732 train-loss: 0.010554345531596078\n",
      "[LOG 20200502-14:13:38] epoch: 5733 train-loss: 0.010554341909786066\n",
      "[LOG 20200502-14:13:39] epoch: 5734 train-loss: 0.010554339012338055\n",
      "[LOG 20200502-14:13:39] epoch: 5735 train-loss: 0.010554335907929473\n",
      "[LOG 20200502-14:13:39] epoch: 5736 train-loss: 0.010554333320922323\n",
      "[LOG 20200502-14:13:39] epoch: 5737 train-loss: 0.010554330526954599\n",
      "[LOG 20200502-14:13:40] epoch: 5738 train-loss: 0.010554328871270021\n",
      "[LOG 20200502-14:13:40] epoch: 5739 train-loss: 0.010554325870341726\n",
      "[LOG 20200502-14:13:40] epoch: 5740 train-loss: 0.010554324214657148\n",
      "[LOG 20200502-14:13:40] epoch: 5740 new best train-loss: 0.010554324214657148 found\n",
      "[LOG 20200502-14:13:40] epoch: 5741 train-loss: 0.010554322145051427\n",
      "[LOG 20200502-14:13:40] epoch: 5742 train-loss: 0.010554320799807707\n",
      "[LOG 20200502-14:13:41] epoch: 5743 train-loss: 0.01055431914412313\n",
      "[LOG 20200502-14:13:41] epoch: 5744 train-loss: 0.010554317902359698\n",
      "[LOG 20200502-14:13:41] epoch: 5745 train-loss: 0.01055431624667512\n",
      "[LOG 20200502-14:13:41] epoch: 5746 train-loss: 0.01055431572927369\n",
      "[LOG 20200502-14:13:42] epoch: 5747 train-loss: 0.010554314280549685\n",
      "[LOG 20200502-14:13:42] epoch: 5748 train-loss: 0.01055431386662854\n",
      "[LOG 20200502-14:13:42] epoch: 5749 train-loss: 0.01055431334922711\n",
      "[LOG 20200502-14:13:42] epoch: 5750 train-loss: 0.010554312728345394\n",
      "[LOG 20200502-14:13:42] epoch: 5750 new best train-loss: 0.010554312728345394 found\n",
      "[LOG 20200502-14:13:43] epoch: 5751 train-loss: 0.010554312107463678\n",
      "[LOG 20200502-14:13:43] epoch: 5752 train-loss: 0.010554311693542533\n",
      "[LOG 20200502-14:13:43] epoch: 5753 train-loss: 0.01055431148658196\n",
      "[LOG 20200502-14:13:43] epoch: 5754 train-loss: 0.010554311590062248\n",
      "[LOG 20200502-14:13:44] epoch: 5755 train-loss: 0.010554311279621389\n",
      "[LOG 20200502-14:13:44] epoch: 5756 train-loss: 0.010554311590062248\n",
      "[LOG 20200502-14:13:44] epoch: 5757 train-loss: 0.010554311693542533\n",
      "[LOG 20200502-14:13:44] epoch: 5758 train-loss: 0.010554311900503106\n",
      "[LOG 20200502-14:13:45] epoch: 5759 train-loss: 0.010554312210943963\n",
      "[LOG 20200502-14:13:45] epoch: 5760 train-loss: 0.010554312624865107\n",
      "[LOG 20200502-14:13:45] epoch: 5760 new best train-loss: 0.010554312624865107 found\n",
      "[LOG 20200502-14:13:45] epoch: 5761 train-loss: 0.010554313142266538\n",
      "[LOG 20200502-14:13:45] epoch: 5762 train-loss: 0.010554313970108828\n",
      "[LOG 20200502-14:13:46] epoch: 5763 train-loss: 0.010554314901431402\n",
      "[LOG 20200502-14:13:46] epoch: 5764 train-loss: 0.010554315522313118\n",
      "[LOG 20200502-14:13:46] epoch: 5765 train-loss: 0.010554316453635693\n",
      "[LOG 20200502-14:13:46] epoch: 5766 train-loss: 0.010554316660596265\n",
      "[LOG 20200502-14:13:46] epoch: 5767 train-loss: 0.010554318212800555\n",
      "[LOG 20200502-14:13:47] epoch: 5768 train-loss: 0.0105543186267217\n",
      "[LOG 20200502-14:13:47] epoch: 5769 train-loss: 0.010554320075445704\n",
      "[LOG 20200502-14:13:47] epoch: 5770 train-loss: 0.010554320592847135\n",
      "[LOG 20200502-14:13:47] epoch: 5771 train-loss: 0.010554320903287994\n",
      "[LOG 20200502-14:13:48] epoch: 5772 train-loss: 0.010554322972893715\n",
      "[LOG 20200502-14:13:48] epoch: 5773 train-loss: 0.010554323490295146\n",
      "[LOG 20200502-14:13:48] epoch: 5774 train-loss: 0.010554324732058577\n",
      "[LOG 20200502-14:13:48] epoch: 5775 train-loss: 0.010554325042499436\n",
      "[LOG 20200502-14:13:48] epoch: 5776 train-loss: 0.010554326180782583\n",
      "[LOG 20200502-14:13:49] epoch: 5777 train-loss: 0.010554327112105157\n",
      "[LOG 20200502-14:13:49] epoch: 5778 train-loss: 0.01055432783646716\n",
      "[LOG 20200502-14:13:49] epoch: 5779 train-loss: 0.01055432938867145\n",
      "[LOG 20200502-14:13:49] epoch: 5780 train-loss: 0.01055433021651374\n",
      "[LOG 20200502-14:13:50] epoch: 5781 train-loss: 0.01055433073391517\n",
      "[LOG 20200502-14:13:50] epoch: 5782 train-loss: 0.0105543312513166\n",
      "[LOG 20200502-14:13:50] epoch: 5783 train-loss: 0.01055433176871803\n",
      "[LOG 20200502-14:13:50] epoch: 5784 train-loss: 0.010554332389599748\n",
      "[LOG 20200502-14:13:51] epoch: 5785 train-loss: 0.010554332907001177\n",
      "[LOG 20200502-14:13:51] epoch: 5786 train-loss: 0.010554333320922323\n",
      "[LOG 20200502-14:13:51] epoch: 5787 train-loss: 0.01055433414876461\n",
      "[LOG 20200502-14:13:51] epoch: 5788 train-loss: 0.01055433414876461\n",
      "[LOG 20200502-14:13:51] epoch: 5789 train-loss: 0.010554334252244897\n",
      "[LOG 20200502-14:13:52] epoch: 5790 train-loss: 0.010554334769646326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:13:52] epoch: 5791 train-loss: 0.010554334459205469\n",
      "[LOG 20200502-14:13:52] epoch: 5792 train-loss: 0.010554333941804038\n",
      "[LOG 20200502-14:13:52] epoch: 5793 train-loss: 0.010554334252244897\n",
      "[LOG 20200502-14:13:53] epoch: 5794 train-loss: 0.010554333424402608\n",
      "[LOG 20200502-14:13:53] epoch: 5795 train-loss: 0.010554333010481464\n",
      "[LOG 20200502-14:13:53] epoch: 5796 train-loss: 0.010554332079158889\n",
      "[LOG 20200502-14:13:53] epoch: 5797 train-loss: 0.010554331354796886\n",
      "[LOG 20200502-14:13:54] epoch: 5798 train-loss: 0.010554330113033453\n",
      "[LOG 20200502-14:13:54] epoch: 5799 train-loss: 0.010554329595632024\n",
      "[LOG 20200502-14:13:54] epoch: 5800 train-loss: 0.01055432783646716\n",
      "[LOG 20200502-14:13:54] epoch: 5801 train-loss: 0.010554326077302298\n",
      "[LOG 20200502-14:13:55] epoch: 5802 train-loss: 0.010554324732058577\n",
      "[LOG 20200502-14:13:55] epoch: 5803 train-loss: 0.010554322352011999\n",
      "[LOG 20200502-14:13:55] epoch: 5804 train-loss: 0.010554319971965419\n",
      "[LOG 20200502-14:13:55] epoch: 5805 train-loss: 0.010554317798879411\n",
      "[LOG 20200502-14:13:55] epoch: 5806 train-loss: 0.01055431521187226\n",
      "[LOG 20200502-14:13:56] epoch: 5807 train-loss: 0.010554312417904535\n",
      "[LOG 20200502-14:13:56] epoch: 5808 train-loss: 0.010554309003055096\n",
      "[LOG 20200502-14:13:56] epoch: 5809 train-loss: 0.010554306312567659\n",
      "[LOG 20200502-14:13:56] epoch: 5810 train-loss: 0.010554302380316787\n",
      "[LOG 20200502-14:13:56] epoch: 5810 new best train-loss: 0.010554302380316787 found\n",
      "[LOG 20200502-14:13:57] epoch: 5811 train-loss: 0.010554298551546203\n",
      "[LOG 20200502-14:13:57] epoch: 5812 train-loss: 0.010554294619295333\n",
      "[LOG 20200502-14:13:57] epoch: 5813 train-loss: 0.010554289859202173\n",
      "[LOG 20200502-14:13:57] epoch: 5814 train-loss: 0.010554285306069586\n",
      "[LOG 20200502-14:13:58] epoch: 5815 train-loss: 0.010554280856417285\n",
      "[LOG 20200502-14:13:58] epoch: 5816 train-loss: 0.010554275889363553\n",
      "[LOG 20200502-14:13:58] epoch: 5817 train-loss: 0.010554269990987249\n",
      "[LOG 20200502-14:13:58] epoch: 5818 train-loss: 0.010554264299571514\n",
      "[LOG 20200502-14:13:58] epoch: 5819 train-loss: 0.010554258297714923\n",
      "[LOG 20200502-14:13:59] epoch: 5820 train-loss: 0.0105542517784569\n",
      "[LOG 20200502-14:13:59] epoch: 5820 new best train-loss: 0.0105542517784569 found\n",
      "[LOG 20200502-14:13:59] epoch: 5821 train-loss: 0.01055424546615945\n",
      "[LOG 20200502-14:13:59] epoch: 5822 train-loss: 0.010554237912098566\n",
      "[LOG 20200502-14:13:59] epoch: 5823 train-loss: 0.010554230771958828\n",
      "[LOG 20200502-14:14:00] epoch: 5824 train-loss: 0.010554223321378231\n",
      "[LOG 20200502-14:14:00] epoch: 5825 train-loss: 0.010554215249915918\n",
      "[LOG 20200502-14:14:00] epoch: 5826 train-loss: 0.010554206868012747\n",
      "[LOG 20200502-14:14:00] epoch: 5827 train-loss: 0.010554198175668716\n",
      "[LOG 20200502-14:14:01] epoch: 5828 train-loss: 0.010554189690285258\n",
      "[LOG 20200502-14:14:01] epoch: 5829 train-loss: 0.010554180066618655\n",
      "[LOG 20200502-14:14:01] epoch: 5830 train-loss: 0.010554170132511191\n",
      "[LOG 20200502-14:14:01] epoch: 5830 new best train-loss: 0.010554170132511191 found\n",
      "[LOG 20200502-14:14:01] epoch: 5831 train-loss: 0.01055415988796287\n",
      "[LOG 20200502-14:14:01] epoch: 5832 train-loss: 0.010554150264296267\n",
      "[LOG 20200502-14:14:02] epoch: 5833 train-loss: 0.010554139088425372\n",
      "[LOG 20200502-14:14:02] epoch: 5834 train-loss: 0.010554128016034761\n",
      "[LOG 20200502-14:14:02] epoch: 5835 train-loss: 0.010554117150604725\n",
      "[LOG 20200502-14:14:02] epoch: 5836 train-loss: 0.010554104836450683\n",
      "[LOG 20200502-14:14:03] epoch: 5837 train-loss: 0.010554092729257213\n",
      "[LOG 20200502-14:14:03] epoch: 5838 train-loss: 0.010554080001182027\n",
      "[LOG 20200502-14:14:03] epoch: 5839 train-loss: 0.010554067480067411\n",
      "[LOG 20200502-14:14:03] epoch: 5840 train-loss: 0.010554054234590795\n",
      "[LOG 20200502-14:14:03] epoch: 5840 new best train-loss: 0.010554054234590795 found\n",
      "[LOG 20200502-14:14:03] epoch: 5841 train-loss: 0.010554040575193034\n",
      "[LOG 20200502-14:14:04] epoch: 5842 train-loss: 0.010554026605354415\n",
      "[LOG 20200502-14:14:04] epoch: 5843 train-loss: 0.010554012221594652\n",
      "[LOG 20200502-14:14:04] epoch: 5844 train-loss: 0.010553997734354602\n",
      "[LOG 20200502-14:14:04] epoch: 5845 train-loss: 0.010553982729713121\n",
      "[LOG 20200502-14:14:05] epoch: 5846 train-loss: 0.010553967725071643\n",
      "[LOG 20200502-14:14:05] epoch: 5847 train-loss: 0.010553952409989305\n",
      "[LOG 20200502-14:14:05] epoch: 5848 train-loss: 0.01055393616358439\n",
      "[LOG 20200502-14:14:05] epoch: 5849 train-loss: 0.010553919503258334\n",
      "[LOG 20200502-14:14:06] epoch: 5850 train-loss: 0.010553902739451991\n",
      "[LOG 20200502-14:14:06] epoch: 5850 new best train-loss: 0.010553902739451991 found\n",
      "[LOG 20200502-14:14:06] epoch: 5851 train-loss: 0.010553885975645648\n",
      "[LOG 20200502-14:14:06] epoch: 5852 train-loss: 0.010553868487477303\n",
      "[LOG 20200502-14:14:06] epoch: 5853 train-loss: 0.010553850585387813\n",
      "[LOG 20200502-14:14:07] epoch: 5854 train-loss: 0.010553833097219467\n",
      "[LOG 20200502-14:14:07] epoch: 5855 train-loss: 0.01055381509164969\n",
      "[LOG 20200502-14:14:07] epoch: 5856 train-loss: 0.010553796465198198\n",
      "[LOG 20200502-14:14:07] epoch: 5857 train-loss: 0.010553777735266421\n",
      "[LOG 20200502-14:14:08] epoch: 5858 train-loss: 0.01055375879837407\n",
      "[LOG 20200502-14:14:08] epoch: 5859 train-loss: 0.010553739447560575\n",
      "[LOG 20200502-14:14:08] epoch: 5860 train-loss: 0.010553720407187939\n",
      "[LOG 20200502-14:14:08] epoch: 5860 new best train-loss: 0.010553720407187939 found\n",
      "[LOG 20200502-14:14:08] epoch: 5861 train-loss: 0.010553700538973013\n",
      "[LOG 20200502-14:14:09] epoch: 5862 train-loss: 0.010553680049876371\n",
      "[LOG 20200502-14:14:09] epoch: 5863 train-loss: 0.010553660285141733\n",
      "[LOG 20200502-14:14:09] epoch: 5864 train-loss: 0.010553639899525378\n",
      "[LOG 20200502-14:14:09] epoch: 5865 train-loss: 0.010553619099987878\n",
      "[LOG 20200502-14:14:10] epoch: 5866 train-loss: 0.010553598093489805\n",
      "[LOG 20200502-14:14:10] epoch: 5867 train-loss: 0.01055357770787345\n",
      "[LOG 20200502-14:14:10] epoch: 5868 train-loss: 0.010553556287454234\n",
      "[LOG 20200502-14:14:10] epoch: 5869 train-loss: 0.010553535280956162\n",
      "[LOG 20200502-14:14:11] epoch: 5870 train-loss: 0.010553513757056661\n",
      "[LOG 20200502-14:14:11] epoch: 5870 new best train-loss: 0.010553513757056661 found\n",
      "[LOG 20200502-14:14:11] epoch: 5871 train-loss: 0.010553492543598017\n",
      "[LOG 20200502-14:14:11] epoch: 5872 train-loss: 0.010553470709257655\n",
      "[LOG 20200502-14:14:11] epoch: 5873 train-loss: 0.010553449288838439\n",
      "[LOG 20200502-14:14:12] epoch: 5874 train-loss: 0.01055342766145865\n",
      "[LOG 20200502-14:14:12] epoch: 5875 train-loss: 0.010553405620157719\n",
      "[LOG 20200502-14:14:12] epoch: 5876 train-loss: 0.010553383578856787\n",
      "[LOG 20200502-14:14:12] epoch: 5877 train-loss: 0.010553361434075568\n",
      "[LOG 20200502-14:14:12] epoch: 5878 train-loss: 0.010553340013656352\n",
      "[LOG 20200502-14:14:13] epoch: 5879 train-loss: 0.010553317868875133\n",
      "[LOG 20200502-14:14:13] epoch: 5880 train-loss: 0.010553295724093914\n",
      "[LOG 20200502-14:14:13] epoch: 5880 new best train-loss: 0.010553295724093914 found\n",
      "[LOG 20200502-14:14:13] epoch: 5881 train-loss: 0.010553273786273267\n",
      "[LOG 20200502-14:14:13] epoch: 5882 train-loss: 0.010553251641492048\n",
      "[LOG 20200502-14:14:14] epoch: 5883 train-loss: 0.010553229600191116\n",
      "[LOG 20200502-14:14:14] epoch: 5884 train-loss: 0.010553207869331041\n",
      "[LOG 20200502-14:14:14] epoch: 5885 train-loss: 0.010553185931510396\n",
      "[LOG 20200502-14:14:14] epoch: 5886 train-loss: 0.010553163890209463\n",
      "[LOG 20200502-14:14:14] epoch: 5887 train-loss: 0.010553142780231105\n",
      "[LOG 20200502-14:14:15] epoch: 5888 train-loss: 0.010553120842410458\n",
      "[LOG 20200502-14:14:15] epoch: 5889 train-loss: 0.010553099008070098\n",
      "[LOG 20200502-14:14:15] epoch: 5890 train-loss: 0.010553076966769166\n",
      "[LOG 20200502-14:14:15] epoch: 5890 new best train-loss: 0.010553076966769166 found\n",
      "[LOG 20200502-14:14:15] epoch: 5891 train-loss: 0.010553055960271094\n",
      "[LOG 20200502-14:14:16] epoch: 5892 train-loss: 0.01055303526421388\n",
      "[LOG 20200502-14:14:16] epoch: 5893 train-loss: 0.010553013843794664\n",
      "[LOG 20200502-14:14:16] epoch: 5894 train-loss: 0.01055299263033602\n",
      "[LOG 20200502-14:14:16] epoch: 5895 train-loss: 0.010552972037759092\n",
      "[LOG 20200502-14:14:17] epoch: 5896 train-loss: 0.010552951134741306\n",
      "[LOG 20200502-14:14:17] epoch: 5897 train-loss: 0.010552930542164378\n",
      "[LOG 20200502-14:14:17] epoch: 5898 train-loss: 0.010552910053067736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:14:17] epoch: 5899 train-loss: 0.010552889770931669\n",
      "[LOG 20200502-14:14:18] epoch: 5900 train-loss: 0.010552870109677315\n",
      "[LOG 20200502-14:14:18] epoch: 5900 new best train-loss: 0.010552870109677315 found\n",
      "[LOG 20200502-14:14:18] epoch: 5901 train-loss: 0.010552850241462389\n",
      "[LOG 20200502-14:14:18] epoch: 5902 train-loss: 0.010552830373247465\n",
      "[LOG 20200502-14:14:18] epoch: 5903 train-loss: 0.010552811125914255\n",
      "[LOG 20200502-14:14:19] epoch: 5904 train-loss: 0.010552791878581047\n",
      "[LOG 20200502-14:14:19] epoch: 5905 train-loss: 0.010552772424287267\n",
      "[LOG 20200502-14:14:19] epoch: 5906 train-loss: 0.010552753797835775\n",
      "[LOG 20200502-14:14:20] epoch: 5907 train-loss: 0.010552735067903996\n",
      "[LOG 20200502-14:14:20] epoch: 5908 train-loss: 0.010552716855373647\n",
      "[LOG 20200502-14:14:20] epoch: 5909 train-loss: 0.010552698953284157\n",
      "[LOG 20200502-14:14:20] epoch: 5910 train-loss: 0.010552680740753809\n",
      "[LOG 20200502-14:14:20] epoch: 5910 new best train-loss: 0.010552680740753809 found\n",
      "[LOG 20200502-14:14:21] epoch: 5911 train-loss: 0.010552663149105178\n",
      "[LOG 20200502-14:14:21] epoch: 5912 train-loss: 0.01055264648877912\n",
      "[LOG 20200502-14:14:21] epoch: 5913 train-loss: 0.010552629311051633\n",
      "[LOG 20200502-14:14:21] epoch: 5914 train-loss: 0.010552611926363574\n",
      "[LOG 20200502-14:14:22] epoch: 5915 train-loss: 0.010552595162557231\n",
      "[LOG 20200502-14:14:22] epoch: 5916 train-loss: 0.01055257860571146\n",
      "[LOG 20200502-14:14:22] epoch: 5917 train-loss: 0.010552562462786833\n",
      "[LOG 20200502-14:14:22] epoch: 5918 train-loss: 0.01055254621638192\n",
      "[LOG 20200502-14:14:23] epoch: 5919 train-loss: 0.010552530797819296\n",
      "[LOG 20200502-14:14:23] epoch: 5920 train-loss: 0.01055251568969753\n",
      "[LOG 20200502-14:14:23] epoch: 5920 new best train-loss: 0.01055251568969753 found\n",
      "[LOG 20200502-14:14:23] epoch: 5921 train-loss: 0.010552499753733477\n",
      "[LOG 20200502-14:14:23] epoch: 5922 train-loss: 0.010552485059532855\n",
      "[LOG 20200502-14:14:24] epoch: 5923 train-loss: 0.010552470572292805\n",
      "[LOG 20200502-14:14:24] epoch: 5924 train-loss: 0.01055245598157247\n",
      "[LOG 20200502-14:14:24] epoch: 5925 train-loss: 0.01055244149433242\n",
      "[LOG 20200502-14:14:24] epoch: 5926 train-loss: 0.010552427938414944\n",
      "[LOG 20200502-14:14:25] epoch: 5927 train-loss: 0.010552414692938328\n",
      "[LOG 20200502-14:14:25] epoch: 5928 train-loss: 0.010552401033540567\n",
      "[LOG 20200502-14:14:25] epoch: 5929 train-loss: 0.010552387891544236\n",
      "[LOG 20200502-14:14:25] epoch: 5930 train-loss: 0.01055237516346905\n",
      "[LOG 20200502-14:14:25] epoch: 5930 new best train-loss: 0.01055237516346905 found\n",
      "[LOG 20200502-14:14:26] epoch: 5931 train-loss: 0.010552362124953005\n",
      "[LOG 20200502-14:14:26] epoch: 5932 train-loss: 0.010552350017759535\n",
      "[LOG 20200502-14:14:26] epoch: 5933 train-loss: 0.010552338221006922\n",
      "[LOG 20200502-14:14:26] epoch: 5934 train-loss: 0.010552326113813452\n",
      "[LOG 20200502-14:14:27] epoch: 5935 train-loss: 0.010552314317060841\n",
      "[LOG 20200502-14:14:27] epoch: 5936 train-loss: 0.01055230303770966\n",
      "[LOG 20200502-14:14:27] epoch: 5937 train-loss: 0.010552291551397907\n",
      "[LOG 20200502-14:14:27] epoch: 5938 train-loss: 0.010552281410329871\n",
      "[LOG 20200502-14:14:28] epoch: 5939 train-loss: 0.010552270337939262\n",
      "[LOG 20200502-14:14:28] epoch: 5940 train-loss: 0.0105522604038318\n",
      "[LOG 20200502-14:14:28] epoch: 5940 new best train-loss: 0.0105522604038318 found\n",
      "[LOG 20200502-14:14:28] epoch: 5941 train-loss: 0.010552250262763765\n",
      "[LOG 20200502-14:14:28] epoch: 5942 train-loss: 0.010552240018215444\n",
      "[LOG 20200502-14:14:28] epoch: 5943 train-loss: 0.010552230808469985\n",
      "[LOG 20200502-14:14:29] epoch: 5944 train-loss: 0.010552220874362521\n",
      "[LOG 20200502-14:14:29] epoch: 5945 train-loss: 0.010552211768097348\n",
      "[LOG 20200502-14:14:29] epoch: 5946 train-loss: 0.01055220276531246\n",
      "[LOG 20200502-14:14:29] epoch: 5947 train-loss: 0.010552194383409288\n",
      "[LOG 20200502-14:14:30] epoch: 5948 train-loss: 0.010552186001506116\n",
      "[LOG 20200502-14:14:30] epoch: 5949 train-loss: 0.01055217751612266\n",
      "[LOG 20200502-14:14:30] epoch: 5950 train-loss: 0.01055216985858149\n",
      "[LOG 20200502-14:14:30] epoch: 5950 new best train-loss: 0.01055216985858149 found\n",
      "[LOG 20200502-14:14:30] epoch: 5951 train-loss: 0.010552161683638891\n",
      "[LOG 20200502-14:14:30] epoch: 5952 train-loss: 0.010552153819137149\n",
      "[LOG 20200502-14:14:31] epoch: 5953 train-loss: 0.01055214667899741\n",
      "[LOG 20200502-14:14:31] epoch: 5954 train-loss: 0.010552139745818244\n",
      "[LOG 20200502-14:14:31] epoch: 5955 train-loss: 0.010552132605678506\n",
      "[LOG 20200502-14:14:31] epoch: 5956 train-loss: 0.010552126086420484\n",
      "[LOG 20200502-14:14:31] epoch: 5957 train-loss: 0.010552119256721603\n",
      "[LOG 20200502-14:14:32] epoch: 5958 train-loss: 0.010552112944424152\n",
      "[LOG 20200502-14:14:32] epoch: 5959 train-loss: 0.010552107046047846\n",
      "[LOG 20200502-14:14:32] epoch: 5960 train-loss: 0.01055210094071097\n",
      "[LOG 20200502-14:14:32] epoch: 5960 new best train-loss: 0.01055210094071097 found\n",
      "[LOG 20200502-14:14:32] epoch: 5961 train-loss: 0.010552095042334663\n",
      "[LOG 20200502-14:14:33] epoch: 5962 train-loss: 0.010552089040478071\n",
      "[LOG 20200502-14:14:33] epoch: 5963 train-loss: 0.010552083659503195\n",
      "[LOG 20200502-14:14:33] epoch: 5964 train-loss: 0.010552078899410035\n",
      "[LOG 20200502-14:14:33] epoch: 5965 train-loss: 0.010552073621915447\n",
      "[LOG 20200502-14:14:33] epoch: 5966 train-loss: 0.010552068758342002\n",
      "[LOG 20200502-14:14:34] epoch: 5967 train-loss: 0.0105520643086897\n",
      "[LOG 20200502-14:14:34] epoch: 5968 train-loss: 0.010552059134675397\n",
      "[LOG 20200502-14:14:34] epoch: 5969 train-loss: 0.010552055305904813\n",
      "[LOG 20200502-14:14:34] epoch: 5970 train-loss: 0.010552051063213084\n",
      "[LOG 20200502-14:14:34] epoch: 5970 new best train-loss: 0.010552051063213084 found\n",
      "[LOG 20200502-14:14:35] epoch: 5971 train-loss: 0.010552047027481927\n",
      "[LOG 20200502-14:14:35] epoch: 5972 train-loss: 0.010552042784790197\n",
      "[LOG 20200502-14:14:35] epoch: 5973 train-loss: 0.010552039266460471\n",
      "[LOG 20200502-14:14:35] epoch: 5974 train-loss: 0.01055203564465046\n",
      "[LOG 20200502-14:14:35] epoch: 5975 train-loss: 0.010552032022840448\n",
      "[LOG 20200502-14:14:36] epoch: 5976 train-loss: 0.010552028711471293\n",
      "[LOG 20200502-14:14:36] epoch: 5977 train-loss: 0.01055202560706271\n",
      "[LOG 20200502-14:14:36] epoch: 5978 train-loss: 0.010552022606134415\n",
      "[LOG 20200502-14:14:36] epoch: 5979 train-loss: 0.010552019501725832\n",
      "[LOG 20200502-14:14:37] epoch: 5980 train-loss: 0.010552016811238395\n",
      "[LOG 20200502-14:14:37] epoch: 5980 new best train-loss: 0.010552016811238395 found\n",
      "[LOG 20200502-14:14:37] epoch: 5981 train-loss: 0.010552014224231243\n",
      "[LOG 20200502-14:14:37] epoch: 5982 train-loss: 0.010552011326783232\n",
      "[LOG 20200502-14:14:37] epoch: 5983 train-loss: 0.010552009360657798\n",
      "[LOG 20200502-14:14:37] epoch: 5984 train-loss: 0.010552006877130933\n",
      "[LOG 20200502-14:14:38] epoch: 5985 train-loss: 0.010552005117966069\n",
      "[LOG 20200502-14:14:38] epoch: 5986 train-loss: 0.010552002634439204\n",
      "[LOG 20200502-14:14:38] epoch: 5987 train-loss: 0.010552000978754627\n",
      "[LOG 20200502-14:14:38] epoch: 5988 train-loss: 0.010551998909148905\n",
      "[LOG 20200502-14:14:38] epoch: 5989 train-loss: 0.010551996839543184\n",
      "[LOG 20200502-14:14:39] epoch: 5990 train-loss: 0.01055199508037832\n",
      "[LOG 20200502-14:14:39] epoch: 5990 new best train-loss: 0.01055199508037832 found\n",
      "[LOG 20200502-14:14:39] epoch: 5991 train-loss: 0.010551993942095174\n",
      "[LOG 20200502-14:14:39] epoch: 5992 train-loss: 0.010551992493371168\n",
      "[LOG 20200502-14:14:39] epoch: 5993 train-loss: 0.010551990837686591\n",
      "[LOG 20200502-14:14:40] epoch: 5994 train-loss: 0.010551989388962587\n",
      "[LOG 20200502-14:14:40] epoch: 5995 train-loss: 0.010551987940238582\n",
      "[LOG 20200502-14:14:40] epoch: 5996 train-loss: 0.010551986491514577\n",
      "[LOG 20200502-14:14:40] epoch: 5997 train-loss: 0.010551985767152574\n",
      "[LOG 20200502-14:14:40] epoch: 5998 train-loss: 0.010551984939310286\n",
      "[LOG 20200502-14:14:41] epoch: 5999 train-loss: 0.010551983904507425\n",
      "[LOG 20200502-14:14:41] epoch: 6000 train-loss: 0.01055198297318485\n",
      "[LOG 20200502-14:14:41] epoch: 6000 new best train-loss: 0.01055198297318485 found\n",
      "[LOG 20200502-14:14:41] epoch: 6001 train-loss: 0.010551981834901704\n",
      "[LOG 20200502-14:14:41] epoch: 6002 train-loss: 0.010551981007059416\n",
      "[LOG 20200502-14:14:42] epoch: 6003 train-loss: 0.01055198059313827\n",
      "[LOG 20200502-14:14:42] epoch: 6004 train-loss: 0.01055197986877627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:14:42] epoch: 6005 train-loss: 0.010551978937453695\n",
      "[LOG 20200502-14:14:42] epoch: 6006 train-loss: 0.01055197800613112\n",
      "[LOG 20200502-14:14:43] epoch: 6007 train-loss: 0.010551978109611405\n",
      "[LOG 20200502-14:14:43] epoch: 6008 train-loss: 0.010551977281769117\n",
      "[LOG 20200502-14:14:43] epoch: 6009 train-loss: 0.010551976350446543\n",
      "[LOG 20200502-14:14:43] epoch: 6010 train-loss: 0.010551976040005684\n",
      "[LOG 20200502-14:14:43] epoch: 6010 new best train-loss: 0.010551976040005684 found\n",
      "[LOG 20200502-14:14:43] epoch: 6011 train-loss: 0.010551975522604253\n",
      "[LOG 20200502-14:14:44] epoch: 6012 train-loss: 0.010551974591281679\n",
      "[LOG 20200502-14:14:44] epoch: 6013 train-loss: 0.010551974591281679\n",
      "[LOG 20200502-14:14:44] epoch: 6014 train-loss: 0.010551974280840822\n",
      "[LOG 20200502-14:14:44] epoch: 6015 train-loss: 0.010551973556478819\n",
      "[LOG 20200502-14:14:45] epoch: 6016 train-loss: 0.010551972935597101\n",
      "[LOG 20200502-14:14:45] epoch: 6017 train-loss: 0.01055197272863653\n",
      "[LOG 20200502-14:14:45] epoch: 6018 train-loss: 0.010551972004274527\n",
      "[LOG 20200502-14:14:45] epoch: 6019 train-loss: 0.010551972211235099\n",
      "[LOG 20200502-14:14:46] epoch: 6020 train-loss: 0.010551971176432239\n",
      "[LOG 20200502-14:14:46] epoch: 6020 new best train-loss: 0.010551971176432239 found\n",
      "[LOG 20200502-14:14:46] epoch: 6021 train-loss: 0.010551970659030808\n",
      "[LOG 20200502-14:14:46] epoch: 6022 train-loss: 0.010551970038149092\n",
      "[LOG 20200502-14:14:46] epoch: 6023 train-loss: 0.010551969624227948\n",
      "[LOG 20200502-14:14:47] epoch: 6024 train-loss: 0.010551969210306803\n",
      "[LOG 20200502-14:14:47] epoch: 6025 train-loss: 0.010551968589425087\n",
      "[LOG 20200502-14:14:47] epoch: 6026 train-loss: 0.010551968175503943\n",
      "[LOG 20200502-14:14:47] epoch: 6027 train-loss: 0.010551967347661654\n",
      "[LOG 20200502-14:14:48] epoch: 6028 train-loss: 0.010551967140701082\n",
      "[LOG 20200502-14:14:48] epoch: 6029 train-loss: 0.010551966209378507\n",
      "[LOG 20200502-14:14:48] epoch: 6030 train-loss: 0.010551965174575647\n",
      "[LOG 20200502-14:14:48] epoch: 6030 new best train-loss: 0.010551965174575647 found\n",
      "[LOG 20200502-14:14:48] epoch: 6031 train-loss: 0.010551964243253073\n",
      "[LOG 20200502-14:14:48] epoch: 6032 train-loss: 0.010551963415410783\n",
      "[LOG 20200502-14:14:49] epoch: 6033 train-loss: 0.010551962794529067\n",
      "[LOG 20200502-14:14:49] epoch: 6034 train-loss: 0.010551961966686778\n",
      "[LOG 20200502-14:14:49] epoch: 6035 train-loss: 0.010551960517962774\n",
      "[LOG 20200502-14:14:49] epoch: 6036 train-loss: 0.010551959483159913\n",
      "[LOG 20200502-14:14:50] epoch: 6037 train-loss: 0.01055195824139648\n",
      "[LOG 20200502-14:14:50] epoch: 6038 train-loss: 0.01055195720659362\n",
      "[LOG 20200502-14:14:50] epoch: 6039 train-loss: 0.010551955550909042\n",
      "[LOG 20200502-14:14:50] epoch: 6040 train-loss: 0.010551954619586468\n",
      "[LOG 20200502-14:14:50] epoch: 6040 new best train-loss: 0.010551954619586468 found\n",
      "[LOG 20200502-14:14:51] epoch: 6041 train-loss: 0.010551952860421605\n",
      "[LOG 20200502-14:14:51] epoch: 6042 train-loss: 0.010551951308217313\n",
      "[LOG 20200502-14:14:51] epoch: 6043 train-loss: 0.010551950066453882\n",
      "[LOG 20200502-14:14:51] epoch: 6044 train-loss: 0.010551947789887587\n",
      "[LOG 20200502-14:14:52] epoch: 6045 train-loss: 0.010551946548124155\n",
      "[LOG 20200502-14:14:52] epoch: 6046 train-loss: 0.010551944685479006\n",
      "[LOG 20200502-14:14:52] epoch: 6047 train-loss: 0.010551942408912711\n",
      "[LOG 20200502-14:14:52] epoch: 6048 train-loss: 0.010551940235826705\n",
      "[LOG 20200502-14:14:52] epoch: 6049 train-loss: 0.010551937959260412\n",
      "[LOG 20200502-14:14:53] epoch: 6050 train-loss: 0.010551935475733545\n",
      "[LOG 20200502-14:14:53] epoch: 6050 new best train-loss: 0.010551935475733545 found\n",
      "[LOG 20200502-14:14:53] epoch: 6051 train-loss: 0.010551933302647538\n",
      "[LOG 20200502-14:14:53] epoch: 6052 train-loss: 0.010551930508679815\n",
      "[LOG 20200502-14:14:53] epoch: 6053 train-loss: 0.01055192771471209\n",
      "[LOG 20200502-14:14:54] epoch: 6054 train-loss: 0.010551925438145796\n",
      "[LOG 20200502-14:14:54] epoch: 6055 train-loss: 0.010551923058099218\n",
      "[LOG 20200502-14:14:54] epoch: 6056 train-loss: 0.010551919125848346\n",
      "[LOG 20200502-14:14:54] epoch: 6057 train-loss: 0.010551916849282052\n",
      "[LOG 20200502-14:14:55] epoch: 6058 train-loss: 0.010551913020511469\n",
      "[LOG 20200502-14:14:55] epoch: 6059 train-loss: 0.01055190929522117\n",
      "[LOG 20200502-14:14:55] epoch: 6060 train-loss: 0.010551905466450585\n",
      "[LOG 20200502-14:14:55] epoch: 6060 new best train-loss: 0.010551905466450585 found\n",
      "[LOG 20200502-14:14:55] epoch: 6061 train-loss: 0.010551902258561717\n",
      "[LOG 20200502-14:14:56] epoch: 6062 train-loss: 0.010551898326310847\n",
      "[LOG 20200502-14:14:56] epoch: 6063 train-loss: 0.010551894187099405\n",
      "[LOG 20200502-14:14:56] epoch: 6064 train-loss: 0.010551889737447103\n",
      "[LOG 20200502-14:14:56] epoch: 6065 train-loss: 0.010551885908676518\n",
      "[LOG 20200502-14:14:57] epoch: 6066 train-loss: 0.010551881355543932\n",
      "[LOG 20200502-14:14:57] epoch: 6067 train-loss: 0.010551876595450772\n",
      "[LOG 20200502-14:14:57] epoch: 6068 train-loss: 0.010551872352759043\n",
      "[LOG 20200502-14:14:57] epoch: 6069 train-loss: 0.010551867385705313\n",
      "[LOG 20200502-14:14:58] epoch: 6070 train-loss: 0.010551862004730437\n",
      "[LOG 20200502-14:14:58] epoch: 6070 new best train-loss: 0.010551862004730437 found\n",
      "[LOG 20200502-14:14:58] epoch: 6071 train-loss: 0.010551857244637277\n",
      "[LOG 20200502-14:14:58] epoch: 6072 train-loss: 0.010551851139300399\n",
      "[LOG 20200502-14:14:58] epoch: 6073 train-loss: 0.010551846068766382\n",
      "[LOG 20200502-14:14:59] epoch: 6074 train-loss: 0.010551840170390077\n",
      "[LOG 20200502-14:14:59] epoch: 6075 train-loss: 0.01055183427201377\n",
      "[LOG 20200502-14:14:59] epoch: 6076 train-loss: 0.010551828270157179\n",
      "[LOG 20200502-14:14:59] epoch: 6077 train-loss: 0.010551822371780872\n",
      "[LOG 20200502-14:15:00] epoch: 6078 train-loss: 0.010551815749042563\n",
      "[LOG 20200502-14:15:00] epoch: 6079 train-loss: 0.010551809229784541\n",
      "[LOG 20200502-14:15:00] epoch: 6080 train-loss: 0.010551802607046233\n",
      "[LOG 20200502-14:15:00] epoch: 6080 new best train-loss: 0.010551802607046233 found\n",
      "[LOG 20200502-14:15:00] epoch: 6081 train-loss: 0.010551795466906495\n",
      "[LOG 20200502-14:15:01] epoch: 6082 train-loss: 0.010551788326766755\n",
      "[LOG 20200502-14:15:01] epoch: 6083 train-loss: 0.010551781290107302\n",
      "[LOG 20200502-14:15:01] epoch: 6084 train-loss: 0.01055177321864499\n",
      "[LOG 20200502-14:15:01] epoch: 6085 train-loss: 0.010551765975024965\n",
      "[LOG 20200502-14:15:02] epoch: 6086 train-loss: 0.010551758007042937\n",
      "[LOG 20200502-14:15:02] epoch: 6087 train-loss: 0.010551749935580624\n",
      "[LOG 20200502-14:15:02] epoch: 6088 train-loss: 0.01055174165715774\n",
      "[LOG 20200502-14:15:02] epoch: 6089 train-loss: 0.01055173348221514\n",
      "[LOG 20200502-14:15:03] epoch: 6090 train-loss: 0.010551724582910538\n",
      "[LOG 20200502-14:15:03] epoch: 6090 new best train-loss: 0.010551724582910538 found\n",
      "[LOG 20200502-14:15:03] epoch: 6091 train-loss: 0.010551715890566507\n",
      "[LOG 20200502-14:15:03] epoch: 6092 train-loss: 0.010551706784301333\n",
      "[LOG 20200502-14:15:03] epoch: 6093 train-loss: 0.01055169767803616\n",
      "[LOG 20200502-14:15:04] epoch: 6094 train-loss: 0.010551688364810415\n",
      "[LOG 20200502-14:15:04] epoch: 6095 train-loss: 0.010551678844624095\n",
      "[LOG 20200502-14:15:04] epoch: 6096 train-loss: 0.01055166901399692\n",
      "[LOG 20200502-14:15:04] epoch: 6097 train-loss: 0.010551659493810601\n",
      "[LOG 20200502-14:15:05] epoch: 6098 train-loss: 0.01055164924926228\n",
      "[LOG 20200502-14:15:05] epoch: 6099 train-loss: 0.01055163900471396\n",
      "[LOG 20200502-14:15:05] epoch: 6100 train-loss: 0.010551628656685352\n",
      "[LOG 20200502-14:15:05] epoch: 6100 new best train-loss: 0.010551628656685352 found\n",
      "[LOG 20200502-14:15:05] epoch: 6101 train-loss: 0.01055161820517646\n",
      "[LOG 20200502-14:15:06] epoch: 6102 train-loss: 0.010551607753667567\n",
      "[LOG 20200502-14:15:06] epoch: 6103 train-loss: 0.0105515963708361\n",
      "[LOG 20200502-14:15:06] epoch: 6104 train-loss: 0.010551585608886348\n",
      "[LOG 20200502-14:15:06] epoch: 6105 train-loss: 0.01055157370865345\n",
      "[LOG 20200502-14:15:07] epoch: 6106 train-loss: 0.010551562325821983\n",
      "[LOG 20200502-14:15:07] epoch: 6107 train-loss: 0.010551551667352518\n",
      "[LOG 20200502-14:15:07] epoch: 6108 train-loss: 0.010551539146237902\n",
      "[LOG 20200502-14:15:07] epoch: 6109 train-loss: 0.010551527452965578\n",
      "[LOG 20200502-14:15:07] epoch: 6110 train-loss: 0.01055151555273268\n",
      "[LOG 20200502-14:15:07] epoch: 6110 new best train-loss: 0.01055151555273268 found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:15:08] epoch: 6111 train-loss: 0.01055150344553921\n",
      "[LOG 20200502-14:15:08] epoch: 6112 train-loss: 0.010551491131385168\n",
      "[LOG 20200502-14:15:08] epoch: 6113 train-loss: 0.010551478817231126\n",
      "[LOG 20200502-14:15:08] epoch: 6114 train-loss: 0.01055146629611651\n",
      "[LOG 20200502-14:15:09] epoch: 6115 train-loss: 0.010551453361080753\n",
      "[LOG 20200502-14:15:09] epoch: 6116 train-loss: 0.010551440736485852\n",
      "[LOG 20200502-14:15:09] epoch: 6117 train-loss: 0.010551428111890951\n",
      "[LOG 20200502-14:15:09] epoch: 6118 train-loss: 0.010551414555973478\n",
      "[LOG 20200502-14:15:10] epoch: 6119 train-loss: 0.010551401207016574\n",
      "[LOG 20200502-14:15:10] epoch: 6120 train-loss: 0.010551387961539958\n",
      "[LOG 20200502-14:15:10] epoch: 6120 new best train-loss: 0.010551387961539958 found\n",
      "[LOG 20200502-14:15:10] epoch: 6121 train-loss: 0.010551375026504198\n",
      "[LOG 20200502-14:15:10] epoch: 6122 train-loss: 0.010551361781027582\n",
      "[LOG 20200502-14:15:11] epoch: 6123 train-loss: 0.010551348328590393\n",
      "[LOG 20200502-14:15:11] epoch: 6124 train-loss: 0.010551334565712346\n",
      "[LOG 20200502-14:15:11] epoch: 6125 train-loss: 0.010551320802834298\n",
      "[LOG 20200502-14:15:11] epoch: 6126 train-loss: 0.010551306626035107\n",
      "[LOG 20200502-14:15:12] epoch: 6127 train-loss: 0.010551293277078204\n",
      "[LOG 20200502-14:15:12] epoch: 6128 train-loss: 0.010551279307239585\n",
      "[LOG 20200502-14:15:12] epoch: 6129 train-loss: 0.010551265440881252\n",
      "[LOG 20200502-14:15:12] epoch: 6130 train-loss: 0.010551251678003205\n",
      "[LOG 20200502-14:15:12] epoch: 6130 new best train-loss: 0.010551251678003205 found\n",
      "[LOG 20200502-14:15:12] epoch: 6131 train-loss: 0.010551237501204014\n",
      "[LOG 20200502-14:15:13] epoch: 6132 train-loss: 0.010551223013963964\n",
      "[LOG 20200502-14:15:13] epoch: 6133 train-loss: 0.010551209354566203\n",
      "[LOG 20200502-14:15:13] epoch: 6134 train-loss: 0.010551195695168443\n",
      "[LOG 20200502-14:15:13] epoch: 6135 train-loss: 0.010551181725329824\n",
      "[LOG 20200502-14:15:14] epoch: 6136 train-loss: 0.01055116754853063\n",
      "[LOG 20200502-14:15:14] epoch: 6137 train-loss: 0.01055115388913287\n",
      "[LOG 20200502-14:15:14] epoch: 6138 train-loss: 0.01055113971233368\n",
      "[LOG 20200502-14:15:14] epoch: 6139 train-loss: 0.010551125535534488\n",
      "[LOG 20200502-14:15:14] epoch: 6140 train-loss: 0.010551111876136728\n",
      "[LOG 20200502-14:15:15] epoch: 6140 new best train-loss: 0.010551111876136728 found\n",
      "[LOG 20200502-14:15:15] epoch: 6141 train-loss: 0.010551097906298108\n",
      "[LOG 20200502-14:15:15] epoch: 6142 train-loss: 0.01055108362601863\n",
      "[LOG 20200502-14:15:15] epoch: 6143 train-loss: 0.010551069863140583\n",
      "[LOG 20200502-14:15:15] epoch: 6144 train-loss: 0.010551056824624538\n",
      "[LOG 20200502-14:15:16] epoch: 6145 train-loss: 0.010551043165226778\n",
      "[LOG 20200502-14:15:16] epoch: 6146 train-loss: 0.010551029712789588\n",
      "[LOG 20200502-14:15:16] epoch: 6147 train-loss: 0.010551016053391827\n",
      "[LOG 20200502-14:15:16] epoch: 6148 train-loss: 0.010551003014875783\n",
      "[LOG 20200502-14:15:17] epoch: 6149 train-loss: 0.01055098966591888\n",
      "[LOG 20200502-14:15:17] epoch: 6150 train-loss: 0.010550976420442263\n",
      "[LOG 20200502-14:15:17] epoch: 6150 new best train-loss: 0.010550976420442263 found\n",
      "[LOG 20200502-14:15:17] epoch: 6151 train-loss: 0.010550963899327649\n",
      "[LOG 20200502-14:15:17] epoch: 6152 train-loss: 0.010550951171252463\n",
      "[LOG 20200502-14:15:18] epoch: 6153 train-loss: 0.010550938132736418\n",
      "[LOG 20200502-14:15:18] epoch: 6154 train-loss: 0.010550925922062662\n",
      "[LOG 20200502-14:15:18] epoch: 6155 train-loss: 0.010550913090507189\n",
      "[LOG 20200502-14:15:18] epoch: 6156 train-loss: 0.010550901293754578\n",
      "[LOG 20200502-14:15:18] epoch: 6157 train-loss: 0.010550889290041394\n",
      "[LOG 20200502-14:15:19] epoch: 6158 train-loss: 0.01055087780372964\n",
      "[LOG 20200502-14:15:19] epoch: 6159 train-loss: 0.01055086569653617\n",
      "[LOG 20200502-14:15:19] epoch: 6160 train-loss: 0.010550854417184988\n",
      "[LOG 20200502-14:15:19] epoch: 6160 new best train-loss: 0.010550854417184988 found\n",
      "[LOG 20200502-14:15:19] epoch: 6161 train-loss: 0.01055084303435352\n",
      "[LOG 20200502-14:15:20] epoch: 6162 train-loss: 0.010550831651522053\n",
      "[LOG 20200502-14:15:20] epoch: 6163 train-loss: 0.010550820786092017\n",
      "[LOG 20200502-14:15:20] epoch: 6164 train-loss: 0.010550810231102837\n",
      "[LOG 20200502-14:15:20] epoch: 6165 train-loss: 0.010550800090034803\n",
      "[LOG 20200502-14:15:21] epoch: 6166 train-loss: 0.010550789328085052\n",
      "[LOG 20200502-14:15:21] epoch: 6167 train-loss: 0.01055077939397759\n",
      "[LOG 20200502-14:15:21] epoch: 6168 train-loss: 0.010550769666830698\n",
      "[LOG 20200502-14:15:21] epoch: 6169 train-loss: 0.010550760457085239\n",
      "[LOG 20200502-14:15:22] epoch: 6170 train-loss: 0.010550751040379206\n",
      "[LOG 20200502-14:15:22] epoch: 6170 new best train-loss: 0.010550751040379206 found\n",
      "[LOG 20200502-14:15:22] epoch: 6171 train-loss: 0.010550741520192888\n",
      "[LOG 20200502-14:15:22] epoch: 6172 train-loss: 0.010550732724368572\n",
      "[LOG 20200502-14:15:22] epoch: 6173 train-loss: 0.010550724549425973\n",
      "[LOG 20200502-14:15:23] epoch: 6174 train-loss: 0.010550715753601657\n",
      "[LOG 20200502-14:15:23] epoch: 6175 train-loss: 0.010550707578659058\n",
      "[LOG 20200502-14:15:23] epoch: 6176 train-loss: 0.010550700231558748\n",
      "[LOG 20200502-14:15:23] epoch: 6177 train-loss: 0.01055069278097815\n",
      "[LOG 20200502-14:15:24] epoch: 6178 train-loss: 0.010550685433877839\n",
      "[LOG 20200502-14:15:24] epoch: 6179 train-loss: 0.01055067777633667\n",
      "[LOG 20200502-14:15:24] epoch: 6180 train-loss: 0.01055067146403922\n",
      "[LOG 20200502-14:15:24] epoch: 6180 new best train-loss: 0.01055067146403922 found\n",
      "[LOG 20200502-14:15:24] epoch: 6181 train-loss: 0.010550664427379767\n",
      "[LOG 20200502-14:15:25] epoch: 6182 train-loss: 0.01055065832204289\n",
      "[LOG 20200502-14:15:25] epoch: 6183 train-loss: 0.010550652941068014\n",
      "[LOG 20200502-14:15:25] epoch: 6184 train-loss: 0.010550646628770564\n",
      "[LOG 20200502-14:15:25] epoch: 6185 train-loss: 0.010550641144315401\n",
      "[LOG 20200502-14:15:26] epoch: 6186 train-loss: 0.010550635970301099\n",
      "[LOG 20200502-14:15:26] epoch: 6187 train-loss: 0.010550630692806508\n",
      "[LOG 20200502-14:15:26] epoch: 6188 train-loss: 0.01055062593271335\n",
      "[LOG 20200502-14:15:26] epoch: 6189 train-loss: 0.010550621586541334\n",
      "[LOG 20200502-14:15:27] epoch: 6190 train-loss: 0.010550616929928461\n",
      "[LOG 20200502-14:15:27] epoch: 6190 new best train-loss: 0.010550616929928461 found\n",
      "[LOG 20200502-14:15:27] epoch: 6191 train-loss: 0.010550613515079021\n",
      "[LOG 20200502-14:15:27] epoch: 6192 train-loss: 0.010550609479347864\n",
      "[LOG 20200502-14:15:27] epoch: 6193 train-loss: 0.010550606167978711\n",
      "[LOG 20200502-14:15:28] epoch: 6194 train-loss: 0.01055060275312927\n",
      "[LOG 20200502-14:15:28] epoch: 6195 train-loss: 0.010550599959161546\n",
      "[LOG 20200502-14:15:28] epoch: 6196 train-loss: 0.010550597268674109\n",
      "[LOG 20200502-14:15:28] epoch: 6197 train-loss: 0.010550593853824668\n",
      "[LOG 20200502-14:15:28] epoch: 6198 train-loss: 0.01055059250858095\n",
      "[LOG 20200502-14:15:29] epoch: 6199 train-loss: 0.010550590232014656\n",
      "[LOG 20200502-14:15:29] epoch: 6200 train-loss: 0.010550587955448363\n",
      "[LOG 20200502-14:15:29] epoch: 6200 new best train-loss: 0.010550587955448363 found\n",
      "[LOG 20200502-14:15:29] epoch: 6201 train-loss: 0.010550586610204644\n",
      "[LOG 20200502-14:15:29] epoch: 6202 train-loss: 0.010550585575401783\n",
      "[LOG 20200502-14:15:30] epoch: 6203 train-loss: 0.010550583919717206\n",
      "[LOG 20200502-14:15:30] epoch: 6204 train-loss: 0.010550583091874918\n",
      "[LOG 20200502-14:15:30] epoch: 6205 train-loss: 0.010550582574473487\n",
      "[LOG 20200502-14:15:30] epoch: 6206 train-loss: 0.010550581746631198\n",
      "[LOG 20200502-14:15:31] epoch: 6207 train-loss: 0.01055058143619034\n",
      "[LOG 20200502-14:15:31] epoch: 6208 train-loss: 0.010550581850111485\n",
      "[LOG 20200502-14:15:31] epoch: 6209 train-loss: 0.010550581539670626\n",
      "[LOG 20200502-14:15:31] epoch: 6210 train-loss: 0.010550581850111485\n",
      "[LOG 20200502-14:15:31] epoch: 6210 new best train-loss: 0.010550581850111485 found\n",
      "[LOG 20200502-14:15:31] epoch: 6211 train-loss: 0.010550581850111485\n",
      "[LOG 20200502-14:15:32] epoch: 6212 train-loss: 0.010550582057072056\n",
      "[LOG 20200502-14:15:32] epoch: 6213 train-loss: 0.01055058278143406\n",
      "[LOG 20200502-14:15:32] epoch: 6214 train-loss: 0.010550584126677778\n",
      "[LOG 20200502-14:15:32] epoch: 6215 train-loss: 0.01055058485103978\n",
      "[LOG 20200502-14:15:33] epoch: 6216 train-loss: 0.010550585782362355\n",
      "[LOG 20200502-14:15:33] epoch: 6217 train-loss: 0.01055058723108636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:15:33] epoch: 6218 train-loss: 0.010550588679810366\n",
      "[LOG 20200502-14:15:33] epoch: 6219 train-loss: 0.010550590232014656\n",
      "[LOG 20200502-14:15:34] epoch: 6220 train-loss: 0.010550592094659805\n",
      "[LOG 20200502-14:15:34] epoch: 6221 train-loss: 0.010550593853824668\n",
      "[LOG 20200502-14:15:34] epoch: 6222 train-loss: 0.010550595612989532\n",
      "[LOG 20200502-14:15:34] epoch: 6223 train-loss: 0.01055059778607554\n",
      "[LOG 20200502-14:15:34] epoch: 6224 train-loss: 0.010550599131319258\n",
      "[LOG 20200502-14:15:35] epoch: 6225 train-loss: 0.010550601821806695\n",
      "[LOG 20200502-14:15:35] epoch: 6226 train-loss: 0.010550604408813847\n",
      "[LOG 20200502-14:15:35] epoch: 6227 train-loss: 0.010550606374939283\n",
      "[LOG 20200502-14:15:35] epoch: 6228 train-loss: 0.010550608858466148\n",
      "[LOG 20200502-14:15:36] epoch: 6229 train-loss: 0.010550611238512728\n",
      "[LOG 20200502-14:15:36] epoch: 6230 train-loss: 0.010550613411598735\n",
      "[LOG 20200502-14:15:36] epoch: 6231 train-loss: 0.010550616619487604\n",
      "[LOG 20200502-14:15:36] epoch: 6232 train-loss: 0.010550618585613038\n",
      "[LOG 20200502-14:15:37] epoch: 6233 train-loss: 0.010550621586541334\n",
      "[LOG 20200502-14:15:37] epoch: 6234 train-loss: 0.010550623863107629\n",
      "[LOG 20200502-14:15:37] epoch: 6235 train-loss: 0.010550626553595066\n",
      "[LOG 20200502-14:15:37] epoch: 6236 train-loss: 0.010550628519720502\n",
      "[LOG 20200502-14:15:37] epoch: 6237 train-loss: 0.01055063172760937\n",
      "[LOG 20200502-14:15:38] epoch: 6238 train-loss: 0.01055063410765595\n",
      "[LOG 20200502-14:15:38] epoch: 6239 train-loss: 0.010550636280741956\n",
      "[LOG 20200502-14:15:38] epoch: 6240 train-loss: 0.010550639178189967\n",
      "[LOG 20200502-14:15:38] epoch: 6241 train-loss: 0.010550641247795688\n",
      "[LOG 20200502-14:15:39] epoch: 6242 train-loss: 0.01055064383480284\n",
      "[LOG 20200502-14:15:39] epoch: 6243 train-loss: 0.01055064621484942\n",
      "[LOG 20200502-14:15:39] epoch: 6244 train-loss: 0.010550648077494569\n",
      "[LOG 20200502-14:15:39] epoch: 6245 train-loss: 0.010550650871462293\n",
      "[LOG 20200502-14:15:39] epoch: 6246 train-loss: 0.010550653251508871\n",
      "[LOG 20200502-14:15:40] epoch: 6247 train-loss: 0.010550654493272305\n",
      "[LOG 20200502-14:15:40] epoch: 6248 train-loss: 0.010550656873318884\n",
      "[LOG 20200502-14:15:40] epoch: 6249 train-loss: 0.01055065832204289\n",
      "[LOG 20200502-14:15:40] epoch: 6250 train-loss: 0.010550659770766893\n",
      "[LOG 20200502-14:15:41] epoch: 6251 train-loss: 0.010550661633412043\n",
      "[LOG 20200502-14:15:41] epoch: 6252 train-loss: 0.010550662978655763\n",
      "[LOG 20200502-14:15:41] epoch: 6253 train-loss: 0.010550664323899481\n",
      "[LOG 20200502-14:15:41] epoch: 6254 train-loss: 0.010550664944781197\n",
      "[LOG 20200502-14:15:42] epoch: 6255 train-loss: 0.010550666290024916\n",
      "[LOG 20200502-14:15:42] epoch: 6256 train-loss: 0.010550667014386918\n",
      "[LOG 20200502-14:15:42] epoch: 6257 train-loss: 0.01055066753178835\n",
      "[LOG 20200502-14:15:42] epoch: 6258 train-loss: 0.010550667945709493\n",
      "[LOG 20200502-14:15:43] epoch: 6259 train-loss: 0.010550668463110924\n",
      "[LOG 20200502-14:15:43] epoch: 6260 train-loss: 0.010550668359630637\n",
      "[LOG 20200502-14:15:43] epoch: 6261 train-loss: 0.010550667738748921\n",
      "[LOG 20200502-14:15:43] epoch: 6262 train-loss: 0.010550667428308062\n",
      "[LOG 20200502-14:15:43] epoch: 6263 train-loss: 0.010550666703946061\n",
      "[LOG 20200502-14:15:44] epoch: 6264 train-loss: 0.01055066618654463\n",
      "[LOG 20200502-14:15:44] epoch: 6265 train-loss: 0.010550664737820625\n",
      "[LOG 20200502-14:15:44] epoch: 6266 train-loss: 0.010550663599537479\n",
      "[LOG 20200502-14:15:44] epoch: 6267 train-loss: 0.010550661943852901\n",
      "[LOG 20200502-14:15:45] epoch: 6268 train-loss: 0.010550660081207752\n",
      "[LOG 20200502-14:15:45] epoch: 6269 train-loss: 0.010550657597680887\n",
      "[LOG 20200502-14:15:45] epoch: 6270 train-loss: 0.01055065542459488\n",
      "[LOG 20200502-14:15:45] epoch: 6271 train-loss: 0.010550652630627155\n",
      "[LOG 20200502-14:15:46] epoch: 6272 train-loss: 0.010550649836659431\n",
      "[LOG 20200502-14:15:46] epoch: 6273 train-loss: 0.010550646732250849\n",
      "[LOG 20200502-14:15:46] epoch: 6274 train-loss: 0.01055064300696055\n",
      "[LOG 20200502-14:15:46] epoch: 6275 train-loss: 0.010550638867749108\n",
      "[LOG 20200502-14:15:47] epoch: 6276 train-loss: 0.010550634625057379\n",
      "[LOG 20200502-14:15:47] epoch: 6277 train-loss: 0.010550629968444506\n",
      "[LOG 20200502-14:15:47] epoch: 6278 train-loss: 0.01055062541531192\n",
      "[LOG 20200502-14:15:47] epoch: 6279 train-loss: 0.010550620344777903\n",
      "[LOG 20200502-14:15:47] epoch: 6280 train-loss: 0.010550614239441024\n",
      "[LOG 20200502-14:15:48] epoch: 6281 train-loss: 0.01055060854802529\n",
      "[LOG 20200502-14:15:48] epoch: 6282 train-loss: 0.010550602649648985\n",
      "[LOG 20200502-14:15:48] epoch: 6283 train-loss: 0.010550595819950104\n",
      "[LOG 20200502-14:15:48] epoch: 6284 train-loss: 0.010550589300692081\n",
      "[LOG 20200502-14:15:49] epoch: 6285 train-loss: 0.010550581850111485\n",
      "[LOG 20200502-14:15:49] epoch: 6286 train-loss: 0.010550574813452031\n",
      "[LOG 20200502-14:15:49] epoch: 6287 train-loss: 0.01055056643154886\n",
      "[LOG 20200502-14:15:49] epoch: 6288 train-loss: 0.010550558360086547\n",
      "[LOG 20200502-14:15:50] epoch: 6289 train-loss: 0.010550549978183376\n",
      "[LOG 20200502-14:15:50] epoch: 6290 train-loss: 0.010550540975398488\n",
      "[LOG 20200502-14:15:50] epoch: 6290 new best train-loss: 0.010550540975398488 found\n",
      "[LOG 20200502-14:15:50] epoch: 6291 train-loss: 0.010550531558692455\n",
      "[LOG 20200502-14:15:50] epoch: 6292 train-loss: 0.010550521831545565\n",
      "[LOG 20200502-14:15:50] epoch: 6293 train-loss: 0.010550511793957816\n",
      "[LOG 20200502-14:15:51] epoch: 6294 train-loss: 0.010550501032008065\n",
      "[LOG 20200502-14:15:51] epoch: 6295 train-loss: 0.010550490580499172\n",
      "[LOG 20200502-14:15:51] epoch: 6296 train-loss: 0.01055047930114799\n",
      "[LOG 20200502-14:15:51] epoch: 6297 train-loss: 0.01055046853919824\n",
      "[LOG 20200502-14:15:52] epoch: 6298 train-loss: 0.010550456845925914\n",
      "[LOG 20200502-14:15:52] epoch: 6299 train-loss: 0.010550444738732444\n",
      "[LOG 20200502-14:15:52] epoch: 6300 train-loss: 0.010550432321098115\n",
      "[LOG 20200502-14:15:52] epoch: 6300 new best train-loss: 0.010550432321098115 found\n",
      "[LOG 20200502-14:15:52] epoch: 6301 train-loss: 0.010550419903463788\n",
      "[LOG 20200502-14:15:53] epoch: 6302 train-loss: 0.010550407278868888\n",
      "[LOG 20200502-14:15:53] epoch: 6303 train-loss: 0.010550393722951412\n",
      "[LOG 20200502-14:15:53] epoch: 6304 train-loss: 0.010550380373994509\n",
      "[LOG 20200502-14:15:54] epoch: 6305 train-loss: 0.010550366507636176\n",
      "[LOG 20200502-14:15:54] epoch: 6306 train-loss: 0.010550352848238416\n",
      "[LOG 20200502-14:15:54] epoch: 6307 train-loss: 0.01055033825751808\n",
      "[LOG 20200502-14:15:54] epoch: 6308 train-loss: 0.010550324391159747\n",
      "[LOG 20200502-14:15:55] epoch: 6309 train-loss: 0.010550309386518266\n",
      "[LOG 20200502-14:15:55] epoch: 6310 train-loss: 0.010550294795797931\n",
      "[LOG 20200502-14:15:55] epoch: 6310 new best train-loss: 0.010550294795797931 found\n",
      "[LOG 20200502-14:15:55] epoch: 6311 train-loss: 0.010550280101597309\n",
      "[LOG 20200502-14:15:55] epoch: 6312 train-loss: 0.010550264683034685\n",
      "[LOG 20200502-14:15:56] epoch: 6313 train-loss: 0.01055024926447206\n",
      "[LOG 20200502-14:15:56] epoch: 6314 train-loss: 0.010550233121547434\n",
      "[LOG 20200502-14:15:56] epoch: 6315 train-loss: 0.010550217496024238\n",
      "[LOG 20200502-14:15:56] epoch: 6316 train-loss: 0.010550201560060183\n",
      "[LOG 20200502-14:15:57] epoch: 6317 train-loss: 0.010550186038017273\n",
      "[LOG 20200502-14:15:57] epoch: 6318 train-loss: 0.010550170309013791\n",
      "[LOG 20200502-14:15:57] epoch: 6319 train-loss: 0.010550154062608877\n",
      "[LOG 20200502-14:15:57] epoch: 6320 train-loss: 0.010550137816203965\n",
      "[LOG 20200502-14:15:57] epoch: 6320 new best train-loss: 0.010550137816203965 found\n",
      "[LOG 20200502-14:15:57] epoch: 6321 train-loss: 0.010550121776759624\n",
      "[LOG 20200502-14:15:58] epoch: 6322 train-loss: 0.010550105426874425\n",
      "[LOG 20200502-14:15:58] epoch: 6323 train-loss: 0.010550088559587797\n",
      "[LOG 20200502-14:15:58] epoch: 6324 train-loss: 0.010550072727104029\n",
      "[LOG 20200502-14:15:58] epoch: 6325 train-loss: 0.01055005637721883\n",
      "[LOG 20200502-14:15:59] epoch: 6326 train-loss: 0.010550040648215346\n",
      "[LOG 20200502-14:15:59] epoch: 6327 train-loss: 0.010550023884409003\n",
      "[LOG 20200502-14:15:59] epoch: 6328 train-loss: 0.01055000794844495\n",
      "[LOG 20200502-14:15:59] epoch: 6329 train-loss: 0.010549991702040037\n",
      "[LOG 20200502-14:16:00] epoch: 6330 train-loss: 0.010549975869556269\n",
      "[LOG 20200502-14:16:00] epoch: 6330 new best train-loss: 0.010549975869556269 found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:16:00] epoch: 6331 train-loss: 0.01054995951967107\n",
      "[LOG 20200502-14:16:00] epoch: 6332 train-loss: 0.01054994399762816\n",
      "[LOG 20200502-14:16:00] epoch: 6333 train-loss: 0.010549928579065535\n",
      "[LOG 20200502-14:16:01] epoch: 6334 train-loss: 0.010549912436140908\n",
      "[LOG 20200502-14:16:01] epoch: 6335 train-loss: 0.010549897224538855\n",
      "[LOG 20200502-14:16:01] epoch: 6336 train-loss: 0.01054988211641709\n",
      "[LOG 20200502-14:16:01] epoch: 6337 train-loss: 0.010549867422216468\n",
      "[LOG 20200502-14:16:01] epoch: 6338 train-loss: 0.010549851900173558\n",
      "[LOG 20200502-14:16:02] epoch: 6339 train-loss: 0.010549837723374367\n",
      "[LOG 20200502-14:16:02] epoch: 6340 train-loss: 0.010549823236134317\n",
      "[LOG 20200502-14:16:02] epoch: 6340 new best train-loss: 0.010549823236134317 found\n",
      "[LOG 20200502-14:16:02] epoch: 6341 train-loss: 0.01054980895585484\n",
      "[LOG 20200502-14:16:02] epoch: 6342 train-loss: 0.010549794572095076\n",
      "[LOG 20200502-14:16:03] epoch: 6343 train-loss: 0.010549780705736743\n",
      "[LOG 20200502-14:16:03] epoch: 6344 train-loss: 0.010549766942858696\n",
      "[LOG 20200502-14:16:03] epoch: 6345 train-loss: 0.010549753490421507\n",
      "[LOG 20200502-14:16:03] epoch: 6346 train-loss: 0.010549739934504032\n",
      "[LOG 20200502-14:16:03] epoch: 6347 train-loss: 0.010549727413389418\n",
      "[LOG 20200502-14:16:04] epoch: 6348 train-loss: 0.010549715202715661\n",
      "[LOG 20200502-14:16:04] epoch: 6349 train-loss: 0.010549702474640476\n",
      "[LOG 20200502-14:16:04] epoch: 6350 train-loss: 0.010549690160486434\n",
      "[LOG 20200502-14:16:04] epoch: 6350 new best train-loss: 0.010549690160486434 found\n",
      "[LOG 20200502-14:16:04] epoch: 6351 train-loss: 0.010549678570694394\n",
      "[LOG 20200502-14:16:05] epoch: 6352 train-loss: 0.010549667291343212\n",
      "[LOG 20200502-14:16:05] epoch: 6353 train-loss: 0.010549655598070886\n",
      "[LOG 20200502-14:16:05] epoch: 6354 train-loss: 0.010549644836121135\n",
      "[LOG 20200502-14:16:05] epoch: 6355 train-loss: 0.010549634074171385\n",
      "[LOG 20200502-14:16:06] epoch: 6356 train-loss: 0.010549623829623064\n",
      "[LOG 20200502-14:16:06] epoch: 6357 train-loss: 0.010549613481594456\n",
      "[LOG 20200502-14:16:06] epoch: 6358 train-loss: 0.010549604271848997\n",
      "[LOG 20200502-14:16:06] epoch: 6359 train-loss: 0.010549594544702105\n",
      "[LOG 20200502-14:16:07] epoch: 6360 train-loss: 0.010549584921035502\n",
      "[LOG 20200502-14:16:07] epoch: 6360 new best train-loss: 0.010549584921035502 found\n",
      "[LOG 20200502-14:16:07] epoch: 6361 train-loss: 0.010549576435652044\n",
      "[LOG 20200502-14:16:07] epoch: 6362 train-loss: 0.010549568053748872\n",
      "[LOG 20200502-14:16:07] epoch: 6363 train-loss: 0.01054955915444427\n",
      "[LOG 20200502-14:16:07] epoch: 6364 train-loss: 0.010549550979501672\n",
      "[LOG 20200502-14:16:08] epoch: 6365 train-loss: 0.010549543528921075\n",
      "[LOG 20200502-14:16:08] epoch: 6366 train-loss: 0.010549536492261622\n",
      "[LOG 20200502-14:16:08] epoch: 6367 train-loss: 0.01054952914516131\n",
      "[LOG 20200502-14:16:08] epoch: 6368 train-loss: 0.010549522418942716\n",
      "[LOG 20200502-14:16:09] epoch: 6369 train-loss: 0.010549515796204409\n",
      "[LOG 20200502-14:16:09] epoch: 6370 train-loss: 0.010549509069985814\n",
      "[LOG 20200502-14:16:09] epoch: 6370 new best train-loss: 0.010549509069985814 found\n",
      "[LOG 20200502-14:16:09] epoch: 6371 train-loss: 0.010549503275089793\n",
      "[LOG 20200502-14:16:09] epoch: 6372 train-loss: 0.010549497997595204\n",
      "[LOG 20200502-14:16:10] epoch: 6373 train-loss: 0.010549492409659756\n",
      "[LOG 20200502-14:16:10] epoch: 6374 train-loss: 0.01054948733912574\n",
      "[LOG 20200502-14:16:10] epoch: 6375 train-loss: 0.010549482475552294\n",
      "[LOG 20200502-14:16:10] epoch: 6376 train-loss: 0.01054947761197885\n",
      "[LOG 20200502-14:16:10] epoch: 6377 train-loss: 0.010549473783208264\n",
      "[LOG 20200502-14:16:11] epoch: 6378 train-loss: 0.010549469643996822\n",
      "[LOG 20200502-14:16:11] epoch: 6379 train-loss: 0.010549465608265664\n",
      "[LOG 20200502-14:16:11] epoch: 6380 train-loss: 0.010549462089935938\n",
      "[LOG 20200502-14:16:11] epoch: 6380 new best train-loss: 0.010549462089935938 found\n",
      "[LOG 20200502-14:16:11] epoch: 6381 train-loss: 0.010549458985527357\n",
      "[LOG 20200502-14:16:12] epoch: 6382 train-loss: 0.010549455881118774\n",
      "[LOG 20200502-14:16:12] epoch: 6383 train-loss: 0.01054945339759191\n",
      "[LOG 20200502-14:16:12] epoch: 6384 train-loss: 0.01054945070710447\n",
      "[LOG 20200502-14:16:12] epoch: 6385 train-loss: 0.010549448844459321\n",
      "[LOG 20200502-14:16:13] epoch: 6386 train-loss: 0.0105494467748536\n",
      "[LOG 20200502-14:16:13] epoch: 6387 train-loss: 0.010549444912208451\n",
      "[LOG 20200502-14:16:13] epoch: 6388 train-loss: 0.01054944284260273\n",
      "[LOG 20200502-14:16:13] epoch: 6389 train-loss: 0.010549441807799868\n",
      "[LOG 20200502-14:16:14] epoch: 6390 train-loss: 0.010549440359075865\n",
      "[LOG 20200502-14:16:14] epoch: 6390 new best train-loss: 0.010549440359075865 found\n",
      "[LOG 20200502-14:16:14] epoch: 6391 train-loss: 0.010549439324273003\n",
      "[LOG 20200502-14:16:14] epoch: 6392 train-loss: 0.010549438496430716\n",
      "[LOG 20200502-14:16:15] epoch: 6393 train-loss: 0.01054943808250957\n",
      "[LOG 20200502-14:16:15] epoch: 6394 train-loss: 0.010549437461627854\n",
      "[LOG 20200502-14:16:15] epoch: 6395 train-loss: 0.010549437668588426\n",
      "[LOG 20200502-14:16:15] epoch: 6396 train-loss: 0.010549438185989857\n",
      "[LOG 20200502-14:16:16] epoch: 6397 train-loss: 0.010549437979029285\n",
      "[LOG 20200502-14:16:16] epoch: 6398 train-loss: 0.01054943808250957\n",
      "[LOG 20200502-14:16:16] epoch: 6399 train-loss: 0.010549438806871573\n",
      "[LOG 20200502-14:16:16] epoch: 6400 train-loss: 0.010549439634713862\n",
      "[LOG 20200502-14:16:16] epoch: 6400 new best train-loss: 0.010549439634713862 found\n",
      "[LOG 20200502-14:16:16] epoch: 6401 train-loss: 0.010549440152115293\n",
      "[LOG 20200502-14:16:17] epoch: 6402 train-loss: 0.010549441704319583\n",
      "[LOG 20200502-14:16:17] epoch: 6403 train-loss: 0.010549442635642158\n",
      "[LOG 20200502-14:16:17] epoch: 6404 train-loss: 0.01054944336000416\n",
      "[LOG 20200502-14:16:18] epoch: 6405 train-loss: 0.01054944522264931\n",
      "[LOG 20200502-14:16:18] epoch: 6406 train-loss: 0.0105494467748536\n",
      "[LOG 20200502-14:16:18] epoch: 6407 train-loss: 0.010549448016617034\n",
      "[LOG 20200502-14:16:18] epoch: 6408 train-loss: 0.010549450396663614\n",
      "[LOG 20200502-14:16:19] epoch: 6409 train-loss: 0.010549451948867904\n",
      "[LOG 20200502-14:16:19] epoch: 6410 train-loss: 0.010549453708032766\n",
      "[LOG 20200502-14:16:19] epoch: 6411 train-loss: 0.010549456191559633\n",
      "[LOG 20200502-14:16:19] epoch: 6412 train-loss: 0.010549458675086498\n",
      "[LOG 20200502-14:16:20] epoch: 6413 train-loss: 0.010549460951652791\n",
      "[LOG 20200502-14:16:20] epoch: 6414 train-loss: 0.0105494631247388\n",
      "[LOG 20200502-14:16:20] epoch: 6415 train-loss: 0.01054946550478538\n",
      "[LOG 20200502-14:16:20] epoch: 6416 train-loss: 0.010549468091792531\n",
      "[LOG 20200502-14:16:21] epoch: 6417 train-loss: 0.010549470885760255\n",
      "[LOG 20200502-14:16:21] epoch: 6418 train-loss: 0.010549473472767405\n",
      "[LOG 20200502-14:16:21] epoch: 6419 train-loss: 0.010549476059774557\n",
      "[LOG 20200502-14:16:21] epoch: 6420 train-loss: 0.010549479267663427\n",
      "[LOG 20200502-14:16:22] epoch: 6421 train-loss: 0.01054948206163115\n",
      "[LOG 20200502-14:16:22] epoch: 6422 train-loss: 0.010549484855598874\n",
      "[LOG 20200502-14:16:22] epoch: 6423 train-loss: 0.010549488166968027\n",
      "[LOG 20200502-14:16:22] epoch: 6424 train-loss: 0.010549490650494894\n",
      "[LOG 20200502-14:16:23] epoch: 6425 train-loss: 0.010549493961864047\n",
      "[LOG 20200502-14:16:23] epoch: 6426 train-loss: 0.010549496652351486\n",
      "[LOG 20200502-14:16:23] epoch: 6427 train-loss: 0.010549500170681212\n",
      "[LOG 20200502-14:16:23] epoch: 6428 train-loss: 0.010549503068129221\n",
      "[LOG 20200502-14:16:23] epoch: 6429 train-loss: 0.01054950627601809\n",
      "[LOG 20200502-14:16:24] epoch: 6430 train-loss: 0.010549509276946386\n",
      "[LOG 20200502-14:16:24] epoch: 6431 train-loss: 0.010549511863953538\n",
      "[LOG 20200502-14:16:24] epoch: 6432 train-loss: 0.010549515071842406\n",
      "[LOG 20200502-14:16:24] epoch: 6433 train-loss: 0.010549518176250987\n",
      "[LOG 20200502-14:16:25] epoch: 6434 train-loss: 0.010549521384139856\n",
      "[LOG 20200502-14:16:25] epoch: 6435 train-loss: 0.010549524488548437\n",
      "[LOG 20200502-14:16:25] epoch: 6436 train-loss: 0.010549527489476733\n",
      "[LOG 20200502-14:16:25] epoch: 6437 train-loss: 0.010549530179964172\n",
      "[LOG 20200502-14:16:26] epoch: 6438 train-loss: 0.010549533284372754\n",
      "[LOG 20200502-14:16:26] epoch: 6439 train-loss: 0.01054953576789962\n",
      "[LOG 20200502-14:16:26] epoch: 6440 train-loss: 0.010549538665347628\n",
      "[LOG 20200502-14:16:26] epoch: 6441 train-loss: 0.010549541562795639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:16:27] epoch: 6442 train-loss: 0.010549543839361932\n",
      "[LOG 20200502-14:16:27] epoch: 6443 train-loss: 0.010549546426369084\n",
      "[LOG 20200502-14:16:27] epoch: 6444 train-loss: 0.010549549220336808\n",
      "[LOG 20200502-14:16:27] epoch: 6445 train-loss: 0.010549551082981957\n",
      "[LOG 20200502-14:16:27] epoch: 6446 train-loss: 0.010549553566508822\n",
      "[LOG 20200502-14:16:28] epoch: 6447 train-loss: 0.010549556153515974\n",
      "[LOG 20200502-14:16:28] epoch: 6448 train-loss: 0.010549558223121695\n",
      "[LOG 20200502-14:16:28] epoch: 6449 train-loss: 0.010549559775325987\n",
      "[LOG 20200502-14:16:28] epoch: 6450 train-loss: 0.010549561948411994\n",
      "[LOG 20200502-14:16:29] epoch: 6451 train-loss: 0.01054956391453743\n",
      "[LOG 20200502-14:16:29] epoch: 6452 train-loss: 0.010549565570222007\n",
      "[LOG 20200502-14:16:29] epoch: 6453 train-loss: 0.010549567122426298\n",
      "[LOG 20200502-14:16:29] epoch: 6454 train-loss: 0.010549568260709444\n",
      "[LOG 20200502-14:16:30] epoch: 6455 train-loss: 0.010549569605953164\n",
      "[LOG 20200502-14:16:30] epoch: 6456 train-loss: 0.010549570744236311\n",
      "[LOG 20200502-14:16:30] epoch: 6457 train-loss: 0.010549572192960315\n",
      "[LOG 20200502-14:16:30] epoch: 6458 train-loss: 0.01054957260688146\n",
      "[LOG 20200502-14:16:31] epoch: 6459 train-loss: 0.010549573331243463\n",
      "[LOG 20200502-14:16:31] epoch: 6460 train-loss: 0.010549574262566037\n",
      "[LOG 20200502-14:16:31] epoch: 6461 train-loss: 0.01054957446952661\n",
      "[LOG 20200502-14:16:31] epoch: 6462 train-loss: 0.01054957446952661\n",
      "[LOG 20200502-14:16:32] epoch: 6463 train-loss: 0.010549574986928038\n",
      "[LOG 20200502-14:16:32] epoch: 6464 train-loss: 0.010549574883447753\n",
      "[LOG 20200502-14:16:32] epoch: 6465 train-loss: 0.010549574883447753\n",
      "[LOG 20200502-14:16:32] epoch: 6466 train-loss: 0.01054957415908575\n",
      "[LOG 20200502-14:16:32] epoch: 6467 train-loss: 0.01054957364168432\n",
      "[LOG 20200502-14:16:33] epoch: 6468 train-loss: 0.010549573124282889\n",
      "[LOG 20200502-14:16:33] epoch: 6469 train-loss: 0.01054957208948003\n",
      "[LOG 20200502-14:16:33] epoch: 6470 train-loss: 0.010549570951196883\n",
      "[LOG 20200502-14:16:33] epoch: 6471 train-loss: 0.010549569916394021\n",
      "[LOG 20200502-14:16:34] epoch: 6472 train-loss: 0.010549568364189731\n",
      "[LOG 20200502-14:16:34] epoch: 6473 train-loss: 0.010549566398064295\n",
      "[LOG 20200502-14:16:34] epoch: 6474 train-loss: 0.010549564949340291\n",
      "[LOG 20200502-14:16:34] epoch: 6475 train-loss: 0.01054956236233314\n",
      "[LOG 20200502-14:16:35] epoch: 6476 train-loss: 0.010549559878806273\n",
      "[LOG 20200502-14:16:35] epoch: 6477 train-loss: 0.01054955760223998\n",
      "[LOG 20200502-14:16:35] epoch: 6478 train-loss: 0.010549555429153971\n",
      "[LOG 20200502-14:16:35] epoch: 6479 train-loss: 0.010549551703863673\n",
      "[LOG 20200502-14:16:35] epoch: 6480 train-loss: 0.010549549220336808\n",
      "[LOG 20200502-14:16:36] epoch: 6481 train-loss: 0.010549545288085938\n",
      "[LOG 20200502-14:16:36] epoch: 6482 train-loss: 0.010549541976716783\n",
      "[LOG 20200502-14:16:36] epoch: 6483 train-loss: 0.010549537527064482\n",
      "[LOG 20200502-14:16:36] epoch: 6484 train-loss: 0.010549534215695329\n",
      "[LOG 20200502-14:16:37] epoch: 6485 train-loss: 0.010549529869523313\n",
      "[LOG 20200502-14:16:37] epoch: 6486 train-loss: 0.010549525316390727\n",
      "[LOG 20200502-14:16:37] epoch: 6487 train-loss: 0.010549520452817282\n",
      "[LOG 20200502-14:16:37] epoch: 6488 train-loss: 0.010549515899684694\n",
      "[LOG 20200502-14:16:38] epoch: 6489 train-loss: 0.01054951072567039\n",
      "[LOG 20200502-14:16:38] epoch: 6490 train-loss: 0.010549505551656088\n",
      "[LOG 20200502-14:16:38] epoch: 6491 train-loss: 0.010549499860240353\n",
      "[LOG 20200502-14:16:38] epoch: 6492 train-loss: 0.010549494272304906\n",
      "[LOG 20200502-14:16:38] epoch: 6493 train-loss: 0.010549488270448314\n",
      "[LOG 20200502-14:16:39] epoch: 6494 train-loss: 0.010549482372072008\n",
      "[LOG 20200502-14:16:39] epoch: 6495 train-loss: 0.0105494757493337\n",
      "[LOG 20200502-14:16:39] epoch: 6496 train-loss: 0.010549468919634819\n",
      "[LOG 20200502-14:16:39] epoch: 6497 train-loss: 0.010549462607337369\n",
      "[LOG 20200502-14:16:40] epoch: 6498 train-loss: 0.010549455363717344\n",
      "[LOG 20200502-14:16:40] epoch: 6499 train-loss: 0.010549448534018464\n",
      "[LOG 20200502-14:16:40] epoch: 6500 train-loss: 0.010549440772997009\n",
      "[LOG 20200502-14:16:40] epoch: 6501 train-loss: 0.010549433943298128\n",
      "[LOG 20200502-14:16:41] epoch: 6502 train-loss: 0.010549425768355528\n",
      "[LOG 20200502-14:16:41] epoch: 6503 train-loss: 0.010549417903853787\n",
      "[LOG 20200502-14:16:41] epoch: 6504 train-loss: 0.010549410039352046\n",
      "[LOG 20200502-14:16:41] epoch: 6505 train-loss: 0.010549402071370019\n",
      "[LOG 20200502-14:16:42] epoch: 6506 train-loss: 0.010549393689466847\n",
      "[LOG 20200502-14:16:42] epoch: 6507 train-loss: 0.010549384997122817\n",
      "[LOG 20200502-14:16:42] epoch: 6508 train-loss: 0.010549376822180219\n",
      "[LOG 20200502-14:16:42] epoch: 6509 train-loss: 0.010549368233316474\n",
      "[LOG 20200502-14:16:43] epoch: 6510 train-loss: 0.010549359127051301\n",
      "[LOG 20200502-14:16:43] epoch: 6510 new best train-loss: 0.010549359127051301 found\n",
      "[LOG 20200502-14:16:43] epoch: 6511 train-loss: 0.010549350434707271\n",
      "[LOG 20200502-14:16:43] epoch: 6512 train-loss: 0.010549341535402669\n",
      "[LOG 20200502-14:16:43] epoch: 6513 train-loss: 0.010549332532617781\n",
      "[LOG 20200502-14:16:43] epoch: 6514 train-loss: 0.010549323115911748\n",
      "[LOG 20200502-14:16:44] epoch: 6515 train-loss: 0.010549313388764858\n",
      "[LOG 20200502-14:16:44] epoch: 6516 train-loss: 0.010549304282499684\n",
      "[LOG 20200502-14:16:44] epoch: 6517 train-loss: 0.010549294865793653\n",
      "[LOG 20200502-14:16:44] epoch: 6518 train-loss: 0.010549285656048192\n",
      "[LOG 20200502-14:16:45] epoch: 6519 train-loss: 0.010549275928901302\n",
      "[LOG 20200502-14:16:45] epoch: 6520 train-loss: 0.010549266201754412\n",
      "[LOG 20200502-14:16:45] epoch: 6520 new best train-loss: 0.010549266201754412 found\n",
      "[LOG 20200502-14:16:45] epoch: 6521 train-loss: 0.010549256681568094\n",
      "[LOG 20200502-14:16:45] epoch: 6522 train-loss: 0.010549247057901489\n",
      "[LOG 20200502-14:16:46] epoch: 6523 train-loss: 0.01054923753771517\n",
      "[LOG 20200502-14:16:46] epoch: 6524 train-loss: 0.010549227707087994\n",
      "[LOG 20200502-14:16:46] epoch: 6525 train-loss: 0.010549218393862247\n",
      "[LOG 20200502-14:16:46] epoch: 6526 train-loss: 0.010549209080636501\n",
      "[LOG 20200502-14:16:46] epoch: 6527 train-loss: 0.010549199353489611\n",
      "[LOG 20200502-14:16:47] epoch: 6528 train-loss: 0.010549190247224437\n",
      "[LOG 20200502-14:16:47] epoch: 6529 train-loss: 0.01054918041659726\n",
      "[LOG 20200502-14:16:47] epoch: 6530 train-loss: 0.010549171103371514\n",
      "[LOG 20200502-14:16:47] epoch: 6530 new best train-loss: 0.010549171103371514 found\n",
      "[LOG 20200502-14:16:47] epoch: 6531 train-loss: 0.010549162100586627\n",
      "[LOG 20200502-14:16:48] epoch: 6532 train-loss: 0.01054915278736088\n",
      "[LOG 20200502-14:16:48] epoch: 6533 train-loss: 0.010549143784575991\n",
      "[LOG 20200502-14:16:48] epoch: 6534 train-loss: 0.010549134781791104\n",
      "[LOG 20200502-14:16:48] epoch: 6535 train-loss: 0.010549125882486502\n",
      "[LOG 20200502-14:16:49] epoch: 6536 train-loss: 0.010549117293622758\n",
      "[LOG 20200502-14:16:49] epoch: 6537 train-loss: 0.010549108808239302\n",
      "[LOG 20200502-14:16:49] epoch: 6538 train-loss: 0.010549100840257274\n",
      "[LOG 20200502-14:16:49] epoch: 6539 train-loss: 0.010549091940952672\n",
      "[LOG 20200502-14:16:49] epoch: 6540 train-loss: 0.01054908438689179\n",
      "[LOG 20200502-14:16:49] epoch: 6540 new best train-loss: 0.01054908438689179 found\n",
      "[LOG 20200502-14:16:50] epoch: 6541 train-loss: 0.01054907621194919\n",
      "[LOG 20200502-14:16:50] epoch: 6542 train-loss: 0.010549068554408021\n",
      "[LOG 20200502-14:16:50] epoch: 6543 train-loss: 0.010549061414268281\n",
      "[LOG 20200502-14:16:50] epoch: 6544 train-loss: 0.010549054170648256\n",
      "[LOG 20200502-14:16:51] epoch: 6545 train-loss: 0.01054904672006766\n",
      "[LOG 20200502-14:16:51] epoch: 6546 train-loss: 0.010549040097329352\n",
      "[LOG 20200502-14:16:51] epoch: 6547 train-loss: 0.010549033681551615\n",
      "[LOG 20200502-14:16:51] epoch: 6548 train-loss: 0.01054902695533302\n",
      "[LOG 20200502-14:16:52] epoch: 6549 train-loss: 0.010549020849996142\n",
      "[LOG 20200502-14:16:52] epoch: 6550 train-loss: 0.010549014951619837\n",
      "[LOG 20200502-14:16:52] epoch: 6550 new best train-loss: 0.010549014951619837 found\n",
      "[LOG 20200502-14:16:52] epoch: 6551 train-loss: 0.010549009053243531\n",
      "[LOG 20200502-14:16:52] epoch: 6552 train-loss: 0.010549003465308083\n",
      "[LOG 20200502-14:16:52] epoch: 6553 train-loss: 0.010548998394774066\n",
      "[LOG 20200502-14:16:53] epoch: 6554 train-loss: 0.010548993634680906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:16:53] epoch: 6555 train-loss: 0.010548988874587748\n",
      "[LOG 20200502-14:16:53] epoch: 6556 train-loss: 0.010548984838856591\n",
      "[LOG 20200502-14:16:54] epoch: 6557 train-loss: 0.010548980699645149\n",
      "[LOG 20200502-14:16:54] epoch: 6558 train-loss: 0.010548976870874563\n",
      "[LOG 20200502-14:16:54] epoch: 6559 train-loss: 0.010548972835143408\n",
      "[LOG 20200502-14:16:54] epoch: 6560 train-loss: 0.010548969834215112\n",
      "[LOG 20200502-14:16:54] epoch: 6560 new best train-loss: 0.010548969834215112 found\n",
      "[LOG 20200502-14:16:55] epoch: 6561 train-loss: 0.010548966522845957\n",
      "[LOG 20200502-14:16:55] epoch: 6562 train-loss: 0.010548963728878234\n",
      "[LOG 20200502-14:16:55] epoch: 6563 train-loss: 0.010548961141871082\n",
      "[LOG 20200502-14:16:55] epoch: 6564 train-loss: 0.010548959279225932\n",
      "[LOG 20200502-14:16:56] epoch: 6565 train-loss: 0.010548957106139924\n",
      "[LOG 20200502-14:16:56] epoch: 6566 train-loss: 0.010548955450455347\n",
      "[LOG 20200502-14:16:56] epoch: 6567 train-loss: 0.010548953898251057\n",
      "[LOG 20200502-14:16:56] epoch: 6568 train-loss: 0.010548952863448195\n",
      "[LOG 20200502-14:16:57] epoch: 6569 train-loss: 0.01054895224256648\n",
      "[LOG 20200502-14:16:57] epoch: 6570 train-loss: 0.010548951414724192\n",
      "[LOG 20200502-14:16:57] epoch: 6570 new best train-loss: 0.010548951414724192 found\n",
      "[LOG 20200502-14:16:57] epoch: 6571 train-loss: 0.010548951414724192\n",
      "[LOG 20200502-14:16:57] epoch: 6572 train-loss: 0.010548951621684764\n",
      "[LOG 20200502-14:16:58] epoch: 6573 train-loss: 0.010548951311243905\n",
      "[LOG 20200502-14:16:58] epoch: 6574 train-loss: 0.010548951725165049\n",
      "[LOG 20200502-14:16:58] epoch: 6575 train-loss: 0.010548952346046766\n",
      "[LOG 20200502-14:16:59] epoch: 6576 train-loss: 0.010548953380849626\n",
      "[LOG 20200502-14:16:59] epoch: 6577 train-loss: 0.010548954519132773\n",
      "[LOG 20200502-14:16:59] epoch: 6578 train-loss: 0.01054895617481735\n",
      "[LOG 20200502-14:16:59] epoch: 6579 train-loss: 0.010548957830501927\n",
      "[LOG 20200502-14:17:00] epoch: 6580 train-loss: 0.01054895907226536\n",
      "[LOG 20200502-14:17:00] epoch: 6581 train-loss: 0.010548961659272512\n",
      "[LOG 20200502-14:17:00] epoch: 6582 train-loss: 0.010548963418437375\n",
      "[LOG 20200502-14:17:00] epoch: 6583 train-loss: 0.010548966212405099\n",
      "[LOG 20200502-14:17:00] epoch: 6584 train-loss: 0.010548968695931964\n",
      "[LOG 20200502-14:17:01] epoch: 6585 train-loss: 0.010548972007301118\n",
      "[LOG 20200502-14:17:01] epoch: 6586 train-loss: 0.010548975008229414\n",
      "[LOG 20200502-14:17:01] epoch: 6587 train-loss: 0.010548978112637997\n",
      "[LOG 20200502-14:17:02] epoch: 6588 train-loss: 0.010548981837928295\n",
      "[LOG 20200502-14:17:02] epoch: 6589 train-loss: 0.010548985252777735\n",
      "[LOG 20200502-14:17:02] epoch: 6590 train-loss: 0.01054898908154832\n",
      "[LOG 20200502-14:17:02] epoch: 6591 train-loss: 0.01054899301379919\n",
      "[LOG 20200502-14:17:03] epoch: 6592 train-loss: 0.010548996532128917\n",
      "[LOG 20200502-14:17:03] epoch: 6593 train-loss: 0.010549001085261503\n",
      "[LOG 20200502-14:17:03] epoch: 6594 train-loss: 0.010549005638394091\n",
      "[LOG 20200502-14:17:03] epoch: 6595 train-loss: 0.010549009777605534\n",
      "[LOG 20200502-14:17:03] epoch: 6596 train-loss: 0.01054901433073812\n",
      "[LOG 20200502-14:17:04] epoch: 6597 train-loss: 0.010549018883870708\n",
      "[LOG 20200502-14:17:04] epoch: 6598 train-loss: 0.010549023540483581\n",
      "[LOG 20200502-14:17:04] epoch: 6599 train-loss: 0.010549028507537313\n",
      "[LOG 20200502-14:17:04] epoch: 6600 train-loss: 0.010549032957189612\n",
      "[LOG 20200502-14:17:05] epoch: 6601 train-loss: 0.010549038338164488\n",
      "[LOG 20200502-14:17:05] epoch: 6602 train-loss: 0.010549043098257648\n",
      "[LOG 20200502-14:17:05] epoch: 6603 train-loss: 0.010549047961831093\n",
      "[LOG 20200502-14:17:06] epoch: 6604 train-loss: 0.010549052928884825\n",
      "[LOG 20200502-14:17:06] epoch: 6605 train-loss: 0.010549058102899127\n",
      "[LOG 20200502-14:17:06] epoch: 6606 train-loss: 0.010549063069952859\n",
      "[LOG 20200502-14:17:06] epoch: 6607 train-loss: 0.010549068761368593\n",
      "[LOG 20200502-14:17:06] epoch: 6608 train-loss: 0.010549073107540607\n",
      "[LOG 20200502-14:17:07] epoch: 6609 train-loss: 0.010549078281554911\n",
      "[LOG 20200502-14:17:07] epoch: 6610 train-loss: 0.010549083455569215\n",
      "[LOG 20200502-14:17:07] epoch: 6611 train-loss: 0.010549088526103232\n",
      "[LOG 20200502-14:17:07] epoch: 6612 train-loss: 0.01054909328619639\n",
      "[LOG 20200502-14:17:08] epoch: 6613 train-loss: 0.010549098356730409\n",
      "[LOG 20200502-14:17:08] epoch: 6614 train-loss: 0.010549102909862995\n",
      "[LOG 20200502-14:17:08] epoch: 6615 train-loss: 0.010549107359515296\n",
      "[LOG 20200502-14:17:08] epoch: 6616 train-loss: 0.010549112326569028\n",
      "[LOG 20200502-14:17:09] epoch: 6617 train-loss: 0.010549116983181901\n",
      "[LOG 20200502-14:17:09] epoch: 6618 train-loss: 0.0105491214328342\n",
      "[LOG 20200502-14:17:09] epoch: 6619 train-loss: 0.010549125572045645\n",
      "[LOG 20200502-14:17:09] epoch: 6620 train-loss: 0.0105491296077768\n",
      "[LOG 20200502-14:17:09] epoch: 6621 train-loss: 0.010549133436547386\n",
      "[LOG 20200502-14:17:10] epoch: 6622 train-loss: 0.010549137265317969\n",
      "[LOG 20200502-14:17:10] epoch: 6623 train-loss: 0.010549140887127982\n",
      "[LOG 20200502-14:17:10] epoch: 6624 train-loss: 0.010549144198497137\n",
      "[LOG 20200502-14:17:10] epoch: 6625 train-loss: 0.010549147716826864\n",
      "[LOG 20200502-14:17:11] epoch: 6626 train-loss: 0.0105491504073143\n",
      "[LOG 20200502-14:17:11] epoch: 6627 train-loss: 0.010549153511722883\n",
      "[LOG 20200502-14:17:11] epoch: 6628 train-loss: 0.010549156098730035\n",
      "[LOG 20200502-14:17:11] epoch: 6629 train-loss: 0.010549158685737185\n",
      "[LOG 20200502-14:17:12] epoch: 6630 train-loss: 0.010549160858823193\n",
      "[LOG 20200502-14:17:12] epoch: 6631 train-loss: 0.010549162721468342\n",
      "[LOG 20200502-14:17:12] epoch: 6632 train-loss: 0.010549164170192348\n",
      "[LOG 20200502-14:17:12] epoch: 6633 train-loss: 0.010549165618916353\n",
      "[LOG 20200502-14:17:13] epoch: 6634 train-loss: 0.010549167067640357\n",
      "[LOG 20200502-14:17:13] epoch: 6635 train-loss: 0.01054916779200236\n",
      "[LOG 20200502-14:17:13] epoch: 6636 train-loss: 0.010549167895482646\n",
      "[LOG 20200502-14:17:13] epoch: 6637 train-loss: 0.010549168516364362\n",
      "[LOG 20200502-14:17:13] epoch: 6638 train-loss: 0.010549168205923505\n",
      "[LOG 20200502-14:17:14] epoch: 6639 train-loss: 0.010549167998962931\n",
      "[LOG 20200502-14:17:14] epoch: 6640 train-loss: 0.010549166964160072\n",
      "[LOG 20200502-14:17:14] epoch: 6641 train-loss: 0.010549166343278356\n",
      "[LOG 20200502-14:17:14] epoch: 6642 train-loss: 0.01054916489455435\n",
      "[LOG 20200502-14:17:15] epoch: 6643 train-loss: 0.010549163342350058\n",
      "[LOG 20200502-14:17:15] epoch: 6644 train-loss: 0.010549160548382334\n",
      "[LOG 20200502-14:17:15] epoch: 6645 train-loss: 0.010549158478776613\n",
      "[LOG 20200502-14:17:15] epoch: 6646 train-loss: 0.010549155891769461\n",
      "[LOG 20200502-14:17:16] epoch: 6647 train-loss: 0.01054915278736088\n",
      "[LOG 20200502-14:17:16] epoch: 6648 train-loss: 0.010549149062070582\n",
      "[LOG 20200502-14:17:16] epoch: 6649 train-loss: 0.010549145233299997\n",
      "[LOG 20200502-14:17:16] epoch: 6650 train-loss: 0.010549140990608268\n",
      "[LOG 20200502-14:17:17] epoch: 6651 train-loss: 0.010549136437475681\n",
      "[LOG 20200502-14:17:17] epoch: 6652 train-loss: 0.010549131780862808\n",
      "[LOG 20200502-14:17:17] epoch: 6653 train-loss: 0.01054912619292736\n",
      "[LOG 20200502-14:17:17] epoch: 6654 train-loss: 0.010549120501511626\n",
      "[LOG 20200502-14:17:17] epoch: 6655 train-loss: 0.010549114706615606\n",
      "[LOG 20200502-14:17:18] epoch: 6656 train-loss: 0.010549108187357584\n",
      "[LOG 20200502-14:17:18] epoch: 6657 train-loss: 0.010549101150698133\n",
      "[LOG 20200502-14:17:18] epoch: 6658 train-loss: 0.010549094527959824\n",
      "[LOG 20200502-14:17:18] epoch: 6659 train-loss: 0.010549086870418655\n",
      "[LOG 20200502-14:17:19] epoch: 6660 train-loss: 0.010549079316357771\n",
      "[LOG 20200502-14:17:19] epoch: 6661 train-loss: 0.010549071244895458\n",
      "[LOG 20200502-14:17:19] epoch: 6662 train-loss: 0.010549062862992287\n",
      "[LOG 20200502-14:17:19] epoch: 6663 train-loss: 0.010549054274128543\n",
      "[LOG 20200502-14:17:20] epoch: 6664 train-loss: 0.010549045478304228\n",
      "[LOG 20200502-14:17:20] epoch: 6665 train-loss: 0.010549035647677051\n",
      "[LOG 20200502-14:17:20] epoch: 6666 train-loss: 0.010549026748372449\n",
      "[LOG 20200502-14:17:20] epoch: 6667 train-loss: 0.010549016503824128\n",
      "[LOG 20200502-14:17:21] epoch: 6668 train-loss: 0.010549006776677238\n",
      "[LOG 20200502-14:17:21] epoch: 6669 train-loss: 0.010548996221688058\n",
      "[LOG 20200502-14:17:21] epoch: 6670 train-loss: 0.010548985770179166\n",
      "[LOG 20200502-14:17:21] epoch: 6671 train-loss: 0.010548975318670273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:17:22] epoch: 6672 train-loss: 0.01054896434975995\n",
      "[LOG 20200502-14:17:22] epoch: 6673 train-loss: 0.010548953380849626\n",
      "[LOG 20200502-14:17:22] epoch: 6674 train-loss: 0.010548942101498445\n",
      "[LOG 20200502-14:17:22] epoch: 6675 train-loss: 0.01054893040822612\n",
      "[LOG 20200502-14:17:22] epoch: 6676 train-loss: 0.010548919749756655\n",
      "[LOG 20200502-14:17:23] epoch: 6677 train-loss: 0.010548907953004042\n",
      "[LOG 20200502-14:17:23] epoch: 6678 train-loss: 0.010548896363212002\n",
      "[LOG 20200502-14:17:23] epoch: 6679 train-loss: 0.010548885187341107\n",
      "[LOG 20200502-14:17:23] epoch: 6680 train-loss: 0.010548873183627924\n",
      "[LOG 20200502-14:17:23] epoch: 6680 new best train-loss: 0.010548873183627924 found\n",
      "[LOG 20200502-14:17:24] epoch: 6681 train-loss: 0.010548861386875311\n",
      "[LOG 20200502-14:17:24] epoch: 6682 train-loss: 0.010548849693602987\n",
      "[LOG 20200502-14:17:24] epoch: 6683 train-loss: 0.010548837896850374\n",
      "[LOG 20200502-14:17:24] epoch: 6684 train-loss: 0.01054882589313719\n",
      "[LOG 20200502-14:17:25] epoch: 6685 train-loss: 0.010548814510305723\n",
      "[LOG 20200502-14:17:25] epoch: 6686 train-loss: 0.010548803023993969\n",
      "[LOG 20200502-14:17:25] epoch: 6687 train-loss: 0.010548791744642787\n",
      "[LOG 20200502-14:17:25] epoch: 6688 train-loss: 0.010548780568771891\n",
      "[LOG 20200502-14:17:26] epoch: 6689 train-loss: 0.01054876928942071\n",
      "[LOG 20200502-14:17:26] epoch: 6690 train-loss: 0.010548758113549815\n",
      "[LOG 20200502-14:17:26] epoch: 6690 new best train-loss: 0.010548758113549815 found\n",
      "[LOG 20200502-14:17:26] epoch: 6691 train-loss: 0.010548747558560636\n",
      "[LOG 20200502-14:17:26] epoch: 6692 train-loss: 0.010548736796610884\n",
      "[LOG 20200502-14:17:26] epoch: 6693 train-loss: 0.010548725517259704\n",
      "[LOG 20200502-14:17:27] epoch: 6694 train-loss: 0.010548715479671955\n",
      "[LOG 20200502-14:17:27] epoch: 6695 train-loss: 0.010548705131643347\n",
      "[LOG 20200502-14:17:27] epoch: 6696 train-loss: 0.010548695404496457\n",
      "[LOG 20200502-14:17:27] epoch: 6697 train-loss: 0.010548685987790426\n",
      "[LOG 20200502-14:17:28] epoch: 6698 train-loss: 0.010548675743242105\n",
      "[LOG 20200502-14:17:28] epoch: 6699 train-loss: 0.010548666533496644\n",
      "[LOG 20200502-14:17:28] epoch: 6700 train-loss: 0.01054865742723147\n",
      "[LOG 20200502-14:17:28] epoch: 6700 new best train-loss: 0.01054865742723147 found\n",
      "[LOG 20200502-14:17:28] epoch: 6701 train-loss: 0.010548648734887442\n",
      "[LOG 20200502-14:17:29] epoch: 6702 train-loss: 0.010548640249503983\n",
      "[LOG 20200502-14:17:29] epoch: 6703 train-loss: 0.010548632385002242\n",
      "[LOG 20200502-14:17:29] epoch: 6704 train-loss: 0.010548624417020215\n",
      "[LOG 20200502-14:17:29] epoch: 6705 train-loss: 0.010548616966439618\n",
      "[LOG 20200502-14:17:30] epoch: 6706 train-loss: 0.01054860982629988\n",
      "[LOG 20200502-14:17:30] epoch: 6707 train-loss: 0.010548603100081285\n",
      "[LOG 20200502-14:17:30] epoch: 6708 train-loss: 0.010548596063421832\n",
      "[LOG 20200502-14:17:30] epoch: 6709 train-loss: 0.01054858954416381\n",
      "[LOG 20200502-14:17:31] epoch: 6710 train-loss: 0.010548583542307218\n",
      "[LOG 20200502-14:17:31] epoch: 6710 new best train-loss: 0.010548583542307218 found\n",
      "[LOG 20200502-14:17:31] epoch: 6711 train-loss: 0.010548578264812628\n",
      "[LOG 20200502-14:17:31] epoch: 6712 train-loss: 0.010548572469916608\n",
      "[LOG 20200502-14:17:31] epoch: 6713 train-loss: 0.010548567088941732\n",
      "[LOG 20200502-14:17:31] epoch: 6714 train-loss: 0.010548562742769718\n",
      "[LOG 20200502-14:17:32] epoch: 6715 train-loss: 0.010548558603558276\n",
      "[LOG 20200502-14:17:32] epoch: 6716 train-loss: 0.010548554153905975\n",
      "[LOG 20200502-14:17:32] epoch: 6717 train-loss: 0.010548550532095961\n",
      "[LOG 20200502-14:17:32] epoch: 6718 train-loss: 0.010548546703325378\n",
      "[LOG 20200502-14:17:33] epoch: 6719 train-loss: 0.01054854349543651\n",
      "[LOG 20200502-14:17:33] epoch: 6720 train-loss: 0.010548541425830789\n",
      "[LOG 20200502-14:17:33] epoch: 6720 new best train-loss: 0.010548541425830789 found\n",
      "[LOG 20200502-14:17:33] epoch: 6721 train-loss: 0.010548538528382778\n",
      "[LOG 20200502-14:17:33] epoch: 6722 train-loss: 0.010548536458777057\n",
      "[LOG 20200502-14:17:33] epoch: 6723 train-loss: 0.010548534389171336\n",
      "[LOG 20200502-14:17:34] epoch: 6724 train-loss: 0.010548533561329046\n",
      "[LOG 20200502-14:17:34] epoch: 6725 train-loss: 0.010548532009124756\n",
      "[LOG 20200502-14:17:34] epoch: 6726 train-loss: 0.01054853056040075\n",
      "[LOG 20200502-14:17:34] epoch: 6727 train-loss: 0.010548530663881037\n",
      "[LOG 20200502-14:17:35] epoch: 6728 train-loss: 0.01054853004299932\n",
      "[LOG 20200502-14:17:35] epoch: 6729 train-loss: 0.010548529939519035\n",
      "[LOG 20200502-14:17:35] epoch: 6730 train-loss: 0.010548530146479607\n",
      "[LOG 20200502-14:17:35] epoch: 6730 new best train-loss: 0.010548530146479607 found\n",
      "[LOG 20200502-14:17:35] epoch: 6731 train-loss: 0.010548530353440179\n",
      "[LOG 20200502-14:17:36] epoch: 6732 train-loss: 0.010548531181282468\n",
      "[LOG 20200502-14:17:36] epoch: 6733 train-loss: 0.010548532733486759\n",
      "[LOG 20200502-14:17:36] epoch: 6734 train-loss: 0.010548533768289618\n",
      "[LOG 20200502-14:17:36] epoch: 6735 train-loss: 0.010548535217013624\n",
      "[LOG 20200502-14:17:36] epoch: 6736 train-loss: 0.010548537286619345\n",
      "[LOG 20200502-14:17:37] epoch: 6737 train-loss: 0.010548539356225066\n",
      "[LOG 20200502-14:17:37] epoch: 6738 train-loss: 0.010548541425830789\n",
      "[LOG 20200502-14:17:37] epoch: 6739 train-loss: 0.010548544219798513\n",
      "[LOG 20200502-14:17:37] epoch: 6740 train-loss: 0.01054854691028595\n",
      "[LOG 20200502-14:17:38] epoch: 6741 train-loss: 0.010548550118174817\n",
      "[LOG 20200502-14:17:38] epoch: 6742 train-loss: 0.01054855270518197\n",
      "[LOG 20200502-14:17:38] epoch: 6743 train-loss: 0.010548556120031409\n",
      "[LOG 20200502-14:17:38] epoch: 6744 train-loss: 0.010548559120959707\n",
      "[LOG 20200502-14:17:38] epoch: 6745 train-loss: 0.010548563674092293\n",
      "[LOG 20200502-14:17:39] epoch: 6746 train-loss: 0.010548567295902304\n",
      "[LOG 20200502-14:17:39] epoch: 6747 train-loss: 0.010548571435113748\n",
      "[LOG 20200502-14:17:39] epoch: 6748 train-loss: 0.01054857588476605\n",
      "[LOG 20200502-14:17:39] epoch: 6749 train-loss: 0.010548580230938064\n",
      "[LOG 20200502-14:17:40] epoch: 6750 train-loss: 0.01054858478407065\n",
      "[LOG 20200502-14:17:40] epoch: 6751 train-loss: 0.010548589751124382\n",
      "[LOG 20200502-14:17:40] epoch: 6752 train-loss: 0.010548594511217542\n",
      "[LOG 20200502-14:17:40] epoch: 6753 train-loss: 0.010548599374790987\n",
      "[LOG 20200502-14:17:41] epoch: 6754 train-loss: 0.010548604548805289\n",
      "[LOG 20200502-14:17:41] epoch: 6755 train-loss: 0.010548610136740737\n",
      "[LOG 20200502-14:17:41] epoch: 6756 train-loss: 0.010548615414235327\n",
      "[LOG 20200502-14:17:41] epoch: 6757 train-loss: 0.010548621002170775\n",
      "[LOG 20200502-14:17:41] epoch: 6758 train-loss: 0.010548626486625936\n",
      "[LOG 20200502-14:17:42] epoch: 6759 train-loss: 0.010548631971081098\n",
      "[LOG 20200502-14:17:42] epoch: 6760 train-loss: 0.010548637662496831\n",
      "[LOG 20200502-14:17:42] epoch: 6761 train-loss: 0.010548643664353423\n",
      "[LOG 20200502-14:17:42] epoch: 6762 train-loss: 0.01054864925228887\n",
      "[LOG 20200502-14:17:43] epoch: 6763 train-loss: 0.010548655461106036\n",
      "[LOG 20200502-14:17:43] epoch: 6764 train-loss: 0.010548661566442914\n",
      "[LOG 20200502-14:17:43] epoch: 6765 train-loss: 0.010548667464819219\n",
      "[LOG 20200502-14:17:43] epoch: 6766 train-loss: 0.010548673570156097\n",
      "[LOG 20200502-14:17:43] epoch: 6767 train-loss: 0.010548679778973261\n",
      "[LOG 20200502-14:17:44] epoch: 6768 train-loss: 0.010548686091270711\n",
      "[LOG 20200502-14:17:44] epoch: 6769 train-loss: 0.010548691782686446\n",
      "[LOG 20200502-14:17:44] epoch: 6770 train-loss: 0.010548698094983896\n",
      "[LOG 20200502-14:17:44] epoch: 6771 train-loss: 0.01054870430380106\n",
      "[LOG 20200502-14:17:45] epoch: 6772 train-loss: 0.010548710719578795\n",
      "[LOG 20200502-14:17:45] epoch: 6773 train-loss: 0.01054871641099453\n",
      "[LOG 20200502-14:17:45] epoch: 6774 train-loss: 0.01054872272329198\n",
      "[LOG 20200502-14:17:45] epoch: 6775 train-loss: 0.010548728621668287\n",
      "[LOG 20200502-14:17:46] epoch: 6776 train-loss: 0.010548734416564306\n",
      "[LOG 20200502-14:17:46] epoch: 6777 train-loss: 0.010548740832342042\n",
      "[LOG 20200502-14:17:46] epoch: 6778 train-loss: 0.01054874642027749\n",
      "[LOG 20200502-14:17:46] epoch: 6779 train-loss: 0.010548752318653796\n",
      "[LOG 20200502-14:17:46] epoch: 6780 train-loss: 0.010548758320510387\n",
      "[LOG 20200502-14:17:47] epoch: 6781 train-loss: 0.010548764425847266\n",
      "[LOG 20200502-14:17:47] epoch: 6782 train-loss: 0.010548769806822142\n",
      "[LOG 20200502-14:17:47] epoch: 6783 train-loss: 0.010548775291277302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:17:47] epoch: 6784 train-loss: 0.01054878036181132\n",
      "[LOG 20200502-14:17:48] epoch: 6785 train-loss: 0.010548785949746767\n",
      "[LOG 20200502-14:17:48] epoch: 6786 train-loss: 0.010548791227241358\n",
      "[LOG 20200502-14:17:48] epoch: 6787 train-loss: 0.010548796608216233\n",
      "[LOG 20200502-14:17:48] epoch: 6788 train-loss: 0.010548801264829107\n",
      "[LOG 20200502-14:17:49] epoch: 6789 train-loss: 0.010548806231882837\n",
      "[LOG 20200502-14:17:49] epoch: 6790 train-loss: 0.01054881088849571\n",
      "[LOG 20200502-14:17:49] epoch: 6791 train-loss: 0.010548816062510014\n",
      "[LOG 20200502-14:17:49] epoch: 6792 train-loss: 0.010548820408682028\n",
      "[LOG 20200502-14:17:50] epoch: 6793 train-loss: 0.010548824858334329\n",
      "[LOG 20200502-14:17:50] epoch: 6794 train-loss: 0.010548828687104914\n",
      "[LOG 20200502-14:17:50] epoch: 6795 train-loss: 0.010548833136757215\n",
      "[LOG 20200502-14:17:50] epoch: 6796 train-loss: 0.010548836551606655\n",
      "[LOG 20200502-14:17:51] epoch: 6797 train-loss: 0.010548840069936382\n",
      "[LOG 20200502-14:17:51] epoch: 6798 train-loss: 0.010548843691746393\n",
      "[LOG 20200502-14:17:51] epoch: 6799 train-loss: 0.01054884721007612\n",
      "[LOG 20200502-14:17:51] epoch: 6800 train-loss: 0.010548850314484702\n",
      "[LOG 20200502-14:17:51] epoch: 6801 train-loss: 0.010548853729334142\n",
      "[LOG 20200502-14:17:52] epoch: 6802 train-loss: 0.010548856005900435\n",
      "[LOG 20200502-14:17:52] epoch: 6803 train-loss: 0.01054885828246673\n",
      "[LOG 20200502-14:17:52] epoch: 6804 train-loss: 0.010548860765993595\n",
      "[LOG 20200502-14:17:52] epoch: 6805 train-loss: 0.01054886324952046\n",
      "[LOG 20200502-14:17:53] epoch: 6806 train-loss: 0.010548864801724752\n",
      "[LOG 20200502-14:17:53] epoch: 6807 train-loss: 0.010548866664369902\n",
      "[LOG 20200502-14:17:53] epoch: 6808 train-loss: 0.01054886800961362\n",
      "[LOG 20200502-14:17:53] epoch: 6809 train-loss: 0.010548869354857339\n",
      "[LOG 20200502-14:17:53] epoch: 6810 train-loss: 0.0105488703896602\n",
      "[LOG 20200502-14:17:54] epoch: 6811 train-loss: 0.010548870803581344\n",
      "[LOG 20200502-14:17:54] epoch: 6812 train-loss: 0.010548872045344777\n",
      "[LOG 20200502-14:17:54] epoch: 6813 train-loss: 0.01054887142446306\n",
      "[LOG 20200502-14:17:54] epoch: 6814 train-loss: 0.01054887194186449\n",
      "[LOG 20200502-14:17:55] epoch: 6815 train-loss: 0.010548871734903919\n",
      "[LOG 20200502-14:17:55] epoch: 6816 train-loss: 0.010548871010541916\n",
      "[LOG 20200502-14:17:55] epoch: 6817 train-loss: 0.010548870803581344\n",
      "[LOG 20200502-14:17:55] epoch: 6818 train-loss: 0.010548869665298197\n",
      "[LOG 20200502-14:17:55] epoch: 6819 train-loss: 0.010548868733975623\n",
      "[LOG 20200502-14:17:56] epoch: 6820 train-loss: 0.010548866974810759\n",
      "[LOG 20200502-14:17:56] epoch: 6821 train-loss: 0.01054886511216561\n",
      "[LOG 20200502-14:17:56] epoch: 6822 train-loss: 0.01054886324952046\n",
      "[LOG 20200502-14:17:56] epoch: 6823 train-loss: 0.010548861593835883\n",
      "[LOG 20200502-14:17:57] epoch: 6824 train-loss: 0.010548859110309018\n",
      "[LOG 20200502-14:17:57] epoch: 6825 train-loss: 0.010548856833742725\n",
      "[LOG 20200502-14:17:57] epoch: 6826 train-loss: 0.01054885352237357\n",
      "[LOG 20200502-14:17:57] epoch: 6827 train-loss: 0.010548850417964988\n",
      "[LOG 20200502-14:17:57] epoch: 6828 train-loss: 0.010548847830957837\n",
      "[LOG 20200502-14:17:58] epoch: 6829 train-loss: 0.010548843484785821\n",
      "[LOG 20200502-14:17:58] epoch: 6830 train-loss: 0.010548839966456095\n",
      "[LOG 20200502-14:17:58] epoch: 6831 train-loss: 0.010548836551606655\n",
      "[LOG 20200502-14:17:58] epoch: 6832 train-loss: 0.010548832308914926\n",
      "[LOG 20200502-14:17:59] epoch: 6833 train-loss: 0.010548828066223197\n",
      "[LOG 20200502-14:17:59] epoch: 6834 train-loss: 0.010548823409610324\n",
      "[LOG 20200502-14:17:59] epoch: 6835 train-loss: 0.010548818959958024\n",
      "[LOG 20200502-14:17:59] epoch: 6836 train-loss: 0.010548813992904292\n",
      "[LOG 20200502-14:17:59] epoch: 6837 train-loss: 0.010548808818889989\n",
      "[LOG 20200502-14:18:00] epoch: 6838 train-loss: 0.010548804162277116\n",
      "[LOG 20200502-14:18:00] epoch: 6839 train-loss: 0.010548798884782527\n",
      "[LOG 20200502-14:18:00] epoch: 6840 train-loss: 0.010548793917728795\n",
      "[LOG 20200502-14:18:00] epoch: 6841 train-loss: 0.010548787708911631\n",
      "[LOG 20200502-14:18:01] epoch: 6842 train-loss: 0.010548782120976184\n",
      "[LOG 20200502-14:18:01] epoch: 6843 train-loss: 0.010548776429560449\n",
      "[LOG 20200502-14:18:01] epoch: 6844 train-loss: 0.010548770531184144\n",
      "[LOG 20200502-14:18:01] epoch: 6845 train-loss: 0.010548764736288123\n",
      "[LOG 20200502-14:18:01] epoch: 6846 train-loss: 0.010548758734431531\n",
      "[LOG 20200502-14:18:02] epoch: 6847 train-loss: 0.010548752939535512\n",
      "[LOG 20200502-14:18:02] epoch: 6848 train-loss: 0.010548746730718348\n",
      "[LOG 20200502-14:18:02] epoch: 6849 train-loss: 0.010548740314940611\n",
      "[LOG 20200502-14:18:02] epoch: 6850 train-loss: 0.010548734209603734\n",
      "[LOG 20200502-14:18:03] epoch: 6851 train-loss: 0.010548728000786569\n",
      "[LOG 20200502-14:18:03] epoch: 6852 train-loss: 0.010548721998929977\n",
      "[LOG 20200502-14:18:03] epoch: 6853 train-loss: 0.010548715376191668\n",
      "[LOG 20200502-14:18:03] epoch: 6854 train-loss: 0.010548709270854792\n",
      "[LOG 20200502-14:18:04] epoch: 6855 train-loss: 0.010548703268998198\n",
      "[LOG 20200502-14:18:04] epoch: 6856 train-loss: 0.010548697060181035\n",
      "[LOG 20200502-14:18:04] epoch: 6857 train-loss: 0.010548691058324443\n",
      "[LOG 20200502-14:18:04] epoch: 6858 train-loss: 0.010548685470388995\n",
      "[LOG 20200502-14:18:05] epoch: 6859 train-loss: 0.010548679572012689\n",
      "[LOG 20200502-14:18:05] epoch: 6860 train-loss: 0.010548674191037813\n",
      "[LOG 20200502-14:18:05] epoch: 6861 train-loss: 0.010548668292661509\n",
      "[LOG 20200502-14:18:05] epoch: 6862 train-loss: 0.010548662601245774\n",
      "[LOG 20200502-14:18:06] epoch: 6863 train-loss: 0.01054865742723147\n",
      "[LOG 20200502-14:18:06] epoch: 6864 train-loss: 0.01054865163233545\n",
      "[LOG 20200502-14:18:06] epoch: 6865 train-loss: 0.010548646872242292\n",
      "[LOG 20200502-14:18:07] epoch: 6866 train-loss: 0.010548642008668847\n",
      "[LOG 20200502-14:18:07] epoch: 6867 train-loss: 0.010548637248575687\n",
      "[LOG 20200502-14:18:07] epoch: 6868 train-loss: 0.010548632902403673\n",
      "[LOG 20200502-14:18:07] epoch: 6869 train-loss: 0.010548628349271085\n",
      "[LOG 20200502-14:18:08] epoch: 6870 train-loss: 0.01054862431353993\n",
      "[LOG 20200502-14:18:08] epoch: 6871 train-loss: 0.010548620588249631\n",
      "[LOG 20200502-14:18:08] epoch: 6872 train-loss: 0.010548617276880477\n",
      "[LOG 20200502-14:18:08] epoch: 6873 train-loss: 0.01054861375855075\n",
      "[LOG 20200502-14:18:09] epoch: 6874 train-loss: 0.010548610240221024\n",
      "[LOG 20200502-14:18:09] epoch: 6875 train-loss: 0.010548606928851869\n",
      "[LOG 20200502-14:18:09] epoch: 6876 train-loss: 0.010548604859246148\n",
      "[LOG 20200502-14:18:09] epoch: 6877 train-loss: 0.01054860216875871\n",
      "[LOG 20200502-14:18:10] epoch: 6878 train-loss: 0.01054860009915299\n",
      "[LOG 20200502-14:18:10] epoch: 6879 train-loss: 0.010548597822586695\n",
      "[LOG 20200502-14:18:10] epoch: 6880 train-loss: 0.010548596373862691\n",
      "[LOG 20200502-14:18:10] epoch: 6881 train-loss: 0.010548595132099258\n",
      "[LOG 20200502-14:18:11] epoch: 6882 train-loss: 0.010548593993816111\n",
      "[LOG 20200502-14:18:11] epoch: 6883 train-loss: 0.010548593269454109\n",
      "[LOG 20200502-14:18:11] epoch: 6884 train-loss: 0.010548592855532965\n",
      "[LOG 20200502-14:18:11] epoch: 6885 train-loss: 0.010548592234651247\n",
      "[LOG 20200502-14:18:11] epoch: 6886 train-loss: 0.01054859295901325\n",
      "[LOG 20200502-14:18:12] epoch: 6887 train-loss: 0.010548593165973822\n",
      "[LOG 20200502-14:18:12] epoch: 6888 train-loss: 0.01054859378685554\n",
      "[LOG 20200502-14:18:12] epoch: 6889 train-loss: 0.010548594097296396\n",
      "[LOG 20200502-14:18:12] epoch: 6890 train-loss: 0.010548595546020402\n",
      "[LOG 20200502-14:18:13] epoch: 6891 train-loss: 0.010548597098224692\n",
      "[LOG 20200502-14:18:13] epoch: 6892 train-loss: 0.010548598650428984\n",
      "[LOG 20200502-14:18:13] epoch: 6893 train-loss: 0.010548600616554419\n",
      "[LOG 20200502-14:18:13] epoch: 6894 train-loss: 0.010548602996600999\n",
      "[LOG 20200502-14:18:14] epoch: 6895 train-loss: 0.010548605790568722\n",
      "[LOG 20200502-14:18:14] epoch: 6896 train-loss: 0.010548607756694158\n",
      "[LOG 20200502-14:18:14] epoch: 6897 train-loss: 0.010548610964583026\n",
      "[LOG 20200502-14:18:14] epoch: 6898 train-loss: 0.010548614172471894\n",
      "[LOG 20200502-14:18:15] epoch: 6899 train-loss: 0.010548617897762192\n",
      "[LOG 20200502-14:18:15] epoch: 6900 train-loss: 0.010548621312611632\n",
      "[LOG 20200502-14:18:15] epoch: 6901 train-loss: 0.01054862586574422\n",
      "[LOG 20200502-14:18:15] epoch: 6902 train-loss: 0.010548629901475377\n",
      "[LOG 20200502-14:18:16] epoch: 6903 train-loss: 0.010548634247647392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:18:16] epoch: 6904 train-loss: 0.010548639111220837\n",
      "[LOG 20200502-14:18:16] epoch: 6905 train-loss: 0.010548643871313997\n",
      "[LOG 20200502-14:18:16] epoch: 6906 train-loss: 0.010548648941848014\n",
      "[LOG 20200502-14:18:16] epoch: 6907 train-loss: 0.010548654219342602\n",
      "[LOG 20200502-14:18:17] epoch: 6908 train-loss: 0.010548659496837191\n",
      "[LOG 20200502-14:18:17] epoch: 6909 train-loss: 0.010548665291733213\n",
      "[LOG 20200502-14:18:17] epoch: 6910 train-loss: 0.01054867139707009\n",
      "[LOG 20200502-14:18:17] epoch: 6911 train-loss: 0.010548676881525252\n",
      "[LOG 20200502-14:18:18] epoch: 6912 train-loss: 0.010548683193822702\n",
      "[LOG 20200502-14:18:18] epoch: 6913 train-loss: 0.010548689506120153\n",
      "[LOG 20200502-14:18:18] epoch: 6914 train-loss: 0.01054869561145703\n",
      "[LOG 20200502-14:18:18] epoch: 6915 train-loss: 0.01054870192375448\n",
      "[LOG 20200502-14:18:19] epoch: 6916 train-loss: 0.01054870875345336\n",
      "[LOG 20200502-14:18:19] epoch: 6917 train-loss: 0.010548715479671955\n",
      "[LOG 20200502-14:18:19] epoch: 6918 train-loss: 0.01054872220589055\n",
      "[LOG 20200502-14:18:19] epoch: 6919 train-loss: 0.010548729139069716\n",
      "[LOG 20200502-14:18:20] epoch: 6920 train-loss: 0.01054873586528831\n",
      "[LOG 20200502-14:18:20] epoch: 6921 train-loss: 0.010548742798467478\n",
      "[LOG 20200502-14:18:20] epoch: 6922 train-loss: 0.010548750042087503\n",
      "[LOG 20200502-14:18:20] epoch: 6923 train-loss: 0.010548757285707526\n",
      "[LOG 20200502-14:18:21] epoch: 6924 train-loss: 0.010548763908445835\n",
      "[LOG 20200502-14:18:21] epoch: 6925 train-loss: 0.010548770738144716\n",
      "[LOG 20200502-14:18:21] epoch: 6926 train-loss: 0.010548777774804168\n",
      "[LOG 20200502-14:18:21] epoch: 6927 train-loss: 0.010548784914943907\n",
      "[LOG 20200502-14:18:22] epoch: 6928 train-loss: 0.010548791848123074\n",
      "[LOG 20200502-14:18:22] epoch: 6929 train-loss: 0.010548798677821955\n",
      "[LOG 20200502-14:18:22] epoch: 6930 train-loss: 0.010548805404040549\n",
      "[LOG 20200502-14:18:22] epoch: 6931 train-loss: 0.010548812647660574\n",
      "[LOG 20200502-14:18:23] epoch: 6932 train-loss: 0.01054881875299745\n",
      "[LOG 20200502-14:18:23] epoch: 6933 train-loss: 0.010548825479216047\n",
      "[LOG 20200502-14:18:23] epoch: 6934 train-loss: 0.010548832101954354\n",
      "[LOG 20200502-14:18:24] epoch: 6935 train-loss: 0.010548838931653235\n",
      "[LOG 20200502-14:18:24] epoch: 6936 train-loss: 0.010548844416108396\n",
      "[LOG 20200502-14:18:24] epoch: 6937 train-loss: 0.010548850624925561\n",
      "[LOG 20200502-14:18:24] epoch: 6938 train-loss: 0.010548856833742725\n",
      "[LOG 20200502-14:18:25] epoch: 6939 train-loss: 0.010548862318197886\n",
      "[LOG 20200502-14:18:25] epoch: 6940 train-loss: 0.010548868113093905\n",
      "[LOG 20200502-14:18:25] epoch: 6941 train-loss: 0.010548873287108209\n",
      "[LOG 20200502-14:18:25] epoch: 6942 train-loss: 0.010548879185484515\n",
      "[LOG 20200502-14:18:26] epoch: 6943 train-loss: 0.010548883945577674\n",
      "[LOG 20200502-14:18:26] epoch: 6944 train-loss: 0.010548888602190547\n",
      "[LOG 20200502-14:18:26] epoch: 6945 train-loss: 0.01054889325880342\n",
      "[LOG 20200502-14:18:26] epoch: 6946 train-loss: 0.010548897604975436\n",
      "[LOG 20200502-14:18:27] epoch: 6947 train-loss: 0.01054890143374602\n",
      "[LOG 20200502-14:18:27] epoch: 6948 train-loss: 0.010548905055556033\n",
      "[LOG 20200502-14:18:27] epoch: 6949 train-loss: 0.010548908677366044\n",
      "[LOG 20200502-14:18:27] epoch: 6950 train-loss: 0.010548911885254912\n",
      "[LOG 20200502-14:18:28] epoch: 6951 train-loss: 0.010548914782702923\n",
      "[LOG 20200502-14:18:28] epoch: 6952 train-loss: 0.010548917059269216\n",
      "[LOG 20200502-14:18:28] epoch: 6953 train-loss: 0.01054891985323694\n",
      "[LOG 20200502-14:18:28] epoch: 6954 train-loss: 0.010548921508921517\n",
      "[LOG 20200502-14:18:29] epoch: 6955 train-loss: 0.010548922957645522\n",
      "[LOG 20200502-14:18:29] epoch: 6956 train-loss: 0.010548924509849813\n",
      "[LOG 20200502-14:18:29] epoch: 6957 train-loss: 0.01054892564813296\n",
      "[LOG 20200502-14:18:29] epoch: 6958 train-loss: 0.010548926269014677\n",
      "[LOG 20200502-14:18:29] epoch: 6959 train-loss: 0.010548926579455534\n",
      "[LOG 20200502-14:18:30] epoch: 6960 train-loss: 0.010548926269014677\n",
      "[LOG 20200502-14:18:30] epoch: 6961 train-loss: 0.010548925855093531\n",
      "[LOG 20200502-14:18:30] epoch: 6962 train-loss: 0.010548925234211816\n",
      "[LOG 20200502-14:18:30] epoch: 6963 train-loss: 0.01054892378548781\n",
      "[LOG 20200502-14:18:31] epoch: 6964 train-loss: 0.01054892223328352\n",
      "[LOG 20200502-14:18:31] epoch: 6965 train-loss: 0.010548920163677799\n",
      "[LOG 20200502-14:18:31] epoch: 6966 train-loss: 0.010548917887111505\n",
      "[LOG 20200502-14:18:31] epoch: 6967 train-loss: 0.010548915507064925\n",
      "[LOG 20200502-14:18:32] epoch: 6968 train-loss: 0.010548912713097202\n",
      "[LOG 20200502-14:18:32] epoch: 6969 train-loss: 0.010548909298247762\n",
      "[LOG 20200502-14:18:32] epoch: 6970 train-loss: 0.01054890536599689\n",
      "[LOG 20200502-14:18:32] epoch: 6971 train-loss: 0.010548901744186878\n",
      "[LOG 20200502-14:18:33] epoch: 6972 train-loss: 0.010548897087574005\n",
      "[LOG 20200502-14:18:33] epoch: 6973 train-loss: 0.010548892327480845\n",
      "[LOG 20200502-14:18:33] epoch: 6974 train-loss: 0.010548887670867972\n",
      "[LOG 20200502-14:18:33] epoch: 6975 train-loss: 0.01054888197945224\n",
      "[LOG 20200502-14:18:33] epoch: 6976 train-loss: 0.010548876805437936\n",
      "[LOG 20200502-14:18:34] epoch: 6977 train-loss: 0.010548870493140485\n",
      "[LOG 20200502-14:18:34] epoch: 6978 train-loss: 0.010548864698244466\n",
      "[LOG 20200502-14:18:34] epoch: 6979 train-loss: 0.010548857972025871\n",
      "[LOG 20200502-14:18:34] epoch: 6980 train-loss: 0.010548851866688993\n",
      "[LOG 20200502-14:18:35] epoch: 6981 train-loss: 0.010548845036990113\n",
      "[LOG 20200502-14:18:35] epoch: 6982 train-loss: 0.010548837896850374\n",
      "[LOG 20200502-14:18:35] epoch: 6983 train-loss: 0.010548830239309205\n",
      "[LOG 20200502-14:18:35] epoch: 6984 train-loss: 0.010548823202649752\n",
      "[LOG 20200502-14:18:36] epoch: 6985 train-loss: 0.010548815441628298\n",
      "[LOG 20200502-14:18:36] epoch: 6986 train-loss: 0.010548807680606842\n",
      "[LOG 20200502-14:18:36] epoch: 6987 train-loss: 0.010548799816105101\n",
      "[LOG 20200502-14:18:36] epoch: 6988 train-loss: 0.010548792262044217\n",
      "[LOG 20200502-14:18:37] epoch: 6989 train-loss: 0.010548783983621333\n",
      "[LOG 20200502-14:18:37] epoch: 6990 train-loss: 0.010548776222599877\n",
      "[LOG 20200502-14:18:37] epoch: 6991 train-loss: 0.010548767840696706\n",
      "[LOG 20200502-14:18:37] epoch: 6992 train-loss: 0.01054876007967525\n",
      "[LOG 20200502-14:18:38] epoch: 6993 train-loss: 0.010548752008212937\n",
      "[LOG 20200502-14:18:38] epoch: 6994 train-loss: 0.010548744143711196\n",
      "[LOG 20200502-14:18:38] epoch: 6995 train-loss: 0.010548736175729169\n",
      "[LOG 20200502-14:18:38] epoch: 6996 train-loss: 0.010548728414707713\n",
      "[LOG 20200502-14:18:39] epoch: 6997 train-loss: 0.010548720136284828\n",
      "[LOG 20200502-14:18:39] epoch: 6998 train-loss: 0.010548712582223944\n",
      "[LOG 20200502-14:18:39] epoch: 6999 train-loss: 0.01054870482120249\n",
      "[LOG 20200502-14:18:39] epoch: 7000 train-loss: 0.010548697888023324\n",
      "[LOG 20200502-14:18:40] epoch: 7001 train-loss: 0.010548690540923012\n",
      "[LOG 20200502-14:18:40] epoch: 7002 train-loss: 0.010548683400783274\n",
      "[LOG 20200502-14:18:40] epoch: 7003 train-loss: 0.010548676260643534\n",
      "[LOG 20200502-14:18:40] epoch: 7004 train-loss: 0.010548669844865799\n",
      "[LOG 20200502-14:18:41] epoch: 7005 train-loss: 0.01054866322212749\n",
      "[LOG 20200502-14:18:41] epoch: 7006 train-loss: 0.010548656288948324\n",
      "[LOG 20200502-14:18:41] epoch: 7007 train-loss: 0.010548650804493163\n",
      "[LOG 20200502-14:18:41] epoch: 7008 train-loss: 0.010548644802636571\n",
      "[LOG 20200502-14:18:42] epoch: 7009 train-loss: 0.010548639318181409\n",
      "[LOG 20200502-14:18:42] epoch: 7010 train-loss: 0.010548633833726248\n",
      "[LOG 20200502-14:18:42] epoch: 7011 train-loss: 0.0105486282457908\n",
      "[LOG 20200502-14:18:42] epoch: 7012 train-loss: 0.01054862431353993\n",
      "[LOG 20200502-14:18:42] epoch: 7013 train-loss: 0.010548619863887628\n",
      "[LOG 20200502-14:18:43] epoch: 7014 train-loss: 0.010548615828156471\n",
      "[LOG 20200502-14:18:43] epoch: 7015 train-loss: 0.0105486118959056\n",
      "[LOG 20200502-14:18:43] epoch: 7016 train-loss: 0.010548608274095587\n",
      "[LOG 20200502-14:18:43] epoch: 7017 train-loss: 0.010548605169687007\n",
      "[LOG 20200502-14:18:44] epoch: 7018 train-loss: 0.010548601754837565\n",
      "[LOG 20200502-14:18:44] epoch: 7019 train-loss: 0.010548599892192416\n",
      "[LOG 20200502-14:18:44] epoch: 7020 train-loss: 0.010548597615626123\n",
      "[LOG 20200502-14:18:44] epoch: 7021 train-loss: 0.01054859533905983\n",
      "[LOG 20200502-14:18:45] epoch: 7022 train-loss: 0.010548593993816111\n",
      "[LOG 20200502-14:18:45] epoch: 7023 train-loss: 0.01054859295901325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:18:45] epoch: 7024 train-loss: 0.010548592441611819\n",
      "[LOG 20200502-14:18:45] epoch: 7025 train-loss: 0.0105485910963681\n",
      "[LOG 20200502-14:18:46] epoch: 7026 train-loss: 0.010548591199848387\n",
      "[LOG 20200502-14:18:46] epoch: 7027 train-loss: 0.0105485910963681\n",
      "[LOG 20200502-14:18:46] epoch: 7028 train-loss: 0.010548591199848387\n",
      "[LOG 20200502-14:18:46] epoch: 7029 train-loss: 0.010548592441611819\n",
      "[LOG 20200502-14:18:46] epoch: 7030 train-loss: 0.010548593165973822\n",
      "[LOG 20200502-14:18:47] epoch: 7031 train-loss: 0.010548594097296396\n",
      "[LOG 20200502-14:18:47] epoch: 7032 train-loss: 0.010548596580823263\n",
      "[LOG 20200502-14:18:47] epoch: 7033 train-loss: 0.01054859823650784\n",
      "[LOG 20200502-14:18:47] epoch: 7034 train-loss: 0.010548599995672703\n",
      "[LOG 20200502-14:18:48] epoch: 7035 train-loss: 0.010548602375719283\n",
      "[LOG 20200502-14:18:48] epoch: 7036 train-loss: 0.010548604962726435\n",
      "[LOG 20200502-14:18:48] epoch: 7037 train-loss: 0.010548608067135016\n",
      "[LOG 20200502-14:18:49] epoch: 7038 train-loss: 0.010548611275023885\n",
      "[LOG 20200502-14:18:49] epoch: 7039 train-loss: 0.010548615000314183\n",
      "[LOG 20200502-14:18:49] epoch: 7040 train-loss: 0.010548619449966483\n",
      "[LOG 20200502-14:18:49] epoch: 7041 train-loss: 0.010548622864815924\n",
      "[LOG 20200502-14:18:50] epoch: 7042 train-loss: 0.010548627314468225\n",
      "[LOG 20200502-14:18:50] epoch: 7043 train-loss: 0.010548631867600812\n",
      "[LOG 20200502-14:18:50] epoch: 7044 train-loss: 0.010548636731174257\n",
      "[LOG 20200502-14:18:50] epoch: 7045 train-loss: 0.01054864138778713\n",
      "[LOG 20200502-14:18:50] epoch: 7046 train-loss: 0.010548646975722577\n",
      "[LOG 20200502-14:18:51] epoch: 7047 train-loss: 0.010548652046256594\n",
      "[LOG 20200502-14:18:51] epoch: 7048 train-loss: 0.010548657944632901\n",
      "[LOG 20200502-14:18:51] epoch: 7049 train-loss: 0.01054866322212749\n",
      "[LOG 20200502-14:18:51] epoch: 7050 train-loss: 0.010548669223984083\n",
      "[LOG 20200502-14:18:52] epoch: 7051 train-loss: 0.010548675432801247\n",
      "[LOG 20200502-14:18:52] epoch: 7052 train-loss: 0.010548681434657838\n",
      "[LOG 20200502-14:18:52] epoch: 7053 train-loss: 0.01054868826435672\n",
      "[LOG 20200502-14:18:52] epoch: 7054 train-loss: 0.010548695197535886\n",
      "[LOG 20200502-14:18:52] epoch: 7055 train-loss: 0.010548701509833336\n",
      "[LOG 20200502-14:18:53] epoch: 7056 train-loss: 0.010548709167374505\n",
      "[LOG 20200502-14:18:53] epoch: 7057 train-loss: 0.010548715479671955\n",
      "[LOG 20200502-14:18:53] epoch: 7058 train-loss: 0.010548723033732839\n",
      "[LOG 20200502-14:18:54] epoch: 7059 train-loss: 0.010548729759951433\n",
      "[LOG 20200502-14:18:54] epoch: 7060 train-loss: 0.010548737727933459\n",
      "[LOG 20200502-14:18:54] epoch: 7061 train-loss: 0.010548744971553484\n",
      "[LOG 20200502-14:18:54] epoch: 7062 train-loss: 0.010548752008212937\n",
      "[LOG 20200502-14:18:55] epoch: 7063 train-loss: 0.010548759665754106\n",
      "[LOG 20200502-14:18:55] epoch: 7064 train-loss: 0.010548767323295275\n",
      "[LOG 20200502-14:18:55] epoch: 7065 train-loss: 0.010548775291277302\n",
      "[LOG 20200502-14:18:55] epoch: 7066 train-loss: 0.01054878325925933\n",
      "[LOG 20200502-14:18:55] epoch: 7067 train-loss: 0.010548790502879355\n",
      "[LOG 20200502-14:18:56] epoch: 7068 train-loss: 0.010548798884782527\n",
      "[LOG 20200502-14:18:56] epoch: 7069 train-loss: 0.010548806852764554\n",
      "[LOG 20200502-14:18:56] epoch: 7070 train-loss: 0.010548814510305723\n",
      "[LOG 20200502-14:18:56] epoch: 7071 train-loss: 0.010548822478287749\n",
      "[LOG 20200502-14:18:57] epoch: 7072 train-loss: 0.010548829928868346\n",
      "[LOG 20200502-14:18:57] epoch: 7073 train-loss: 0.01054883800033066\n",
      "[LOG 20200502-14:18:57] epoch: 7074 train-loss: 0.010548845968312688\n",
      "[LOG 20200502-14:18:57] epoch: 7075 train-loss: 0.010548853936294714\n",
      "[LOG 20200502-14:18:58] epoch: 7076 train-loss: 0.010548861800796457\n",
      "[LOG 20200502-14:18:58] epoch: 7077 train-loss: 0.010548869354857339\n",
      "[LOG 20200502-14:18:58] epoch: 7078 train-loss: 0.01054887670195765\n",
      "[LOG 20200502-14:18:58] epoch: 7079 train-loss: 0.010548885187341107\n",
      "[LOG 20200502-14:18:59] epoch: 7080 train-loss: 0.010548892534441419\n",
      "[LOG 20200502-14:18:59] epoch: 7081 train-loss: 0.010548899881541729\n",
      "[LOG 20200502-14:18:59] epoch: 7082 train-loss: 0.010548907953004042\n",
      "[LOG 20200502-14:18:59] epoch: 7083 train-loss: 0.010548915093143782\n",
      "[LOG 20200502-14:19:00] epoch: 7084 train-loss: 0.010548921922842661\n",
      "[LOG 20200502-14:19:00] epoch: 7085 train-loss: 0.010548929166462686\n",
      "[LOG 20200502-14:19:00] epoch: 7086 train-loss: 0.010548936617043283\n",
      "[LOG 20200502-14:19:00] epoch: 7087 train-loss: 0.010548943446742164\n",
      "[LOG 20200502-14:19:01] epoch: 7088 train-loss: 0.010548950690362189\n",
      "[LOG 20200502-14:19:01] epoch: 7089 train-loss: 0.01054895700265964\n",
      "[LOG 20200502-14:19:01] epoch: 7090 train-loss: 0.01054896403931909\n",
      "[LOG 20200502-14:19:01] epoch: 7091 train-loss: 0.010548970248136256\n",
      "[LOG 20200502-14:19:02] epoch: 7092 train-loss: 0.010548976043032275\n",
      "[LOG 20200502-14:19:02] epoch: 7093 train-loss: 0.010548982355329726\n",
      "[LOG 20200502-14:19:02] epoch: 7094 train-loss: 0.01054898856414689\n",
      "[LOG 20200502-14:19:02] epoch: 7095 train-loss: 0.010548994669483768\n",
      "[LOG 20200502-14:19:02] epoch: 7096 train-loss: 0.010549000257419216\n",
      "[LOG 20200502-14:19:03] epoch: 7097 train-loss: 0.010549005224472947\n",
      "[LOG 20200502-14:19:03] epoch: 7098 train-loss: 0.010549011019368967\n",
      "[LOG 20200502-14:19:03] epoch: 7099 train-loss: 0.010549015779462125\n",
      "[LOG 20200502-14:19:03] epoch: 7100 train-loss: 0.010549020953476429\n",
      "[LOG 20200502-14:19:04] epoch: 7101 train-loss: 0.010549025713569589\n",
      "[LOG 20200502-14:19:04] epoch: 7102 train-loss: 0.010549029956261316\n",
      "[LOG 20200502-14:19:04] epoch: 7103 train-loss: 0.010549034198953046\n",
      "[LOG 20200502-14:19:04] epoch: 7104 train-loss: 0.010549038752085634\n",
      "[LOG 20200502-14:19:05] epoch: 7105 train-loss: 0.010549042063454786\n",
      "[LOG 20200502-14:19:05] epoch: 7106 train-loss: 0.010549045892225372\n",
      "[LOG 20200502-14:19:05] epoch: 7107 train-loss: 0.010549049410555098\n",
      "[LOG 20200502-14:19:05] epoch: 7108 train-loss: 0.010549052618443966\n",
      "[LOG 20200502-14:19:06] epoch: 7109 train-loss: 0.01054905541241169\n",
      "[LOG 20200502-14:19:06] epoch: 7110 train-loss: 0.010549058413339986\n",
      "[LOG 20200502-14:19:06] epoch: 7111 train-loss: 0.010549060689906279\n",
      "[LOG 20200502-14:19:06] epoch: 7112 train-loss: 0.010549062966472574\n",
      "[LOG 20200502-14:19:06] epoch: 7113 train-loss: 0.010549065036078295\n",
      "[LOG 20200502-14:19:07] epoch: 7114 train-loss: 0.010549067416124873\n",
      "[LOG 20200502-14:19:07] epoch: 7115 train-loss: 0.010549068243967162\n",
      "[LOG 20200502-14:19:07] epoch: 7116 train-loss: 0.010549070106612312\n",
      "[LOG 20200502-14:19:07] epoch: 7117 train-loss: 0.010549071141415171\n",
      "[LOG 20200502-14:19:08] epoch: 7118 train-loss: 0.010549071969257461\n",
      "[LOG 20200502-14:19:08] epoch: 7119 train-loss: 0.010549072486658892\n",
      "[LOG 20200502-14:19:08] epoch: 7120 train-loss: 0.010549072590139177\n",
      "[LOG 20200502-14:19:08] epoch: 7121 train-loss: 0.010549073211020894\n",
      "[LOG 20200502-14:19:09] epoch: 7122 train-loss: 0.010549073211020894\n",
      "[LOG 20200502-14:19:09] epoch: 7123 train-loss: 0.010549073417981466\n",
      "[LOG 20200502-14:19:09] epoch: 7124 train-loss: 0.010549072383178605\n",
      "[LOG 20200502-14:19:09] epoch: 7125 train-loss: 0.010549071865777174\n",
      "[LOG 20200502-14:19:10] epoch: 7126 train-loss: 0.010549070830974314\n",
      "[LOG 20200502-14:19:10] epoch: 7127 train-loss: 0.010549069796171453\n",
      "[LOG 20200502-14:19:10] epoch: 7128 train-loss: 0.010549068243967162\n",
      "[LOG 20200502-14:19:11] epoch: 7129 train-loss: 0.010549067002203729\n",
      "[LOG 20200502-14:19:11] epoch: 7130 train-loss: 0.010549065760440297\n",
      "[LOG 20200502-14:19:11] epoch: 7131 train-loss: 0.010549063380393717\n",
      "[LOG 20200502-14:19:11] epoch: 7132 train-loss: 0.01054906172470914\n",
      "[LOG 20200502-14:19:12] epoch: 7133 train-loss: 0.010549059551623132\n",
      "[LOG 20200502-14:19:12] epoch: 7134 train-loss: 0.010549057482017411\n",
      "[LOG 20200502-14:19:12] epoch: 7135 train-loss: 0.010549054688049687\n",
      "[LOG 20200502-14:19:12] epoch: 7136 train-loss: 0.010549051894081963\n",
      "[LOG 20200502-14:19:13] epoch: 7137 train-loss: 0.010549049720995955\n",
      "[LOG 20200502-14:19:13] epoch: 7138 train-loss: 0.010549046409626802\n",
      "[LOG 20200502-14:19:13] epoch: 7139 train-loss: 0.010549043719139364\n",
      "[LOG 20200502-14:19:13] epoch: 7140 train-loss: 0.010549040718211068\n",
      "[LOG 20200502-14:19:13] epoch: 7141 train-loss: 0.010549037406841913\n",
      "[LOG 20200502-14:19:14] epoch: 7142 train-loss: 0.010549034509393904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:19:14] epoch: 7143 train-loss: 0.010549031404985322\n",
      "[LOG 20200502-14:19:14] epoch: 7144 train-loss: 0.010549027886655595\n",
      "[LOG 20200502-14:19:14] epoch: 7145 train-loss: 0.010549024678766727\n",
      "[LOG 20200502-14:19:15] epoch: 7146 train-loss: 0.01054902147087786\n",
      "[LOG 20200502-14:19:15] epoch: 7147 train-loss: 0.010549018159508705\n",
      "[LOG 20200502-14:19:15] epoch: 7148 train-loss: 0.01054901484813955\n",
      "[LOG 20200502-14:19:15] epoch: 7149 train-loss: 0.010549011536770396\n",
      "[LOG 20200502-14:19:16] epoch: 7150 train-loss: 0.010549008742802672\n",
      "[LOG 20200502-14:19:16] epoch: 7151 train-loss: 0.010549005224472947\n",
      "[LOG 20200502-14:19:16] epoch: 7152 train-loss: 0.010549002430505224\n",
      "[LOG 20200502-14:19:16] epoch: 7153 train-loss: 0.010548998912175497\n",
      "[LOG 20200502-14:19:17] epoch: 7154 train-loss: 0.010548996221688058\n",
      "[LOG 20200502-14:19:17] epoch: 7155 train-loss: 0.010548993117279477\n",
      "[LOG 20200502-14:19:17] epoch: 7156 train-loss: 0.010548990323311754\n",
      "[LOG 20200502-14:19:17] epoch: 7157 train-loss: 0.010548987943265174\n",
      "[LOG 20200502-14:19:17] epoch: 7158 train-loss: 0.010548985252777735\n",
      "[LOG 20200502-14:19:18] epoch: 7159 train-loss: 0.010548983183172014\n",
      "[LOG 20200502-14:19:18] epoch: 7160 train-loss: 0.01054898121704658\n",
      "[LOG 20200502-14:19:18] epoch: 7161 train-loss: 0.010548978940480284\n",
      "[LOG 20200502-14:19:18] epoch: 7162 train-loss: 0.010548976767394278\n",
      "[LOG 20200502-14:19:19] epoch: 7163 train-loss: 0.01054897542215056\n",
      "[LOG 20200502-14:19:19] epoch: 7164 train-loss: 0.010548973973426554\n",
      "[LOG 20200502-14:19:19] epoch: 7165 train-loss: 0.010548972628182836\n",
      "[LOG 20200502-14:19:19] epoch: 7166 train-loss: 0.010548971696860261\n",
      "[LOG 20200502-14:19:20] epoch: 7167 train-loss: 0.010548970869017972\n",
      "[LOG 20200502-14:19:20] epoch: 7168 train-loss: 0.010548970351616541\n",
      "[LOG 20200502-14:19:20] epoch: 7169 train-loss: 0.010548969937695397\n",
      "[LOG 20200502-14:19:20] epoch: 7170 train-loss: 0.010548970248136256\n",
      "[LOG 20200502-14:19:20] epoch: 7171 train-loss: 0.010548970144655969\n",
      "[LOG 20200502-14:19:21] epoch: 7172 train-loss: 0.0105489706620574\n",
      "[LOG 20200502-14:19:21] epoch: 7173 train-loss: 0.010548971386419402\n",
      "[LOG 20200502-14:19:21] epoch: 7174 train-loss: 0.010548972524702549\n",
      "[LOG 20200502-14:19:21] epoch: 7175 train-loss: 0.010548973662985696\n",
      "[LOG 20200502-14:19:22] epoch: 7176 train-loss: 0.010548974801268842\n",
      "[LOG 20200502-14:19:22] epoch: 7177 train-loss: 0.01054897697435485\n",
      "[LOG 20200502-14:19:22] epoch: 7178 train-loss: 0.010548978940480284\n",
      "[LOG 20200502-14:19:22] epoch: 7179 train-loss: 0.010548981010086007\n",
      "[LOG 20200502-14:19:23] epoch: 7180 train-loss: 0.0105489832866523\n",
      "[LOG 20200502-14:19:23] epoch: 7181 train-loss: 0.010548986391060881\n",
      "[LOG 20200502-14:19:23] epoch: 7182 train-loss: 0.010548989288508892\n",
      "[LOG 20200502-14:19:23] epoch: 7183 train-loss: 0.010548992289437188\n",
      "[LOG 20200502-14:19:23] epoch: 7184 train-loss: 0.010548996221688058\n",
      "[LOG 20200502-14:19:24] epoch: 7185 train-loss: 0.010548999843498072\n",
      "[LOG 20200502-14:19:24] epoch: 7186 train-loss: 0.010549004189670086\n",
      "[LOG 20200502-14:19:24] epoch: 7187 train-loss: 0.010549008949763246\n",
      "[LOG 20200502-14:19:24] epoch: 7188 train-loss: 0.010549013192454973\n",
      "[LOG 20200502-14:19:25] epoch: 7189 train-loss: 0.010549018159508705\n",
      "[LOG 20200502-14:19:25] epoch: 7190 train-loss: 0.010549023333523009\n",
      "[LOG 20200502-14:19:25] epoch: 7191 train-loss: 0.01054902881797817\n",
      "[LOG 20200502-14:19:25] epoch: 7192 train-loss: 0.010549034198953046\n",
      "[LOG 20200502-14:19:25] epoch: 7193 train-loss: 0.010549039579927921\n",
      "[LOG 20200502-14:19:26] epoch: 7194 train-loss: 0.010549045892225372\n",
      "[LOG 20200502-14:19:26] epoch: 7195 train-loss: 0.01054905199756225\n",
      "[LOG 20200502-14:19:26] epoch: 7196 train-loss: 0.010549058413339986\n",
      "[LOG 20200502-14:19:26] epoch: 7197 train-loss: 0.010549064725637436\n",
      "[LOG 20200502-14:19:27] epoch: 7198 train-loss: 0.01054907145185603\n",
      "[LOG 20200502-14:19:27] epoch: 7199 train-loss: 0.010549078488515483\n",
      "[LOG 20200502-14:19:27] epoch: 7200 train-loss: 0.01054908542169465\n",
      "[LOG 20200502-14:19:27] epoch: 7201 train-loss: 0.010549092458354102\n",
      "[LOG 20200502-14:19:28] epoch: 7202 train-loss: 0.010549099701974127\n",
      "[LOG 20200502-14:19:28] epoch: 7203 train-loss: 0.010549107152554724\n",
      "[LOG 20200502-14:19:28] epoch: 7204 train-loss: 0.010549114810095893\n",
      "[LOG 20200502-14:19:28] epoch: 7205 train-loss: 0.010549122364156775\n",
      "[LOG 20200502-14:19:28] epoch: 7206 train-loss: 0.010549130125178231\n",
      "[LOG 20200502-14:19:29] epoch: 7207 train-loss: 0.010549137989679972\n",
      "[LOG 20200502-14:19:29] epoch: 7208 train-loss: 0.010549145957662\n",
      "[LOG 20200502-14:19:29] epoch: 7209 train-loss: 0.010549153615203168\n",
      "[LOG 20200502-14:19:29] epoch: 7210 train-loss: 0.010549161686665483\n",
      "[LOG 20200502-14:19:30] epoch: 7211 train-loss: 0.010549169447686937\n",
      "[LOG 20200502-14:19:30] epoch: 7212 train-loss: 0.010549177415668964\n",
      "[LOG 20200502-14:19:30] epoch: 7213 train-loss: 0.010549185694091849\n",
      "[LOG 20200502-14:19:30] epoch: 7214 train-loss: 0.01054919335163302\n",
      "[LOG 20200502-14:19:30] epoch: 7215 train-loss: 0.010549201526575618\n",
      "[LOG 20200502-14:19:31] epoch: 7216 train-loss: 0.01054920887367593\n",
      "[LOG 20200502-14:19:31] epoch: 7217 train-loss: 0.010549216945138242\n",
      "[LOG 20200502-14:19:31] epoch: 7218 train-loss: 0.010549224809639983\n",
      "[LOG 20200502-14:19:31] epoch: 7219 train-loss: 0.010549232570661439\n",
      "[LOG 20200502-14:19:32] epoch: 7220 train-loss: 0.010549239814281464\n",
      "[LOG 20200502-14:19:32] epoch: 7221 train-loss: 0.010549247368342347\n",
      "[LOG 20200502-14:19:32] epoch: 7222 train-loss: 0.010549254508482086\n",
      "[LOG 20200502-14:19:32] epoch: 7223 train-loss: 0.010549261648621824\n",
      "[LOG 20200502-14:19:32] epoch: 7224 train-loss: 0.010549268788761564\n",
      "[LOG 20200502-14:19:33] epoch: 7225 train-loss: 0.010549275514980158\n",
      "[LOG 20200502-14:19:33] epoch: 7226 train-loss: 0.010549281930757893\n",
      "[LOG 20200502-14:19:33] epoch: 7227 train-loss: 0.010549288553496202\n",
      "[LOG 20200502-14:19:33] epoch: 7228 train-loss: 0.010549294969273938\n",
      "[LOG 20200502-14:19:34] epoch: 7229 train-loss: 0.010549300350248814\n",
      "[LOG 20200502-14:19:34] epoch: 7230 train-loss: 0.010549306145144833\n",
      "[LOG 20200502-14:19:34] epoch: 7231 train-loss: 0.010549311836560568\n",
      "[LOG 20200502-14:19:34] epoch: 7232 train-loss: 0.010549316700134013\n",
      "[LOG 20200502-14:19:34] epoch: 7233 train-loss: 0.010549321563707458\n",
      "[LOG 20200502-14:19:35] epoch: 7234 train-loss: 0.010549326737721762\n",
      "[LOG 20200502-14:19:35] epoch: 7235 train-loss: 0.010549330566492345\n",
      "[LOG 20200502-14:19:35] epoch: 7236 train-loss: 0.010549335016144646\n",
      "[LOG 20200502-14:19:35] epoch: 7237 train-loss: 0.010549338327513801\n",
      "[LOG 20200502-14:19:36] epoch: 7238 train-loss: 0.010549341845843527\n",
      "[LOG 20200502-14:19:36] epoch: 7239 train-loss: 0.010549344846771823\n",
      "[LOG 20200502-14:19:36] epoch: 7240 train-loss: 0.01054934753725926\n",
      "[LOG 20200502-14:19:36] epoch: 7241 train-loss: 0.010549349710345268\n",
      "[LOG 20200502-14:19:36] epoch: 7242 train-loss: 0.010549351676470704\n",
      "[LOG 20200502-14:19:37] epoch: 7243 train-loss: 0.010549353021714423\n",
      "[LOG 20200502-14:19:37] epoch: 7244 train-loss: 0.010549354263477854\n",
      "[LOG 20200502-14:19:37] epoch: 7245 train-loss: 0.010549355608721575\n",
      "[LOG 20200502-14:19:37] epoch: 7246 train-loss: 0.01054935571220186\n",
      "[LOG 20200502-14:19:38] epoch: 7247 train-loss: 0.01054935622960329\n",
      "[LOG 20200502-14:19:38] epoch: 7248 train-loss: 0.010549355608721575\n",
      "[LOG 20200502-14:19:38] epoch: 7249 train-loss: 0.010549355194800429\n",
      "[LOG 20200502-14:19:38] epoch: 7250 train-loss: 0.010549354263477854\n",
      "[LOG 20200502-14:19:39] epoch: 7251 train-loss: 0.010549352918234136\n",
      "[LOG 20200502-14:19:39] epoch: 7252 train-loss: 0.010549350848628415\n",
      "[LOG 20200502-14:19:39] epoch: 7253 train-loss: 0.010549349089463552\n",
      "[LOG 20200502-14:19:39] epoch: 7254 train-loss: 0.010549346812897258\n",
      "[LOG 20200502-14:19:39] epoch: 7255 train-loss: 0.01054934360500839\n",
      "[LOG 20200502-14:19:40] epoch: 7256 train-loss: 0.010549340914520953\n",
      "[LOG 20200502-14:19:40] epoch: 7257 train-loss: 0.01054933729271094\n",
      "[LOG 20200502-14:19:40] epoch: 7258 train-loss: 0.010549334188302359\n",
      "[LOG 20200502-14:19:40] epoch: 7259 train-loss: 0.010549330152571201\n",
      "[LOG 20200502-14:19:41] epoch: 7260 train-loss: 0.010549325909879472\n",
      "[LOG 20200502-14:19:41] epoch: 7261 train-loss: 0.010549321563707458\n",
      "[LOG 20200502-14:19:41] epoch: 7262 train-loss: 0.010549316596653726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:19:41] epoch: 7263 train-loss: 0.010549311733080281\n",
      "[LOG 20200502-14:19:41] epoch: 7264 train-loss: 0.010549306559065977\n",
      "[LOG 20200502-14:19:42] epoch: 7265 train-loss: 0.01054930148853196\n",
      "[LOG 20200502-14:19:42] epoch: 7266 train-loss: 0.010549296211037371\n",
      "[LOG 20200502-14:19:42] epoch: 7267 train-loss: 0.010549290312661065\n",
      "[LOG 20200502-14:19:42] epoch: 7268 train-loss: 0.010549284207324186\n",
      "[LOG 20200502-14:19:43] epoch: 7269 train-loss: 0.010549279033309884\n",
      "[LOG 20200502-14:19:43] epoch: 7270 train-loss: 0.010549273445374437\n",
      "[LOG 20200502-14:19:43] epoch: 7271 train-loss: 0.010549267443517843\n",
      "[LOG 20200502-14:19:43] epoch: 7272 train-loss: 0.010549261855582396\n",
      "[LOG 20200502-14:19:43] epoch: 7273 train-loss: 0.010549255543284945\n",
      "[LOG 20200502-14:19:44] epoch: 7274 train-loss: 0.01054924964490864\n",
      "[LOG 20200502-14:19:44] epoch: 7275 train-loss: 0.010549243953492906\n",
      "[LOG 20200502-14:19:44] epoch: 7276 train-loss: 0.010549238262077173\n",
      "[LOG 20200502-14:19:44] epoch: 7277 train-loss: 0.010549232363700867\n",
      "[LOG 20200502-14:19:45] epoch: 7278 train-loss: 0.010549227396647135\n",
      "[LOG 20200502-14:19:45] epoch: 7279 train-loss: 0.010549221601751115\n",
      "[LOG 20200502-14:19:45] epoch: 7280 train-loss: 0.010549215910335382\n",
      "[LOG 20200502-14:19:45] epoch: 7281 train-loss: 0.010549211357202794\n",
      "[LOG 20200502-14:19:46] epoch: 7282 train-loss: 0.010549206390149064\n",
      "[LOG 20200502-14:19:46] epoch: 7283 train-loss: 0.010549201630055904\n",
      "[LOG 20200502-14:19:46] epoch: 7284 train-loss: 0.010549196973443031\n",
      "[LOG 20200502-14:19:46] epoch: 7285 train-loss: 0.010549192730751302\n",
      "[LOG 20200502-14:19:46] epoch: 7286 train-loss: 0.010549189005461004\n",
      "[LOG 20200502-14:19:47] epoch: 7287 train-loss: 0.010549184866249561\n",
      "[LOG 20200502-14:19:47] epoch: 7288 train-loss: 0.010549181347919835\n",
      "[LOG 20200502-14:19:47] epoch: 7289 train-loss: 0.010549178243511252\n",
      "[LOG 20200502-14:19:47] epoch: 7290 train-loss: 0.0105491749321421\n",
      "[LOG 20200502-14:19:48] epoch: 7291 train-loss: 0.01054917255209552\n",
      "[LOG 20200502-14:19:48] epoch: 7292 train-loss: 0.010549170068568654\n",
      "[LOG 20200502-14:19:48] epoch: 7293 train-loss: 0.01054916779200236\n",
      "[LOG 20200502-14:19:48] epoch: 7294 train-loss: 0.01054916592935721\n",
      "[LOG 20200502-14:19:48] epoch: 7295 train-loss: 0.010549164687593779\n",
      "[LOG 20200502-14:19:49] epoch: 7296 train-loss: 0.01054916251450777\n",
      "[LOG 20200502-14:19:49] epoch: 7297 train-loss: 0.010549162721468342\n",
      "[LOG 20200502-14:19:49] epoch: 7298 train-loss: 0.010549161583185196\n",
      "[LOG 20200502-14:19:49] epoch: 7299 train-loss: 0.010549161583185196\n",
      "[LOG 20200502-14:19:49] epoch: 7300 train-loss: 0.010549161583185196\n",
      "[LOG 20200502-14:19:50] epoch: 7301 train-loss: 0.010549161479704909\n",
      "[LOG 20200502-14:19:50] epoch: 7302 train-loss: 0.010549162411027484\n",
      "[LOG 20200502-14:19:50] epoch: 7303 train-loss: 0.010549163342350058\n",
      "[LOG 20200502-14:19:50] epoch: 7304 train-loss: 0.01054916489455435\n",
      "[LOG 20200502-14:19:51] epoch: 7305 train-loss: 0.010549166446758641\n",
      "[LOG 20200502-14:19:51] epoch: 7306 train-loss: 0.010549168412884077\n",
      "[LOG 20200502-14:19:51] epoch: 7307 train-loss: 0.010549170585970083\n",
      "[LOG 20200502-14:19:51] epoch: 7308 train-loss: 0.010549172966016663\n",
      "[LOG 20200502-14:19:51] epoch: 7309 train-loss: 0.010549175863464674\n",
      "[LOG 20200502-14:19:52] epoch: 7310 train-loss: 0.0105491793817944\n",
      "[LOG 20200502-14:19:52] epoch: 7311 train-loss: 0.01054918227924241\n",
      "[LOG 20200502-14:19:52] epoch: 7312 train-loss: 0.010549185901052423\n",
      "[LOG 20200502-14:19:52] epoch: 7313 train-loss: 0.010549189833303293\n",
      "[LOG 20200502-14:19:53] epoch: 7314 train-loss: 0.010549194282955594\n",
      "[LOG 20200502-14:19:53] epoch: 7315 train-loss: 0.010549198422167037\n",
      "[LOG 20200502-14:19:53] epoch: 7316 train-loss: 0.01054920307877991\n",
      "[LOG 20200502-14:19:53] epoch: 7317 train-loss: 0.010549208252794214\n",
      "[LOG 20200502-14:19:54] epoch: 7318 train-loss: 0.010549213530288802\n",
      "[LOG 20200502-14:19:54] epoch: 7319 train-loss: 0.010549218911263678\n",
      "[LOG 20200502-14:19:54] epoch: 7320 train-loss: 0.010549223981797695\n",
      "[LOG 20200502-14:19:54] epoch: 7321 train-loss: 0.010549230294095146\n",
      "[LOG 20200502-14:19:54] epoch: 7322 train-loss: 0.010549235985510878\n",
      "[LOG 20200502-14:19:55] epoch: 7323 train-loss: 0.010549242297808329\n",
      "[LOG 20200502-14:19:55] epoch: 7324 train-loss: 0.010549248506625494\n",
      "[LOG 20200502-14:19:55] epoch: 7325 train-loss: 0.010549255336324373\n",
      "[LOG 20200502-14:19:55] epoch: 7326 train-loss: 0.010549261855582396\n",
      "[LOG 20200502-14:19:56] epoch: 7327 train-loss: 0.010549268995722135\n",
      "[LOG 20200502-14:19:56] epoch: 7328 train-loss: 0.01054927623934216\n",
      "[LOG 20200502-14:19:56] epoch: 7329 train-loss: 0.010549283276001612\n",
      "[LOG 20200502-14:19:56] epoch: 7330 train-loss: 0.010549290726582209\n",
      "[LOG 20200502-14:19:56] epoch: 7331 train-loss: 0.010549298177162806\n",
      "[LOG 20200502-14:19:57] epoch: 7332 train-loss: 0.010549305627743403\n",
      "[LOG 20200502-14:19:57] epoch: 7333 train-loss: 0.010549313285284571\n",
      "[LOG 20200502-14:19:57] epoch: 7334 train-loss: 0.010549321460227171\n",
      "[LOG 20200502-14:19:57] epoch: 7335 train-loss: 0.010549328703847196\n",
      "[LOG 20200502-14:19:58] epoch: 7336 train-loss: 0.010549336671829224\n",
      "[LOG 20200502-14:19:58] epoch: 7337 train-loss: 0.010549344950252108\n",
      "[LOG 20200502-14:19:58] epoch: 7338 train-loss: 0.010549353125194708\n",
      "[LOG 20200502-14:19:58] epoch: 7339 train-loss: 0.010549360886216164\n",
      "[LOG 20200502-14:19:58] epoch: 7340 train-loss: 0.010549369268119335\n",
      "[LOG 20200502-14:19:59] epoch: 7341 train-loss: 0.010549377650022507\n",
      "[LOG 20200502-14:19:59] epoch: 7342 train-loss: 0.010549385411043962\n",
      "[LOG 20200502-14:19:59] epoch: 7343 train-loss: 0.010549393482506275\n",
      "[LOG 20200502-14:19:59] epoch: 7344 train-loss: 0.01054940227833059\n",
      "[LOG 20200502-14:19:59] epoch: 7345 train-loss: 0.010549410349792905\n",
      "[LOG 20200502-14:20:00] epoch: 7346 train-loss: 0.010549418835176362\n",
      "[LOG 20200502-14:20:00] epoch: 7347 train-loss: 0.01054942732055982\n",
      "[LOG 20200502-14:20:00] epoch: 7348 train-loss: 0.010549435185061561\n",
      "[LOG 20200502-14:20:00] epoch: 7349 train-loss: 0.010549443566964732\n",
      "[LOG 20200502-14:20:01] epoch: 7350 train-loss: 0.010549451431466473\n",
      "[LOG 20200502-14:20:01] epoch: 7351 train-loss: 0.010549459606409073\n",
      "[LOG 20200502-14:20:01] epoch: 7352 train-loss: 0.010549467677871386\n",
      "[LOG 20200502-14:20:01] epoch: 7353 train-loss: 0.010549475645853413\n",
      "[LOG 20200502-14:20:01] epoch: 7354 train-loss: 0.010549483510355154\n",
      "[LOG 20200502-14:20:02] epoch: 7355 train-loss: 0.010549491478337182\n",
      "[LOG 20200502-14:20:02] epoch: 7356 train-loss: 0.010549499342838923\n",
      "[LOG 20200502-14:20:02] epoch: 7357 train-loss: 0.010549507103860378\n",
      "[LOG 20200502-14:20:02] epoch: 7358 train-loss: 0.010549514554440975\n",
      "[LOG 20200502-14:20:03] epoch: 7359 train-loss: 0.010549522418942716\n",
      "[LOG 20200502-14:20:03] epoch: 7360 train-loss: 0.010549529766043028\n",
      "[LOG 20200502-14:20:03] epoch: 7361 train-loss: 0.010549536492261622\n",
      "[LOG 20200502-14:20:03] epoch: 7362 train-loss: 0.010549543942842219\n",
      "[LOG 20200502-14:20:04] epoch: 7363 train-loss: 0.010549550772541098\n",
      "[LOG 20200502-14:20:04] epoch: 7364 train-loss: 0.01054955729179912\n",
      "[LOG 20200502-14:20:04] epoch: 7365 train-loss: 0.010549563707576858\n",
      "[LOG 20200502-14:20:04] epoch: 7366 train-loss: 0.010549570640756024\n",
      "[LOG 20200502-14:20:04] epoch: 7367 train-loss: 0.010549576746092902\n",
      "[LOG 20200502-14:20:05] epoch: 7368 train-loss: 0.010549583161870638\n",
      "[LOG 20200502-14:20:05] epoch: 7369 train-loss: 0.010549589060246944\n",
      "[LOG 20200502-14:20:05] epoch: 7370 train-loss: 0.010549595062103536\n",
      "[LOG 20200502-14:20:05] epoch: 7371 train-loss: 0.010549600443078412\n",
      "[LOG 20200502-14:20:06] epoch: 7372 train-loss: 0.010549605617092716\n",
      "[LOG 20200502-14:20:06] epoch: 7373 train-loss: 0.010549611101547876\n",
      "[LOG 20200502-14:20:06] epoch: 7374 train-loss: 0.010549616482522752\n",
      "[LOG 20200502-14:20:06] epoch: 7375 train-loss: 0.01054962103565534\n",
      "[LOG 20200502-14:20:06] epoch: 7376 train-loss: 0.01054962600270907\n",
      "[LOG 20200502-14:20:07] epoch: 7377 train-loss: 0.010549630452361371\n",
      "[LOG 20200502-14:20:07] epoch: 7378 train-loss: 0.010549634902013673\n",
      "[LOG 20200502-14:20:07] epoch: 7379 train-loss: 0.010549638730784258\n",
      "[LOG 20200502-14:20:07] epoch: 7380 train-loss: 0.01054964204215341\n",
      "[LOG 20200502-14:20:08] epoch: 7381 train-loss: 0.010549646388325427\n",
      "[LOG 20200502-14:20:08] epoch: 7382 train-loss: 0.01054964918229315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:20:08] epoch: 7383 train-loss: 0.010549652700622877\n",
      "[LOG 20200502-14:20:08] epoch: 7384 train-loss: 0.010549655598070886\n",
      "[LOG 20200502-14:20:08] epoch: 7385 train-loss: 0.010549658288558325\n",
      "[LOG 20200502-14:20:09] epoch: 7386 train-loss: 0.010549660668604903\n",
      "[LOG 20200502-14:20:09] epoch: 7387 train-loss: 0.010549662738210626\n",
      "[LOG 20200502-14:20:09] epoch: 7388 train-loss: 0.010549665014776919\n",
      "[LOG 20200502-14:20:09] epoch: 7389 train-loss: 0.010549666980902353\n",
      "[LOG 20200502-14:20:10] epoch: 7390 train-loss: 0.010549668326146074\n",
      "[LOG 20200502-14:20:10] epoch: 7391 train-loss: 0.010549669774870077\n",
      "[LOG 20200502-14:20:10] epoch: 7392 train-loss: 0.010549671120113797\n",
      "[LOG 20200502-14:20:10] epoch: 7393 train-loss: 0.010549672465357516\n",
      "[LOG 20200502-14:20:11] epoch: 7394 train-loss: 0.010549672672318088\n",
      "[LOG 20200502-14:20:11] epoch: 7395 train-loss: 0.01054967339668009\n",
      "[LOG 20200502-14:20:11] epoch: 7396 train-loss: 0.010549673914081521\n",
      "[LOG 20200502-14:20:11] epoch: 7397 train-loss: 0.010549674328002665\n",
      "[LOG 20200502-14:20:12] epoch: 7398 train-loss: 0.01054967443148295\n",
      "[LOG 20200502-14:20:12] epoch: 7399 train-loss: 0.010549674017561806\n",
      "[LOG 20200502-14:20:12] epoch: 7400 train-loss: 0.010549673707120948\n",
      "[LOG 20200502-14:20:12] epoch: 7401 train-loss: 0.010549673086239232\n",
      "[LOG 20200502-14:20:12] epoch: 7402 train-loss: 0.010549672465357516\n",
      "[LOG 20200502-14:20:13] epoch: 7403 train-loss: 0.010549671844475798\n",
      "[LOG 20200502-14:20:13] epoch: 7404 train-loss: 0.010549671223594083\n",
      "[LOG 20200502-14:20:13] epoch: 7405 train-loss: 0.010549669774870077\n",
      "[LOG 20200502-14:20:13] epoch: 7406 train-loss: 0.010549668533106646\n",
      "[LOG 20200502-14:20:14] epoch: 7407 train-loss: 0.010549667394823499\n",
      "[LOG 20200502-14:20:14] epoch: 7408 train-loss: 0.010549666049579779\n",
      "[LOG 20200502-14:20:14] epoch: 7409 train-loss: 0.010549664497375488\n",
      "[LOG 20200502-14:20:14] epoch: 7410 train-loss: 0.01054966315213177\n",
      "[LOG 20200502-14:20:14] epoch: 7411 train-loss: 0.010549661703407764\n",
      "[LOG 20200502-14:20:15] epoch: 7412 train-loss: 0.010549659633802043\n",
      "[LOG 20200502-14:20:15] epoch: 7413 train-loss: 0.010549658495518897\n",
      "[LOG 20200502-14:20:15] epoch: 7414 train-loss: 0.010549656632873747\n",
      "[LOG 20200502-14:20:16] epoch: 7415 train-loss: 0.010549654356307454\n",
      "[LOG 20200502-14:20:16] epoch: 7416 train-loss: 0.010549653011063734\n",
      "[LOG 20200502-14:20:16] epoch: 7417 train-loss: 0.010549651355379157\n",
      "[LOG 20200502-14:20:17] epoch: 7418 train-loss: 0.010549649699694581\n",
      "[LOG 20200502-14:20:17] epoch: 7419 train-loss: 0.010549648147490289\n",
      "[LOG 20200502-14:20:17] epoch: 7420 train-loss: 0.010549646595285999\n",
      "[LOG 20200502-14:20:17] epoch: 7421 train-loss: 0.010549644629160563\n",
      "[LOG 20200502-14:20:18] epoch: 7422 train-loss: 0.010549643387397131\n",
      "[LOG 20200502-14:20:18] epoch: 7423 train-loss: 0.010549641731712554\n",
      "[LOG 20200502-14:20:18] epoch: 7424 train-loss: 0.010549640179508261\n",
      "[LOG 20200502-14:20:18] epoch: 7425 train-loss: 0.010549639662106832\n",
      "[LOG 20200502-14:20:19] epoch: 7426 train-loss: 0.010549638213382827\n",
      "[LOG 20200502-14:20:19] epoch: 7427 train-loss: 0.010549636868139109\n",
      "[LOG 20200502-14:20:19] epoch: 7428 train-loss: 0.010549636040296819\n",
      "[LOG 20200502-14:20:19] epoch: 7429 train-loss: 0.010549635522895388\n",
      "[LOG 20200502-14:20:19] epoch: 7430 train-loss: 0.010549635108974244\n",
      "[LOG 20200502-14:20:20] epoch: 7431 train-loss: 0.010549634798533387\n",
      "[LOG 20200502-14:20:20] epoch: 7432 train-loss: 0.010549634591572814\n",
      "[LOG 20200502-14:20:20] epoch: 7433 train-loss: 0.010549634798533387\n",
      "[LOG 20200502-14:20:20] epoch: 7434 train-loss: 0.01054963500549396\n",
      "[LOG 20200502-14:20:21] epoch: 7435 train-loss: 0.01054963500549396\n",
      "[LOG 20200502-14:20:21] epoch: 7436 train-loss: 0.010549636247257391\n",
      "[LOG 20200502-14:20:21] epoch: 7437 train-loss: 0.010549636661178537\n",
      "[LOG 20200502-14:20:21] epoch: 7438 train-loss: 0.010549637592501111\n",
      "[LOG 20200502-14:20:21] epoch: 7439 train-loss: 0.010549639041225115\n",
      "[LOG 20200502-14:20:22] epoch: 7440 train-loss: 0.010549641007350551\n",
      "[LOG 20200502-14:20:22] epoch: 7441 train-loss: 0.010549642249113984\n",
      "[LOG 20200502-14:20:22] epoch: 7442 train-loss: 0.010549644525680277\n",
      "[LOG 20200502-14:20:22] epoch: 7443 train-loss: 0.01054964628484514\n",
      "[LOG 20200502-14:20:23] epoch: 7444 train-loss: 0.010549648975332579\n",
      "[LOG 20200502-14:20:23] epoch: 7445 train-loss: 0.010549651769300302\n",
      "[LOG 20200502-14:20:23] epoch: 7446 train-loss: 0.01054965497718917\n",
      "[LOG 20200502-14:20:23] epoch: 7447 train-loss: 0.010549658288558325\n",
      "[LOG 20200502-14:20:24] epoch: 7448 train-loss: 0.01054966128948662\n",
      "[LOG 20200502-14:20:24] epoch: 7449 train-loss: 0.01054966470433606\n",
      "[LOG 20200502-14:20:24] epoch: 7450 train-loss: 0.010549668533106646\n",
      "[LOG 20200502-14:20:24] epoch: 7451 train-loss: 0.010549673293199804\n",
      "[LOG 20200502-14:20:24] epoch: 7452 train-loss: 0.010549677432411246\n",
      "[LOG 20200502-14:20:25] epoch: 7453 train-loss: 0.010549682192504406\n",
      "[LOG 20200502-14:20:25] epoch: 7454 train-loss: 0.010549687159558138\n",
      "[LOG 20200502-14:20:25] epoch: 7455 train-loss: 0.010549691919651296\n",
      "[LOG 20200502-14:20:25] epoch: 7456 train-loss: 0.010549696886705028\n",
      "[LOG 20200502-14:20:26] epoch: 7457 train-loss: 0.010549702992041906\n",
      "[LOG 20200502-14:20:26] epoch: 7458 train-loss: 0.010549708373016782\n",
      "[LOG 20200502-14:20:26] epoch: 7459 train-loss: 0.010549713857471943\n",
      "[LOG 20200502-14:20:26] epoch: 7460 train-loss: 0.010549720480210252\n",
      "[LOG 20200502-14:20:27] epoch: 7461 train-loss: 0.010549726378586557\n",
      "[LOG 20200502-14:20:27] epoch: 7462 train-loss: 0.010549733001324866\n",
      "[LOG 20200502-14:20:27] epoch: 7463 train-loss: 0.010549739003181458\n",
      "[LOG 20200502-14:20:27] epoch: 7464 train-loss: 0.010549745832880339\n",
      "[LOG 20200502-14:20:27] epoch: 7465 train-loss: 0.010549752559098933\n",
      "[LOG 20200502-14:20:28] epoch: 7466 train-loss: 0.01054975918183724\n",
      "[LOG 20200502-14:20:28] epoch: 7467 train-loss: 0.01054976632197698\n",
      "[LOG 20200502-14:20:28] epoch: 7468 train-loss: 0.010549773462116718\n",
      "[LOG 20200502-14:20:28] epoch: 7469 train-loss: 0.010549780395295884\n",
      "[LOG 20200502-14:20:29] epoch: 7470 train-loss: 0.010549788052837053\n",
      "[LOG 20200502-14:20:29] epoch: 7471 train-loss: 0.010549795192976793\n",
      "[LOG 20200502-14:20:29] epoch: 7472 train-loss: 0.010549802333116531\n",
      "[LOG 20200502-14:20:29] epoch: 7473 train-loss: 0.010549810404578844\n",
      "[LOG 20200502-14:20:30] epoch: 7474 train-loss: 0.010549817337758012\n",
      "[LOG 20200502-14:20:30] epoch: 7475 train-loss: 0.01054982447789775\n",
      "[LOG 20200502-14:20:30] epoch: 7476 train-loss: 0.010549832031958632\n",
      "[LOG 20200502-14:20:30] epoch: 7477 train-loss: 0.010549839379058944\n",
      "[LOG 20200502-14:20:30] epoch: 7478 train-loss: 0.010549846622678969\n",
      "[LOG 20200502-14:20:31] epoch: 7479 train-loss: 0.010549853348897563\n",
      "[LOG 20200502-14:20:31] epoch: 7480 train-loss: 0.01054986079947816\n",
      "[LOG 20200502-14:20:31] epoch: 7481 train-loss: 0.01054986814657847\n",
      "[LOG 20200502-14:20:31] epoch: 7482 train-loss: 0.010549875597159067\n",
      "[LOG 20200502-14:20:32] epoch: 7483 train-loss: 0.010549881702495946\n",
      "[LOG 20200502-14:20:32] epoch: 7484 train-loss: 0.010549888221753968\n",
      "[LOG 20200502-14:20:32] epoch: 7485 train-loss: 0.010549894844492277\n",
      "[LOG 20200502-14:20:32] epoch: 7486 train-loss: 0.01054990105330944\n",
      "[LOG 20200502-14:20:32] epoch: 7487 train-loss: 0.010549907365606891\n",
      "[LOG 20200502-14:20:33] epoch: 7488 train-loss: 0.010549913263983197\n",
      "[LOG 20200502-14:20:33] epoch: 7489 train-loss: 0.010549919265839789\n",
      "[LOG 20200502-14:20:33] epoch: 7490 train-loss: 0.010549924439854093\n",
      "[LOG 20200502-14:20:33] epoch: 7491 train-loss: 0.010549929820828967\n",
      "[LOG 20200502-14:20:34] epoch: 7492 train-loss: 0.010549935098323557\n",
      "[LOG 20200502-14:20:34] epoch: 7493 train-loss: 0.010549939651456144\n",
      "[LOG 20200502-14:20:34] epoch: 7494 train-loss: 0.010549943894147873\n",
      "[LOG 20200502-14:20:34] epoch: 7495 train-loss: 0.010549948240319887\n",
      "[LOG 20200502-14:20:34] epoch: 7496 train-loss: 0.010549952069090473\n",
      "[LOG 20200502-14:20:35] epoch: 7497 train-loss: 0.010549955690900484\n",
      "[LOG 20200502-14:20:35] epoch: 7498 train-loss: 0.010549958484868208\n",
      "[LOG 20200502-14:20:35] epoch: 7499 train-loss: 0.01054996158927679\n",
      "[LOG 20200502-14:20:35] epoch: 7500 train-loss: 0.010549963762362799\n",
      "[LOG 20200502-14:20:36] epoch: 7501 train-loss: 0.01054996634936995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:20:36] epoch: 7502 train-loss: 0.010549967901574241\n",
      "[LOG 20200502-14:20:36] epoch: 7503 train-loss: 0.010549969557258818\n",
      "[LOG 20200502-14:20:36] epoch: 7504 train-loss: 0.010549971005982824\n",
      "[LOG 20200502-14:20:36] epoch: 7505 train-loss: 0.010549971419903968\n",
      "[LOG 20200502-14:20:37] epoch: 7506 train-loss: 0.010549971833825111\n",
      "[LOG 20200502-14:20:37] epoch: 7507 train-loss: 0.010549971937305398\n",
      "[LOG 20200502-14:20:37] epoch: 7508 train-loss: 0.01054997162686454\n",
      "[LOG 20200502-14:20:37] epoch: 7509 train-loss: 0.010549971109463109\n",
      "[LOG 20200502-14:20:38] epoch: 7510 train-loss: 0.010549970695541965\n",
      "[LOG 20200502-14:20:38] epoch: 7511 train-loss: 0.0105499689363771\n",
      "[LOG 20200502-14:20:38] epoch: 7512 train-loss: 0.010549967177212238\n",
      "[LOG 20200502-14:20:38] epoch: 7513 train-loss: 0.010549965211086802\n",
      "[LOG 20200502-14:20:38] epoch: 7514 train-loss: 0.01054996345192194\n",
      "[LOG 20200502-14:20:39] epoch: 7515 train-loss: 0.010549960657954216\n",
      "[LOG 20200502-14:20:39] epoch: 7516 train-loss: 0.010549957967466779\n",
      "[LOG 20200502-14:20:39] epoch: 7517 train-loss: 0.010549954863058196\n",
      "[LOG 20200502-14:20:39] epoch: 7518 train-loss: 0.01054995134472847\n",
      "[LOG 20200502-14:20:40] epoch: 7519 train-loss: 0.010549948136839602\n",
      "[LOG 20200502-14:20:40] epoch: 7520 train-loss: 0.010549944411549304\n",
      "[LOG 20200502-14:20:40] epoch: 7521 train-loss: 0.010549940272337861\n",
      "[LOG 20200502-14:20:40] epoch: 7522 train-loss: 0.010549936029646132\n",
      "[LOG 20200502-14:20:41] epoch: 7523 train-loss: 0.010549932097395262\n",
      "[LOG 20200502-14:20:41] epoch: 7524 train-loss: 0.010549927337302102\n",
      "[LOG 20200502-14:20:41] epoch: 7525 train-loss: 0.010549923301570945\n",
      "[LOG 20200502-14:20:41] epoch: 7526 train-loss: 0.010549918644958071\n",
      "[LOG 20200502-14:20:41] epoch: 7527 train-loss: 0.010549914195305772\n",
      "[LOG 20200502-14:20:42] epoch: 7528 train-loss: 0.010549909642173184\n",
      "[LOG 20200502-14:20:42] epoch: 7529 train-loss: 0.010549904985560311\n",
      "[LOG 20200502-14:20:42] epoch: 7530 train-loss: 0.01054990053590801\n",
      "[LOG 20200502-14:20:42] epoch: 7531 train-loss: 0.010549895879295137\n",
      "[LOG 20200502-14:20:43] epoch: 7532 train-loss: 0.010549891015721692\n",
      "[LOG 20200502-14:20:43] epoch: 7533 train-loss: 0.01054988656606939\n",
      "[LOG 20200502-14:20:43] epoch: 7534 train-loss: 0.010549882426857948\n",
      "[LOG 20200502-14:20:43] epoch: 7535 train-loss: 0.010549878805047937\n",
      "[LOG 20200502-14:20:43] epoch: 7536 train-loss: 0.010549874148435064\n",
      "[LOG 20200502-14:20:44] epoch: 7537 train-loss: 0.010549869698782762\n",
      "[LOG 20200502-14:20:44] epoch: 7538 train-loss: 0.010549866283933321\n",
      "[LOG 20200502-14:20:44] epoch: 7539 train-loss: 0.010549862558643023\n",
      "[LOG 20200502-14:20:44] epoch: 7540 train-loss: 0.010549859143793583\n",
      "[LOG 20200502-14:20:44] epoch: 7541 train-loss: 0.010549856142865287\n",
      "[LOG 20200502-14:20:45] epoch: 7542 train-loss: 0.01054985293497642\n",
      "[LOG 20200502-14:20:45] epoch: 7543 train-loss: 0.010549850141008696\n",
      "[LOG 20200502-14:20:45] epoch: 7544 train-loss: 0.010549847967922688\n",
      "[LOG 20200502-14:20:45] epoch: 7545 train-loss: 0.010549845173954964\n",
      "[LOG 20200502-14:20:46] epoch: 7546 train-loss: 0.010549843518270386\n",
      "[LOG 20200502-14:20:46] epoch: 7547 train-loss: 0.010549841759105524\n",
      "[LOG 20200502-14:20:46] epoch: 7548 train-loss: 0.010549840103420947\n",
      "[LOG 20200502-14:20:46] epoch: 7549 train-loss: 0.01054983813729551\n",
      "[LOG 20200502-14:20:47] epoch: 7550 train-loss: 0.010549837412933508\n",
      "[LOG 20200502-14:20:47] epoch: 7551 train-loss: 0.010549837205972936\n",
      "[LOG 20200502-14:20:47] epoch: 7552 train-loss: 0.010549836792051792\n",
      "[LOG 20200502-14:20:47] epoch: 7553 train-loss: 0.010549836792051792\n",
      "[LOG 20200502-14:20:47] epoch: 7554 train-loss: 0.01054983658509122\n",
      "[LOG 20200502-14:20:48] epoch: 7555 train-loss: 0.010549836999012364\n",
      "[LOG 20200502-14:20:48] epoch: 7556 train-loss: 0.01054983813729551\n",
      "[LOG 20200502-14:20:48] epoch: 7557 train-loss: 0.010549839172098372\n",
      "[LOG 20200502-14:20:48] epoch: 7558 train-loss: 0.010549840103420947\n",
      "[LOG 20200502-14:20:49] epoch: 7559 train-loss: 0.010549841552144952\n",
      "[LOG 20200502-14:20:49] epoch: 7560 train-loss: 0.010549843518270386\n",
      "[LOG 20200502-14:20:49] epoch: 7561 train-loss: 0.010549845484395822\n",
      "[LOG 20200502-14:20:49] epoch: 7562 train-loss: 0.010549848071402974\n",
      "[LOG 20200502-14:20:49] epoch: 7563 train-loss: 0.01054985024448898\n",
      "[LOG 20200502-14:20:50] epoch: 7564 train-loss: 0.01054985293497642\n",
      "[LOG 20200502-14:20:50] epoch: 7565 train-loss: 0.010549856453306146\n",
      "[LOG 20200502-14:20:50] epoch: 7566 train-loss: 0.010549859557714727\n",
      "[LOG 20200502-14:20:50] epoch: 7567 train-loss: 0.010549862869083881\n",
      "[LOG 20200502-14:20:51] epoch: 7568 train-loss: 0.010549866180453036\n",
      "[LOG 20200502-14:20:51] epoch: 7569 train-loss: 0.010549871147506766\n",
      "[LOG 20200502-14:20:51] epoch: 7570 train-loss: 0.010549875079757638\n",
      "[LOG 20200502-14:20:51] epoch: 7571 train-loss: 0.010549879012008509\n",
      "[LOG 20200502-14:20:52] epoch: 7572 train-loss: 0.010549884082542526\n",
      "[LOG 20200502-14:20:52] epoch: 7573 train-loss: 0.010549888532194827\n",
      "[LOG 20200502-14:20:52] epoch: 7574 train-loss: 0.010549893602728844\n",
      "[LOG 20200502-14:20:52] epoch: 7575 train-loss: 0.010549899087184005\n",
      "[LOG 20200502-14:20:53] epoch: 7576 train-loss: 0.010549904157718023\n",
      "[LOG 20200502-14:20:53] epoch: 7577 train-loss: 0.010549909745653471\n",
      "[LOG 20200502-14:20:53] epoch: 7578 train-loss: 0.010549914816187488\n",
      "[LOG 20200502-14:20:53] epoch: 7579 train-loss: 0.01054992050760322\n",
      "[LOG 20200502-14:20:54] epoch: 7580 train-loss: 0.010549926716420386\n",
      "[LOG 20200502-14:20:54] epoch: 7581 train-loss: 0.01054993292523755\n",
      "[LOG 20200502-14:20:54] epoch: 7582 train-loss: 0.010549939134054713\n",
      "[LOG 20200502-14:20:55] epoch: 7583 train-loss: 0.010549945756793022\n",
      "[LOG 20200502-14:20:55] epoch: 7584 train-loss: 0.010549951448208757\n",
      "[LOG 20200502-14:20:55] epoch: 7585 train-loss: 0.010549958381387923\n",
      "[LOG 20200502-14:20:55] epoch: 7586 train-loss: 0.010549964693685373\n",
      "[LOG 20200502-14:20:55] epoch: 7587 train-loss: 0.010549971523384253\n",
      "[LOG 20200502-14:20:56] epoch: 7588 train-loss: 0.010549978249602847\n",
      "[LOG 20200502-14:20:56] epoch: 7589 train-loss: 0.010549984975821443\n",
      "[LOG 20200502-14:20:56] epoch: 7590 train-loss: 0.010549991184638606\n",
      "[LOG 20200502-14:20:56] epoch: 7591 train-loss: 0.010549998531738916\n",
      "[LOG 20200502-14:20:57] epoch: 7592 train-loss: 0.010550005361437798\n",
      "[LOG 20200502-14:20:57] epoch: 7593 train-loss: 0.010550012294616964\n",
      "[LOG 20200502-14:20:57] epoch: 7594 train-loss: 0.010550019124315845\n",
      "[LOG 20200502-14:20:57] epoch: 7595 train-loss: 0.010550026678376727\n",
      "[LOG 20200502-14:20:58] epoch: 7596 train-loss: 0.010550033094154464\n",
      "[LOG 20200502-14:20:58] epoch: 7597 train-loss: 0.010550040441254774\n",
      "[LOG 20200502-14:20:58] epoch: 7598 train-loss: 0.01055004665007194\n",
      "[LOG 20200502-14:20:58] epoch: 7599 train-loss: 0.01055005368673139\n",
      "[LOG 20200502-14:20:58] epoch: 7600 train-loss: 0.010550061033831703\n",
      "[LOG 20200502-14:20:59] epoch: 7601 train-loss: 0.010550067967010869\n",
      "[LOG 20200502-14:20:59] epoch: 7602 train-loss: 0.010550074382788606\n",
      "[LOG 20200502-14:20:59] epoch: 7603 train-loss: 0.010550081522928344\n",
      "[LOG 20200502-14:20:59] epoch: 7604 train-loss: 0.010550088663068082\n",
      "[LOG 20200502-14:20:59] epoch: 7605 train-loss: 0.010550095492766963\n",
      "[LOG 20200502-14:21:00] epoch: 7606 train-loss: 0.010550101805064414\n",
      "[LOG 20200502-14:21:00] epoch: 7607 train-loss: 0.010550108427802721\n",
      "[LOG 20200502-14:21:00] epoch: 7608 train-loss: 0.01055011505054103\n",
      "[LOG 20200502-14:21:00] epoch: 7609 train-loss: 0.010550121052397622\n",
      "[LOG 20200502-14:21:01] epoch: 7610 train-loss: 0.010550127468175359\n",
      "[LOG 20200502-14:21:01] epoch: 7611 train-loss: 0.010550134297874238\n",
      "[LOG 20200502-14:21:01] epoch: 7612 train-loss: 0.010550140403211117\n",
      "[LOG 20200502-14:21:01] epoch: 7613 train-loss: 0.010550146198107136\n",
      "[LOG 20200502-14:21:02] epoch: 7614 train-loss: 0.010550152303444015\n",
      "[LOG 20200502-14:21:02] epoch: 7615 train-loss: 0.010550158305300606\n",
      "[LOG 20200502-14:21:02] epoch: 7616 train-loss: 0.010550163686275482\n",
      "[LOG 20200502-14:21:02] epoch: 7617 train-loss: 0.010550169584651789\n",
      "[LOG 20200502-14:21:02] epoch: 7618 train-loss: 0.010550174965626664\n",
      "[LOG 20200502-14:21:03] epoch: 7619 train-loss: 0.010550180036160681\n",
      "[LOG 20200502-14:21:03] epoch: 7620 train-loss: 0.010550185417135557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:21:03] epoch: 7621 train-loss: 0.010550190384189287\n",
      "[LOG 20200502-14:21:03] epoch: 7622 train-loss: 0.010550194937321875\n",
      "[LOG 20200502-14:21:04] epoch: 7623 train-loss: 0.01055019980089532\n",
      "[LOG 20200502-14:21:04] epoch: 7624 train-loss: 0.010550205181870196\n",
      "[LOG 20200502-14:21:04] epoch: 7625 train-loss: 0.010550209114121066\n",
      "[LOG 20200502-14:21:04] epoch: 7626 train-loss: 0.010550213667253653\n",
      "[LOG 20200502-14:21:05] epoch: 7627 train-loss: 0.010550217806465097\n",
      "[LOG 20200502-14:21:05] epoch: 7628 train-loss: 0.010550221738715967\n",
      "[LOG 20200502-14:21:05] epoch: 7629 train-loss: 0.010550225464006266\n",
      "[LOG 20200502-14:21:05] epoch: 7630 train-loss: 0.010550229396257136\n",
      "[LOG 20200502-14:21:05] epoch: 7631 train-loss: 0.010550232914586863\n",
      "[LOG 20200502-14:21:06] epoch: 7632 train-loss: 0.010550236536396874\n",
      "[LOG 20200502-14:21:06] epoch: 7633 train-loss: 0.010550239330364598\n",
      "[LOG 20200502-14:21:06] epoch: 7634 train-loss: 0.010550242331292894\n",
      "[LOG 20200502-14:21:06] epoch: 7635 train-loss: 0.010550245435701476\n",
      "[LOG 20200502-14:21:07] epoch: 7636 train-loss: 0.010550247919228341\n",
      "[LOG 20200502-14:21:07] epoch: 7637 train-loss: 0.010550250816676352\n",
      "[LOG 20200502-14:21:07] epoch: 7638 train-loss: 0.010550253093242645\n",
      "[LOG 20200502-14:21:07] epoch: 7639 train-loss: 0.010550254955887794\n",
      "[LOG 20200502-14:21:07] epoch: 7640 train-loss: 0.010550257128973803\n",
      "[LOG 20200502-14:21:08] epoch: 7641 train-loss: 0.010550258991618952\n",
      "[LOG 20200502-14:21:08] epoch: 7642 train-loss: 0.010550260750783814\n",
      "[LOG 20200502-14:21:08] epoch: 7643 train-loss: 0.010550262406468391\n",
      "[LOG 20200502-14:21:08] epoch: 7644 train-loss: 0.010550263337790966\n",
      "[LOG 20200502-14:21:08] epoch: 7645 train-loss: 0.010550264683034685\n",
      "[LOG 20200502-14:21:09] epoch: 7646 train-loss: 0.010550265924798118\n",
      "[LOG 20200502-14:21:09] epoch: 7647 train-loss: 0.01055026664916012\n",
      "[LOG 20200502-14:21:09] epoch: 7648 train-loss: 0.010550267166561551\n",
      "[LOG 20200502-14:21:09] epoch: 7649 train-loss: 0.010550268201364411\n",
      "[LOG 20200502-14:21:10] epoch: 7650 train-loss: 0.010550268615285555\n",
      "[LOG 20200502-14:21:10] epoch: 7651 train-loss: 0.010550269132686986\n",
      "[LOG 20200502-14:21:10] epoch: 7652 train-loss: 0.0105502690292067\n",
      "[LOG 20200502-14:21:10] epoch: 7653 train-loss: 0.010550269339647558\n",
      "[LOG 20200502-14:21:11] epoch: 7654 train-loss: 0.010550269339647558\n",
      "[LOG 20200502-14:21:11] epoch: 7655 train-loss: 0.010550269650088416\n",
      "[LOG 20200502-14:21:11] epoch: 7656 train-loss: 0.010550269339647558\n",
      "[LOG 20200502-14:21:11] epoch: 7657 train-loss: 0.010550268925726414\n",
      "[LOG 20200502-14:21:11] epoch: 7658 train-loss: 0.010550268925726414\n",
      "[LOG 20200502-14:21:12] epoch: 7659 train-loss: 0.010550268718765842\n",
      "[LOG 20200502-14:21:12] epoch: 7660 train-loss: 0.010550267580482695\n",
      "[LOG 20200502-14:21:12] epoch: 7661 train-loss: 0.010550267580482695\n",
      "[LOG 20200502-14:21:13] epoch: 7662 train-loss: 0.010550266959600978\n",
      "[LOG 20200502-14:21:13] epoch: 7663 train-loss: 0.010550266545679834\n",
      "[LOG 20200502-14:21:13] epoch: 7664 train-loss: 0.010550265717837546\n",
      "[LOG 20200502-14:21:13] epoch: 7665 train-loss: 0.010550265096955828\n",
      "[LOG 20200502-14:21:14] epoch: 7666 train-loss: 0.010550264372593827\n",
      "[LOG 20200502-14:21:14] epoch: 7667 train-loss: 0.010550263648231825\n",
      "[LOG 20200502-14:21:14] epoch: 7668 train-loss: 0.01055026323431068\n",
      "[LOG 20200502-14:21:14] epoch: 7669 train-loss: 0.010550262509948678\n",
      "[LOG 20200502-14:21:15] epoch: 7670 train-loss: 0.01055026188906696\n",
      "[LOG 20200502-14:21:15] epoch: 7671 train-loss: 0.010550261061224673\n",
      "[LOG 20200502-14:21:15] epoch: 7672 train-loss: 0.010550260854264101\n",
      "[LOG 20200502-14:21:16] epoch: 7673 train-loss: 0.010550260543823242\n",
      "[LOG 20200502-14:21:16] epoch: 7674 train-loss: 0.010550260026421811\n",
      "[LOG 20200502-14:21:16] epoch: 7675 train-loss: 0.01055025981946124\n",
      "[LOG 20200502-14:21:16] epoch: 7676 train-loss: 0.010550259302059809\n",
      "[LOG 20200502-14:21:17] epoch: 7677 train-loss: 0.010550259198579524\n",
      "[LOG 20200502-14:21:17] epoch: 7678 train-loss: 0.010550259612500668\n",
      "[LOG 20200502-14:21:17] epoch: 7679 train-loss: 0.010550258888138665\n",
      "[LOG 20200502-14:21:17] epoch: 7680 train-loss: 0.010550259612500668\n",
      "[LOG 20200502-14:21:18] epoch: 7681 train-loss: 0.010550260233382383\n",
      "[LOG 20200502-14:21:18] epoch: 7682 train-loss: 0.01055026033686267\n",
      "[LOG 20200502-14:21:18] epoch: 7683 train-loss: 0.010550260543823242\n",
      "[LOG 20200502-14:21:18] epoch: 7684 train-loss: 0.01055026219950782\n",
      "[LOG 20200502-14:21:19] epoch: 7685 train-loss: 0.010550262923869822\n",
      "[LOG 20200502-14:21:19] epoch: 7686 train-loss: 0.010550263648231825\n",
      "[LOG 20200502-14:21:19] epoch: 7687 train-loss: 0.010550265200436115\n",
      "[LOG 20200502-14:21:19] epoch: 7688 train-loss: 0.010550266235238977\n",
      "[LOG 20200502-14:21:19] epoch: 7689 train-loss: 0.010550268097884126\n",
      "[LOG 20200502-14:21:20] epoch: 7690 train-loss: 0.010550269753568702\n",
      "[LOG 20200502-14:21:20] epoch: 7691 train-loss: 0.01055027161621385\n",
      "[LOG 20200502-14:21:20] epoch: 7692 train-loss: 0.010550273685819574\n",
      "[LOG 20200502-14:21:20] epoch: 7693 train-loss: 0.010550275755425295\n",
      "[LOG 20200502-14:21:21] epoch: 7694 train-loss: 0.010550277928511301\n",
      "[LOG 20200502-14:21:21] epoch: 7695 train-loss: 0.01055028061899874\n",
      "[LOG 20200502-14:21:21] epoch: 7696 train-loss: 0.010550283412966464\n",
      "[LOG 20200502-14:21:21] epoch: 7697 train-loss: 0.0105502861034539\n",
      "[LOG 20200502-14:21:21] epoch: 7698 train-loss: 0.010550289414823055\n",
      "[LOG 20200502-14:21:22] epoch: 7699 train-loss: 0.010550292933152782\n",
      "[LOG 20200502-14:21:22] epoch: 7700 train-loss: 0.010550296451482508\n",
      "[LOG 20200502-14:21:22] epoch: 7701 train-loss: 0.010550300176772807\n",
      "[LOG 20200502-14:21:22] epoch: 7702 train-loss: 0.010550304212503962\n",
      "[LOG 20200502-14:21:23] epoch: 7703 train-loss: 0.010550308144754834\n",
      "[LOG 20200502-14:21:23] epoch: 7704 train-loss: 0.010550312283966277\n",
      "[LOG 20200502-14:21:23] epoch: 7705 train-loss: 0.010550316423177719\n",
      "[LOG 20200502-14:21:23] epoch: 7706 train-loss: 0.010550321079790592\n",
      "[LOG 20200502-14:21:24] epoch: 7707 train-loss: 0.010550325943364037\n",
      "[LOG 20200502-14:21:24] epoch: 7708 train-loss: 0.010550330186055766\n",
      "[LOG 20200502-14:21:24] epoch: 7709 train-loss: 0.010550335670510927\n",
      "[LOG 20200502-14:21:24] epoch: 7710 train-loss: 0.010550340430604087\n",
      "[LOG 20200502-14:21:24] epoch: 7711 train-loss: 0.010550345397657819\n",
      "[LOG 20200502-14:21:25] epoch: 7712 train-loss: 0.010550350778632693\n",
      "[LOG 20200502-14:21:25] epoch: 7713 train-loss: 0.010550356159607569\n",
      "[LOG 20200502-14:21:25] epoch: 7714 train-loss: 0.01055036143710216\n",
      "[LOG 20200502-14:21:25] epoch: 7715 train-loss: 0.010550366818077035\n",
      "[LOG 20200502-14:21:26] epoch: 7716 train-loss: 0.01055037219905191\n",
      "[LOG 20200502-14:21:26] epoch: 7717 train-loss: 0.010550378200908503\n",
      "[LOG 20200502-14:21:26] epoch: 7718 train-loss: 0.010550383581883378\n",
      "[LOG 20200502-14:21:26] epoch: 7719 train-loss: 0.010550388962858252\n",
      "[LOG 20200502-14:21:26] epoch: 7720 train-loss: 0.010550394654273987\n",
      "[LOG 20200502-14:21:27] epoch: 7721 train-loss: 0.010550400552650293\n",
      "[LOG 20200502-14:21:27] epoch: 7722 train-loss: 0.010550405933625169\n",
      "[LOG 20200502-14:21:27] epoch: 7723 train-loss: 0.010550411728521189\n",
      "[LOG 20200502-14:21:27] epoch: 7724 train-loss: 0.010550417109496064\n",
      "[LOG 20200502-14:21:28] epoch: 7725 train-loss: 0.010550422490470938\n",
      "[LOG 20200502-14:21:28] epoch: 7726 train-loss: 0.010550427561004957\n",
      "[LOG 20200502-14:21:28] epoch: 7727 train-loss: 0.010550433355900977\n",
      "[LOG 20200502-14:21:28] epoch: 7728 train-loss: 0.010550438115994135\n",
      "[LOG 20200502-14:21:28] epoch: 7729 train-loss: 0.010550443290008439\n",
      "[LOG 20200502-14:21:29] epoch: 7730 train-loss: 0.010550448567503028\n",
      "[LOG 20200502-14:21:29] epoch: 7731 train-loss: 0.010550453638037046\n",
      "[LOG 20200502-14:21:29] epoch: 7732 train-loss: 0.010550457880728774\n",
      "[LOG 20200502-14:21:29] epoch: 7733 train-loss: 0.010550462640821934\n",
      "[LOG 20200502-14:21:30] epoch: 7734 train-loss: 0.010550466469592519\n",
      "[LOG 20200502-14:21:30] epoch: 7735 train-loss: 0.010550471022725105\n",
      "[LOG 20200502-14:21:30] epoch: 7736 train-loss: 0.010550474748015404\n",
      "[LOG 20200502-14:21:30] epoch: 7737 train-loss: 0.010550478162864843\n",
      "[LOG 20200502-14:21:30] epoch: 7738 train-loss: 0.010550482198596\n",
      "[LOG 20200502-14:21:31] epoch: 7739 train-loss: 0.010550484785603153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:21:31] epoch: 7740 train-loss: 0.010550488200452592\n",
      "[LOG 20200502-14:21:31] epoch: 7741 train-loss: 0.010550490787459744\n",
      "[LOG 20200502-14:21:31] epoch: 7742 train-loss: 0.010550493064026037\n",
      "[LOG 20200502-14:21:32] epoch: 7743 train-loss: 0.010550495547552904\n",
      "[LOG 20200502-14:21:32] epoch: 7744 train-loss: 0.010550496996276908\n",
      "[LOG 20200502-14:21:32] epoch: 7745 train-loss: 0.010550498651961485\n",
      "[LOG 20200502-14:21:32] epoch: 7746 train-loss: 0.010550500411126349\n",
      "[LOG 20200502-14:21:32] epoch: 7747 train-loss: 0.010550501445929209\n",
      "[LOG 20200502-14:21:33] epoch: 7748 train-loss: 0.01055050196333064\n",
      "[LOG 20200502-14:21:33] epoch: 7749 train-loss: 0.010550502273771498\n",
      "[LOG 20200502-14:21:33] epoch: 7750 train-loss: 0.01055050248073207\n",
      "[LOG 20200502-14:21:33] epoch: 7751 train-loss: 0.010550502377251783\n",
      "[LOG 20200502-14:21:34] epoch: 7752 train-loss: 0.01055050196333064\n",
      "[LOG 20200502-14:21:34] epoch: 7753 train-loss: 0.010550501445929209\n",
      "[LOG 20200502-14:21:34] epoch: 7754 train-loss: 0.010550500204165777\n",
      "[LOG 20200502-14:21:34] epoch: 7755 train-loss: 0.01055049906588263\n",
      "[LOG 20200502-14:21:34] epoch: 7756 train-loss: 0.010550497617158625\n",
      "[LOG 20200502-14:21:35] epoch: 7757 train-loss: 0.010550496271914907\n",
      "[LOG 20200502-14:21:35] epoch: 7758 train-loss: 0.010550494202309184\n",
      "[LOG 20200502-14:21:35] epoch: 7759 train-loss: 0.010550492339664035\n",
      "[LOG 20200502-14:21:35] epoch: 7760 train-loss: 0.010550489649176598\n",
      "[LOG 20200502-14:21:36] epoch: 7761 train-loss: 0.010550486751728587\n",
      "[LOG 20200502-14:21:36] epoch: 7762 train-loss: 0.010550484475162294\n",
      "[LOG 20200502-14:21:36] epoch: 7763 train-loss: 0.010550481474233998\n",
      "[LOG 20200502-14:21:36] epoch: 7764 train-loss: 0.01055047826634513\n",
      "[LOG 20200502-14:21:36] epoch: 7765 train-loss: 0.010550475472377406\n",
      "[LOG 20200502-14:21:37] epoch: 7766 train-loss: 0.010550472161008252\n",
      "[LOG 20200502-14:21:37] epoch: 7767 train-loss: 0.01055046853919824\n",
      "[LOG 20200502-14:21:37] epoch: 7768 train-loss: 0.01055046533130937\n",
      "[LOG 20200502-14:21:37] epoch: 7769 train-loss: 0.010550461916459931\n",
      "[LOG 20200502-14:21:38] epoch: 7770 train-loss: 0.010550458501610491\n",
      "[LOG 20200502-14:21:38] epoch: 7771 train-loss: 0.010550454672839906\n",
      "[LOG 20200502-14:21:38] epoch: 7772 train-loss: 0.010550451464951038\n",
      "[LOG 20200502-14:21:38] epoch: 7773 train-loss: 0.010550447636180453\n",
      "[LOG 20200502-14:21:39] epoch: 7774 train-loss: 0.010550444945693016\n",
      "[LOG 20200502-14:21:39] epoch: 7775 train-loss: 0.010550441634323861\n",
      "[LOG 20200502-14:21:39] epoch: 7776 train-loss: 0.010550438115994135\n",
      "[LOG 20200502-14:21:39] epoch: 7777 train-loss: 0.010550434908105267\n",
      "[LOG 20200502-14:21:40] epoch: 7778 train-loss: 0.010550431907176971\n",
      "[LOG 20200502-14:21:40] epoch: 7779 train-loss: 0.01055042900972896\n",
      "[LOG 20200502-14:21:40] epoch: 7780 train-loss: 0.010550426319241524\n",
      "[LOG 20200502-14:21:40] epoch: 7781 train-loss: 0.010550423628754087\n",
      "[LOG 20200502-14:21:40] epoch: 7782 train-loss: 0.010550421559148364\n",
      "[LOG 20200502-14:21:41] epoch: 7783 train-loss: 0.010550419179101786\n",
      "[LOG 20200502-14:21:41] epoch: 7784 train-loss: 0.010550417626897493\n",
      "[LOG 20200502-14:21:41] epoch: 7785 train-loss: 0.010550415660772059\n",
      "[LOG 20200502-14:21:41] epoch: 7786 train-loss: 0.010550413901607195\n",
      "[LOG 20200502-14:21:42] epoch: 7787 train-loss: 0.010550412452883191\n",
      "[LOG 20200502-14:21:42] epoch: 7788 train-loss: 0.01055041141808033\n",
      "[LOG 20200502-14:21:42] epoch: 7789 train-loss: 0.010550410176316896\n",
      "[LOG 20200502-14:21:42] epoch: 7790 train-loss: 0.010550410279797183\n",
      "[LOG 20200502-14:21:43] epoch: 7791 train-loss: 0.010550409762395753\n",
      "[LOG 20200502-14:21:43] epoch: 7792 train-loss: 0.010550409451954894\n",
      "[LOG 20200502-14:21:43] epoch: 7793 train-loss: 0.010550409451954894\n",
      "[LOG 20200502-14:21:43] epoch: 7794 train-loss: 0.010550409658915468\n",
      "[LOG 20200502-14:21:43] epoch: 7795 train-loss: 0.010550410176316896\n",
      "[LOG 20200502-14:21:44] epoch: 7796 train-loss: 0.0105504109006789\n",
      "[LOG 20200502-14:21:44] epoch: 7797 train-loss: 0.010550412142442333\n",
      "[LOG 20200502-14:21:44] epoch: 7798 train-loss: 0.010550413384205766\n",
      "[LOG 20200502-14:21:44] epoch: 7799 train-loss: 0.010550414729449484\n",
      "[LOG 20200502-14:21:45] epoch: 7800 train-loss: 0.010550415971212916\n",
      "[LOG 20200502-14:21:45] epoch: 7801 train-loss: 0.010550418351259496\n",
      "[LOG 20200502-14:21:45] epoch: 7802 train-loss: 0.010550420213904645\n",
      "[LOG 20200502-14:21:45] epoch: 7803 train-loss: 0.010550422904392084\n",
      "[LOG 20200502-14:21:45] epoch: 7804 train-loss: 0.010550425284438662\n",
      "[LOG 20200502-14:21:46] epoch: 7805 train-loss: 0.010550427767965529\n",
      "[LOG 20200502-14:21:46] epoch: 7806 train-loss: 0.010550430458452966\n",
      "[LOG 20200502-14:21:46] epoch: 7807 train-loss: 0.010550433873302408\n",
      "[LOG 20200502-14:21:46] epoch: 7808 train-loss: 0.010550437391632132\n",
      "[LOG 20200502-14:21:47] epoch: 7809 train-loss: 0.010550440909961859\n",
      "[LOG 20200502-14:21:47] epoch: 7810 train-loss: 0.010550445152653588\n",
      "[LOG 20200502-14:21:47] epoch: 7811 train-loss: 0.010550448670983315\n",
      "[LOG 20200502-14:21:47] epoch: 7812 train-loss: 0.010550452810194757\n",
      "[LOG 20200502-14:21:47] epoch: 7813 train-loss: 0.010550456845925914\n",
      "[LOG 20200502-14:21:48] epoch: 7814 train-loss: 0.010550461088617643\n",
      "[LOG 20200502-14:21:48] epoch: 7815 train-loss: 0.010550466055671373\n",
      "[LOG 20200502-14:21:48] epoch: 7816 train-loss: 0.01055047040184339\n",
      "[LOG 20200502-14:21:48] epoch: 7817 train-loss: 0.010550474644535117\n",
      "[LOG 20200502-14:21:49] epoch: 7818 train-loss: 0.010550479922029708\n",
      "[LOG 20200502-14:21:49] epoch: 7819 train-loss: 0.010550484785603153\n",
      "[LOG 20200502-14:21:49] epoch: 7820 train-loss: 0.010550489959617456\n",
      "[LOG 20200502-14:21:49] epoch: 7821 train-loss: 0.01055049461623033\n",
      "[LOG 20200502-14:21:49] epoch: 7822 train-loss: 0.010550500618086921\n",
      "[LOG 20200502-14:21:50] epoch: 7823 train-loss: 0.010550505999061797\n",
      "[LOG 20200502-14:21:50] epoch: 7824 train-loss: 0.0105505111730761\n",
      "[LOG 20200502-14:21:50] epoch: 7825 train-loss: 0.010550516554050975\n",
      "[LOG 20200502-14:21:50] epoch: 7826 train-loss: 0.010550522141986422\n",
      "[LOG 20200502-14:21:51] epoch: 7827 train-loss: 0.010550527109040154\n",
      "[LOG 20200502-14:21:51] epoch: 7828 train-loss: 0.010550533317857318\n",
      "[LOG 20200502-14:21:51] epoch: 7829 train-loss: 0.01055053880231248\n",
      "[LOG 20200502-14:21:51] epoch: 7830 train-loss: 0.0105505445972085\n",
      "[LOG 20200502-14:21:52] epoch: 7831 train-loss: 0.01055055008166366\n",
      "[LOG 20200502-14:21:52] epoch: 7832 train-loss: 0.010550556290480826\n",
      "[LOG 20200502-14:21:52] epoch: 7833 train-loss: 0.010550562085376846\n",
      "[LOG 20200502-14:21:52] epoch: 7834 train-loss: 0.010550567052430578\n",
      "[LOG 20200502-14:21:52] epoch: 7835 train-loss: 0.010550573157767454\n",
      "[LOG 20200502-14:21:53] epoch: 7836 train-loss: 0.010550578952663474\n",
      "[LOG 20200502-14:21:53] epoch: 7837 train-loss: 0.010550584747559495\n",
      "[LOG 20200502-14:21:53] epoch: 7838 train-loss: 0.010550590232014656\n",
      "[LOG 20200502-14:21:53] epoch: 7839 train-loss: 0.01055059540602896\n",
      "[LOG 20200502-14:21:54] epoch: 7840 train-loss: 0.01055060120092498\n",
      "[LOG 20200502-14:21:54] epoch: 7841 train-loss: 0.010550607202781571\n",
      "[LOG 20200502-14:21:54] epoch: 7842 train-loss: 0.010550612169835303\n",
      "[LOG 20200502-14:21:54] epoch: 7843 train-loss: 0.010550617447329892\n",
      "[LOG 20200502-14:21:54] epoch: 7844 train-loss: 0.010550622828304768\n",
      "[LOG 20200502-14:21:55] epoch: 7845 train-loss: 0.010550627691878213\n",
      "[LOG 20200502-14:21:55] epoch: 7846 train-loss: 0.010550632969372802\n",
      "[LOG 20200502-14:21:55] epoch: 7847 train-loss: 0.010550638350347677\n",
      "[LOG 20200502-14:21:55] epoch: 7848 train-loss: 0.010550643420881696\n",
      "[LOG 20200502-14:21:56] epoch: 7849 train-loss: 0.010550648387935426\n",
      "[LOG 20200502-14:21:56] epoch: 7850 train-loss: 0.010550652941068014\n",
      "[LOG 20200502-14:21:56] epoch: 7851 train-loss: 0.0105506574942006\n",
      "[LOG 20200502-14:21:56] epoch: 7852 train-loss: 0.010550662668214904\n",
      "[LOG 20200502-14:21:57] epoch: 7853 train-loss: 0.010550666807426346\n",
      "[LOG 20200502-14:21:57] epoch: 7854 train-loss: 0.010550671670999792\n",
      "[LOG 20200502-14:21:57] epoch: 7855 train-loss: 0.010550675603250662\n",
      "[LOG 20200502-14:21:57] epoch: 7856 train-loss: 0.01055067963898182\n",
      "[LOG 20200502-14:21:57] epoch: 7857 train-loss: 0.010550683674712976\n",
      "[LOG 20200502-14:21:58] epoch: 7858 train-loss: 0.010550687606963847\n",
      "[LOG 20200502-14:21:58] epoch: 7859 train-loss: 0.010550691125293573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:21:58] epoch: 7860 train-loss: 0.010550694333182441\n",
      "[LOG 20200502-14:21:58] epoch: 7861 train-loss: 0.010550698472393883\n",
      "[LOG 20200502-14:21:59] epoch: 7862 train-loss: 0.010550701576802466\n",
      "[LOG 20200502-14:21:59] epoch: 7863 train-loss: 0.010550704474250475\n",
      "[LOG 20200502-14:21:59] epoch: 7864 train-loss: 0.01055070747517877\n",
      "[LOG 20200502-14:21:59] epoch: 7865 train-loss: 0.01055071068306764\n",
      "[LOG 20200502-14:21:59] epoch: 7866 train-loss: 0.010550713477035364\n",
      "[LOG 20200502-14:22:00] epoch: 7867 train-loss: 0.010550716064042516\n",
      "[LOG 20200502-14:22:00] epoch: 7868 train-loss: 0.010550718444089094\n",
      "[LOG 20200502-14:22:00] epoch: 7869 train-loss: 0.010550720927615961\n",
      "[LOG 20200502-14:22:00] epoch: 7870 train-loss: 0.010550722893741395\n",
      "[LOG 20200502-14:22:01] epoch: 7871 train-loss: 0.010550724756386545\n",
      "[LOG 20200502-14:22:01] epoch: 7872 train-loss: 0.010550726929472553\n",
      "[LOG 20200502-14:22:01] epoch: 7873 train-loss: 0.01055072858515713\n",
      "[LOG 20200502-14:22:01] epoch: 7874 train-loss: 0.010550729930400848\n",
      "[LOG 20200502-14:22:02] epoch: 7875 train-loss: 0.010550731275644567\n",
      "[LOG 20200502-14:22:02] epoch: 7876 train-loss: 0.010550732931329144\n",
      "[LOG 20200502-14:22:02] epoch: 7877 train-loss: 0.010550734173092578\n",
      "[LOG 20200502-14:22:02] epoch: 7878 train-loss: 0.010550735104415152\n",
      "[LOG 20200502-14:22:02] epoch: 7879 train-loss: 0.010550736346178584\n",
      "[LOG 20200502-14:22:03] epoch: 7880 train-loss: 0.010550736863580015\n",
      "[LOG 20200502-14:22:03] epoch: 7881 train-loss: 0.01055073779490259\n",
      "[LOG 20200502-14:22:03] epoch: 7882 train-loss: 0.010550738001863161\n",
      "[LOG 20200502-14:22:03] epoch: 7883 train-loss: 0.010550738519264592\n",
      "[LOG 20200502-14:22:03] epoch: 7884 train-loss: 0.010550739347106881\n",
      "[LOG 20200502-14:22:04] epoch: 7885 train-loss: 0.010550739140146308\n",
      "[LOG 20200502-14:22:04] epoch: 7886 train-loss: 0.010550739554067453\n",
      "[LOG 20200502-14:22:04] epoch: 7887 train-loss: 0.010550739450587166\n",
      "[LOG 20200502-14:22:04] epoch: 7888 train-loss: 0.010550739657547738\n",
      "[LOG 20200502-14:22:05] epoch: 7889 train-loss: 0.010550739554067453\n",
      "[LOG 20200502-14:22:05] epoch: 7890 train-loss: 0.010550739967988597\n",
      "[LOG 20200502-14:22:05] epoch: 7891 train-loss: 0.010550739657547738\n",
      "[LOG 20200502-14:22:05] epoch: 7892 train-loss: 0.010550739140146308\n",
      "[LOG 20200502-14:22:06] epoch: 7893 train-loss: 0.010550738726225164\n",
      "[LOG 20200502-14:22:06] epoch: 7894 train-loss: 0.010550738622744879\n",
      "[LOG 20200502-14:22:06] epoch: 7895 train-loss: 0.010550738105343448\n",
      "[LOG 20200502-14:22:06] epoch: 7896 train-loss: 0.010550738105343448\n",
      "[LOG 20200502-14:22:07] epoch: 7897 train-loss: 0.01055073779490259\n",
      "[LOG 20200502-14:22:07] epoch: 7898 train-loss: 0.010550737484461732\n",
      "[LOG 20200502-14:22:07] epoch: 7899 train-loss: 0.010550737070540587\n",
      "[LOG 20200502-14:22:07] epoch: 7900 train-loss: 0.01055073644965887\n",
      "[LOG 20200502-14:22:07] epoch: 7901 train-loss: 0.010550736967060301\n",
      "[LOG 20200502-14:22:08] epoch: 7902 train-loss: 0.010550737070540587\n",
      "[LOG 20200502-14:22:08] epoch: 7903 train-loss: 0.010550736346178584\n",
      "[LOG 20200502-14:22:08] epoch: 7904 train-loss: 0.010550736035737727\n",
      "[LOG 20200502-14:22:08] epoch: 7905 train-loss: 0.010550735725296868\n",
      "[LOG 20200502-14:22:09] epoch: 7906 train-loss: 0.010550736035737727\n",
      "[LOG 20200502-14:22:09] epoch: 7907 train-loss: 0.010550736035737727\n",
      "[LOG 20200502-14:22:09] epoch: 7908 train-loss: 0.010550736242698299\n",
      "[LOG 20200502-14:22:09] epoch: 7909 train-loss: 0.010550736346178584\n",
      "[LOG 20200502-14:22:09] epoch: 7910 train-loss: 0.01055073676009973\n",
      "[LOG 20200502-14:22:10] epoch: 7911 train-loss: 0.010550736863580015\n",
      "[LOG 20200502-14:22:10] epoch: 7912 train-loss: 0.010550737691422304\n",
      "[LOG 20200502-14:22:10] epoch: 7913 train-loss: 0.010550738105343448\n",
      "[LOG 20200502-14:22:10] epoch: 7914 train-loss: 0.010550738726225164\n",
      "[LOG 20200502-14:22:11] epoch: 7915 train-loss: 0.010550739657547738\n",
      "[LOG 20200502-14:22:11] epoch: 7916 train-loss: 0.010550740899311172\n",
      "[LOG 20200502-14:22:11] epoch: 7917 train-loss: 0.010550742037594318\n",
      "[LOG 20200502-14:22:11] epoch: 7918 train-loss: 0.010550742865436606\n",
      "[LOG 20200502-14:22:11] epoch: 7919 train-loss: 0.010550744728081755\n",
      "[LOG 20200502-14:22:12] epoch: 7920 train-loss: 0.01055074617680576\n",
      "[LOG 20200502-14:22:12] epoch: 7921 train-loss: 0.01055074803945091\n",
      "[LOG 20200502-14:22:12] epoch: 7922 train-loss: 0.010550750212536918\n",
      "[LOG 20200502-14:22:12] epoch: 7923 train-loss: 0.010550751764741208\n",
      "[LOG 20200502-14:22:13] epoch: 7924 train-loss: 0.010550753937827216\n",
      "[LOG 20200502-14:22:13] epoch: 7925 train-loss: 0.010550756421354082\n",
      "[LOG 20200502-14:22:13] epoch: 7926 train-loss: 0.010550759008361233\n",
      "[LOG 20200502-14:22:13] epoch: 7927 train-loss: 0.010550761077966955\n",
      "[LOG 20200502-14:22:14] epoch: 7928 train-loss: 0.010550763458013535\n",
      "[LOG 20200502-14:22:14] epoch: 7929 train-loss: 0.010550766872862974\n",
      "[LOG 20200502-14:22:14] epoch: 7930 train-loss: 0.01055076987379127\n",
      "[LOG 20200502-14:22:14] epoch: 7931 train-loss: 0.01055077308168014\n",
      "[LOG 20200502-14:22:14] epoch: 7932 train-loss: 0.010550776289569007\n",
      "[LOG 20200502-14:22:15] epoch: 7933 train-loss: 0.010550779187017016\n",
      "[LOG 20200502-14:22:15] epoch: 7934 train-loss: 0.01055078249838617\n",
      "[LOG 20200502-14:22:15] epoch: 7935 train-loss: 0.010550785913235612\n",
      "[LOG 20200502-14:22:15] epoch: 7936 train-loss: 0.010550789845486483\n",
      "[LOG 20200502-14:22:16] epoch: 7937 train-loss: 0.010550793467296494\n",
      "[LOG 20200502-14:22:16] epoch: 7938 train-loss: 0.010550796778665649\n",
      "[LOG 20200502-14:22:16] epoch: 7939 train-loss: 0.01055080071091652\n",
      "[LOG 20200502-14:22:16] epoch: 7940 train-loss: 0.010550805057088533\n",
      "[LOG 20200502-14:22:16] epoch: 7941 train-loss: 0.01055080857541826\n",
      "[LOG 20200502-14:22:17] epoch: 7942 train-loss: 0.01055081333551142\n",
      "[LOG 20200502-14:22:17] epoch: 7943 train-loss: 0.01055081675036086\n",
      "[LOG 20200502-14:22:17] epoch: 7944 train-loss: 0.010550821096532874\n",
      "[LOG 20200502-14:22:17] epoch: 7945 train-loss: 0.010550824718342887\n",
      "[LOG 20200502-14:22:18] epoch: 7946 train-loss: 0.010550828650593758\n",
      "[LOG 20200502-14:22:18] epoch: 7947 train-loss: 0.010550833203726344\n",
      "[LOG 20200502-14:22:18] epoch: 7948 train-loss: 0.010550836929016642\n",
      "[LOG 20200502-14:22:18] epoch: 7949 train-loss: 0.010550840861267514\n",
      "[LOG 20200502-14:22:19] epoch: 7950 train-loss: 0.01055084437959724\n",
      "[LOG 20200502-14:22:19] epoch: 7951 train-loss: 0.010550848208367825\n",
      "[LOG 20200502-14:22:19] epoch: 7952 train-loss: 0.010550851726697551\n",
      "[LOG 20200502-14:22:19] epoch: 7953 train-loss: 0.01055085545198785\n",
      "[LOG 20200502-14:22:20] epoch: 7954 train-loss: 0.01055085886683729\n",
      "[LOG 20200502-14:22:20] epoch: 7955 train-loss: 0.010550862488647303\n",
      "[LOG 20200502-14:22:20] epoch: 7956 train-loss: 0.010550865593055883\n",
      "[LOG 20200502-14:22:20] epoch: 7957 train-loss: 0.010550868490503894\n",
      "[LOG 20200502-14:22:20] epoch: 7958 train-loss: 0.010550870767070187\n",
      "[LOG 20200502-14:22:21] epoch: 7959 train-loss: 0.010550873767998483\n",
      "[LOG 20200502-14:22:21] epoch: 7960 train-loss: 0.010550876044564776\n",
      "[LOG 20200502-14:22:21] epoch: 7961 train-loss: 0.010550878114170499\n",
      "[LOG 20200502-14:22:21] epoch: 7962 train-loss: 0.010550880287256505\n",
      "[LOG 20200502-14:22:22] epoch: 7963 train-loss: 0.010550882563822798\n",
      "[LOG 20200502-14:22:22] epoch: 7964 train-loss: 0.010550883805586232\n",
      "[LOG 20200502-14:22:22] epoch: 7965 train-loss: 0.010550885357790522\n",
      "[LOG 20200502-14:22:22] epoch: 7966 train-loss: 0.010550886806514528\n",
      "[LOG 20200502-14:22:22] epoch: 7967 train-loss: 0.010550887116955386\n",
      "[LOG 20200502-14:22:23] epoch: 7968 train-loss: 0.010550888048277961\n",
      "[LOG 20200502-14:22:23] epoch: 7969 train-loss: 0.010550888772639964\n",
      "[LOG 20200502-14:22:23] epoch: 7970 train-loss: 0.010550888876120249\n",
      "[LOG 20200502-14:22:23] epoch: 7971 train-loss: 0.010550888772639964\n",
      "[LOG 20200502-14:22:24] epoch: 7972 train-loss: 0.010550888255238533\n",
      "[LOG 20200502-14:22:24] epoch: 7973 train-loss: 0.01055088753087653\n",
      "[LOG 20200502-14:22:24] epoch: 7974 train-loss: 0.010550886599553956\n",
      "[LOG 20200502-14:22:24] epoch: 7975 train-loss: 0.010550885668231381\n",
      "[LOG 20200502-14:22:25] epoch: 7976 train-loss: 0.010550884529948235\n",
      "[LOG 20200502-14:22:25] epoch: 7977 train-loss: 0.010550882770783372\n",
      "[LOG 20200502-14:22:25] epoch: 7978 train-loss: 0.010550881529019939\n",
      "[LOG 20200502-14:22:25] epoch: 7979 train-loss: 0.010550879976815648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:22:26] epoch: 7980 train-loss: 0.01055087728632821\n",
      "[LOG 20200502-14:22:26] epoch: 7981 train-loss: 0.010550874802801345\n",
      "[LOG 20200502-14:22:26] epoch: 7982 train-loss: 0.010550872836675908\n",
      "[LOG 20200502-14:22:26] epoch: 7983 train-loss: 0.0105508699392279\n",
      "[LOG 20200502-14:22:27] epoch: 7984 train-loss: 0.01055086755918132\n",
      "[LOG 20200502-14:22:27] epoch: 7985 train-loss: 0.010550864765213596\n",
      "[LOG 20200502-14:22:27] epoch: 7986 train-loss: 0.010550861350364156\n",
      "[LOG 20200502-14:22:27] epoch: 7987 train-loss: 0.010550858659876717\n",
      "[LOG 20200502-14:22:28] epoch: 7988 train-loss: 0.010550855658948421\n",
      "[LOG 20200502-14:22:28] epoch: 7989 train-loss: 0.010550852347579267\n",
      "[LOG 20200502-14:22:28] epoch: 7990 train-loss: 0.0105508491396904\n",
      "[LOG 20200502-14:22:28] epoch: 7991 train-loss: 0.010550846242242388\n",
      "[LOG 20200502-14:22:29] epoch: 7992 train-loss: 0.01055084303435352\n",
      "[LOG 20200502-14:22:29] epoch: 7993 train-loss: 0.010550840136905512\n",
      "[LOG 20200502-14:22:29] epoch: 7994 train-loss: 0.010550836618575785\n",
      "[LOG 20200502-14:22:29] epoch: 7995 train-loss: 0.010550833928088347\n",
      "[LOG 20200502-14:22:30] epoch: 7996 train-loss: 0.01055083092716005\n",
      "[LOG 20200502-14:22:30] epoch: 7997 train-loss: 0.010550828133192327\n",
      "[LOG 20200502-14:22:30] epoch: 7998 train-loss: 0.010550825753145747\n",
      "[LOG 20200502-14:22:31] epoch: 7999 train-loss: 0.010550823166138597\n",
      "[LOG 20200502-14:22:31] epoch: 8000 train-loss: 0.010550820579131445\n",
      "[LOG 20200502-14:22:31] epoch: 8001 train-loss: 0.010550817888644006\n",
      "[LOG 20200502-14:22:31] epoch: 8002 train-loss: 0.010550815508597426\n",
      "[LOG 20200502-14:22:32] epoch: 8003 train-loss: 0.010550813542471992\n",
      "[LOG 20200502-14:22:32] epoch: 8004 train-loss: 0.010550812197228273\n",
      "[LOG 20200502-14:22:32] epoch: 8005 train-loss: 0.010550810541543696\n",
      "[LOG 20200502-14:22:32] epoch: 8006 train-loss: 0.01055080909281969\n",
      "[LOG 20200502-14:22:33] epoch: 8007 train-loss: 0.010550807851056257\n",
      "[LOG 20200502-14:22:33] epoch: 8008 train-loss: 0.010550806609292826\n",
      "[LOG 20200502-14:22:33] epoch: 8009 train-loss: 0.010550805677970251\n",
      "[LOG 20200502-14:22:33] epoch: 8010 train-loss: 0.010550805057088533\n",
      "[LOG 20200502-14:22:34] epoch: 8011 train-loss: 0.010550804746647676\n",
      "[LOG 20200502-14:22:34] epoch: 8012 train-loss: 0.010550804746647676\n",
      "[LOG 20200502-14:22:34] epoch: 8013 train-loss: 0.010550804125765959\n",
      "[LOG 20200502-14:22:34] epoch: 8014 train-loss: 0.010550805057088533\n",
      "[LOG 20200502-14:22:35] epoch: 8015 train-loss: 0.010550805367529392\n",
      "[LOG 20200502-14:22:35] epoch: 8016 train-loss: 0.010550805884930823\n",
      "[LOG 20200502-14:22:35] epoch: 8017 train-loss: 0.01055080671277311\n",
      "[LOG 20200502-14:22:35] epoch: 8018 train-loss: 0.0105508075406154\n",
      "[LOG 20200502-14:22:36] epoch: 8019 train-loss: 0.01055080909281969\n",
      "[LOG 20200502-14:22:36] epoch: 8020 train-loss: 0.01055081095546484\n",
      "[LOG 20200502-14:22:36] epoch: 8021 train-loss: 0.010550812197228273\n",
      "[LOG 20200502-14:22:36] epoch: 8022 train-loss: 0.010550814059873423\n",
      "[LOG 20200502-14:22:37] epoch: 8023 train-loss: 0.010550816025998857\n",
      "[LOG 20200502-14:22:37] epoch: 8024 train-loss: 0.01055081830256515\n",
      "[LOG 20200502-14:22:37] epoch: 8025 train-loss: 0.010550820786092017\n",
      "[LOG 20200502-14:22:37] epoch: 8026 train-loss: 0.010550823269618882\n",
      "[LOG 20200502-14:22:38] epoch: 8027 train-loss: 0.010550825856626034\n",
      "[LOG 20200502-14:22:38] epoch: 8028 train-loss: 0.01055082885755433\n",
      "[LOG 20200502-14:22:38] epoch: 8029 train-loss: 0.010550831651522053\n",
      "[LOG 20200502-14:22:38] epoch: 8030 train-loss: 0.010550835066371493\n",
      "[LOG 20200502-14:22:39] epoch: 8031 train-loss: 0.01055083858470122\n",
      "[LOG 20200502-14:22:39] epoch: 8032 train-loss: 0.010550842103030946\n",
      "[LOG 20200502-14:22:39] epoch: 8033 train-loss: 0.010550845310919814\n",
      "[LOG 20200502-14:22:39] epoch: 8034 train-loss: 0.010550849036210112\n",
      "[LOG 20200502-14:22:40] epoch: 8035 train-loss: 0.010550852864980698\n",
      "[LOG 20200502-14:22:40] epoch: 8036 train-loss: 0.010550857107672427\n",
      "[LOG 20200502-14:22:40] epoch: 8037 train-loss: 0.01055086093644301\n",
      "[LOG 20200502-14:22:40] epoch: 8038 train-loss: 0.01055086486869388\n",
      "[LOG 20200502-14:22:41] epoch: 8039 train-loss: 0.010550869007905325\n",
      "[LOG 20200502-14:22:41] epoch: 8040 train-loss: 0.010550872940156195\n",
      "[LOG 20200502-14:22:41] epoch: 8041 train-loss: 0.010550877079367638\n",
      "[LOG 20200502-14:22:41] epoch: 8042 train-loss: 0.010550881322059367\n",
      "[LOG 20200502-14:22:42] epoch: 8043 train-loss: 0.01055088597867224\n",
      "[LOG 20200502-14:22:42] epoch: 8044 train-loss: 0.010550890117883682\n",
      "[LOG 20200502-14:22:42] epoch: 8045 train-loss: 0.010550894671016268\n",
      "[LOG 20200502-14:22:42] epoch: 8046 train-loss: 0.010550898810227713\n",
      "[LOG 20200502-14:22:43] epoch: 8047 train-loss: 0.010550903259880014\n",
      "[LOG 20200502-14:22:43] epoch: 8048 train-loss: 0.010550907606052028\n",
      "[LOG 20200502-14:22:43] epoch: 8049 train-loss: 0.01055091205570433\n",
      "[LOG 20200502-14:22:43] epoch: 8050 train-loss: 0.010550916401876343\n",
      "[LOG 20200502-14:22:43] epoch: 8051 train-loss: 0.010550921265449788\n",
      "[LOG 20200502-14:22:44] epoch: 8052 train-loss: 0.01055092540466123\n",
      "[LOG 20200502-14:22:44] epoch: 8053 train-loss: 0.010550929336912103\n",
      "[LOG 20200502-14:22:44] epoch: 8054 train-loss: 0.010550934097005261\n",
      "[LOG 20200502-14:22:44] epoch: 8055 train-loss: 0.010550937925775846\n",
      "[LOG 20200502-14:22:45] epoch: 8056 train-loss: 0.010550942685869005\n",
      "[LOG 20200502-14:22:45] epoch: 8057 train-loss: 0.01055094651463959\n",
      "[LOG 20200502-14:22:45] epoch: 8058 train-loss: 0.010550950550370745\n",
      "[LOG 20200502-14:22:45] epoch: 8059 train-loss: 0.01055095468958219\n",
      "[LOG 20200502-14:22:46] epoch: 8060 train-loss: 0.010550958725313345\n",
      "[LOG 20200502-14:22:46] epoch: 8061 train-loss: 0.010550962657564215\n",
      "[LOG 20200502-14:22:46] epoch: 8062 train-loss: 0.010550966175893942\n",
      "[LOG 20200502-14:22:46] epoch: 8063 train-loss: 0.010550970315105386\n",
      "[LOG 20200502-14:22:47] epoch: 8064 train-loss: 0.010550973936915398\n",
      "[LOG 20200502-14:22:47] epoch: 8065 train-loss: 0.010550977248284552\n",
      "[LOG 20200502-14:22:47] epoch: 8066 train-loss: 0.010550980870094564\n",
      "[LOG 20200502-14:22:47] epoch: 8067 train-loss: 0.01055098387102286\n",
      "[LOG 20200502-14:22:48] epoch: 8068 train-loss: 0.010550987596313158\n",
      "[LOG 20200502-14:22:48] epoch: 8069 train-loss: 0.01055099070072174\n",
      "[LOG 20200502-14:22:48] epoch: 8070 train-loss: 0.010550993805130323\n",
      "[LOG 20200502-14:22:49] epoch: 8071 train-loss: 0.010550996288657188\n",
      "[LOG 20200502-14:22:49] epoch: 8072 train-loss: 0.010550999393065771\n",
      "[LOG 20200502-14:22:49] epoch: 8073 train-loss: 0.010551002187033495\n",
      "[LOG 20200502-14:22:49] epoch: 8074 train-loss: 0.010551004567080073\n",
      "[LOG 20200502-14:22:49] epoch: 8075 train-loss: 0.010551006947126653\n",
      "[LOG 20200502-14:22:50] epoch: 8076 train-loss: 0.010551009223692946\n",
      "[LOG 20200502-14:22:50] epoch: 8077 train-loss: 0.010551011707219813\n",
      "[LOG 20200502-14:22:50] epoch: 8078 train-loss: 0.010551013259424103\n",
      "[LOG 20200502-14:22:50] epoch: 8079 train-loss: 0.010551015432510111\n",
      "[LOG 20200502-14:22:51] epoch: 8080 train-loss: 0.010551017191674974\n",
      "[LOG 20200502-14:22:51] epoch: 8081 train-loss: 0.01055101915780041\n",
      "[LOG 20200502-14:22:51] epoch: 8082 train-loss: 0.0105510207100047\n",
      "[LOG 20200502-14:22:51] epoch: 8083 train-loss: 0.01055102153784699\n",
      "[LOG 20200502-14:22:52] epoch: 8084 train-loss: 0.01055102309005128\n",
      "[LOG 20200502-14:22:52] epoch: 8085 train-loss: 0.010551024538775286\n",
      "[LOG 20200502-14:22:52] epoch: 8086 train-loss: 0.010551025159657001\n",
      "[LOG 20200502-14:22:52] epoch: 8087 train-loss: 0.010551026815341579\n",
      "[LOG 20200502-14:22:53] epoch: 8088 train-loss: 0.01055102733274301\n",
      "[LOG 20200502-14:22:53] epoch: 8089 train-loss: 0.010551027953624725\n",
      "[LOG 20200502-14:22:53] epoch: 8090 train-loss: 0.010551028574506441\n",
      "[LOG 20200502-14:22:53] epoch: 8091 train-loss: 0.010551029091907872\n",
      "[LOG 20200502-14:22:54] epoch: 8092 train-loss: 0.010551029505829016\n",
      "[LOG 20200502-14:22:54] epoch: 8093 train-loss: 0.010551029712789588\n",
      "[LOG 20200502-14:22:54] epoch: 8094 train-loss: 0.010551029816269875\n",
      "[LOG 20200502-14:22:54] epoch: 8095 train-loss: 0.01055103043715159\n",
      "[LOG 20200502-14:22:55] epoch: 8096 train-loss: 0.010551029712789588\n",
      "[LOG 20200502-14:22:55] epoch: 8097 train-loss: 0.010551030126710733\n",
      "[LOG 20200502-14:22:55] epoch: 8098 train-loss: 0.010551030023230447\n",
      "[LOG 20200502-14:22:55] epoch: 8099 train-loss: 0.010551029712789588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:22:56] epoch: 8100 train-loss: 0.010551029609309303\n",
      "[LOG 20200502-14:22:56] epoch: 8101 train-loss: 0.01055102940234873\n",
      "[LOG 20200502-14:22:56] epoch: 8102 train-loss: 0.010551028574506441\n",
      "[LOG 20200502-14:22:56] epoch: 8103 train-loss: 0.010551028574506441\n",
      "[LOG 20200502-14:22:56] epoch: 8104 train-loss: 0.010551027746664153\n",
      "[LOG 20200502-14:22:57] epoch: 8105 train-loss: 0.010551027643183867\n",
      "[LOG 20200502-14:22:57] epoch: 8106 train-loss: 0.010551027539703581\n",
      "[LOG 20200502-14:22:57] epoch: 8107 train-loss: 0.010551026608381007\n",
      "[LOG 20200502-14:22:57] epoch: 8108 train-loss: 0.010551026090979576\n",
      "[LOG 20200502-14:22:58] epoch: 8109 train-loss: 0.010551025884019004\n",
      "[LOG 20200502-14:22:58] epoch: 8110 train-loss: 0.01055102547009786\n",
      "[LOG 20200502-14:22:58] epoch: 8111 train-loss: 0.010551025056176715\n",
      "[LOG 20200502-14:22:58] epoch: 8112 train-loss: 0.010551024849216143\n",
      "[LOG 20200502-14:22:59] epoch: 8113 train-loss: 0.01055102464225557\n",
      "[LOG 20200502-14:22:59] epoch: 8114 train-loss: 0.010551024228334427\n",
      "[LOG 20200502-14:22:59] epoch: 8115 train-loss: 0.010551024021373855\n",
      "[LOG 20200502-14:22:59] epoch: 8116 train-loss: 0.010551023917893568\n",
      "[LOG 20200502-14:23:00] epoch: 8117 train-loss: 0.010551023607452711\n",
      "[LOG 20200502-14:23:00] epoch: 8118 train-loss: 0.010551023607452711\n",
      "[LOG 20200502-14:23:00] epoch: 8119 train-loss: 0.010551023400492139\n",
      "[LOG 20200502-14:23:00] epoch: 8120 train-loss: 0.010551023400492139\n",
      "[LOG 20200502-14:23:01] epoch: 8121 train-loss: 0.010551023917893568\n",
      "[LOG 20200502-14:23:01] epoch: 8122 train-loss: 0.010551024228334427\n",
      "[LOG 20200502-14:23:01] epoch: 8123 train-loss: 0.01055102412485414\n",
      "[LOG 20200502-14:23:01] epoch: 8124 train-loss: 0.010551024538775286\n",
      "[LOG 20200502-14:23:02] epoch: 8125 train-loss: 0.010551025780538717\n",
      "[LOG 20200502-14:23:02] epoch: 8126 train-loss: 0.01055102650490072\n",
      "[LOG 20200502-14:23:02] epoch: 8127 train-loss: 0.010551026918821864\n",
      "[LOG 20200502-14:23:02] epoch: 8128 train-loss: 0.010551027953624725\n",
      "[LOG 20200502-14:23:03] epoch: 8129 train-loss: 0.0105510288849473\n",
      "[LOG 20200502-14:23:03] epoch: 8130 train-loss: 0.010551029919750161\n",
      "[LOG 20200502-14:23:03] epoch: 8131 train-loss: 0.010551031368474165\n",
      "[LOG 20200502-14:23:03] epoch: 8132 train-loss: 0.010551032403277026\n",
      "[LOG 20200502-14:23:04] epoch: 8133 train-loss: 0.01055103436940246\n",
      "[LOG 20200502-14:23:04] epoch: 8134 train-loss: 0.010551035611165894\n",
      "[LOG 20200502-14:23:04] epoch: 8135 train-loss: 0.010551037163370185\n",
      "[LOG 20200502-14:23:04] epoch: 8136 train-loss: 0.01055103912949562\n",
      "[LOG 20200502-14:23:04] epoch: 8137 train-loss: 0.01055104047473934\n",
      "[LOG 20200502-14:23:05] epoch: 8138 train-loss: 0.010551042751305632\n",
      "[LOG 20200502-14:23:05] epoch: 8139 train-loss: 0.010551044717431068\n",
      "[LOG 20200502-14:23:05] epoch: 8140 train-loss: 0.01055104730443822\n",
      "[LOG 20200502-14:23:05] epoch: 8141 train-loss: 0.01055104916708337\n",
      "[LOG 20200502-14:23:06] epoch: 8142 train-loss: 0.010551051443649663\n",
      "[LOG 20200502-14:23:06] epoch: 8143 train-loss: 0.010551053409775099\n",
      "[LOG 20200502-14:23:06] epoch: 8144 train-loss: 0.010551055582861105\n",
      "[LOG 20200502-14:23:06] epoch: 8145 train-loss: 0.010551058066387972\n",
      "[LOG 20200502-14:23:07] epoch: 8146 train-loss: 0.010551060239473978\n",
      "[LOG 20200502-14:23:07] epoch: 8147 train-loss: 0.010551062929961417\n",
      "[LOG 20200502-14:23:07] epoch: 8148 train-loss: 0.010551065516968569\n",
      "[LOG 20200502-14:23:07] epoch: 8149 train-loss: 0.010551067483094003\n",
      "[LOG 20200502-14:23:07] epoch: 8150 train-loss: 0.010551069863140583\n",
      "[LOG 20200502-14:23:08] epoch: 8151 train-loss: 0.010551072760588594\n",
      "[LOG 20200502-14:23:08] epoch: 8152 train-loss: 0.010551075037154887\n",
      "[LOG 20200502-14:23:08] epoch: 8153 train-loss: 0.010551076692839464\n",
      "[LOG 20200502-14:23:08] epoch: 8154 train-loss: 0.010551078762445185\n",
      "[LOG 20200502-14:23:09] epoch: 8155 train-loss: 0.010551081452932622\n",
      "[LOG 20200502-14:23:09] epoch: 8156 train-loss: 0.010551083212097486\n",
      "[LOG 20200502-14:23:09] epoch: 8157 train-loss: 0.010551085385183493\n",
      "[LOG 20200502-14:23:09] epoch: 8158 train-loss: 0.010551086937387785\n",
      "[LOG 20200502-14:23:10] epoch: 8159 train-loss: 0.010551089006993506\n",
      "[LOG 20200502-14:23:10] epoch: 8160 train-loss: 0.010551090455717511\n",
      "[LOG 20200502-14:23:10] epoch: 8161 train-loss: 0.010551091904441515\n",
      "[LOG 20200502-14:23:10] epoch: 8162 train-loss: 0.010551093146204948\n",
      "[LOG 20200502-14:23:11] epoch: 8163 train-loss: 0.010551093974047236\n",
      "[LOG 20200502-14:23:11] epoch: 8164 train-loss: 0.010551095008850098\n",
      "[LOG 20200502-14:23:11] epoch: 8165 train-loss: 0.010551096043652959\n",
      "[LOG 20200502-14:23:11] epoch: 8166 train-loss: 0.010551096561054388\n",
      "[LOG 20200502-14:23:11] epoch: 8167 train-loss: 0.010551096561054388\n",
      "[LOG 20200502-14:23:12] epoch: 8168 train-loss: 0.010551097181936106\n",
      "[LOG 20200502-14:23:12] epoch: 8169 train-loss: 0.010551097181936106\n",
      "[LOG 20200502-14:23:12] epoch: 8170 train-loss: 0.010551096664534675\n",
      "[LOG 20200502-14:23:12] epoch: 8171 train-loss: 0.010551096043652959\n",
      "[LOG 20200502-14:23:13] epoch: 8172 train-loss: 0.010551095629731813\n",
      "[LOG 20200502-14:23:13] epoch: 8173 train-loss: 0.01055109490536981\n",
      "[LOG 20200502-14:23:13] epoch: 8174 train-loss: 0.01055109366360638\n",
      "[LOG 20200502-14:23:13] epoch: 8175 train-loss: 0.010551092421842946\n",
      "[LOG 20200502-14:23:14] epoch: 8176 train-loss: 0.010551091076599227\n",
      "[LOG 20200502-14:23:14] epoch: 8177 train-loss: 0.010551089317434363\n",
      "[LOG 20200502-14:23:14] epoch: 8178 train-loss: 0.0105510875582695\n",
      "[LOG 20200502-14:23:14] epoch: 8179 train-loss: 0.01055108517822292\n",
      "[LOG 20200502-14:23:14] epoch: 8180 train-loss: 0.0105510831086172\n",
      "[LOG 20200502-14:23:15] epoch: 8181 train-loss: 0.01055108072857062\n",
      "[LOG 20200502-14:23:15] epoch: 8182 train-loss: 0.010551078452004327\n",
      "[LOG 20200502-14:23:15] epoch: 8183 train-loss: 0.010551075244115459\n",
      "[LOG 20200502-14:23:15] epoch: 8184 train-loss: 0.01055107307102945\n",
      "[LOG 20200502-14:23:16] epoch: 8185 train-loss: 0.01055106996662087\n",
      "[LOG 20200502-14:23:16] epoch: 8186 train-loss: 0.010551066862212287\n",
      "[LOG 20200502-14:23:16] epoch: 8187 train-loss: 0.010551063964764277\n",
      "[LOG 20200502-14:23:16] epoch: 8188 train-loss: 0.010551060756875409\n",
      "[LOG 20200502-14:23:16] epoch: 8189 train-loss: 0.010551057548986541\n",
      "[LOG 20200502-14:23:17] epoch: 8190 train-loss: 0.010551054755018817\n",
      "[LOG 20200502-14:23:17] epoch: 8191 train-loss: 0.010551051340169378\n",
      "[LOG 20200502-14:23:17] epoch: 8192 train-loss: 0.010551048132280508\n",
      "[LOG 20200502-14:23:17] epoch: 8193 train-loss: 0.010551044717431068\n",
      "[LOG 20200502-14:23:18] epoch: 8194 train-loss: 0.010551041199101342\n",
      "[LOG 20200502-14:23:18] epoch: 8195 train-loss: 0.010551038301653333\n",
      "[LOG 20200502-14:23:18] epoch: 8196 train-loss: 0.010551035714646181\n",
      "[LOG 20200502-14:23:18] epoch: 8197 train-loss: 0.01055103229979674\n",
      "[LOG 20200502-14:23:19] epoch: 8198 train-loss: 0.010551030230191018\n",
      "[LOG 20200502-14:23:19] epoch: 8199 train-loss: 0.010551026711861292\n",
      "[LOG 20200502-14:23:19] epoch: 8200 train-loss: 0.01055102412485414\n",
      "[LOG 20200502-14:23:19] epoch: 8201 train-loss: 0.010551021641327275\n",
      "[LOG 20200502-14:23:19] epoch: 8202 train-loss: 0.010551018847359551\n",
      "[LOG 20200502-14:23:20] epoch: 8203 train-loss: 0.010551016881234117\n",
      "[LOG 20200502-14:23:20] epoch: 8204 train-loss: 0.010551014604667822\n",
      "[LOG 20200502-14:23:20] epoch: 8205 train-loss: 0.010551012742022673\n",
      "[LOG 20200502-14:23:20] epoch: 8206 train-loss: 0.010551011396778954\n",
      "[LOG 20200502-14:23:21] epoch: 8207 train-loss: 0.010551009534133805\n",
      "[LOG 20200502-14:23:21] epoch: 8208 train-loss: 0.0105510080854098\n",
      "[LOG 20200502-14:23:21] epoch: 8209 train-loss: 0.010551006947126653\n",
      "[LOG 20200502-14:23:21] epoch: 8210 train-loss: 0.01055100622276465\n",
      "[LOG 20200502-14:23:21] epoch: 8211 train-loss: 0.010551005291442076\n",
      "[LOG 20200502-14:23:22] epoch: 8212 train-loss: 0.01055100467056036\n",
      "[LOG 20200502-14:23:22] epoch: 8213 train-loss: 0.01055100415315893\n",
      "[LOG 20200502-14:23:22] epoch: 8214 train-loss: 0.010551004049678644\n",
      "[LOG 20200502-14:23:23] epoch: 8215 train-loss: 0.010551004463599788\n",
      "[LOG 20200502-14:23:23] epoch: 8216 train-loss: 0.010551004049678644\n",
      "[LOG 20200502-14:23:23] epoch: 8217 train-loss: 0.010551004256639216\n",
      "[LOG 20200502-14:23:23] epoch: 8218 train-loss: 0.010551004567080073\n",
      "[LOG 20200502-14:23:23] epoch: 8219 train-loss: 0.010551005601882935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:23:24] epoch: 8220 train-loss: 0.01055100653320551\n",
      "[LOG 20200502-14:23:24] epoch: 8221 train-loss: 0.010551007878449228\n",
      "[LOG 20200502-14:23:24] epoch: 8222 train-loss: 0.010551008395850658\n",
      "[LOG 20200502-14:23:24] epoch: 8223 train-loss: 0.010551009741094377\n",
      "[LOG 20200502-14:23:25] epoch: 8224 train-loss: 0.010551010879377523\n",
      "[LOG 20200502-14:23:25] epoch: 8225 train-loss: 0.010551013052463531\n",
      "[LOG 20200502-14:23:25] epoch: 8226 train-loss: 0.010551014501187537\n",
      "[LOG 20200502-14:23:25] epoch: 8227 train-loss: 0.010551016363832686\n",
      "[LOG 20200502-14:23:26] epoch: 8228 train-loss: 0.010551018226477835\n",
      "[LOG 20200502-14:23:26] epoch: 8229 train-loss: 0.0105510207100047\n",
      "[LOG 20200502-14:23:26] epoch: 8230 train-loss: 0.01055102257264985\n",
      "[LOG 20200502-14:23:26] epoch: 8231 train-loss: 0.010551024745735858\n",
      "[LOG 20200502-14:23:26] epoch: 8232 train-loss: 0.010551027539703581\n",
      "[LOG 20200502-14:23:27] epoch: 8233 train-loss: 0.010551029609309303\n",
      "[LOG 20200502-14:23:27] epoch: 8234 train-loss: 0.010551032196316455\n",
      "[LOG 20200502-14:23:27] epoch: 8235 train-loss: 0.010551035300725035\n",
      "[LOG 20200502-14:23:27] epoch: 8236 train-loss: 0.010551037784251902\n",
      "[LOG 20200502-14:23:28] epoch: 8237 train-loss: 0.010551040267778767\n",
      "[LOG 20200502-14:23:28] epoch: 8238 train-loss: 0.010551043061746491\n",
      "[LOG 20200502-14:23:28] epoch: 8239 train-loss: 0.010551045959194502\n",
      "[LOG 20200502-14:23:28] epoch: 8240 train-loss: 0.010551048960122798\n",
      "[LOG 20200502-14:23:28] epoch: 8241 train-loss: 0.010551051650610235\n",
      "[LOG 20200502-14:23:29] epoch: 8242 train-loss: 0.010551054341097673\n",
      "[LOG 20200502-14:23:29] epoch: 8243 train-loss: 0.0105510578594274\n",
      "[LOG 20200502-14:23:29] epoch: 8244 train-loss: 0.010551060653395124\n",
      "[LOG 20200502-14:23:29] epoch: 8245 train-loss: 0.010551063757803705\n",
      "[LOG 20200502-14:23:30] epoch: 8246 train-loss: 0.010551066758732\n",
      "[LOG 20200502-14:23:30] epoch: 8247 train-loss: 0.01055106944921944\n",
      "[LOG 20200502-14:23:30] epoch: 8248 train-loss: 0.010551072139706876\n",
      "[LOG 20200502-14:23:30] epoch: 8249 train-loss: 0.010551075037154887\n",
      "[LOG 20200502-14:23:31] epoch: 8250 train-loss: 0.010551078348524041\n",
      "[LOG 20200502-14:23:31] epoch: 8251 train-loss: 0.010551080935531192\n",
      "[LOG 20200502-14:23:31] epoch: 8252 train-loss: 0.010551083832979202\n",
      "[LOG 20200502-14:23:31] epoch: 8253 train-loss: 0.010551086419986354\n",
      "[LOG 20200502-14:23:32] epoch: 8254 train-loss: 0.010551088800032934\n",
      "[LOG 20200502-14:23:32] epoch: 8255 train-loss: 0.01055109231836266\n",
      "[LOG 20200502-14:23:32] epoch: 8256 train-loss: 0.010551094077527523\n",
      "[LOG 20200502-14:23:32] epoch: 8257 train-loss: 0.010551096561054388\n",
      "[LOG 20200502-14:23:33] epoch: 8258 train-loss: 0.010551099355022112\n",
      "[LOG 20200502-14:23:33] epoch: 8259 train-loss: 0.01055110152810812\n",
      "[LOG 20200502-14:23:34] epoch: 8260 train-loss: 0.0105511039081547\n",
      "[LOG 20200502-14:23:34] epoch: 8261 train-loss: 0.010551105874280134\n",
      "[LOG 20200502-14:23:34] epoch: 8262 train-loss: 0.010551108357807001\n",
      "[LOG 20200502-14:23:34] epoch: 8263 train-loss: 0.01055111022045215\n",
      "[LOG 20200502-14:23:35] epoch: 8264 train-loss: 0.010551112186577585\n",
      "[LOG 20200502-14:23:35] epoch: 8265 train-loss: 0.010551113324860731\n",
      "[LOG 20200502-14:23:35] epoch: 8266 train-loss: 0.010551115084025595\n",
      "[LOG 20200502-14:23:36] epoch: 8267 train-loss: 0.010551116843190458\n",
      "[LOG 20200502-14:23:36] epoch: 8268 train-loss: 0.010551117981473604\n",
      "[LOG 20200502-14:23:36] epoch: 8269 train-loss: 0.010551119740638468\n",
      "[LOG 20200502-14:23:36] epoch: 8270 train-loss: 0.0105511209824019\n",
      "[LOG 20200502-14:23:37] epoch: 8271 train-loss: 0.01055112232764562\n",
      "[LOG 20200502-14:23:37] epoch: 8272 train-loss: 0.010551123672889339\n",
      "[LOG 20200502-14:23:37] epoch: 8273 train-loss: 0.01055112387984991\n",
      "[LOG 20200502-14:23:37] epoch: 8274 train-loss: 0.010551125432054201\n",
      "[LOG 20200502-14:23:38] epoch: 8275 train-loss: 0.010551125639014773\n",
      "[LOG 20200502-14:23:38] epoch: 8276 train-loss: 0.010551126363376776\n",
      "[LOG 20200502-14:23:38] epoch: 8277 train-loss: 0.01055112729469935\n",
      "[LOG 20200502-14:23:38] epoch: 8278 train-loss: 0.010551127915581068\n",
      "[LOG 20200502-14:23:39] epoch: 8279 train-loss: 0.010551128019061353\n",
      "[LOG 20200502-14:23:39] epoch: 8280 train-loss: 0.010551127398179637\n",
      "[LOG 20200502-14:23:39] epoch: 8281 train-loss: 0.010551128226021925\n",
      "[LOG 20200502-14:23:39] epoch: 8282 train-loss: 0.010551128536462784\n",
      "[LOG 20200502-14:23:40] epoch: 8283 train-loss: 0.010551128329502212\n",
      "[LOG 20200502-14:23:40] epoch: 8284 train-loss: 0.010551128432982497\n",
      "[LOG 20200502-14:23:40] epoch: 8285 train-loss: 0.010551127812100781\n",
      "[LOG 20200502-14:23:40] epoch: 8286 train-loss: 0.010551127708620496\n",
      "[LOG 20200502-14:23:41] epoch: 8287 train-loss: 0.010551127191219065\n",
      "[LOG 20200502-14:23:41] epoch: 8288 train-loss: 0.010551126466857063\n",
      "[LOG 20200502-14:23:41] epoch: 8289 train-loss: 0.010551126156416204\n",
      "[LOG 20200502-14:23:41] epoch: 8290 train-loss: 0.010551125432054201\n",
      "[LOG 20200502-14:23:42] epoch: 8291 train-loss: 0.010551124811172485\n",
      "[LOG 20200502-14:23:42] epoch: 8292 train-loss: 0.01055112419029077\n",
      "[LOG 20200502-14:23:42] epoch: 8293 train-loss: 0.010551124086810483\n",
      "[LOG 20200502-14:23:42] epoch: 8294 train-loss: 0.010551122638086477\n",
      "[LOG 20200502-14:23:42] epoch: 8295 train-loss: 0.010551122120685048\n",
      "[LOG 20200502-14:23:43] epoch: 8296 train-loss: 0.010551121085882187\n",
      "[LOG 20200502-14:23:43] epoch: 8297 train-loss: 0.010551120051079325\n",
      "[LOG 20200502-14:23:43] epoch: 8298 train-loss: 0.010551119223237038\n",
      "[LOG 20200502-14:23:43] epoch: 8299 train-loss: 0.010551118602355322\n",
      "[LOG 20200502-14:23:43] epoch: 8300 train-loss: 0.01055111756755246\n",
      "[LOG 20200502-14:23:44] epoch: 8301 train-loss: 0.010551116636229886\n",
      "[LOG 20200502-14:23:44] epoch: 8302 train-loss: 0.010551115911867883\n",
      "[LOG 20200502-14:23:44] epoch: 8303 train-loss: 0.010551115290986167\n",
      "[LOG 20200502-14:23:44] epoch: 8304 train-loss: 0.010551114049222734\n",
      "[LOG 20200502-14:23:45] epoch: 8305 train-loss: 0.010551113014419874\n",
      "[LOG 20200502-14:23:45] epoch: 8306 train-loss: 0.010551112910939587\n",
      "[LOG 20200502-14:23:45] epoch: 8307 train-loss: 0.010551111876136728\n",
      "[LOG 20200502-14:23:45] epoch: 8308 train-loss: 0.01055111125525501\n",
      "[LOG 20200502-14:23:46] epoch: 8309 train-loss: 0.010551110427412722\n",
      "[LOG 20200502-14:23:46] epoch: 8310 train-loss: 0.010551109599570433\n",
      "[LOG 20200502-14:23:46] epoch: 8311 train-loss: 0.010551108978688717\n",
      "[LOG 20200502-14:23:46] epoch: 8312 train-loss: 0.01055110887520843\n",
      "[LOG 20200502-14:23:46] epoch: 8313 train-loss: 0.010551108357807001\n",
      "[LOG 20200502-14:23:47] epoch: 8314 train-loss: 0.010551108564767573\n",
      "[LOG 20200502-14:23:47] epoch: 8315 train-loss: 0.01055110784040557\n",
      "[LOG 20200502-14:23:47] epoch: 8316 train-loss: 0.01055110732300414\n",
      "[LOG 20200502-14:23:47] epoch: 8317 train-loss: 0.010551107529964712\n",
      "[LOG 20200502-14:23:48] epoch: 8318 train-loss: 0.01055110732300414\n",
      "[LOG 20200502-14:23:48] epoch: 8319 train-loss: 0.010551107943885855\n",
      "[LOG 20200502-14:23:48] epoch: 8320 train-loss: 0.010551107426484426\n",
      "[LOG 20200502-14:23:48] epoch: 8321 train-loss: 0.010551107633444998\n",
      "[LOG 20200502-14:23:48] epoch: 8322 train-loss: 0.010551108254326714\n",
      "[LOG 20200502-14:23:49] epoch: 8323 train-loss: 0.010551108461287286\n",
      "[LOG 20200502-14:23:49] epoch: 8324 train-loss: 0.010551108771728145\n",
      "[LOG 20200502-14:23:49] epoch: 8325 train-loss: 0.010551108978688717\n",
      "[LOG 20200502-14:23:49] epoch: 8326 train-loss: 0.010551109599570433\n",
      "[LOG 20200502-14:23:50] epoch: 8327 train-loss: 0.01055111022045215\n",
      "[LOG 20200502-14:23:50] epoch: 8328 train-loss: 0.01055111073785358\n",
      "[LOG 20200502-14:23:50] epoch: 8329 train-loss: 0.010551111358735297\n",
      "[LOG 20200502-14:23:50] epoch: 8330 train-loss: 0.010551112497018443\n",
      "[LOG 20200502-14:23:50] epoch: 8331 train-loss: 0.010551112186577585\n",
      "[LOG 20200502-14:23:51] epoch: 8332 train-loss: 0.010551113324860731\n",
      "[LOG 20200502-14:23:51] epoch: 8333 train-loss: 0.010551114049222734\n",
      "[LOG 20200502-14:23:51] epoch: 8334 train-loss: 0.010551114463143878\n",
      "[LOG 20200502-14:23:51] epoch: 8335 train-loss: 0.010551115601427026\n",
      "[LOG 20200502-14:23:51] epoch: 8336 train-loss: 0.010551116222308742\n",
      "[LOG 20200502-14:23:52] epoch: 8337 train-loss: 0.010551116739710173\n",
      "[LOG 20200502-14:23:52] epoch: 8338 train-loss: 0.010551117671032747\n",
      "[LOG 20200502-14:23:52] epoch: 8339 train-loss: 0.010551118084953891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:23:52] epoch: 8340 train-loss: 0.010551118809315894\n",
      "[LOG 20200502-14:23:53] epoch: 8341 train-loss: 0.010551118809315894\n",
      "[LOG 20200502-14:23:53] epoch: 8342 train-loss: 0.010551119326717324\n",
      "[LOG 20200502-14:23:53] epoch: 8343 train-loss: 0.01055111943019761\n",
      "[LOG 20200502-14:23:53] epoch: 8344 train-loss: 0.010551120568480756\n",
      "[LOG 20200502-14:23:53] epoch: 8345 train-loss: 0.010551119740638468\n",
      "[LOG 20200502-14:23:54] epoch: 8346 train-loss: 0.010551120154559612\n",
      "[LOG 20200502-14:23:54] epoch: 8347 train-loss: 0.01055111994759904\n",
      "[LOG 20200502-14:23:54] epoch: 8348 train-loss: 0.010551120154559612\n",
      "[LOG 20200502-14:23:54] epoch: 8349 train-loss: 0.01055111911975675\n",
      "[LOG 20200502-14:23:55] epoch: 8350 train-loss: 0.010551119223237038\n",
      "[LOG 20200502-14:23:55] epoch: 8351 train-loss: 0.01055111839539475\n",
      "[LOG 20200502-14:23:55] epoch: 8352 train-loss: 0.010551117671032747\n",
      "[LOG 20200502-14:23:55] epoch: 8353 train-loss: 0.010551116843190458\n",
      "[LOG 20200502-14:23:55] epoch: 8354 train-loss: 0.01055111601534817\n",
      "[LOG 20200502-14:23:56] epoch: 8355 train-loss: 0.010551114877065023\n",
      "[LOG 20200502-14:23:56] epoch: 8356 train-loss: 0.010551113324860731\n",
      "[LOG 20200502-14:23:56] epoch: 8357 train-loss: 0.010551111979617013\n",
      "[LOG 20200502-14:23:56] epoch: 8358 train-loss: 0.010551110013491578\n",
      "[LOG 20200502-14:23:57] epoch: 8359 train-loss: 0.010551108357807001\n",
      "[LOG 20200502-14:23:57] epoch: 8360 train-loss: 0.010551106081240706\n",
      "[LOG 20200502-14:23:57] epoch: 8361 train-loss: 0.010551103597713841\n",
      "[LOG 20200502-14:23:57] epoch: 8362 train-loss: 0.010551101838548979\n",
      "[LOG 20200502-14:23:57] epoch: 8363 train-loss: 0.010551099251541827\n",
      "[LOG 20200502-14:23:58] epoch: 8364 train-loss: 0.010551096354093816\n",
      "[LOG 20200502-14:23:58] epoch: 8365 train-loss: 0.010551093974047236\n",
      "[LOG 20200502-14:23:58] epoch: 8366 train-loss: 0.0105510912835598\n",
      "[LOG 20200502-14:23:58] epoch: 8367 train-loss: 0.010551087765230073\n",
      "[LOG 20200502-14:23:58] epoch: 8368 train-loss: 0.010551084971262349\n",
      "[LOG 20200502-14:23:59] epoch: 8369 train-loss: 0.01055108155641291\n",
      "[LOG 20200502-14:23:59] epoch: 8370 train-loss: 0.010551078348524041\n",
      "[LOG 20200502-14:23:59] epoch: 8371 train-loss: 0.010551075244115459\n",
      "[LOG 20200502-14:23:59] epoch: 8372 train-loss: 0.010551072139706876\n",
      "[LOG 20200502-14:24:00] epoch: 8373 train-loss: 0.01055106862137715\n",
      "[LOG 20200502-14:24:00] epoch: 8374 train-loss: 0.010551065103047423\n",
      "[LOG 20200502-14:24:00] epoch: 8375 train-loss: 0.010551061584717698\n",
      "[LOG 20200502-14:24:00] epoch: 8376 train-loss: 0.010551058066387972\n",
      "[LOG 20200502-14:24:00] epoch: 8377 train-loss: 0.010551055375900533\n",
      "[LOG 20200502-14:24:01] epoch: 8378 train-loss: 0.010551052064531379\n",
      "[LOG 20200502-14:24:01] epoch: 8379 train-loss: 0.010551048649681939\n",
      "[LOG 20200502-14:24:01] epoch: 8380 train-loss: 0.01055104492439164\n",
      "[LOG 20200502-14:24:01] epoch: 8381 train-loss: 0.010551041406061914\n",
      "[LOG 20200502-14:24:02] epoch: 8382 train-loss: 0.010551038819054762\n",
      "[LOG 20200502-14:24:02] epoch: 8383 train-loss: 0.010551036025087038\n",
      "[LOG 20200502-14:24:02] epoch: 8384 train-loss: 0.010551033231119314\n",
      "[LOG 20200502-14:24:02] epoch: 8385 train-loss: 0.01055103043715159\n",
      "[LOG 20200502-14:24:02] epoch: 8386 train-loss: 0.010551027436223295\n",
      "[LOG 20200502-14:24:03] epoch: 8387 train-loss: 0.010551025056176715\n",
      "[LOG 20200502-14:24:03] epoch: 8388 train-loss: 0.010551022676130136\n",
      "[LOG 20200502-14:24:03] epoch: 8389 train-loss: 0.010551020606524415\n",
      "[LOG 20200502-14:24:03] epoch: 8390 train-loss: 0.010551018019517263\n",
      "[LOG 20200502-14:24:03] epoch: 8391 train-loss: 0.0105510162603524\n",
      "[LOG 20200502-14:24:04] epoch: 8392 train-loss: 0.01055101439770725\n",
      "[LOG 20200502-14:24:04] epoch: 8393 train-loss: 0.010551012431581816\n",
      "[LOG 20200502-14:24:04] epoch: 8394 train-loss: 0.010551011396778954\n",
      "[LOG 20200502-14:24:04] epoch: 8395 train-loss: 0.010551009844574664\n",
      "[LOG 20200502-14:24:05] epoch: 8396 train-loss: 0.01055100891325209\n",
      "[LOG 20200502-14:24:05] epoch: 8397 train-loss: 0.010551007878449228\n",
      "[LOG 20200502-14:24:05] epoch: 8398 train-loss: 0.010551006843646368\n",
      "[LOG 20200502-14:24:05] epoch: 8399 train-loss: 0.010551006326244937\n",
      "[LOG 20200502-14:24:05] epoch: 8400 train-loss: 0.010551004981001219\n",
      "[LOG 20200502-14:24:06] epoch: 8401 train-loss: 0.010551004256639216\n",
      "[LOG 20200502-14:24:06] epoch: 8402 train-loss: 0.010551004463599788\n",
      "[LOG 20200502-14:24:06] epoch: 8403 train-loss: 0.010551004360119501\n",
      "[LOG 20200502-14:24:06] epoch: 8404 train-loss: 0.010551004463599788\n",
      "[LOG 20200502-14:24:07] epoch: 8405 train-loss: 0.010551004567080073\n",
      "[LOG 20200502-14:24:07] epoch: 8406 train-loss: 0.010551004567080073\n",
      "[LOG 20200502-14:24:07] epoch: 8407 train-loss: 0.01055100518796179\n",
      "[LOG 20200502-14:24:07] epoch: 8408 train-loss: 0.010551005601882935\n",
      "[LOG 20200502-14:24:07] epoch: 8409 train-loss: 0.010551006015804078\n",
      "[LOG 20200502-14:24:08] epoch: 8410 train-loss: 0.010551006740166081\n",
      "[LOG 20200502-14:24:08] epoch: 8411 train-loss: 0.010551007464528084\n",
      "[LOG 20200502-14:24:08] epoch: 8412 train-loss: 0.010551008499330945\n",
      "[LOG 20200502-14:24:08] epoch: 8413 train-loss: 0.010551009120212661\n",
      "[LOG 20200502-14:24:09] epoch: 8414 train-loss: 0.010551009948054949\n",
      "[LOG 20200502-14:24:09] epoch: 8415 train-loss: 0.010551011086338095\n",
      "[LOG 20200502-14:24:09] epoch: 8416 train-loss: 0.010551012328101529\n",
      "[LOG 20200502-14:24:09] epoch: 8417 train-loss: 0.010551013569864962\n",
      "[LOG 20200502-14:24:09] epoch: 8418 train-loss: 0.010551015018588968\n",
      "[LOG 20200502-14:24:10] epoch: 8419 train-loss: 0.010551016053391827\n",
      "[LOG 20200502-14:24:10] epoch: 8420 train-loss: 0.010551017812556691\n",
      "[LOG 20200502-14:24:10] epoch: 8421 train-loss: 0.010551018743879266\n",
      "[LOG 20200502-14:24:10] epoch: 8422 train-loss: 0.010551020296083556\n",
      "[LOG 20200502-14:24:11] epoch: 8423 train-loss: 0.010551022365689278\n",
      "[LOG 20200502-14:24:11] epoch: 8424 train-loss: 0.010551022883090708\n",
      "[LOG 20200502-14:24:11] epoch: 8425 train-loss: 0.010551024538775286\n",
      "[LOG 20200502-14:24:11] epoch: 8426 train-loss: 0.010551025884019004\n",
      "[LOG 20200502-14:24:11] epoch: 8427 train-loss: 0.010551028057105012\n",
      "[LOG 20200502-14:24:12] epoch: 8428 train-loss: 0.01055102940234873\n",
      "[LOG 20200502-14:24:12] epoch: 8429 train-loss: 0.010551030333671305\n",
      "[LOG 20200502-14:24:12] epoch: 8430 train-loss: 0.010551032092836168\n",
      "[LOG 20200502-14:24:12] epoch: 8431 train-loss: 0.010551033541560173\n",
      "[LOG 20200502-14:24:12] epoch: 8432 train-loss: 0.010551034886803892\n",
      "[LOG 20200502-14:24:13] epoch: 8433 train-loss: 0.010551035611165894\n",
      "[LOG 20200502-14:24:13] epoch: 8434 train-loss: 0.010551037473811043\n",
      "[LOG 20200502-14:24:13] epoch: 8435 train-loss: 0.010551037784251902\n",
      "[LOG 20200502-14:24:13] epoch: 8436 train-loss: 0.010551039543416765\n",
      "[LOG 20200502-14:24:14] epoch: 8437 train-loss: 0.010551040371259054\n",
      "[LOG 20200502-14:24:14] epoch: 8438 train-loss: 0.0105510415095422\n",
      "[LOG 20200502-14:24:14] epoch: 8439 train-loss: 0.01055104254434506\n",
      "[LOG 20200502-14:24:14] epoch: 8440 train-loss: 0.010551043165226778\n",
      "[LOG 20200502-14:24:15] epoch: 8441 train-loss: 0.010551044303509925\n",
      "[LOG 20200502-14:24:15] epoch: 8442 train-loss: 0.010551045027871927\n",
      "[LOG 20200502-14:24:15] epoch: 8443 train-loss: 0.010551045648753643\n",
      "[LOG 20200502-14:24:15] epoch: 8444 train-loss: 0.010551046166155074\n",
      "[LOG 20200502-14:24:15] epoch: 8445 train-loss: 0.01055104678703679\n",
      "[LOG 20200502-14:24:16] epoch: 8446 train-loss: 0.010551047200957933\n",
      "[LOG 20200502-14:24:16] epoch: 8447 train-loss: 0.010551047614879079\n",
      "[LOG 20200502-14:24:16] epoch: 8448 train-loss: 0.010551047511398792\n",
      "[LOG 20200502-14:24:16] epoch: 8449 train-loss: 0.010551048132280508\n",
      "[LOG 20200502-14:24:16] epoch: 8450 train-loss: 0.010551047925319936\n",
      "[LOG 20200502-14:24:17] epoch: 8451 train-loss: 0.010551047718359364\n",
      "[LOG 20200502-14:24:17] epoch: 8452 train-loss: 0.010551048132280508\n",
      "[LOG 20200502-14:24:17] epoch: 8453 train-loss: 0.010551047511398792\n",
      "[LOG 20200502-14:24:17] epoch: 8454 train-loss: 0.010551047200957933\n",
      "[LOG 20200502-14:24:18] epoch: 8455 train-loss: 0.010551047511398792\n",
      "[LOG 20200502-14:24:18] epoch: 8456 train-loss: 0.010551046890517076\n",
      "[LOG 20200502-14:24:18] epoch: 8457 train-loss: 0.010551046890517076\n",
      "[LOG 20200502-14:24:18] epoch: 8458 train-loss: 0.010551045855714215\n",
      "[LOG 20200502-14:24:19] epoch: 8459 train-loss: 0.010551045545273356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:24:19] epoch: 8460 train-loss: 0.010551045131352212\n",
      "[LOG 20200502-14:24:19] epoch: 8461 train-loss: 0.010551043786108494\n",
      "[LOG 20200502-14:24:19] epoch: 8462 train-loss: 0.010551043579147922\n",
      "[LOG 20200502-14:24:19] epoch: 8463 train-loss: 0.010551042026943631\n",
      "[LOG 20200502-14:24:20] epoch: 8464 train-loss: 0.010551041406061914\n",
      "[LOG 20200502-14:24:20] epoch: 8465 train-loss: 0.010551040578219626\n",
      "[LOG 20200502-14:24:20] epoch: 8466 train-loss: 0.010551039232975908\n",
      "[LOG 20200502-14:24:20] epoch: 8467 train-loss: 0.010551038405133618\n",
      "[LOG 20200502-14:24:21] epoch: 8468 train-loss: 0.010551037163370185\n",
      "[LOG 20200502-14:24:21] epoch: 8469 train-loss: 0.010551035300725035\n",
      "[LOG 20200502-14:24:21] epoch: 8470 train-loss: 0.010551034472882748\n",
      "[LOG 20200502-14:24:21] epoch: 8471 train-loss: 0.01055103312763903\n",
      "[LOG 20200502-14:24:21] epoch: 8472 train-loss: 0.010551032506757312\n",
      "[LOG 20200502-14:24:22] epoch: 8473 train-loss: 0.010551030644112162\n",
      "[LOG 20200502-14:24:22] epoch: 8474 train-loss: 0.010551029505829016\n",
      "[LOG 20200502-14:24:22] epoch: 8475 train-loss: 0.010551027850144439\n",
      "[LOG 20200502-14:24:22] epoch: 8476 train-loss: 0.010551026401420435\n",
      "[LOG 20200502-14:24:23] epoch: 8477 train-loss: 0.010551025263137288\n",
      "[LOG 20200502-14:24:23] epoch: 8478 train-loss: 0.010551024331814714\n",
      "[LOG 20200502-14:24:23] epoch: 8479 train-loss: 0.010551022158728706\n",
      "[LOG 20200502-14:24:23] epoch: 8480 train-loss: 0.010551020296083556\n",
      "[LOG 20200502-14:24:23] epoch: 8481 train-loss: 0.01055101967520184\n",
      "[LOG 20200502-14:24:24] epoch: 8482 train-loss: 0.01055101864039898\n",
      "[LOG 20200502-14:24:24] epoch: 8483 train-loss: 0.010551016881234117\n",
      "[LOG 20200502-14:24:24] epoch: 8484 train-loss: 0.010551015639470683\n",
      "[LOG 20200502-14:24:24] epoch: 8485 train-loss: 0.010551014708148109\n",
      "[LOG 20200502-14:24:25] epoch: 8486 train-loss: 0.010551013155943818\n",
      "[LOG 20200502-14:24:25] epoch: 8487 train-loss: 0.010551011914180385\n",
      "[LOG 20200502-14:24:25] epoch: 8488 train-loss: 0.010551010672416952\n",
      "[LOG 20200502-14:24:25] epoch: 8489 train-loss: 0.010551010051535236\n",
      "[LOG 20200502-14:24:25] epoch: 8490 train-loss: 0.010551008188890086\n",
      "[LOG 20200502-14:24:26] epoch: 8491 train-loss: 0.010551007878449228\n",
      "[LOG 20200502-14:24:26] epoch: 8492 train-loss: 0.010551006429725222\n",
      "[LOG 20200502-14:24:26] epoch: 8493 train-loss: 0.010551005912323793\n",
      "[LOG 20200502-14:24:26] epoch: 8494 train-loss: 0.01055100467056036\n",
      "[LOG 20200502-14:24:27] epoch: 8495 train-loss: 0.010551003739237785\n",
      "[LOG 20200502-14:24:27] epoch: 8496 train-loss: 0.010551002600954639\n",
      "[LOG 20200502-14:24:27] epoch: 8497 train-loss: 0.010551002083553208\n",
      "[LOG 20200502-14:24:27] epoch: 8498 train-loss: 0.010551001152230633\n",
      "[LOG 20200502-14:24:28] epoch: 8499 train-loss: 0.01055100042786863\n",
      "[LOG 20200502-14:24:28] epoch: 8500 train-loss: 0.010550999289585484\n",
      "[LOG 20200502-14:24:28] epoch: 8501 train-loss: 0.010550999082624912\n",
      "[LOG 20200502-14:24:28] epoch: 8502 train-loss: 0.010550997737381194\n",
      "[LOG 20200502-14:24:29] epoch: 8503 train-loss: 0.010550997116499476\n",
      "[LOG 20200502-14:24:29] epoch: 8504 train-loss: 0.010550997219979763\n",
      "[LOG 20200502-14:24:29] epoch: 8505 train-loss: 0.010550996185176902\n",
      "[LOG 20200502-14:24:29] epoch: 8506 train-loss: 0.0105509954608149\n",
      "[LOG 20200502-14:24:29] epoch: 8507 train-loss: 0.010550995253854327\n",
      "[LOG 20200502-14:24:30] epoch: 8508 train-loss: 0.01055099411557118\n",
      "[LOG 20200502-14:24:30] epoch: 8509 train-loss: 0.010550993184248606\n",
      "[LOG 20200502-14:24:30] epoch: 8510 train-loss: 0.010550992459886603\n",
      "[LOG 20200502-14:24:30] epoch: 8511 train-loss: 0.010550991839004887\n",
      "[LOG 20200502-14:24:31] epoch: 8512 train-loss: 0.0105509910111626\n",
      "[LOG 20200502-14:24:31] epoch: 8513 train-loss: 0.010550989769399166\n",
      "[LOG 20200502-14:24:31] epoch: 8514 train-loss: 0.010550988941556878\n",
      "[LOG 20200502-14:24:31] epoch: 8515 train-loss: 0.010550987699793445\n",
      "[LOG 20200502-14:24:31] epoch: 8516 train-loss: 0.010550987492832873\n",
      "[LOG 20200502-14:24:32] epoch: 8517 train-loss: 0.010550985630187724\n",
      "[LOG 20200502-14:24:32] epoch: 8518 train-loss: 0.010550984802345434\n",
      "[LOG 20200502-14:24:32] epoch: 8519 train-loss: 0.010550983560582003\n",
      "[LOG 20200502-14:24:32] epoch: 8520 train-loss: 0.010550982215338282\n",
      "[LOG 20200502-14:24:33] epoch: 8521 train-loss: 0.010550980870094564\n",
      "[LOG 20200502-14:24:33] epoch: 8522 train-loss: 0.010550979110929701\n",
      "[LOG 20200502-14:24:33] epoch: 8523 train-loss: 0.01055097755872541\n",
      "[LOG 20200502-14:24:33] epoch: 8524 train-loss: 0.01055097569608026\n",
      "[LOG 20200502-14:24:34] epoch: 8525 train-loss: 0.01055097414387597\n",
      "[LOG 20200502-14:24:34] epoch: 8526 train-loss: 0.010550971970789962\n",
      "[LOG 20200502-14:24:34] epoch: 8527 train-loss: 0.010550970108144812\n",
      "[LOG 20200502-14:24:34] epoch: 8528 train-loss: 0.010550967624617947\n",
      "[LOG 20200502-14:24:34] epoch: 8529 train-loss: 0.010550965141091082\n",
      "[LOG 20200502-14:24:35] epoch: 8530 train-loss: 0.010550962864524789\n",
      "[LOG 20200502-14:24:35] epoch: 8531 train-loss: 0.010550960691438781\n",
      "[LOG 20200502-14:24:35] epoch: 8532 train-loss: 0.010550958207911916\n",
      "[LOG 20200502-14:24:35] epoch: 8533 train-loss: 0.01055095468958219\n",
      "[LOG 20200502-14:24:36] epoch: 8534 train-loss: 0.010550952206055323\n",
      "[LOG 20200502-14:24:36] epoch: 8535 train-loss: 0.010550949205127027\n",
      "[LOG 20200502-14:24:36] epoch: 8536 train-loss: 0.010550946204198731\n",
      "[LOG 20200502-14:24:36] epoch: 8537 train-loss: 0.010550942789349291\n",
      "[LOG 20200502-14:24:36] epoch: 8538 train-loss: 0.010550939581460424\n",
      "[LOG 20200502-14:24:37] epoch: 8539 train-loss: 0.010550936994453272\n",
      "[LOG 20200502-14:24:37] epoch: 8540 train-loss: 0.010550932855241828\n",
      "[LOG 20200502-14:24:37] epoch: 8541 train-loss: 0.010550929854313532\n",
      "[LOG 20200502-14:24:37] epoch: 8542 train-loss: 0.01055092623250352\n",
      "[LOG 20200502-14:24:38] epoch: 8543 train-loss: 0.010550923438535796\n",
      "[LOG 20200502-14:24:38] epoch: 8544 train-loss: 0.01055091992020607\n",
      "[LOG 20200502-14:24:38] epoch: 8545 train-loss: 0.010550916091435485\n",
      "[LOG 20200502-14:24:38] epoch: 8546 train-loss: 0.010550912883546617\n",
      "[LOG 20200502-14:24:38] epoch: 8547 train-loss: 0.010550909468697177\n",
      "[LOG 20200502-14:24:39] epoch: 8548 train-loss: 0.010550905743406879\n",
      "[LOG 20200502-14:24:39] epoch: 8549 train-loss: 0.010550902432037724\n",
      "[LOG 20200502-14:24:39] epoch: 8550 train-loss: 0.01055089912066857\n",
      "[LOG 20200502-14:24:39] epoch: 8551 train-loss: 0.01055089570581913\n",
      "[LOG 20200502-14:24:40] epoch: 8552 train-loss: 0.01055089280837112\n",
      "[LOG 20200502-14:24:40] epoch: 8553 train-loss: 0.010550889807442823\n",
      "[LOG 20200502-14:24:40] epoch: 8554 train-loss: 0.01055088649607367\n",
      "[LOG 20200502-14:24:40] epoch: 8555 train-loss: 0.010550883805586232\n",
      "[LOG 20200502-14:24:41] epoch: 8556 train-loss: 0.010550881115098795\n",
      "[LOG 20200502-14:24:41] epoch: 8557 train-loss: 0.010550878114170499\n",
      "[LOG 20200502-14:24:41] epoch: 8558 train-loss: 0.010550875320202775\n",
      "[LOG 20200502-14:24:41] epoch: 8559 train-loss: 0.010550872733195623\n",
      "[LOG 20200502-14:24:42] epoch: 8560 train-loss: 0.010550870042708185\n",
      "[LOG 20200502-14:24:42] epoch: 8561 train-loss: 0.01055086807658275\n",
      "[LOG 20200502-14:24:42] epoch: 8562 train-loss: 0.01055086569653617\n",
      "[LOG 20200502-14:24:42] epoch: 8563 train-loss: 0.010550863523450162\n",
      "[LOG 20200502-14:24:42] epoch: 8564 train-loss: 0.010550861660805013\n",
      "[LOG 20200502-14:24:43] epoch: 8565 train-loss: 0.010550860108600723\n",
      "[LOG 20200502-14:24:43] epoch: 8566 train-loss: 0.010550858245955573\n",
      "[LOG 20200502-14:24:43] epoch: 8567 train-loss: 0.010550856176349852\n",
      "[LOG 20200502-14:24:43] epoch: 8568 train-loss: 0.010550855038066706\n",
      "[LOG 20200502-14:24:44] epoch: 8569 train-loss: 0.0105508535893427\n",
      "[LOG 20200502-14:24:44] epoch: 8570 train-loss: 0.010550852554539839\n",
      "[LOG 20200502-14:24:44] epoch: 8571 train-loss: 0.010550851312776407\n",
      "[LOG 20200502-14:24:44] epoch: 8572 train-loss: 0.01055084965709183\n",
      "[LOG 20200502-14:24:45] epoch: 8573 train-loss: 0.010550849346650971\n",
      "[LOG 20200502-14:24:45] epoch: 8574 train-loss: 0.010550848518808683\n",
      "[LOG 20200502-14:24:45] epoch: 8575 train-loss: 0.010550848001407253\n",
      "[LOG 20200502-14:24:45] epoch: 8576 train-loss: 0.010550847380525537\n",
      "[LOG 20200502-14:24:45] epoch: 8577 train-loss: 0.010550846656163534\n",
      "[LOG 20200502-14:24:46] epoch: 8578 train-loss: 0.010550846138762103\n",
      "[LOG 20200502-14:24:46] epoch: 8579 train-loss: 0.010550846138762103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:24:46] epoch: 8580 train-loss: 0.010550845517880388\n",
      "[LOG 20200502-14:24:46] epoch: 8581 train-loss: 0.0105508454144001\n",
      "[LOG 20200502-14:24:47] epoch: 8582 train-loss: 0.010550844690038098\n",
      "[LOG 20200502-14:24:47] epoch: 8583 train-loss: 0.01055084489699867\n",
      "[LOG 20200502-14:24:47] epoch: 8584 train-loss: 0.010550845000478957\n",
      "[LOG 20200502-14:24:47] epoch: 8585 train-loss: 0.010550845517880388\n",
      "[LOG 20200502-14:24:48] epoch: 8586 train-loss: 0.010550845000478957\n",
      "[LOG 20200502-14:24:48] epoch: 8587 train-loss: 0.010550845000478957\n",
      "[LOG 20200502-14:24:48] epoch: 8588 train-loss: 0.0105508454144001\n",
      "[LOG 20200502-14:24:48] epoch: 8589 train-loss: 0.010550845517880388\n",
      "[LOG 20200502-14:24:48] epoch: 8590 train-loss: 0.010550845621360673\n",
      "[LOG 20200502-14:24:49] epoch: 8591 train-loss: 0.010550846138762103\n",
      "[LOG 20200502-14:24:49] epoch: 8592 train-loss: 0.010550846449202962\n",
      "[LOG 20200502-14:24:49] epoch: 8593 train-loss: 0.010550846966604391\n",
      "[LOG 20200502-14:24:49] epoch: 8594 train-loss: 0.01055084727704525\n",
      "[LOG 20200502-14:24:50] epoch: 8595 train-loss: 0.01055084727704525\n",
      "[LOG 20200502-14:24:50] epoch: 8596 train-loss: 0.010550847484005822\n",
      "[LOG 20200502-14:24:50] epoch: 8597 train-loss: 0.010550847484005822\n",
      "[LOG 20200502-14:24:50] epoch: 8598 train-loss: 0.010550847897926966\n",
      "[LOG 20200502-14:24:50] epoch: 8599 train-loss: 0.010550848104887538\n",
      "[LOG 20200502-14:24:51] epoch: 8600 train-loss: 0.010550848311848111\n",
      "[LOG 20200502-14:24:51] epoch: 8601 train-loss: 0.010550848725769255\n",
      "[LOG 20200502-14:24:51] epoch: 8602 train-loss: 0.010550848001407253\n",
      "[LOG 20200502-14:24:51] epoch: 8603 train-loss: 0.010550848311848111\n",
      "[LOG 20200502-14:24:52] epoch: 8604 train-loss: 0.010550848932729827\n",
      "[LOG 20200502-14:24:52] epoch: 8605 train-loss: 0.010550848932729827\n",
      "[LOG 20200502-14:24:52] epoch: 8606 train-loss: 0.010550848518808683\n",
      "[LOG 20200502-14:24:52] epoch: 8607 train-loss: 0.010550848932729827\n",
      "[LOG 20200502-14:24:52] epoch: 8608 train-loss: 0.010550848622288968\n",
      "[LOG 20200502-14:24:53] epoch: 8609 train-loss: 0.010550848001407253\n",
      "[LOG 20200502-14:24:53] epoch: 8610 train-loss: 0.010550848208367825\n",
      "[LOG 20200502-14:24:53] epoch: 8611 train-loss: 0.01055084779444668\n",
      "[LOG 20200502-14:24:53] epoch: 8612 train-loss: 0.010550847587486109\n",
      "[LOG 20200502-14:24:54] epoch: 8613 train-loss: 0.01055084727704525\n",
      "[LOG 20200502-14:24:54] epoch: 8614 train-loss: 0.010550846656163534\n",
      "[LOG 20200502-14:24:54] epoch: 8615 train-loss: 0.010550846242242388\n",
      "[LOG 20200502-14:24:54] epoch: 8616 train-loss: 0.010550845621360673\n",
      "[LOG 20200502-14:24:55] epoch: 8617 train-loss: 0.010550845103959242\n",
      "[LOG 20200502-14:24:55] epoch: 8618 train-loss: 0.010550844586557813\n",
      "[LOG 20200502-14:24:55] epoch: 8619 train-loss: 0.010550844172636667\n",
      "[LOG 20200502-14:24:55] epoch: 8620 train-loss: 0.010550842827392949\n",
      "[LOG 20200502-14:24:56] epoch: 8621 train-loss: 0.010550842103030946\n",
      "[LOG 20200502-14:24:56] epoch: 8622 train-loss: 0.010550841171708372\n",
      "[LOG 20200502-14:24:56] epoch: 8623 train-loss: 0.010550840343866084\n",
      "[LOG 20200502-14:24:56] epoch: 8624 train-loss: 0.010550839619504081\n",
      "[LOG 20200502-14:24:57] epoch: 8625 train-loss: 0.010550838170780076\n",
      "[LOG 20200502-14:24:57] epoch: 8626 train-loss: 0.010550836825536357\n",
      "[LOG 20200502-14:24:57] epoch: 8627 train-loss: 0.010550835583772924\n",
      "[LOG 20200502-14:24:57] epoch: 8628 train-loss: 0.010550834445489777\n",
      "[LOG 20200502-14:24:57] epoch: 8629 train-loss: 0.010550833410686918\n",
      "[LOG 20200502-14:24:58] epoch: 8630 train-loss: 0.010550831858482625\n",
      "[LOG 20200502-14:24:58] epoch: 8631 train-loss: 0.010550830306278335\n",
      "[LOG 20200502-14:24:58] epoch: 8632 train-loss: 0.010550829271475473\n",
      "[LOG 20200502-14:24:58] epoch: 8633 train-loss: 0.010550828133192327\n",
      "[LOG 20200502-14:24:59] epoch: 8634 train-loss: 0.01055082596010632\n",
      "[LOG 20200502-14:24:59] epoch: 8635 train-loss: 0.010550825235744318\n",
      "[LOG 20200502-14:24:59] epoch: 8636 train-loss: 0.010550823476579454\n",
      "[LOG 20200502-14:25:00] epoch: 8637 train-loss: 0.010550821613934305\n",
      "[LOG 20200502-14:25:00] epoch: 8638 train-loss: 0.010550820061730014\n",
      "[LOG 20200502-14:25:00] epoch: 8639 train-loss: 0.010550817992124293\n",
      "[LOG 20200502-14:25:00] epoch: 8640 train-loss: 0.01055081643992\n",
      "[LOG 20200502-14:25:01] epoch: 8641 train-loss: 0.010550815405117141\n",
      "[LOG 20200502-14:25:01] epoch: 8642 train-loss: 0.01055081333551142\n",
      "[LOG 20200502-14:25:01] epoch: 8643 train-loss: 0.010550812404188845\n",
      "[LOG 20200502-14:25:01] epoch: 8644 train-loss: 0.010550810645023981\n",
      "[LOG 20200502-14:25:02] epoch: 8645 train-loss: 0.01055080857541826\n",
      "[LOG 20200502-14:25:02] epoch: 8646 train-loss: 0.01055080702321397\n",
      "[LOG 20200502-14:25:02] epoch: 8647 train-loss: 0.010550805367529392\n",
      "[LOG 20200502-14:25:02] epoch: 8648 train-loss: 0.010550803918805387\n",
      "[LOG 20200502-14:25:03] epoch: 8649 train-loss: 0.010550802159640525\n",
      "[LOG 20200502-14:25:03] epoch: 8650 train-loss: 0.010550800607436232\n",
      "[LOG 20200502-14:25:03] epoch: 8651 train-loss: 0.010550798641310798\n",
      "[LOG 20200502-14:25:03] epoch: 8652 train-loss: 0.010550797503027651\n",
      "[LOG 20200502-14:25:04] epoch: 8653 train-loss: 0.010550795640382502\n",
      "[LOG 20200502-14:25:04] epoch: 8654 train-loss: 0.0105507949160205\n",
      "[LOG 20200502-14:25:04] epoch: 8655 train-loss: 0.010550792639454206\n",
      "[LOG 20200502-14:25:04] epoch: 8656 train-loss: 0.010550791397690773\n",
      "[LOG 20200502-14:25:05] epoch: 8657 train-loss: 0.010550789328085052\n",
      "[LOG 20200502-14:25:05] epoch: 8658 train-loss: 0.01055078829328219\n",
      "[LOG 20200502-14:25:05] epoch: 8659 train-loss: 0.010550786637597613\n",
      "[LOG 20200502-14:25:05] epoch: 8660 train-loss: 0.010550785085393323\n",
      "[LOG 20200502-14:25:05] epoch: 8661 train-loss: 0.01055078384362989\n",
      "[LOG 20200502-14:25:06] epoch: 8662 train-loss: 0.010550782394905886\n",
      "[LOG 20200502-14:25:06] epoch: 8663 train-loss: 0.010550781153142452\n",
      "[LOG 20200502-14:25:06] epoch: 8664 train-loss: 0.010550779807898734\n",
      "[LOG 20200502-14:25:06] epoch: 8665 train-loss: 0.010550777324371867\n",
      "[LOG 20200502-14:25:07] epoch: 8666 train-loss: 0.010550776393049292\n",
      "[LOG 20200502-14:25:07] epoch: 8667 train-loss: 0.010550774944325289\n",
      "[LOG 20200502-14:25:07] epoch: 8668 train-loss: 0.010550773495601283\n",
      "[LOG 20200502-14:25:07] epoch: 8669 train-loss: 0.010550771943396993\n",
      "[LOG 20200502-14:25:07] epoch: 8670 train-loss: 0.010550770287712416\n",
      "[LOG 20200502-14:25:08] epoch: 8671 train-loss: 0.010550768735508123\n",
      "[LOG 20200502-14:25:08] epoch: 8672 train-loss: 0.010550767183303833\n",
      "[LOG 20200502-14:25:08] epoch: 8673 train-loss: 0.010550765631099543\n",
      "[LOG 20200502-14:25:08] epoch: 8674 train-loss: 0.010550764182375537\n",
      "[LOG 20200502-14:25:09] epoch: 8675 train-loss: 0.010550762319730388\n",
      "[LOG 20200502-14:25:09] epoch: 8676 train-loss: 0.01055076097448667\n",
      "[LOG 20200502-14:25:09] epoch: 8677 train-loss: 0.010550759318802092\n",
      "[LOG 20200502-14:25:09] epoch: 8678 train-loss: 0.010550756835275225\n",
      "[LOG 20200502-14:25:09] epoch: 8679 train-loss: 0.010550755800472366\n",
      "[LOG 20200502-14:25:10] epoch: 8680 train-loss: 0.0105507533169455\n",
      "[LOG 20200502-14:25:10] epoch: 8681 train-loss: 0.010550751247339778\n",
      "[LOG 20200502-14:25:10] epoch: 8682 train-loss: 0.010550749488174915\n",
      "[LOG 20200502-14:25:10] epoch: 8683 train-loss: 0.010550747625529766\n",
      "[LOG 20200502-14:25:11] epoch: 8684 train-loss: 0.010550745245483186\n",
      "[LOG 20200502-14:25:11] epoch: 8685 train-loss: 0.010550742658476034\n",
      "[LOG 20200502-14:25:11] epoch: 8686 train-loss: 0.010550740795830885\n",
      "[LOG 20200502-14:25:11] epoch: 8687 train-loss: 0.01055073882970545\n",
      "[LOG 20200502-14:25:12] epoch: 8688 train-loss: 0.01055073644965887\n",
      "[LOG 20200502-14:25:12] epoch: 8689 train-loss: 0.010550733448730575\n",
      "[LOG 20200502-14:25:12] epoch: 8690 train-loss: 0.010550730344321992\n",
      "[LOG 20200502-14:25:12] epoch: 8691 train-loss: 0.010550728481676843\n",
      "[LOG 20200502-14:25:12] epoch: 8692 train-loss: 0.01055072568770912\n",
      "[LOG 20200502-14:25:13] epoch: 8693 train-loss: 0.010550722686780823\n",
      "[LOG 20200502-14:25:13] epoch: 8694 train-loss: 0.010550720099773672\n",
      "[LOG 20200502-14:25:13] epoch: 8695 train-loss: 0.010550717098845376\n",
      "[LOG 20200502-14:25:13] epoch: 8696 train-loss: 0.010550714201397367\n",
      "[LOG 20200502-14:25:14] epoch: 8697 train-loss: 0.01055071120046907\n",
      "[LOG 20200502-14:25:14] epoch: 8698 train-loss: 0.01055070830302106\n",
      "[LOG 20200502-14:25:14] epoch: 8699 train-loss: 0.010550705198612478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:25:14] epoch: 8700 train-loss: 0.010550702301164469\n",
      "[LOG 20200502-14:25:14] epoch: 8701 train-loss: 0.010550698989795314\n",
      "[LOG 20200502-14:25:15] epoch: 8702 train-loss: 0.010550695988867018\n",
      "[LOG 20200502-14:25:15] epoch: 8703 train-loss: 0.010550692884458436\n",
      "[LOG 20200502-14:25:15] epoch: 8704 train-loss: 0.010550689159168137\n",
      "[LOG 20200502-14:25:15] epoch: 8705 train-loss: 0.010550686675641272\n",
      "[LOG 20200502-14:25:16] epoch: 8706 train-loss: 0.01055068357123269\n",
      "[LOG 20200502-14:25:16] epoch: 8707 train-loss: 0.01055068015638325\n",
      "[LOG 20200502-14:25:16] epoch: 8708 train-loss: 0.010550677051974667\n",
      "[LOG 20200502-14:25:16] epoch: 8709 train-loss: 0.010550674464967515\n",
      "[LOG 20200502-14:25:16] epoch: 8710 train-loss: 0.010550671257078648\n",
      "[LOG 20200502-14:25:17] epoch: 8711 train-loss: 0.010550667842229208\n",
      "[LOG 20200502-14:25:17] epoch: 8712 train-loss: 0.010550664841300912\n",
      "[LOG 20200502-14:25:17] epoch: 8713 train-loss: 0.010550661840372615\n",
      "[LOG 20200502-14:25:17] epoch: 8714 train-loss: 0.010550659253365464\n",
      "[LOG 20200502-14:25:18] epoch: 8715 train-loss: 0.010550656562878026\n",
      "[LOG 20200502-14:25:18] epoch: 8716 train-loss: 0.01055065407935116\n",
      "[LOG 20200502-14:25:18] epoch: 8717 train-loss: 0.010550651595824294\n",
      "[LOG 20200502-14:25:18] epoch: 8718 train-loss: 0.010550649008817144\n",
      "[LOG 20200502-14:25:19] epoch: 8719 train-loss: 0.010550647042691708\n",
      "[LOG 20200502-14:25:19] epoch: 8720 train-loss: 0.01055064383480284\n",
      "[LOG 20200502-14:25:19] epoch: 8721 train-loss: 0.01055064197215769\n",
      "[LOG 20200502-14:25:19] epoch: 8722 train-loss: 0.010550640006032255\n",
      "[LOG 20200502-14:25:19] epoch: 8723 train-loss: 0.010550637625985675\n",
      "[LOG 20200502-14:25:20] epoch: 8724 train-loss: 0.010550635038978524\n",
      "[LOG 20200502-14:25:20] epoch: 8725 train-loss: 0.010550633590254519\n",
      "[LOG 20200502-14:25:20] epoch: 8726 train-loss: 0.010550632038050227\n",
      "[LOG 20200502-14:25:20] epoch: 8727 train-loss: 0.010550630175405078\n",
      "[LOG 20200502-14:25:21] epoch: 8728 train-loss: 0.010550628519720502\n",
      "[LOG 20200502-14:25:21] epoch: 8729 train-loss: 0.010550627174476782\n",
      "[LOG 20200502-14:25:21] epoch: 8730 train-loss: 0.010550624897910489\n",
      "[LOG 20200502-14:25:21] epoch: 8731 train-loss: 0.010550623966587914\n",
      "[LOG 20200502-14:25:21] epoch: 8732 train-loss: 0.010550622621344196\n",
      "[LOG 20200502-14:25:22] epoch: 8733 train-loss: 0.010550621379580762\n",
      "[LOG 20200502-14:25:22] epoch: 8734 train-loss: 0.0105506196204159\n",
      "[LOG 20200502-14:25:22] epoch: 8735 train-loss: 0.010550618896053897\n",
      "[LOG 20200502-14:25:22] epoch: 8736 train-loss: 0.010550618068211608\n",
      "[LOG 20200502-14:25:23] epoch: 8737 train-loss: 0.01055061672296789\n",
      "[LOG 20200502-14:25:23] epoch: 8738 train-loss: 0.010550615998605887\n",
      "[LOG 20200502-14:25:23] epoch: 8739 train-loss: 0.010550615584684743\n",
      "[LOG 20200502-14:25:23] epoch: 8740 train-loss: 0.010550614446401596\n",
      "[LOG 20200502-14:25:24] epoch: 8741 train-loss: 0.010550614239441024\n",
      "[LOG 20200502-14:25:24] epoch: 8742 train-loss: 0.010550613101157878\n",
      "[LOG 20200502-14:25:24] epoch: 8743 train-loss: 0.010550612687236734\n",
      "[LOG 20200502-14:25:24] epoch: 8744 train-loss: 0.010550611859394444\n",
      "[LOG 20200502-14:25:24] epoch: 8745 train-loss: 0.010550612169835303\n",
      "[LOG 20200502-14:25:25] epoch: 8746 train-loss: 0.010550610824591584\n",
      "[LOG 20200502-14:25:25] epoch: 8747 train-loss: 0.010550610514150726\n",
      "[LOG 20200502-14:25:25] epoch: 8748 train-loss: 0.010550610410670439\n",
      "[LOG 20200502-14:25:25] epoch: 8749 train-loss: 0.01055060989326901\n",
      "[LOG 20200502-14:25:26] epoch: 8750 train-loss: 0.010550609375867579\n",
      "[LOG 20200502-14:25:26] epoch: 8751 train-loss: 0.010550609168907007\n",
      "[LOG 20200502-14:25:26] epoch: 8752 train-loss: 0.010550608237584433\n",
      "[LOG 20200502-14:25:26] epoch: 8753 train-loss: 0.010550608341064718\n",
      "[LOG 20200502-14:25:26] epoch: 8754 train-loss: 0.01055060803062386\n",
      "[LOG 20200502-14:25:27] epoch: 8755 train-loss: 0.010550607409742143\n",
      "[LOG 20200502-14:25:27] epoch: 8756 train-loss: 0.010550607306261858\n",
      "[LOG 20200502-14:25:27] epoch: 8757 train-loss: 0.010550606788860427\n",
      "[LOG 20200502-14:25:27] epoch: 8758 train-loss: 0.010550606478419568\n",
      "[LOG 20200502-14:25:28] epoch: 8759 train-loss: 0.010550605961018138\n",
      "[LOG 20200502-14:25:28] epoch: 8760 train-loss: 0.010550605547096994\n",
      "[LOG 20200502-14:25:28] epoch: 8761 train-loss: 0.01055060565057728\n",
      "[LOG 20200502-14:25:28] epoch: 8762 train-loss: 0.010550604305333562\n",
      "[LOG 20200502-14:25:28] epoch: 8763 train-loss: 0.010550604305333562\n",
      "[LOG 20200502-14:25:29] epoch: 8764 train-loss: 0.010550604201853275\n",
      "[LOG 20200502-14:25:29] epoch: 8765 train-loss: 0.01055060358097156\n",
      "[LOG 20200502-14:25:29] epoch: 8766 train-loss: 0.01055060275312927\n",
      "[LOG 20200502-14:25:29] epoch: 8767 train-loss: 0.010550602339208126\n",
      "[LOG 20200502-14:25:30] epoch: 8768 train-loss: 0.01055060171832641\n",
      "[LOG 20200502-14:25:30] epoch: 8769 train-loss: 0.010550600580043264\n",
      "[LOG 20200502-14:25:30] epoch: 8770 train-loss: 0.010550600269602405\n",
      "[LOG 20200502-14:25:30] epoch: 8771 train-loss: 0.010550599855681261\n",
      "[LOG 20200502-14:25:31] epoch: 8772 train-loss: 0.010550599131319258\n",
      "[LOG 20200502-14:25:31] epoch: 8773 train-loss: 0.010550598406957256\n",
      "[LOG 20200502-14:25:31] epoch: 8774 train-loss: 0.01055059778607554\n",
      "[LOG 20200502-14:25:31] epoch: 8775 train-loss: 0.010550596751272678\n",
      "[LOG 20200502-14:25:32] epoch: 8776 train-loss: 0.01055059540602896\n",
      "[LOG 20200502-14:25:32] epoch: 8777 train-loss: 0.010550594267745813\n",
      "[LOG 20200502-14:25:32] epoch: 8778 train-loss: 0.010550594060785241\n",
      "[LOG 20200502-14:25:32] epoch: 8779 train-loss: 0.010550592715541521\n",
      "[LOG 20200502-14:25:32] epoch: 8780 train-loss: 0.010550591887699233\n",
      "[LOG 20200502-14:25:33] epoch: 8781 train-loss: 0.0105505906459358\n",
      "[LOG 20200502-14:25:33] epoch: 8782 train-loss: 0.010550590025054084\n",
      "[LOG 20200502-14:25:33] epoch: 8783 train-loss: 0.010550587955448363\n",
      "[LOG 20200502-14:25:33] epoch: 8784 train-loss: 0.010550586920645501\n",
      "[LOG 20200502-14:25:33] epoch: 8785 train-loss: 0.010550586299763786\n",
      "[LOG 20200502-14:25:34] epoch: 8786 train-loss: 0.010550584747559495\n",
      "[LOG 20200502-14:25:34] epoch: 8787 train-loss: 0.010550583402315775\n",
      "[LOG 20200502-14:25:34] epoch: 8788 train-loss: 0.010550582160552343\n",
      "[LOG 20200502-14:25:34] epoch: 8789 train-loss: 0.010550580711828338\n",
      "[LOG 20200502-14:25:35] epoch: 8790 train-loss: 0.010550579677025477\n",
      "[LOG 20200502-14:25:35] epoch: 8791 train-loss: 0.010550578124821186\n",
      "[LOG 20200502-14:25:35] epoch: 8792 train-loss: 0.010550577607419755\n",
      "[LOG 20200502-14:25:35] epoch: 8793 train-loss: 0.010550575641294321\n",
      "[LOG 20200502-14:25:35] epoch: 8794 train-loss: 0.010550573882129457\n",
      "[LOG 20200502-14:25:36] epoch: 8795 train-loss: 0.010550572950806882\n",
      "[LOG 20200502-14:25:36] epoch: 8796 train-loss: 0.010550571088161733\n",
      "[LOG 20200502-14:25:36] epoch: 8797 train-loss: 0.010550569742918015\n",
      "[LOG 20200502-14:25:36] epoch: 8798 train-loss: 0.010550568501154581\n",
      "[LOG 20200502-14:25:37] epoch: 8799 train-loss: 0.010550567052430578\n",
      "[LOG 20200502-14:25:37] epoch: 8800 train-loss: 0.010550565189785428\n",
      "[LOG 20200502-14:25:37] epoch: 8801 train-loss: 0.01055056405150228\n",
      "[LOG 20200502-14:25:37] epoch: 8802 train-loss: 0.010550562602778276\n",
      "[LOG 20200502-14:25:37] epoch: 8803 train-loss: 0.010550561154054271\n",
      "[LOG 20200502-14:25:38] epoch: 8804 train-loss: 0.010550559705330266\n",
      "[LOG 20200502-14:25:38] epoch: 8805 train-loss: 0.010550557946165403\n",
      "[LOG 20200502-14:25:38] epoch: 8806 train-loss: 0.01055055670440197\n",
      "[LOG 20200502-14:25:38] epoch: 8807 train-loss: 0.010550555462638537\n",
      "[LOG 20200502-14:25:39] epoch: 8808 train-loss: 0.010550553703473674\n",
      "[LOG 20200502-14:25:39] epoch: 8809 train-loss: 0.010550552358229956\n",
      "[LOG 20200502-14:25:39] epoch: 8810 train-loss: 0.010550550495584806\n",
      "[LOG 20200502-14:25:39] epoch: 8811 train-loss: 0.010550549460781945\n",
      "[LOG 20200502-14:25:39] epoch: 8812 train-loss: 0.010550547908577654\n",
      "[LOG 20200502-14:25:40] epoch: 8813 train-loss: 0.01055054614941279\n",
      "[LOG 20200502-14:25:40] epoch: 8814 train-loss: 0.01055054511460993\n",
      "[LOG 20200502-14:25:40] epoch: 8815 train-loss: 0.010550543148484495\n",
      "[LOG 20200502-14:25:40] epoch: 8816 train-loss: 0.010550541492799917\n",
      "[LOG 20200502-14:25:41] epoch: 8817 train-loss: 0.010550540768437915\n",
      "[LOG 20200502-14:25:41] epoch: 8818 train-loss: 0.010550538698832193\n",
      "[LOG 20200502-14:25:41] epoch: 8819 train-loss: 0.010550537457068762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:25:41] epoch: 8820 train-loss: 0.010550535594423613\n",
      "[LOG 20200502-14:25:42] epoch: 8821 train-loss: 0.010550534559620751\n",
      "[LOG 20200502-14:25:42] epoch: 8822 train-loss: 0.01055053300741646\n",
      "[LOG 20200502-14:25:42] epoch: 8823 train-loss: 0.010550531351731883\n",
      "[LOG 20200502-14:25:42] epoch: 8824 train-loss: 0.010550529696047306\n",
      "[LOG 20200502-14:25:43] epoch: 8825 train-loss: 0.010550528143843016\n",
      "[LOG 20200502-14:25:43] epoch: 8826 train-loss: 0.010550526074237294\n",
      "[LOG 20200502-14:25:43] epoch: 8827 train-loss: 0.010550524418552717\n",
      "[LOG 20200502-14:25:43] epoch: 8828 train-loss: 0.01055052328026957\n",
      "[LOG 20200502-14:25:44] epoch: 8829 train-loss: 0.010550521210663848\n",
      "[LOG 20200502-14:25:44] epoch: 8830 train-loss: 0.010550519658459557\n",
      "[LOG 20200502-14:25:44] epoch: 8831 train-loss: 0.010550518106255267\n",
      "[LOG 20200502-14:25:44] epoch: 8832 train-loss: 0.010550516243610118\n",
      "[LOG 20200502-14:25:44] epoch: 8833 train-loss: 0.01055051407052411\n",
      "[LOG 20200502-14:25:45] epoch: 8834 train-loss: 0.010550512518319819\n",
      "[LOG 20200502-14:25:45] epoch: 8835 train-loss: 0.010550510862635242\n",
      "[LOG 20200502-14:25:45] epoch: 8836 train-loss: 0.010550508896509806\n",
      "[LOG 20200502-14:25:45] epoch: 8837 train-loss: 0.010550506930384371\n",
      "[LOG 20200502-14:25:46] epoch: 8838 train-loss: 0.010550504757298363\n",
      "[LOG 20200502-14:25:46] epoch: 8839 train-loss: 0.010550502791172929\n",
      "[LOG 20200502-14:25:46] epoch: 8840 train-loss: 0.010550500721567206\n",
      "[LOG 20200502-14:25:46] epoch: 8841 train-loss: 0.0105504985484812\n",
      "[LOG 20200502-14:25:47] epoch: 8842 train-loss: 0.010550496375395192\n",
      "[LOG 20200502-14:25:47] epoch: 8843 train-loss: 0.010550494202309184\n",
      "[LOG 20200502-14:25:47] epoch: 8844 train-loss: 0.010550492132703463\n",
      "[LOG 20200502-14:25:47] epoch: 8845 train-loss: 0.010550489959617456\n",
      "[LOG 20200502-14:25:48] epoch: 8846 train-loss: 0.010550487579570876\n",
      "[LOG 20200502-14:25:48] epoch: 8847 train-loss: 0.010550485406484868\n",
      "[LOG 20200502-14:25:48] epoch: 8848 train-loss: 0.010550483129918575\n",
      "[LOG 20200502-14:25:48] epoch: 8849 train-loss: 0.010550480025509993\n",
      "[LOG 20200502-14:25:48] epoch: 8850 train-loss: 0.010550478473305702\n",
      "[LOG 20200502-14:25:49] epoch: 8851 train-loss: 0.010550476300219694\n",
      "[LOG 20200502-14:25:49] epoch: 8852 train-loss: 0.010550473713212542\n",
      "[LOG 20200502-14:25:49] epoch: 8853 train-loss: 0.010550471229685677\n",
      "[LOG 20200502-14:25:50] epoch: 8854 train-loss: 0.01055046905659967\n",
      "[LOG 20200502-14:25:50] epoch: 8855 train-loss: 0.010550466676553091\n",
      "[LOG 20200502-14:25:50] epoch: 8856 train-loss: 0.010550464399986796\n",
      "[LOG 20200502-14:25:50] epoch: 8857 train-loss: 0.010550461606019072\n",
      "[LOG 20200502-14:25:51] epoch: 8858 train-loss: 0.01055045932945278\n",
      "[LOG 20200502-14:25:51] epoch: 8859 train-loss: 0.010550456638965342\n",
      "[LOG 20200502-14:25:51] epoch: 8860 train-loss: 0.010550454362399049\n",
      "[LOG 20200502-14:25:51] epoch: 8861 train-loss: 0.010550452189313041\n",
      "[LOG 20200502-14:25:51] epoch: 8862 train-loss: 0.010550449705786176\n",
      "[LOG 20200502-14:25:52] epoch: 8863 train-loss: 0.010550448153581884\n",
      "[LOG 20200502-14:25:52] epoch: 8864 train-loss: 0.010550446083976163\n",
      "[LOG 20200502-14:25:52] epoch: 8865 train-loss: 0.010550443910890155\n",
      "[LOG 20200502-14:25:52] epoch: 8866 train-loss: 0.010550441220402718\n",
      "[LOG 20200502-14:25:53] epoch: 8867 train-loss: 0.010550439254277282\n",
      "[LOG 20200502-14:25:53] epoch: 8868 train-loss: 0.010550436977710988\n",
      "[LOG 20200502-14:25:53] epoch: 8869 train-loss: 0.01055043511506584\n",
      "[LOG 20200502-14:25:53] epoch: 8870 train-loss: 0.010550433355900977\n",
      "[LOG 20200502-14:25:54] epoch: 8871 train-loss: 0.010550431493255828\n",
      "[LOG 20200502-14:25:54] epoch: 8872 train-loss: 0.010550429734090963\n",
      "[LOG 20200502-14:25:54] epoch: 8873 train-loss: 0.010550428181886673\n",
      "[LOG 20200502-14:25:54] epoch: 8874 train-loss: 0.010550426112280952\n",
      "[LOG 20200502-14:25:54] epoch: 8875 train-loss: 0.010550424456596375\n",
      "[LOG 20200502-14:25:55] epoch: 8876 train-loss: 0.010550422800911797\n",
      "[LOG 20200502-14:25:55] epoch: 8877 train-loss: 0.010550421352187792\n",
      "[LOG 20200502-14:25:55] epoch: 8878 train-loss: 0.010550420213904645\n",
      "[LOG 20200502-14:25:55] epoch: 8879 train-loss: 0.010550418972141214\n",
      "[LOG 20200502-14:25:56] epoch: 8880 train-loss: 0.010550417419936921\n",
      "[LOG 20200502-14:25:56] epoch: 8881 train-loss: 0.010550415971212916\n",
      "[LOG 20200502-14:25:56] epoch: 8882 train-loss: 0.010550414729449484\n",
      "[LOG 20200502-14:25:56] epoch: 8883 train-loss: 0.01055041379812691\n",
      "[LOG 20200502-14:25:57] epoch: 8884 train-loss: 0.010550412866804335\n",
      "[LOG 20200502-14:25:57] epoch: 8885 train-loss: 0.010550412038962046\n",
      "[LOG 20200502-14:25:57] epoch: 8886 train-loss: 0.010550411314600043\n",
      "[LOG 20200502-14:25:57] epoch: 8887 train-loss: 0.01055040986587604\n",
      "[LOG 20200502-14:25:57] epoch: 8888 train-loss: 0.010550409658915468\n",
      "[LOG 20200502-14:25:58] epoch: 8889 train-loss: 0.010550408831073178\n",
      "[LOG 20200502-14:25:58] epoch: 8890 train-loss: 0.010550407899750603\n",
      "[LOG 20200502-14:25:58] epoch: 8891 train-loss: 0.010550407071908316\n",
      "[LOG 20200502-14:25:58] epoch: 8892 train-loss: 0.010550406140585741\n",
      "[LOG 20200502-14:25:59] epoch: 8893 train-loss: 0.010550405830144882\n",
      "[LOG 20200502-14:25:59] epoch: 8894 train-loss: 0.010550405519704023\n",
      "[LOG 20200502-14:25:59] epoch: 8895 train-loss: 0.010550405209263166\n",
      "[LOG 20200502-14:25:59] epoch: 8896 train-loss: 0.010550404484901164\n",
      "[LOG 20200502-14:26:00] epoch: 8897 train-loss: 0.01055040407098002\n",
      "[LOG 20200502-14:26:00] epoch: 8898 train-loss: 0.010550403553578589\n",
      "[LOG 20200502-14:26:00] epoch: 8899 train-loss: 0.010550402829216586\n",
      "[LOG 20200502-14:26:00] epoch: 8900 train-loss: 0.010550402415295443\n",
      "[LOG 20200502-14:26:01] epoch: 8901 train-loss: 0.010550402518775728\n",
      "[LOG 20200502-14:26:01] epoch: 8902 train-loss: 0.010550402104854584\n",
      "[LOG 20200502-14:26:01] epoch: 8903 train-loss: 0.010550401380492581\n",
      "[LOG 20200502-14:26:01] epoch: 8904 train-loss: 0.010550401483972868\n",
      "[LOG 20200502-14:26:02] epoch: 8905 train-loss: 0.010550401380492581\n",
      "[LOG 20200502-14:26:02] epoch: 8906 train-loss: 0.010550400759610865\n",
      "[LOG 20200502-14:26:02] epoch: 8907 train-loss: 0.010550400242209435\n",
      "[LOG 20200502-14:26:02] epoch: 8908 train-loss: 0.010550400242209435\n",
      "[LOG 20200502-14:26:02] epoch: 8909 train-loss: 0.010550400035248863\n",
      "[LOG 20200502-14:26:03] epoch: 8910 train-loss: 0.010550399517847432\n",
      "[LOG 20200502-14:26:03] epoch: 8911 train-loss: 0.010550399414367147\n",
      "[LOG 20200502-14:26:03] epoch: 8912 train-loss: 0.010550399207406573\n",
      "[LOG 20200502-14:26:03] epoch: 8913 train-loss: 0.010550399103926288\n",
      "[LOG 20200502-14:26:04] epoch: 8914 train-loss: 0.010550398483044572\n",
      "[LOG 20200502-14:26:04] epoch: 8915 train-loss: 0.010550397862162855\n",
      "[LOG 20200502-14:26:04] epoch: 8916 train-loss: 0.010550398069123426\n",
      "[LOG 20200502-14:26:04] epoch: 8917 train-loss: 0.010550397241281139\n",
      "[LOG 20200502-14:26:04] epoch: 8918 train-loss: 0.010550397344761424\n",
      "[LOG 20200502-14:26:05] epoch: 8919 train-loss: 0.010550396723879708\n",
      "[LOG 20200502-14:26:05] epoch: 8920 train-loss: 0.010550396516919136\n",
      "[LOG 20200502-14:26:05] epoch: 8921 train-loss: 0.010550396206478277\n",
      "[LOG 20200502-14:26:05] epoch: 8922 train-loss: 0.010550395585596561\n",
      "[LOG 20200502-14:26:06] epoch: 8923 train-loss: 0.010550395275155703\n",
      "[LOG 20200502-14:26:06] epoch: 8924 train-loss: 0.010550394757754274\n",
      "[LOG 20200502-14:26:06] epoch: 8925 train-loss: 0.0105503945507937\n",
      "[LOG 20200502-14:26:06] epoch: 8926 train-loss: 0.010550393722951412\n",
      "[LOG 20200502-14:26:07] epoch: 8927 train-loss: 0.010550393722951412\n",
      "[LOG 20200502-14:26:07] epoch: 8928 train-loss: 0.010550392791628838\n",
      "[LOG 20200502-14:26:07] epoch: 8929 train-loss: 0.010550392377707694\n",
      "[LOG 20200502-14:26:07] epoch: 8930 train-loss: 0.010550391653345691\n",
      "[LOG 20200502-14:26:08] epoch: 8931 train-loss: 0.010550391032463975\n",
      "[LOG 20200502-14:26:08] epoch: 8932 train-loss: 0.010550390411582258\n",
      "[LOG 20200502-14:26:08] epoch: 8933 train-loss: 0.010550389894180827\n",
      "[LOG 20200502-14:26:08] epoch: 8934 train-loss: 0.010550389169818826\n",
      "[LOG 20200502-14:26:08] epoch: 8935 train-loss: 0.010550388445456823\n",
      "[LOG 20200502-14:26:09] epoch: 8936 train-loss: 0.010550387514134249\n",
      "[LOG 20200502-14:26:09] epoch: 8937 train-loss: 0.010550386893252531\n",
      "[LOG 20200502-14:26:09] epoch: 8938 train-loss: 0.010550386789772246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:26:09] epoch: 8939 train-loss: 0.010550385961929956\n",
      "[LOG 20200502-14:26:10] epoch: 8940 train-loss: 0.010550385030607382\n",
      "[LOG 20200502-14:26:10] epoch: 8941 train-loss: 0.010550384513205953\n",
      "[LOG 20200502-14:26:10] epoch: 8942 train-loss: 0.01055038327144252\n",
      "[LOG 20200502-14:26:10] epoch: 8943 train-loss: 0.010550382547080517\n",
      "[LOG 20200502-14:26:10] epoch: 8944 train-loss: 0.010550381305317083\n",
      "[LOG 20200502-14:26:11] epoch: 8945 train-loss: 0.010550380787915654\n",
      "[LOG 20200502-14:26:11] epoch: 8946 train-loss: 0.010550380167033937\n",
      "[LOG 20200502-14:26:11] epoch: 8947 train-loss: 0.010550379235711362\n",
      "[LOG 20200502-14:26:11] epoch: 8948 train-loss: 0.01055037851134936\n",
      "[LOG 20200502-14:26:12] epoch: 8949 train-loss: 0.010550378097428216\n",
      "[LOG 20200502-14:26:12] epoch: 8950 train-loss: 0.010550376959145069\n",
      "[LOG 20200502-14:26:12] epoch: 8951 train-loss: 0.010550376338263353\n",
      "[LOG 20200502-14:26:12] epoch: 8952 train-loss: 0.01055037509649992\n",
      "[LOG 20200502-14:26:12] epoch: 8953 train-loss: 0.010550373854736486\n",
      "[LOG 20200502-14:26:13] epoch: 8954 train-loss: 0.010550373130374484\n",
      "[LOG 20200502-14:26:13] epoch: 8955 train-loss: 0.010550372302532196\n",
      "[LOG 20200502-14:26:13] epoch: 8956 train-loss: 0.010550371785130765\n",
      "[LOG 20200502-14:26:13] epoch: 8957 train-loss: 0.01055037085380819\n",
      "[LOG 20200502-14:26:14] epoch: 8958 train-loss: 0.010550369612044759\n",
      "[LOG 20200502-14:26:14] epoch: 8959 train-loss: 0.010550368577241898\n",
      "[LOG 20200502-14:26:14] epoch: 8960 train-loss: 0.010550367852879895\n",
      "[LOG 20200502-14:26:14] epoch: 8961 train-loss: 0.010550366818077035\n",
      "[LOG 20200502-14:26:15] epoch: 8962 train-loss: 0.010550366611116462\n",
      "[LOG 20200502-14:26:15] epoch: 8963 train-loss: 0.010550365058912171\n",
      "[LOG 20200502-14:26:15] epoch: 8964 train-loss: 0.010550364127589596\n",
      "[LOG 20200502-14:26:15] epoch: 8965 train-loss: 0.010550363196267022\n",
      "[LOG 20200502-14:26:15] epoch: 8966 train-loss: 0.010550362264944447\n",
      "[LOG 20200502-14:26:16] epoch: 8967 train-loss: 0.010550361230141587\n",
      "[LOG 20200502-14:26:16] epoch: 8968 train-loss: 0.010550359988378154\n",
      "[LOG 20200502-14:26:16] epoch: 8969 train-loss: 0.010550358850095008\n",
      "[LOG 20200502-14:26:16] epoch: 8970 train-loss: 0.010550358850095008\n",
      "[LOG 20200502-14:26:17] epoch: 8971 train-loss: 0.010550356677009\n",
      "[LOG 20200502-14:26:17] epoch: 8972 train-loss: 0.010550356263087856\n",
      "[LOG 20200502-14:26:17] epoch: 8973 train-loss: 0.010550355331765281\n",
      "[LOG 20200502-14:26:17] epoch: 8974 train-loss: 0.010550354503922992\n",
      "[LOG 20200502-14:26:17] epoch: 8975 train-loss: 0.010550353158679273\n",
      "[LOG 20200502-14:26:18] epoch: 8976 train-loss: 0.010550351709955268\n",
      "[LOG 20200502-14:26:18] epoch: 8977 train-loss: 0.010550350778632693\n",
      "[LOG 20200502-14:26:18] epoch: 8978 train-loss: 0.010550349847310118\n",
      "[LOG 20200502-14:26:18] epoch: 8979 train-loss: 0.010550348812507259\n",
      "[LOG 20200502-14:26:19] epoch: 8980 train-loss: 0.01055034746726354\n",
      "[LOG 20200502-14:26:19] epoch: 8981 train-loss: 0.01055034612201982\n",
      "[LOG 20200502-14:26:19] epoch: 8982 train-loss: 0.01055034508721696\n",
      "[LOG 20200502-14:26:19] epoch: 8983 train-loss: 0.01055034425937467\n",
      "[LOG 20200502-14:26:20] epoch: 8984 train-loss: 0.010550342914130952\n",
      "[LOG 20200502-14:26:20] epoch: 8985 train-loss: 0.010550341465406947\n",
      "[LOG 20200502-14:26:20] epoch: 8986 train-loss: 0.010550340120163228\n",
      "[LOG 20200502-14:26:20] epoch: 8987 train-loss: 0.010550339188840654\n",
      "[LOG 20200502-14:26:20] epoch: 8988 train-loss: 0.010550337947077222\n",
      "[LOG 20200502-14:26:21] epoch: 8989 train-loss: 0.010550336705313789\n",
      "[LOG 20200502-14:26:21] epoch: 8990 train-loss: 0.010550335153109498\n",
      "[LOG 20200502-14:26:21] epoch: 8991 train-loss: 0.010550333704385493\n",
      "[LOG 20200502-14:26:21] epoch: 8992 train-loss: 0.01055033246262206\n",
      "[LOG 20200502-14:26:22] epoch: 8993 train-loss: 0.010550331117378341\n",
      "[LOG 20200502-14:26:22] epoch: 8994 train-loss: 0.010550329875614908\n",
      "[LOG 20200502-14:26:22] epoch: 8995 train-loss: 0.010550328426890902\n",
      "[LOG 20200502-14:26:22] epoch: 8996 train-loss: 0.010550326874686612\n",
      "[LOG 20200502-14:26:22] epoch: 8997 train-loss: 0.010550325322482321\n",
      "[LOG 20200502-14:26:23] epoch: 8998 train-loss: 0.01055032428767946\n",
      "[LOG 20200502-14:26:23] epoch: 8999 train-loss: 0.010550322838955455\n",
      "[LOG 20200502-14:26:23] epoch: 9000 train-loss: 0.010550321700672308\n",
      "[LOG 20200502-14:26:23] epoch: 9001 train-loss: 0.010550320148468018\n",
      "[LOG 20200502-14:26:24] epoch: 9002 train-loss: 0.010550318596263727\n",
      "[LOG 20200502-14:26:24] epoch: 9003 train-loss: 0.010550317147539722\n",
      "[LOG 20200502-14:26:24] epoch: 9004 train-loss: 0.010550315698815716\n",
      "[LOG 20200502-14:26:24] epoch: 9005 train-loss: 0.010550314664012857\n",
      "[LOG 20200502-14:26:25] epoch: 9006 train-loss: 0.010550313318769136\n",
      "[LOG 20200502-14:26:25] epoch: 9007 train-loss: 0.010550311870045133\n",
      "[LOG 20200502-14:26:25] epoch: 9008 train-loss: 0.010550310524801413\n",
      "[LOG 20200502-14:26:25] epoch: 9009 train-loss: 0.010550309386518266\n",
      "[LOG 20200502-14:26:25] epoch: 9010 train-loss: 0.010550308144754834\n",
      "[LOG 20200502-14:26:26] epoch: 9011 train-loss: 0.010550306799511114\n",
      "[LOG 20200502-14:26:26] epoch: 9012 train-loss: 0.01055030586818854\n",
      "[LOG 20200502-14:26:26] epoch: 9013 train-loss: 0.010550305040346252\n",
      "[LOG 20200502-14:26:26] epoch: 9014 train-loss: 0.010550303384661674\n",
      "[LOG 20200502-14:26:27] epoch: 9015 train-loss: 0.010550302763779959\n",
      "[LOG 20200502-14:26:27] epoch: 9016 train-loss: 0.010550301315055953\n",
      "[LOG 20200502-14:26:27] epoch: 9017 train-loss: 0.01055030007329252\n",
      "[LOG 20200502-14:26:27] epoch: 9018 train-loss: 0.010550299762851663\n",
      "[LOG 20200502-14:26:28] epoch: 9019 train-loss: 0.010550298107167086\n",
      "[LOG 20200502-14:26:28] epoch: 9020 train-loss: 0.010550297900206514\n",
      "[LOG 20200502-14:26:28] epoch: 9021 train-loss: 0.010550297175844511\n",
      "[LOG 20200502-14:26:28] epoch: 9022 train-loss: 0.01055029614104165\n",
      "[LOG 20200502-14:26:28] epoch: 9023 train-loss: 0.010550295002758503\n",
      "[LOG 20200502-14:26:29] epoch: 9024 train-loss: 0.010550294588837359\n",
      "[LOG 20200502-14:26:29] epoch: 9025 train-loss: 0.010550293554034498\n",
      "[LOG 20200502-14:26:29] epoch: 9026 train-loss: 0.010550293347073926\n",
      "[LOG 20200502-14:26:29] epoch: 9027 train-loss: 0.01055029324359364\n",
      "[LOG 20200502-14:26:30] epoch: 9028 train-loss: 0.010550292519231638\n",
      "[LOG 20200502-14:26:30] epoch: 9029 train-loss: 0.010550292208790779\n",
      "[LOG 20200502-14:26:30] epoch: 9030 train-loss: 0.010550291794869635\n",
      "[LOG 20200502-14:26:30] epoch: 9031 train-loss: 0.010550291484428776\n",
      "[LOG 20200502-14:26:31] epoch: 9032 train-loss: 0.010550291173987918\n",
      "[LOG 20200502-14:26:31] epoch: 9033 train-loss: 0.010550290760066774\n",
      "[LOG 20200502-14:26:31] epoch: 9034 train-loss: 0.010550291070507633\n",
      "[LOG 20200502-14:26:31] epoch: 9035 train-loss: 0.010550290553106202\n",
      "[LOG 20200502-14:26:31] epoch: 9036 train-loss: 0.010550290553106202\n",
      "[LOG 20200502-14:26:32] epoch: 9037 train-loss: 0.010550290242665343\n",
      "[LOG 20200502-14:26:32] epoch: 9038 train-loss: 0.010550289932224486\n",
      "[LOG 20200502-14:26:32] epoch: 9039 train-loss: 0.010550290449625917\n",
      "[LOG 20200502-14:26:32] epoch: 9040 train-loss: 0.010550290760066774\n",
      "[LOG 20200502-14:26:33] epoch: 9041 train-loss: 0.010550290449625917\n",
      "[LOG 20200502-14:26:33] epoch: 9042 train-loss: 0.01055029034614563\n",
      "[LOG 20200502-14:26:33] epoch: 9043 train-loss: 0.01055029034614563\n",
      "[LOG 20200502-14:26:33] epoch: 9044 train-loss: 0.01055029086354706\n",
      "[LOG 20200502-14:26:34] epoch: 9045 train-loss: 0.010550290760066774\n",
      "[LOG 20200502-14:26:34] epoch: 9046 train-loss: 0.01055029086354706\n",
      "[LOG 20200502-14:26:34] epoch: 9047 train-loss: 0.010550290656586489\n",
      "[LOG 20200502-14:26:34] epoch: 9048 train-loss: 0.010550291380948491\n",
      "[LOG 20200502-14:26:34] epoch: 9049 train-loss: 0.010550291173987918\n",
      "[LOG 20200502-14:26:35] epoch: 9050 train-loss: 0.010550291794869635\n",
      "[LOG 20200502-14:26:35] epoch: 9051 train-loss: 0.010550292105310492\n",
      "[LOG 20200502-14:26:35] epoch: 9052 train-loss: 0.010550292208790779\n",
      "[LOG 20200502-14:26:35] epoch: 9053 train-loss: 0.010550292519231638\n",
      "[LOG 20200502-14:26:36] epoch: 9054 train-loss: 0.010550293036633067\n",
      "[LOG 20200502-14:26:36] epoch: 9055 train-loss: 0.010550292933152782\n",
      "[LOG 20200502-14:26:36] epoch: 9056 train-loss: 0.010550293347073926\n",
      "[LOG 20200502-14:26:36] epoch: 9057 train-loss: 0.010550293864475356\n",
      "[LOG 20200502-14:26:36] epoch: 9058 train-loss: 0.010550293967955642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:26:37] epoch: 9059 train-loss: 0.0105502942783965\n",
      "[LOG 20200502-14:26:37] epoch: 9060 train-loss: 0.010550294795797931\n",
      "[LOG 20200502-14:26:37] epoch: 9061 train-loss: 0.01055029510623879\n",
      "[LOG 20200502-14:26:37] epoch: 9062 train-loss: 0.010550295209719075\n",
      "[LOG 20200502-14:26:38] epoch: 9063 train-loss: 0.010550295727120506\n",
      "[LOG 20200502-14:26:38] epoch: 9064 train-loss: 0.010550296037561364\n",
      "[LOG 20200502-14:26:38] epoch: 9065 train-loss: 0.01055029614104165\n",
      "[LOG 20200502-14:26:38] epoch: 9066 train-loss: 0.010550296451482508\n",
      "[LOG 20200502-14:26:39] epoch: 9067 train-loss: 0.010550296348002221\n",
      "[LOG 20200502-14:26:39] epoch: 9068 train-loss: 0.010550296865403652\n",
      "[LOG 20200502-14:26:39] epoch: 9069 train-loss: 0.010550297382805083\n",
      "[LOG 20200502-14:26:39] epoch: 9070 train-loss: 0.01055029821064737\n",
      "[LOG 20200502-14:26:39] epoch: 9071 train-loss: 0.010550297382805083\n",
      "[LOG 20200502-14:26:40] epoch: 9072 train-loss: 0.010550298003686799\n",
      "[LOG 20200502-14:26:40] epoch: 9073 train-loss: 0.01055029821064737\n",
      "[LOG 20200502-14:26:40] epoch: 9074 train-loss: 0.010550298935009373\n",
      "[LOG 20200502-14:26:40] epoch: 9075 train-loss: 0.010550298935009373\n",
      "[LOG 20200502-14:26:41] epoch: 9076 train-loss: 0.010550298935009373\n",
      "[LOG 20200502-14:26:41] epoch: 9077 train-loss: 0.010550298624568515\n",
      "[LOG 20200502-14:26:41] epoch: 9078 train-loss: 0.010550299452410804\n",
      "[LOG 20200502-14:26:41] epoch: 9079 train-loss: 0.01055029955589109\n",
      "[LOG 20200502-14:26:41] epoch: 9080 train-loss: 0.010550299866331948\n",
      "[LOG 20200502-14:26:42] epoch: 9081 train-loss: 0.01055030007329252\n",
      "[LOG 20200502-14:26:42] epoch: 9082 train-loss: 0.01055030059069395\n",
      "[LOG 20200502-14:26:42] epoch: 9083 train-loss: 0.010550300487213664\n",
      "[LOG 20200502-14:26:42] epoch: 9084 train-loss: 0.010550300176772807\n",
      "[LOG 20200502-14:26:43] epoch: 9085 train-loss: 0.010550300487213664\n",
      "[LOG 20200502-14:26:43] epoch: 9086 train-loss: 0.010550300383733379\n",
      "[LOG 20200502-14:26:43] epoch: 9087 train-loss: 0.010550300694174237\n",
      "[LOG 20200502-14:26:43] epoch: 9088 train-loss: 0.010550301108095381\n",
      "[LOG 20200502-14:26:44] epoch: 9089 train-loss: 0.010550301211575666\n",
      "[LOG 20200502-14:26:44] epoch: 9090 train-loss: 0.010550301418536238\n",
      "[LOG 20200502-14:26:44] epoch: 9091 train-loss: 0.010550301108095381\n",
      "[LOG 20200502-14:26:44] epoch: 9092 train-loss: 0.010550301832457384\n",
      "[LOG 20200502-14:26:44] epoch: 9093 train-loss: 0.010550301625496812\n",
      "[LOG 20200502-14:26:45] epoch: 9094 train-loss: 0.010550301728977097\n",
      "[LOG 20200502-14:26:45] epoch: 9095 train-loss: 0.010550301522016525\n",
      "[LOG 20200502-14:26:45] epoch: 9096 train-loss: 0.010550302039417956\n",
      "[LOG 20200502-14:26:45] epoch: 9097 train-loss: 0.010550301728977097\n",
      "[LOG 20200502-14:26:46] epoch: 9098 train-loss: 0.010550301625496812\n",
      "[LOG 20200502-14:26:46] epoch: 9099 train-loss: 0.010550301315055953\n",
      "[LOG 20200502-14:26:46] epoch: 9100 train-loss: 0.010550302142898241\n",
      "[LOG 20200502-14:26:46] epoch: 9101 train-loss: 0.010550302349858813\n",
      "[LOG 20200502-14:26:46] epoch: 9102 train-loss: 0.010550301832457384\n",
      "[LOG 20200502-14:26:47] epoch: 9103 train-loss: 0.010550301832457384\n",
      "[LOG 20200502-14:26:47] epoch: 9104 train-loss: 0.010550302039417956\n",
      "[LOG 20200502-14:26:47] epoch: 9105 train-loss: 0.010550301728977097\n",
      "[LOG 20200502-14:26:47] epoch: 9106 train-loss: 0.01055030193593767\n",
      "[LOG 20200502-14:26:48] epoch: 9107 train-loss: 0.010550302349858813\n",
      "[LOG 20200502-14:26:48] epoch: 9108 train-loss: 0.010550302039417956\n",
      "[LOG 20200502-14:26:48] epoch: 9109 train-loss: 0.010550302142898241\n",
      "[LOG 20200502-14:26:48] epoch: 9110 train-loss: 0.010550302246378528\n",
      "[LOG 20200502-14:26:49] epoch: 9111 train-loss: 0.010550302246378528\n",
      "[LOG 20200502-14:26:49] epoch: 9112 train-loss: 0.010550302142898241\n",
      "[LOG 20200502-14:26:49] epoch: 9113 train-loss: 0.010550302349858813\n",
      "[LOG 20200502-14:26:49] epoch: 9114 train-loss: 0.01055030193593767\n",
      "[LOG 20200502-14:26:50] epoch: 9115 train-loss: 0.0105503024533391\n",
      "[LOG 20200502-14:26:50] epoch: 9116 train-loss: 0.010550302039417956\n",
      "[LOG 20200502-14:26:50] epoch: 9117 train-loss: 0.01055030193593767\n",
      "[LOG 20200502-14:26:50] epoch: 9118 train-loss: 0.010550302142898241\n",
      "[LOG 20200502-14:26:51] epoch: 9119 train-loss: 0.010550301625496812\n",
      "[LOG 20200502-14:26:51] epoch: 9120 train-loss: 0.010550301832457384\n",
      "[LOG 20200502-14:26:51] epoch: 9121 train-loss: 0.010550301522016525\n",
      "[LOG 20200502-14:26:51] epoch: 9122 train-loss: 0.010550301728977097\n",
      "[LOG 20200502-14:26:52] epoch: 9123 train-loss: 0.010550301625496812\n",
      "[LOG 20200502-14:26:52] epoch: 9124 train-loss: 0.01055030193593767\n",
      "[LOG 20200502-14:26:52] epoch: 9125 train-loss: 0.010550301832457384\n",
      "[LOG 20200502-14:26:52] epoch: 9126 train-loss: 0.010550301418536238\n",
      "[LOG 20200502-14:26:53] epoch: 9127 train-loss: 0.010550301315055953\n",
      "[LOG 20200502-14:26:53] epoch: 9128 train-loss: 0.010550301625496812\n",
      "[LOG 20200502-14:26:53] epoch: 9129 train-loss: 0.010550301108095381\n",
      "[LOG 20200502-14:26:53] epoch: 9130 train-loss: 0.01055030090113481\n",
      "[LOG 20200502-14:26:53] epoch: 9131 train-loss: 0.010550301211575666\n",
      "[LOG 20200502-14:26:54] epoch: 9132 train-loss: 0.01055030059069395\n",
      "[LOG 20200502-14:26:54] epoch: 9133 train-loss: 0.010550300797654523\n",
      "[LOG 20200502-14:26:54] epoch: 9134 train-loss: 0.01055030059069395\n",
      "[LOG 20200502-14:26:54] epoch: 9135 train-loss: 0.010550300280253092\n",
      "[LOG 20200502-14:26:55] epoch: 9136 train-loss: 0.010550299659371376\n",
      "[LOG 20200502-14:26:55] epoch: 9137 train-loss: 0.01055030007329252\n",
      "[LOG 20200502-14:26:55] epoch: 9138 train-loss: 0.010550299762851663\n",
      "[LOG 20200502-14:26:55] epoch: 9139 train-loss: 0.010550299866331948\n",
      "[LOG 20200502-14:26:55] epoch: 9140 train-loss: 0.010550299452410804\n",
      "[LOG 20200502-14:26:56] epoch: 9141 train-loss: 0.010550299452410804\n",
      "[LOG 20200502-14:26:56] epoch: 9142 train-loss: 0.010550299141969945\n",
      "[LOG 20200502-14:26:56] epoch: 9143 train-loss: 0.010550299452410804\n",
      "[LOG 20200502-14:26:56] epoch: 9144 train-loss: 0.010550299348930517\n",
      "[LOG 20200502-14:26:56] epoch: 9145 train-loss: 0.010550298935009373\n",
      "[LOG 20200502-14:26:57] epoch: 9146 train-loss: 0.010550298728048801\n",
      "[LOG 20200502-14:26:57] epoch: 9147 train-loss: 0.01055029903848966\n",
      "[LOG 20200502-14:26:57] epoch: 9148 train-loss: 0.01055029852108823\n",
      "[LOG 20200502-14:26:57] epoch: 9149 train-loss: 0.010550299141969945\n",
      "[LOG 20200502-14:26:58] epoch: 9150 train-loss: 0.010550298831529088\n",
      "[LOG 20200502-14:26:58] epoch: 9151 train-loss: 0.010550298831529088\n",
      "[LOG 20200502-14:26:58] epoch: 9152 train-loss: 0.010550298831529088\n",
      "[LOG 20200502-14:26:58] epoch: 9153 train-loss: 0.01055029903848966\n",
      "[LOG 20200502-14:26:58] epoch: 9154 train-loss: 0.01055029903848966\n",
      "[LOG 20200502-14:26:59] epoch: 9155 train-loss: 0.010550299348930517\n",
      "[LOG 20200502-14:26:59] epoch: 9156 train-loss: 0.010550299348930517\n",
      "[LOG 20200502-14:26:59] epoch: 9157 train-loss: 0.010550299659371376\n",
      "[LOG 20200502-14:26:59] epoch: 9158 train-loss: 0.010550299866331948\n",
      "[LOG 20200502-14:27:00] epoch: 9159 train-loss: 0.010550300176772807\n",
      "[LOG 20200502-14:27:00] epoch: 9160 train-loss: 0.010550300487213664\n",
      "[LOG 20200502-14:27:00] epoch: 9161 train-loss: 0.010550300487213664\n",
      "[LOG 20200502-14:27:00] epoch: 9162 train-loss: 0.010550301108095381\n",
      "[LOG 20200502-14:27:00] epoch: 9163 train-loss: 0.010550301315055953\n",
      "[LOG 20200502-14:27:01] epoch: 9164 train-loss: 0.010550302246378528\n",
      "[LOG 20200502-14:27:01] epoch: 9165 train-loss: 0.010550302660299672\n",
      "[LOG 20200502-14:27:01] epoch: 9166 train-loss: 0.010550302763779959\n",
      "[LOG 20200502-14:27:01] epoch: 9167 train-loss: 0.010550303488141961\n",
      "[LOG 20200502-14:27:01] epoch: 9168 train-loss: 0.010550303902063105\n",
      "[LOG 20200502-14:27:02] epoch: 9169 train-loss: 0.010550304109023677\n",
      "[LOG 20200502-14:27:02] epoch: 9170 train-loss: 0.01055030535078711\n",
      "[LOG 20200502-14:27:02] epoch: 9171 train-loss: 0.010550306075149112\n",
      "[LOG 20200502-14:27:02] epoch: 9172 train-loss: 0.01055030638558997\n",
      "[LOG 20200502-14:27:03] epoch: 9173 train-loss: 0.010550306489070257\n",
      "[LOG 20200502-14:27:03] epoch: 9174 train-loss: 0.010550307523873117\n",
      "[LOG 20200502-14:27:03] epoch: 9175 train-loss: 0.010550308558675978\n",
      "[LOG 20200502-14:27:03] epoch: 9176 train-loss: 0.010550309593478838\n",
      "[LOG 20200502-14:27:03] epoch: 9177 train-loss: 0.010550310214360556\n",
      "[LOG 20200502-14:27:04] epoch: 9178 train-loss: 0.01055031114568313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:27:04] epoch: 9179 train-loss: 0.010550311456123987\n",
      "[LOG 20200502-14:27:04] epoch: 9180 train-loss: 0.01055031300832828\n",
      "[LOG 20200502-14:27:04] epoch: 9181 train-loss: 0.010550313939650854\n",
      "[LOG 20200502-14:27:05] epoch: 9182 train-loss: 0.010550314664012857\n",
      "[LOG 20200502-14:27:05] epoch: 9183 train-loss: 0.01055031611273686\n",
      "[LOG 20200502-14:27:05] epoch: 9184 train-loss: 0.010550317044059435\n",
      "[LOG 20200502-14:27:05] epoch: 9185 train-loss: 0.010550318285822868\n",
      "[LOG 20200502-14:27:05] epoch: 9186 train-loss: 0.010550319527586302\n",
      "[LOG 20200502-14:27:06] epoch: 9187 train-loss: 0.01055032004498773\n",
      "[LOG 20200502-14:27:06] epoch: 9188 train-loss: 0.010550321597192023\n",
      "[LOG 20200502-14:27:06] epoch: 9189 train-loss: 0.010550322942435741\n",
      "[LOG 20200502-14:27:06] epoch: 9190 train-loss: 0.010550323666797744\n",
      "[LOG 20200502-14:27:06] epoch: 9191 train-loss: 0.010550325529442893\n",
      "[LOG 20200502-14:27:07] epoch: 9192 train-loss: 0.01055032666772604\n",
      "[LOG 20200502-14:27:07] epoch: 9193 train-loss: 0.010550327702528901\n",
      "[LOG 20200502-14:27:07] epoch: 9194 train-loss: 0.010550329358213477\n",
      "[LOG 20200502-14:27:07] epoch: 9195 train-loss: 0.010550330186055766\n",
      "[LOG 20200502-14:27:08] epoch: 9196 train-loss: 0.0105503314278192\n",
      "[LOG 20200502-14:27:08] epoch: 9197 train-loss: 0.010550332773062918\n",
      "[LOG 20200502-14:27:08] epoch: 9198 train-loss: 0.010550334428747496\n",
      "[LOG 20200502-14:27:08] epoch: 9199 train-loss: 0.010550335256589783\n",
      "[LOG 20200502-14:27:08] epoch: 9200 train-loss: 0.010550336705313789\n",
      "[LOG 20200502-14:27:09] epoch: 9201 train-loss: 0.010550338360998366\n",
      "[LOG 20200502-14:27:09] epoch: 9202 train-loss: 0.010550339395801226\n",
      "[LOG 20200502-14:27:09] epoch: 9203 train-loss: 0.010550341051485803\n",
      "[LOG 20200502-14:27:09] epoch: 9204 train-loss: 0.010550342500209808\n",
      "[LOG 20200502-14:27:09] epoch: 9205 train-loss: 0.010550343638492955\n",
      "[LOG 20200502-14:27:10] epoch: 9206 train-loss: 0.010550345190697245\n",
      "[LOG 20200502-14:27:10] epoch: 9207 train-loss: 0.01055034612201982\n",
      "[LOG 20200502-14:27:10] epoch: 9208 train-loss: 0.010550347777704397\n",
      "[LOG 20200502-14:27:10] epoch: 9209 train-loss: 0.010550349122948117\n",
      "[LOG 20200502-14:27:11] epoch: 9210 train-loss: 0.01055035088211298\n",
      "[LOG 20200502-14:27:11] epoch: 9211 train-loss: 0.010550351916915841\n",
      "[LOG 20200502-14:27:11] epoch: 9212 train-loss: 0.010550353055198988\n",
      "[LOG 20200502-14:27:11] epoch: 9213 train-loss: 0.010550354710883565\n",
      "[LOG 20200502-14:27:12] epoch: 9214 train-loss: 0.010550356056127284\n",
      "[LOG 20200502-14:27:12] epoch: 9215 train-loss: 0.010550357918772433\n",
      "[LOG 20200502-14:27:12] epoch: 9216 train-loss: 0.010550359160535865\n",
      "[LOG 20200502-14:27:12] epoch: 9217 train-loss: 0.010550359781417582\n",
      "[LOG 20200502-14:27:12] epoch: 9218 train-loss: 0.010550361851023303\n",
      "[LOG 20200502-14:27:13] epoch: 9219 train-loss: 0.010550363092786737\n",
      "[LOG 20200502-14:27:13] epoch: 9220 train-loss: 0.010550364644991027\n",
      "[LOG 20200502-14:27:13] epoch: 9221 train-loss: 0.010550365990234746\n",
      "[LOG 20200502-14:27:13] epoch: 9222 train-loss: 0.010550367025037607\n",
      "[LOG 20200502-14:27:14] epoch: 9223 train-loss: 0.010550368887682756\n",
      "[LOG 20200502-14:27:14] epoch: 9224 train-loss: 0.010550369922485616\n",
      "[LOG 20200502-14:27:14] epoch: 9225 train-loss: 0.010550371267729335\n",
      "[LOG 20200502-14:27:14] epoch: 9226 train-loss: 0.010550372302532196\n",
      "[LOG 20200502-14:27:15] epoch: 9227 train-loss: 0.010550373751256201\n",
      "[LOG 20200502-14:27:15] epoch: 9228 train-loss: 0.01055037561390135\n",
      "[LOG 20200502-14:27:15] epoch: 9229 train-loss: 0.010550376959145069\n",
      "[LOG 20200502-14:27:15] epoch: 9230 train-loss: 0.010550378200908503\n",
      "[LOG 20200502-14:27:15] epoch: 9231 train-loss: 0.010550379546152221\n",
      "[LOG 20200502-14:27:16] epoch: 9232 train-loss: 0.010550380787915654\n",
      "[LOG 20200502-14:27:16] epoch: 9233 train-loss: 0.010550382547080517\n",
      "[LOG 20200502-14:27:16] epoch: 9234 train-loss: 0.010550383685363663\n",
      "[LOG 20200502-14:27:16] epoch: 9235 train-loss: 0.010550384616686238\n",
      "[LOG 20200502-14:27:17] epoch: 9236 train-loss: 0.010550385961929956\n",
      "[LOG 20200502-14:27:17] epoch: 9237 train-loss: 0.010550387514134249\n",
      "[LOG 20200502-14:27:17] epoch: 9238 train-loss: 0.010550389169818826\n",
      "[LOG 20200502-14:27:17] epoch: 9239 train-loss: 0.010550390308101973\n",
      "[LOG 20200502-14:27:17] epoch: 9240 train-loss: 0.010550391549865404\n",
      "[LOG 20200502-14:27:18] epoch: 9241 train-loss: 0.010550392895109124\n",
      "[LOG 20200502-14:27:18] epoch: 9242 train-loss: 0.010550394136872556\n",
      "[LOG 20200502-14:27:18] epoch: 9243 train-loss: 0.010550395482116275\n",
      "[LOG 20200502-14:27:18] epoch: 9244 train-loss: 0.010550396827359995\n",
      "[LOG 20200502-14:27:19] epoch: 9245 train-loss: 0.010550398379564285\n",
      "[LOG 20200502-14:27:19] epoch: 9246 train-loss: 0.010550400035248863\n",
      "[LOG 20200502-14:27:19] epoch: 9247 train-loss: 0.010550401380492581\n",
      "[LOG 20200502-14:27:19] epoch: 9248 train-loss: 0.010550402518775728\n",
      "[LOG 20200502-14:27:20] epoch: 9249 train-loss: 0.01055040324313773\n",
      "[LOG 20200502-14:27:20] epoch: 9250 train-loss: 0.010550405209263166\n",
      "[LOG 20200502-14:27:20] epoch: 9251 train-loss: 0.01055040665798717\n",
      "[LOG 20200502-14:27:20] epoch: 9252 train-loss: 0.010550407899750603\n",
      "[LOG 20200502-14:27:21] epoch: 9253 train-loss: 0.01055040903803375\n",
      "[LOG 20200502-14:27:21] epoch: 9254 train-loss: 0.010550410797198614\n",
      "[LOG 20200502-14:27:21] epoch: 9255 train-loss: 0.010550412038962046\n",
      "[LOG 20200502-14:27:21] epoch: 9256 train-loss: 0.010550412763324048\n",
      "[LOG 20200502-14:27:21] epoch: 9257 train-loss: 0.010550413901607195\n",
      "[LOG 20200502-14:27:22] epoch: 9258 train-loss: 0.010550415867732631\n",
      "[LOG 20200502-14:27:22] epoch: 9259 train-loss: 0.010550417109496064\n",
      "[LOG 20200502-14:27:22] epoch: 9260 train-loss: 0.010550418454739783\n",
      "[LOG 20200502-14:27:22] epoch: 9261 train-loss: 0.01055041959302293\n",
      "[LOG 20200502-14:27:23] epoch: 9262 train-loss: 0.01055042114522722\n",
      "[LOG 20200502-14:27:23] epoch: 9263 train-loss: 0.010550422076549795\n",
      "[LOG 20200502-14:27:23] epoch: 9264 train-loss: 0.0105504235252738\n",
      "[LOG 20200502-14:27:23] epoch: 9265 train-loss: 0.010550424663556946\n",
      "[LOG 20200502-14:27:23] epoch: 9266 train-loss: 0.010550426215761237\n",
      "[LOG 20200502-14:27:24] epoch: 9267 train-loss: 0.010550427974926101\n",
      "[LOG 20200502-14:27:24] epoch: 9268 train-loss: 0.010550428492327532\n",
      "[LOG 20200502-14:27:24] epoch: 9269 train-loss: 0.010550430665413538\n",
      "[LOG 20200502-14:27:24] epoch: 9270 train-loss: 0.01055043138977554\n",
      "[LOG 20200502-14:27:25] epoch: 9271 train-loss: 0.010550433148940405\n",
      "[LOG 20200502-14:27:25] epoch: 9272 train-loss: 0.010550433976782693\n",
      "[LOG 20200502-14:27:25] epoch: 9273 train-loss: 0.010550435735947557\n",
      "[LOG 20200502-14:27:25] epoch: 9274 train-loss: 0.010550436874230703\n",
      "[LOG 20200502-14:27:25] epoch: 9275 train-loss: 0.010550438426434994\n",
      "[LOG 20200502-14:27:26] epoch: 9276 train-loss: 0.010550439978639284\n",
      "[LOG 20200502-14:27:26] epoch: 9277 train-loss: 0.01055044111692243\n",
      "[LOG 20200502-14:27:26] epoch: 9278 train-loss: 0.010550442462166151\n",
      "[LOG 20200502-14:27:26] epoch: 9279 train-loss: 0.010550443910890155\n",
      "[LOG 20200502-14:27:27] epoch: 9280 train-loss: 0.010550445256133875\n",
      "[LOG 20200502-14:27:27] epoch: 9281 train-loss: 0.010550446497897306\n",
      "[LOG 20200502-14:27:27] epoch: 9282 train-loss: 0.010550448050101599\n",
      "[LOG 20200502-14:27:27] epoch: 9283 train-loss: 0.010550449395345317\n",
      "[LOG 20200502-14:27:28] epoch: 9284 train-loss: 0.010550450430148177\n",
      "[LOG 20200502-14:27:28] epoch: 9285 train-loss: 0.010550452085832754\n",
      "[LOG 20200502-14:27:28] epoch: 9286 train-loss: 0.010550453431076474\n",
      "[LOG 20200502-14:27:28] epoch: 9287 train-loss: 0.010550454983280765\n",
      "[LOG 20200502-14:27:28] epoch: 9288 train-loss: 0.010550456742445627\n",
      "[LOG 20200502-14:27:29] epoch: 9289 train-loss: 0.01055045798420906\n",
      "[LOG 20200502-14:27:29] epoch: 9290 train-loss: 0.010550459639893638\n",
      "[LOG 20200502-14:27:29] epoch: 9291 train-loss: 0.010550460881657071\n",
      "[LOG 20200502-14:27:29] epoch: 9292 train-loss: 0.010550462951262793\n",
      "[LOG 20200502-14:27:30] epoch: 9293 train-loss: 0.010550464710427655\n",
      "[LOG 20200502-14:27:30] epoch: 9294 train-loss: 0.010550465952191088\n",
      "[LOG 20200502-14:27:30] epoch: 9295 train-loss: 0.010550467814836238\n",
      "[LOG 20200502-14:27:30] epoch: 9296 train-loss: 0.010550469470520815\n",
      "[LOG 20200502-14:27:30] epoch: 9297 train-loss: 0.010550470712284247\n",
      "[LOG 20200502-14:27:31] epoch: 9298 train-loss: 0.010550472781889968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:27:31] epoch: 9299 train-loss: 0.010550474541054832\n",
      "[LOG 20200502-14:27:31] epoch: 9300 train-loss: 0.01055047588629855\n",
      "[LOG 20200502-14:27:31] epoch: 9301 train-loss: 0.010550478473305702\n",
      "[LOG 20200502-14:27:32] epoch: 9302 train-loss: 0.010550479611588849\n",
      "[LOG 20200502-14:27:32] epoch: 9303 train-loss: 0.010550481370753713\n",
      "[LOG 20200502-14:27:32] epoch: 9304 train-loss: 0.010550483233398862\n",
      "[LOG 20200502-14:27:32] epoch: 9305 train-loss: 0.01055048561344544\n",
      "[LOG 20200502-14:27:33] epoch: 9306 train-loss: 0.01055048695868916\n",
      "[LOG 20200502-14:27:33] epoch: 9307 train-loss: 0.01055048954569631\n",
      "[LOG 20200502-14:27:33] epoch: 9308 train-loss: 0.010550491304861175\n",
      "[LOG 20200502-14:27:33] epoch: 9309 train-loss: 0.010550493477947183\n",
      "[LOG 20200502-14:27:33] epoch: 9310 train-loss: 0.010550495547552904\n",
      "[LOG 20200502-14:27:34] epoch: 9311 train-loss: 0.010550497410198053\n",
      "[LOG 20200502-14:27:34] epoch: 9312 train-loss: 0.010550499376323488\n",
      "[LOG 20200502-14:27:34] epoch: 9313 train-loss: 0.010550501859850354\n",
      "[LOG 20200502-14:27:34] epoch: 9314 train-loss: 0.010550503619015217\n",
      "[LOG 20200502-14:27:35] epoch: 9315 train-loss: 0.010550506102542082\n",
      "[LOG 20200502-14:27:35] epoch: 9316 train-loss: 0.010550507965187231\n",
      "[LOG 20200502-14:27:35] epoch: 9317 train-loss: 0.01055051013827324\n",
      "[LOG 20200502-14:27:35] epoch: 9318 train-loss: 0.010550512828760676\n",
      "[LOG 20200502-14:27:36] epoch: 9319 train-loss: 0.010550515105326971\n",
      "[LOG 20200502-14:27:36] epoch: 9320 train-loss: 0.01055051696797212\n",
      "[LOG 20200502-14:27:36] epoch: 9321 train-loss: 0.010550519554979272\n",
      "[LOG 20200502-14:27:36] epoch: 9322 train-loss: 0.010550521624584993\n",
      "[LOG 20200502-14:27:37] epoch: 9323 train-loss: 0.010550524108111858\n",
      "[LOG 20200502-14:27:37] epoch: 9324 train-loss: 0.010550526488158438\n",
      "[LOG 20200502-14:27:37] epoch: 9325 train-loss: 0.010550528661244445\n",
      "[LOG 20200502-14:27:37] epoch: 9326 train-loss: 0.010550531248251597\n",
      "[LOG 20200502-14:27:38] epoch: 9327 train-loss: 0.010550533628298176\n",
      "[LOG 20200502-14:27:38] epoch: 9328 train-loss: 0.010550536008344756\n",
      "[LOG 20200502-14:27:38] epoch: 9329 train-loss: 0.010550538905792765\n",
      "[LOG 20200502-14:27:38] epoch: 9330 train-loss: 0.010550541078878773\n",
      "[LOG 20200502-14:27:38] epoch: 9331 train-loss: 0.010550543872846497\n",
      "[LOG 20200502-14:27:39] epoch: 9332 train-loss: 0.01055054614941279\n",
      "[LOG 20200502-14:27:39] epoch: 9333 train-loss: 0.010550548632939657\n",
      "[LOG 20200502-14:27:39] epoch: 9334 train-loss: 0.010550551737348238\n",
      "[LOG 20200502-14:27:39] epoch: 9335 train-loss: 0.010550553910434246\n",
      "[LOG 20200502-14:27:40] epoch: 9336 train-loss: 0.010550556393961111\n",
      "[LOG 20200502-14:27:40] epoch: 9337 train-loss: 0.010550559705330266\n",
      "[LOG 20200502-14:27:40] epoch: 9338 train-loss: 0.01055056218885713\n",
      "[LOG 20200502-14:27:40] epoch: 9339 train-loss: 0.01055056456890371\n",
      "[LOG 20200502-14:27:40] epoch: 9340 train-loss: 0.010550567362871435\n",
      "[LOG 20200502-14:27:41] epoch: 9341 train-loss: 0.010550569846398301\n",
      "[LOG 20200502-14:27:41] epoch: 9342 train-loss: 0.01055057274384631\n",
      "[LOG 20200502-14:27:41] epoch: 9343 train-loss: 0.01055057460649146\n",
      "[LOG 20200502-14:27:41] epoch: 9344 train-loss: 0.010550577917860614\n",
      "[LOG 20200502-14:27:42] epoch: 9345 train-loss: 0.01055058091878891\n",
      "[LOG 20200502-14:27:42] epoch: 9346 train-loss: 0.010550584230158064\n",
      "[LOG 20200502-14:27:42] epoch: 9347 train-loss: 0.010550586506724358\n",
      "[LOG 20200502-14:27:42] epoch: 9348 train-loss: 0.010550588679810366\n",
      "[LOG 20200502-14:27:43] epoch: 9349 train-loss: 0.010550591784218947\n",
      "[LOG 20200502-14:27:43] epoch: 9350 train-loss: 0.010550594785147242\n",
      "[LOG 20200502-14:27:43] epoch: 9351 train-loss: 0.010550597165193822\n",
      "[LOG 20200502-14:27:43] epoch: 9352 train-loss: 0.010550599959161546\n",
      "[LOG 20200502-14:27:44] epoch: 9353 train-loss: 0.010550602546168698\n",
      "[LOG 20200502-14:27:44] epoch: 9354 train-loss: 0.010550605236656137\n",
      "[LOG 20200502-14:27:44] epoch: 9355 train-loss: 0.010550607927143574\n",
      "[LOG 20200502-14:27:44] epoch: 9356 train-loss: 0.010550610721111298\n",
      "[LOG 20200502-14:27:45] epoch: 9357 train-loss: 0.01055061330811845\n",
      "[LOG 20200502-14:27:45] epoch: 9358 train-loss: 0.010550616412527032\n",
      "[LOG 20200502-14:27:45] epoch: 9359 train-loss: 0.010550618896053897\n",
      "[LOG 20200502-14:27:45] epoch: 9360 train-loss: 0.01055062200046248\n",
      "[LOG 20200502-14:27:45] epoch: 9361 train-loss: 0.010550624690949917\n",
      "[LOG 20200502-14:27:46] epoch: 9362 train-loss: 0.010550627898838785\n",
      "[LOG 20200502-14:27:46] epoch: 9363 train-loss: 0.010550630589326223\n",
      "[LOG 20200502-14:27:46] epoch: 9364 train-loss: 0.01055063327981366\n",
      "[LOG 20200502-14:27:46] epoch: 9365 train-loss: 0.010550636177261671\n",
      "[LOG 20200502-14:27:47] epoch: 9366 train-loss: 0.010550638143387105\n",
      "[LOG 20200502-14:27:47] epoch: 9367 train-loss: 0.010550641351275973\n",
      "[LOG 20200502-14:27:47] epoch: 9368 train-loss: 0.010550644455684556\n",
      "[LOG 20200502-14:27:47] epoch: 9369 train-loss: 0.010550647353132566\n",
      "[LOG 20200502-14:27:48] epoch: 9370 train-loss: 0.010550649940139718\n",
      "[LOG 20200502-14:27:48] epoch: 9371 train-loss: 0.0105506530445483\n",
      "[LOG 20200502-14:27:48] epoch: 9372 train-loss: 0.01055065542459488\n",
      "[LOG 20200502-14:27:48] epoch: 9373 train-loss: 0.01055065832204289\n",
      "[LOG 20200502-14:27:48] epoch: 9374 train-loss: 0.010550661529931758\n",
      "[LOG 20200502-14:27:49] epoch: 9375 train-loss: 0.010550664323899481\n",
      "[LOG 20200502-14:27:49] epoch: 9376 train-loss: 0.010550666910906633\n",
      "[LOG 20200502-14:27:49] epoch: 9377 train-loss: 0.010550669704874357\n",
      "[LOG 20200502-14:27:49] epoch: 9378 train-loss: 0.010550672602322366\n",
      "[LOG 20200502-14:27:50] epoch: 9379 train-loss: 0.010550675085849233\n",
      "[LOG 20200502-14:27:50] epoch: 9380 train-loss: 0.010550678397218386\n",
      "[LOG 20200502-14:27:50] epoch: 9381 train-loss: 0.01055068119118611\n",
      "[LOG 20200502-14:27:50] epoch: 9382 train-loss: 0.010550683674712976\n",
      "[LOG 20200502-14:27:51] epoch: 9383 train-loss: 0.010550686261720128\n",
      "[LOG 20200502-14:27:51] epoch: 9384 train-loss: 0.010550689469608996\n",
      "[LOG 20200502-14:27:51] epoch: 9385 train-loss: 0.010550692056616148\n",
      "[LOG 20200502-14:27:51] epoch: 9386 train-loss: 0.010550695057544444\n",
      "[LOG 20200502-14:27:52] epoch: 9387 train-loss: 0.010550697954992453\n",
      "[LOG 20200502-14:27:52] epoch: 9388 train-loss: 0.010550700748960177\n",
      "[LOG 20200502-14:27:52] epoch: 9389 train-loss: 0.010550703749888472\n",
      "[LOG 20200502-14:27:52] epoch: 9390 train-loss: 0.010550706440375911\n",
      "[LOG 20200502-14:27:52] epoch: 9391 train-loss: 0.010550709234343635\n",
      "[LOG 20200502-14:27:53] epoch: 9392 train-loss: 0.010550712131791644\n",
      "[LOG 20200502-14:27:53] epoch: 9393 train-loss: 0.010550715132719941\n",
      "[LOG 20200502-14:27:53] epoch: 9394 train-loss: 0.010550717616246806\n",
      "[LOG 20200502-14:27:53] epoch: 9395 train-loss: 0.01055072041021453\n",
      "[LOG 20200502-14:27:54] epoch: 9396 train-loss: 0.010550723514623113\n",
      "[LOG 20200502-14:27:54] epoch: 9397 train-loss: 0.010550725998149978\n",
      "[LOG 20200502-14:27:54] epoch: 9398 train-loss: 0.01055072910255856\n",
      "[LOG 20200502-14:27:54] epoch: 9399 train-loss: 0.010550732206967142\n",
      "[LOG 20200502-14:27:55] epoch: 9400 train-loss: 0.010550734690494008\n",
      "[LOG 20200502-14:27:55] epoch: 9401 train-loss: 0.010550737587942017\n",
      "[LOG 20200502-14:27:55] epoch: 9402 train-loss: 0.010550740278429456\n",
      "[LOG 20200502-14:27:55] epoch: 9403 train-loss: 0.010550742865436606\n",
      "[LOG 20200502-14:27:56] epoch: 9404 train-loss: 0.010550746073325476\n",
      "[LOG 20200502-14:27:56] epoch: 9405 train-loss: 0.010550749177734057\n",
      "[LOG 20200502-14:27:56] epoch: 9406 train-loss: 0.010550751454300351\n",
      "[LOG 20200502-14:27:56] epoch: 9407 train-loss: 0.010550754869149791\n",
      "[LOG 20200502-14:27:57] epoch: 9408 train-loss: 0.010550757870078087\n",
      "[LOG 20200502-14:27:57] epoch: 9409 train-loss: 0.010550760250124667\n",
      "[LOG 20200502-14:27:57] epoch: 9410 train-loss: 0.010550763871934678\n",
      "[LOG 20200502-14:27:57] epoch: 9411 train-loss: 0.01055076645894183\n",
      "[LOG 20200502-14:27:57] epoch: 9412 train-loss: 0.010550769252909554\n",
      "[LOG 20200502-14:27:58] epoch: 9413 train-loss: 0.010550772357318137\n",
      "[LOG 20200502-14:27:58] epoch: 9414 train-loss: 0.010550774737364717\n",
      "[LOG 20200502-14:27:58] epoch: 9415 train-loss: 0.010550778255694442\n",
      "[LOG 20200502-14:27:58] epoch: 9416 train-loss: 0.01055078094618188\n",
      "[LOG 20200502-14:27:59] epoch: 9417 train-loss: 0.010550783533189032\n",
      "[LOG 20200502-14:27:59] epoch: 9418 train-loss: 0.010550787051518759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:27:59] epoch: 9419 train-loss: 0.010550789535045624\n",
      "[LOG 20200502-14:27:59] epoch: 9420 train-loss: 0.010550793260335922\n",
      "[LOG 20200502-14:28:00] epoch: 9421 train-loss: 0.01055079595082336\n",
      "[LOG 20200502-14:28:00] epoch: 9422 train-loss: 0.010550798951751657\n",
      "[LOG 20200502-14:28:00] epoch: 9423 train-loss: 0.010550802159640525\n",
      "[LOG 20200502-14:28:00] epoch: 9424 train-loss: 0.010550804850127962\n",
      "[LOG 20200502-14:28:01] epoch: 9425 train-loss: 0.010550808161497116\n",
      "[LOG 20200502-14:28:01] epoch: 9426 train-loss: 0.010550810541543696\n",
      "[LOG 20200502-14:28:01] epoch: 9427 train-loss: 0.01055081385291285\n",
      "[LOG 20200502-14:28:01] epoch: 9428 train-loss: 0.010550817474722862\n",
      "[LOG 20200502-14:28:01] epoch: 9429 train-loss: 0.010550820061730014\n",
      "[LOG 20200502-14:28:02] epoch: 9430 train-loss: 0.010550823890500598\n",
      "[LOG 20200502-14:28:02] epoch: 9431 train-loss: 0.01055082647750775\n",
      "[LOG 20200502-14:28:02] epoch: 9432 train-loss: 0.010550829685396619\n",
      "[LOG 20200502-14:28:02] epoch: 9433 train-loss: 0.010550832996765772\n",
      "[LOG 20200502-14:28:03] epoch: 9434 train-loss: 0.010550836101174355\n",
      "[LOG 20200502-14:28:03] epoch: 9435 train-loss: 0.010550839722984366\n",
      "[LOG 20200502-14:28:03] epoch: 9436 train-loss: 0.01055084251695209\n",
      "[LOG 20200502-14:28:03] epoch: 9437 train-loss: 0.010550845931801531\n",
      "[LOG 20200502-14:28:03] epoch: 9438 train-loss: 0.010550848932729827\n",
      "[LOG 20200502-14:28:04] epoch: 9439 train-loss: 0.010550852658020126\n",
      "[LOG 20200502-14:28:04] epoch: 9440 train-loss: 0.010550856072869565\n",
      "[LOG 20200502-14:28:04] epoch: 9441 train-loss: 0.010550859280758433\n",
      "[LOG 20200502-14:28:04] epoch: 9442 train-loss: 0.010550862902568446\n",
      "[LOG 20200502-14:28:05] epoch: 9443 train-loss: 0.010550866834819317\n",
      "[LOG 20200502-14:28:05] epoch: 9444 train-loss: 0.010550869732267328\n",
      "[LOG 20200502-14:28:05] epoch: 9445 train-loss: 0.010550872940156195\n",
      "[LOG 20200502-14:28:05] epoch: 9446 train-loss: 0.010550876355005635\n",
      "[LOG 20200502-14:28:06] epoch: 9447 train-loss: 0.01055087966637479\n",
      "[LOG 20200502-14:28:06] epoch: 9448 train-loss: 0.010550883702105947\n",
      "[LOG 20200502-14:28:06] epoch: 9449 train-loss: 0.010550886909994815\n",
      "[LOG 20200502-14:28:06] epoch: 9450 train-loss: 0.010550890324844254\n",
      "[LOG 20200502-14:28:06] epoch: 9451 train-loss: 0.010550894050134553\n",
      "[LOG 20200502-14:28:07] epoch: 9452 train-loss: 0.010550897878905138\n",
      "[LOG 20200502-14:28:07] epoch: 9453 train-loss: 0.010550901811156008\n",
      "[LOG 20200502-14:28:07] epoch: 9454 train-loss: 0.010550905226005448\n",
      "[LOG 20200502-14:28:07] epoch: 9455 train-loss: 0.010550909261736605\n",
      "[LOG 20200502-14:28:08] epoch: 9456 train-loss: 0.010550912883546617\n",
      "[LOG 20200502-14:28:08] epoch: 9457 train-loss: 0.0105509159879552\n",
      "[LOG 20200502-14:28:08] epoch: 9458 train-loss: 0.010550919713245498\n",
      "[LOG 20200502-14:28:08] epoch: 9459 train-loss: 0.01055092333505551\n",
      "[LOG 20200502-14:28:09] epoch: 9460 train-loss: 0.010550927888188098\n",
      "[LOG 20200502-14:28:09] epoch: 9461 train-loss: 0.010550931199557252\n",
      "[LOG 20200502-14:28:09] epoch: 9462 train-loss: 0.010550935338768695\n",
      "[LOG 20200502-14:28:09] epoch: 9463 train-loss: 0.010550938857098421\n",
      "[LOG 20200502-14:28:09] epoch: 9464 train-loss: 0.010550943306750722\n",
      "[LOG 20200502-14:28:10] epoch: 9465 train-loss: 0.010550946928560734\n",
      "[LOG 20200502-14:28:10] epoch: 9466 train-loss: 0.010550950653851032\n",
      "[LOG 20200502-14:28:10] epoch: 9467 train-loss: 0.010550954586101903\n",
      "[LOG 20200502-14:28:10] epoch: 9468 train-loss: 0.010550958207911916\n",
      "[LOG 20200502-14:28:11] epoch: 9469 train-loss: 0.010550962761044502\n",
      "[LOG 20200502-14:28:11] epoch: 9470 train-loss: 0.010550965865453085\n",
      "[LOG 20200502-14:28:11] epoch: 9471 train-loss: 0.010550970625546243\n",
      "[LOG 20200502-14:28:11] epoch: 9472 train-loss: 0.010550974040395684\n",
      "[LOG 20200502-14:28:11] epoch: 9473 train-loss: 0.010550978386567699\n",
      "[LOG 20200502-14:28:12] epoch: 9474 train-loss: 0.010550982111857997\n",
      "[LOG 20200502-14:28:12] epoch: 9475 train-loss: 0.010550986871951155\n",
      "[LOG 20200502-14:28:12] epoch: 9476 train-loss: 0.010550990390280882\n",
      "[LOG 20200502-14:28:12] epoch: 9477 train-loss: 0.010550994322531752\n",
      "[LOG 20200502-14:28:13] epoch: 9478 train-loss: 0.010550998772184053\n",
      "[LOG 20200502-14:28:13] epoch: 9479 train-loss: 0.010551002704434924\n",
      "[LOG 20200502-14:28:13] epoch: 9480 train-loss: 0.010551006326244937\n",
      "[LOG 20200502-14:28:13] epoch: 9481 train-loss: 0.010551010258495808\n",
      "[LOG 20200502-14:28:13] epoch: 9482 train-loss: 0.010551015018588968\n",
      "[LOG 20200502-14:28:14] epoch: 9483 train-loss: 0.010551019468241267\n",
      "[LOG 20200502-14:28:14] epoch: 9484 train-loss: 0.010551023503972424\n",
      "[LOG 20200502-14:28:14] epoch: 9485 train-loss: 0.010551027229262723\n",
      "[LOG 20200502-14:28:14] epoch: 9486 train-loss: 0.010551031575434737\n",
      "[LOG 20200502-14:28:15] epoch: 9487 train-loss: 0.010551035611165894\n",
      "[LOG 20200502-14:28:15] epoch: 9488 train-loss: 0.010551039853857623\n",
      "[LOG 20200502-14:28:15] epoch: 9489 train-loss: 0.010551043682628207\n",
      "[LOG 20200502-14:28:15] epoch: 9490 train-loss: 0.010551047821839651\n",
      "[LOG 20200502-14:28:16] epoch: 9491 train-loss: 0.010551052374972237\n",
      "[LOG 20200502-14:28:16] epoch: 9492 train-loss: 0.010551056307223108\n",
      "[LOG 20200502-14:28:16] epoch: 9493 train-loss: 0.010551060653395124\n",
      "[LOG 20200502-14:28:16] epoch: 9494 train-loss: 0.010551064378685422\n",
      "[LOG 20200502-14:28:16] epoch: 9495 train-loss: 0.010551069035298295\n",
      "[LOG 20200502-14:28:17] epoch: 9496 train-loss: 0.01055107338147031\n",
      "[LOG 20200502-14:28:17] epoch: 9497 train-loss: 0.01055107731372118\n",
      "[LOG 20200502-14:28:17] epoch: 9498 train-loss: 0.010551081039011478\n",
      "[LOG 20200502-14:28:17] epoch: 9499 train-loss: 0.010551085385183493\n",
      "[LOG 20200502-14:28:18] epoch: 9500 train-loss: 0.010551090455717511\n",
      "[LOG 20200502-14:28:18] epoch: 9501 train-loss: 0.010551094077527523\n",
      "[LOG 20200502-14:28:18] epoch: 9502 train-loss: 0.010551098423699537\n",
      "[LOG 20200502-14:28:18] epoch: 9503 train-loss: 0.010551102769871553\n",
      "[LOG 20200502-14:28:18] epoch: 9504 train-loss: 0.010551106598642137\n",
      "[LOG 20200502-14:28:19] epoch: 9505 train-loss: 0.010551111565695869\n",
      "[LOG 20200502-14:28:19] epoch: 9506 train-loss: 0.010551115290986167\n",
      "[LOG 20200502-14:28:19] epoch: 9507 train-loss: 0.010551119844118753\n",
      "[LOG 20200502-14:28:19] epoch: 9508 train-loss: 0.01055112419029077\n",
      "[LOG 20200502-14:28:20] epoch: 9509 train-loss: 0.010551127708620496\n",
      "[LOG 20200502-14:28:20] epoch: 9510 train-loss: 0.010551132675674226\n",
      "[LOG 20200502-14:28:20] epoch: 9511 train-loss: 0.010551136711405383\n",
      "[LOG 20200502-14:28:20] epoch: 9512 train-loss: 0.010551140540175967\n",
      "[LOG 20200502-14:28:20] epoch: 9513 train-loss: 0.01055114571419027\n",
      "[LOG 20200502-14:28:21] epoch: 9514 train-loss: 0.010551149232519997\n",
      "[LOG 20200502-14:28:21] epoch: 9515 train-loss: 0.010551153682172298\n",
      "[LOG 20200502-14:28:21] epoch: 9516 train-loss: 0.01055115782138374\n",
      "[LOG 20200502-14:28:21] epoch: 9517 train-loss: 0.010551161960595183\n",
      "[LOG 20200502-14:28:21] epoch: 9518 train-loss: 0.0105511663067672\n",
      "[LOG 20200502-14:28:22] epoch: 9519 train-loss: 0.010551170652939213\n",
      "[LOG 20200502-14:28:22] epoch: 9520 train-loss: 0.010551175206071801\n",
      "[LOG 20200502-14:28:22] epoch: 9521 train-loss: 0.010551179448763529\n",
      "[LOG 20200502-14:28:22] epoch: 9522 train-loss: 0.010551183691455258\n",
      "[LOG 20200502-14:28:23] epoch: 9523 train-loss: 0.010551187727186415\n",
      "[LOG 20200502-14:28:23] epoch: 9524 train-loss: 0.010551192383799288\n",
      "[LOG 20200502-14:28:23] epoch: 9525 train-loss: 0.01055119652301073\n",
      "[LOG 20200502-14:28:23] epoch: 9526 train-loss: 0.010551200869182745\n",
      "[LOG 20200502-14:28:23] epoch: 9527 train-loss: 0.010551205111874474\n",
      "[LOG 20200502-14:28:24] epoch: 9528 train-loss: 0.010551209871967634\n",
      "[LOG 20200502-14:28:24] epoch: 9529 train-loss: 0.01055121390769879\n",
      "[LOG 20200502-14:28:24] epoch: 9530 train-loss: 0.010551217943429947\n",
      "[LOG 20200502-14:28:24] epoch: 9531 train-loss: 0.010551222186121676\n",
      "[LOG 20200502-14:28:25] epoch: 9532 train-loss: 0.010551226739254262\n",
      "[LOG 20200502-14:28:25] epoch: 9533 train-loss: 0.010551231085426278\n",
      "[LOG 20200502-14:28:25] epoch: 9534 train-loss: 0.010551235431598293\n",
      "[LOG 20200502-14:28:25] epoch: 9535 train-loss: 0.010551239674290022\n",
      "[LOG 20200502-14:28:26] epoch: 9536 train-loss: 0.010551243813501464\n",
      "[LOG 20200502-14:28:26] epoch: 9537 train-loss: 0.010551248056193193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:28:26] epoch: 9538 train-loss: 0.010551252402365208\n",
      "[LOG 20200502-14:28:26] epoch: 9539 train-loss: 0.010551256852017509\n",
      "[LOG 20200502-14:28:27] epoch: 9540 train-loss: 0.010551261508630382\n",
      "[LOG 20200502-14:28:27] epoch: 9541 train-loss: 0.010551265544361539\n",
      "[LOG 20200502-14:28:27] epoch: 9542 train-loss: 0.010551270097494125\n",
      "[LOG 20200502-14:28:27] epoch: 9543 train-loss: 0.010551274133225283\n",
      "[LOG 20200502-14:28:27] epoch: 9544 train-loss: 0.010551278479397297\n",
      "[LOG 20200502-14:28:28] epoch: 9545 train-loss: 0.010551282929049598\n",
      "[LOG 20200502-14:28:28] epoch: 9546 train-loss: 0.010551287689142756\n",
      "[LOG 20200502-14:28:28] epoch: 9547 train-loss: 0.010551291724873913\n",
      "[LOG 20200502-14:28:29] epoch: 9548 train-loss: 0.010551296174526215\n",
      "[LOG 20200502-14:28:29] epoch: 9549 train-loss: 0.010551300313737657\n",
      "[LOG 20200502-14:28:29] epoch: 9550 train-loss: 0.01055130497035053\n",
      "[LOG 20200502-14:28:29] epoch: 9551 train-loss: 0.010551309316522546\n",
      "[LOG 20200502-14:28:30] epoch: 9552 train-loss: 0.010551314180095991\n",
      "[LOG 20200502-14:28:30] epoch: 9553 train-loss: 0.010551318112346861\n",
      "[LOG 20200502-14:28:30] epoch: 9554 train-loss: 0.010551322975920307\n",
      "[LOG 20200502-14:28:30] epoch: 9555 train-loss: 0.010551327218612036\n",
      "[LOG 20200502-14:28:30] epoch: 9556 train-loss: 0.010551331668264337\n",
      "[LOG 20200502-14:28:31] epoch: 9557 train-loss: 0.010551335910956064\n",
      "[LOG 20200502-14:28:31] epoch: 9558 train-loss: 0.010551340360608365\n",
      "[LOG 20200502-14:28:31] epoch: 9559 train-loss: 0.010551345017221238\n",
      "[LOG 20200502-14:28:31] epoch: 9560 train-loss: 0.01055134946687354\n",
      "[LOG 20200502-14:28:32] epoch: 9561 train-loss: 0.01055135391652584\n",
      "[LOG 20200502-14:28:32] epoch: 9562 train-loss: 0.010551358469658427\n",
      "[LOG 20200502-14:28:32] epoch: 9563 train-loss: 0.0105513631262713\n",
      "[LOG 20200502-14:28:32] epoch: 9564 train-loss: 0.010551367782884173\n",
      "[LOG 20200502-14:28:33] epoch: 9565 train-loss: 0.010551372025575902\n",
      "[LOG 20200502-14:28:33] epoch: 9566 train-loss: 0.010551376992629634\n",
      "[LOG 20200502-14:28:33] epoch: 9567 train-loss: 0.01055138154576222\n",
      "[LOG 20200502-14:28:33] epoch: 9568 train-loss: 0.010551386202375094\n",
      "[LOG 20200502-14:28:34] epoch: 9569 train-loss: 0.01055139054854711\n",
      "[LOG 20200502-14:28:34] epoch: 9570 train-loss: 0.010551395412120555\n",
      "[LOG 20200502-14:28:34] epoch: 9571 train-loss: 0.010551400172213713\n",
      "[LOG 20200502-14:28:34] epoch: 9572 train-loss: 0.010551405035787158\n",
      "[LOG 20200502-14:28:35] epoch: 9573 train-loss: 0.01055140969240003\n",
      "[LOG 20200502-14:28:35] epoch: 9574 train-loss: 0.01055141393509176\n",
      "[LOG 20200502-14:28:35] epoch: 9575 train-loss: 0.010551418902145492\n",
      "[LOG 20200502-14:28:35] epoch: 9576 train-loss: 0.01055142366223865\n",
      "[LOG 20200502-14:28:36] epoch: 9577 train-loss: 0.01055142893973324\n",
      "[LOG 20200502-14:28:36] epoch: 9578 train-loss: 0.010551433803306686\n",
      "[LOG 20200502-14:28:36] epoch: 9579 train-loss: 0.0105514381494787\n",
      "[LOG 20200502-14:28:36] epoch: 9580 train-loss: 0.010551443220012717\n",
      "[LOG 20200502-14:28:37] epoch: 9581 train-loss: 0.010551448083586164\n",
      "[LOG 20200502-14:28:37] epoch: 9582 train-loss: 0.010551452947159609\n",
      "[LOG 20200502-14:28:37] epoch: 9583 train-loss: 0.010551457810733054\n",
      "[LOG 20200502-14:28:37] epoch: 9584 train-loss: 0.010551462984747358\n",
      "[LOG 20200502-14:28:38] epoch: 9585 train-loss: 0.010551467537879944\n",
      "[LOG 20200502-14:28:38] epoch: 9586 train-loss: 0.010551472504933676\n",
      "[LOG 20200502-14:28:38] epoch: 9587 train-loss: 0.010551477885908551\n",
      "[LOG 20200502-14:28:38] epoch: 9588 train-loss: 0.010551482749481996\n",
      "[LOG 20200502-14:28:38] epoch: 9589 train-loss: 0.010551487509575155\n",
      "[LOG 20200502-14:28:39] epoch: 9590 train-loss: 0.0105514923731486\n",
      "[LOG 20200502-14:28:39] epoch: 9591 train-loss: 0.010551498064564334\n",
      "[LOG 20200502-14:28:39] epoch: 9592 train-loss: 0.01055150292813778\n",
      "[LOG 20200502-14:28:39] epoch: 9593 train-loss: 0.010551507481270365\n",
      "[LOG 20200502-14:28:40] epoch: 9594 train-loss: 0.010551512758764956\n",
      "[LOG 20200502-14:28:40] epoch: 9595 train-loss: 0.010551518139739832\n",
      "[LOG 20200502-14:28:40] epoch: 9596 train-loss: 0.010551522692872418\n",
      "[LOG 20200502-14:28:40] epoch: 9597 train-loss: 0.010551528591248725\n",
      "[LOG 20200502-14:28:41] epoch: 9598 train-loss: 0.010551533765263028\n",
      "[LOG 20200502-14:28:41] epoch: 9599 train-loss: 0.010551538421875901\n",
      "[LOG 20200502-14:28:41] epoch: 9600 train-loss: 0.01055154369937049\n",
      "[LOG 20200502-14:28:41] epoch: 9601 train-loss: 0.010551549183825651\n",
      "[LOG 20200502-14:28:42] epoch: 9602 train-loss: 0.010551554668280814\n",
      "[LOG 20200502-14:28:42] epoch: 9603 train-loss: 0.010551559531854259\n",
      "[LOG 20200502-14:28:42] epoch: 9604 train-loss: 0.010551564602388276\n",
      "[LOG 20200502-14:28:42] epoch: 9605 train-loss: 0.010551569879882865\n",
      "[LOG 20200502-14:28:43] epoch: 9606 train-loss: 0.01055157526085774\n",
      "[LOG 20200502-14:28:43] epoch: 9607 train-loss: 0.010551580848793188\n",
      "[LOG 20200502-14:28:43] epoch: 9608 train-loss: 0.010551586022807492\n",
      "[LOG 20200502-14:28:43] epoch: 9609 train-loss: 0.010551590989861224\n",
      "[LOG 20200502-14:28:43] epoch: 9610 train-loss: 0.010551596577796672\n",
      "[LOG 20200502-14:28:44] epoch: 9611 train-loss: 0.01055160216573212\n",
      "[LOG 20200502-14:28:44] epoch: 9612 train-loss: 0.010551607546706995\n",
      "[LOG 20200502-14:28:44] epoch: 9613 train-loss: 0.010551612720721297\n",
      "[LOG 20200502-14:28:44] epoch: 9614 train-loss: 0.010551617687775029\n",
      "[LOG 20200502-14:28:45] epoch: 9615 train-loss: 0.01055162368963162\n",
      "[LOG 20200502-14:28:45] epoch: 9616 train-loss: 0.010551628863645924\n",
      "[LOG 20200502-14:28:45] epoch: 9617 train-loss: 0.010551633934179941\n",
      "[LOG 20200502-14:28:45] epoch: 9618 train-loss: 0.010551639418635104\n",
      "[LOG 20200502-14:28:46] epoch: 9619 train-loss: 0.010551644903090265\n",
      "[LOG 20200502-14:28:46] epoch: 9620 train-loss: 0.010551649973624282\n",
      "[LOG 20200502-14:28:46] epoch: 9621 train-loss: 0.010551655665040016\n",
      "[LOG 20200502-14:28:46] epoch: 9622 train-loss: 0.01055166135645575\n",
      "[LOG 20200502-14:28:46] epoch: 9623 train-loss: 0.010551666737430625\n",
      "[LOG 20200502-14:28:47] epoch: 9624 train-loss: 0.010551672325366072\n",
      "[LOG 20200502-14:28:47] epoch: 9625 train-loss: 0.010551677499380376\n",
      "[LOG 20200502-14:28:47] epoch: 9626 train-loss: 0.010551682983835539\n",
      "[LOG 20200502-14:28:47] epoch: 9627 train-loss: 0.010551688157849841\n",
      "[LOG 20200502-14:28:48] epoch: 9628 train-loss: 0.010551693745785289\n",
      "[LOG 20200502-14:28:48] epoch: 9629 train-loss: 0.010551699437201023\n",
      "[LOG 20200502-14:28:48] epoch: 9630 train-loss: 0.010551704611215327\n",
      "[LOG 20200502-14:28:48] epoch: 9631 train-loss: 0.010551709992190203\n",
      "[LOG 20200502-14:28:48] epoch: 9632 train-loss: 0.010551715994046794\n",
      "[LOG 20200502-14:28:49] epoch: 9633 train-loss: 0.010551720961100526\n",
      "[LOG 20200502-14:28:49] epoch: 9634 train-loss: 0.010551726652516259\n",
      "[LOG 20200502-14:28:49] epoch: 9635 train-loss: 0.010551731930010848\n",
      "[LOG 20200502-14:28:49] epoch: 9636 train-loss: 0.010551737517946295\n",
      "[LOG 20200502-14:28:50] epoch: 9637 train-loss: 0.010551742795440886\n",
      "[LOG 20200502-14:28:50] epoch: 9638 train-loss: 0.010551748486856619\n",
      "[LOG 20200502-14:28:50] epoch: 9639 train-loss: 0.010551754178272353\n",
      "[LOG 20200502-14:28:50] epoch: 9640 train-loss: 0.010551759352286657\n",
      "[LOG 20200502-14:28:51] epoch: 9641 train-loss: 0.010551764836741818\n",
      "[LOG 20200502-14:28:51] epoch: 9642 train-loss: 0.010551770528157553\n",
      "[LOG 20200502-14:28:51] epoch: 9643 train-loss: 0.010551775909132428\n",
      "[LOG 20200502-14:28:51] epoch: 9644 train-loss: 0.01055178139358759\n",
      "[LOG 20200502-14:28:51] epoch: 9645 train-loss: 0.010551786671082178\n",
      "[LOG 20200502-14:28:52] epoch: 9646 train-loss: 0.010551792776419057\n",
      "[LOG 20200502-14:28:52] epoch: 9647 train-loss: 0.010551797846953074\n",
      "[LOG 20200502-14:28:52] epoch: 9648 train-loss: 0.010551803538368808\n",
      "[LOG 20200502-14:28:52] epoch: 9649 train-loss: 0.010551809022823969\n",
      "[LOG 20200502-14:28:53] epoch: 9650 train-loss: 0.010551814714239703\n",
      "[LOG 20200502-14:28:53] epoch: 9651 train-loss: 0.010551819991734292\n",
      "[LOG 20200502-14:28:53] epoch: 9652 train-loss: 0.010551825269228883\n",
      "[LOG 20200502-14:28:53] epoch: 9653 train-loss: 0.010551831167605188\n",
      "[LOG 20200502-14:28:53] epoch: 9654 train-loss: 0.010551836341619492\n",
      "[LOG 20200502-14:28:54] epoch: 9655 train-loss: 0.010551842033035226\n",
      "[LOG 20200502-14:28:54] epoch: 9656 train-loss: 0.010551847103569243\n",
      "[LOG 20200502-14:28:54] epoch: 9657 train-loss: 0.010551853105425835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:28:54] epoch: 9658 train-loss: 0.010551858279440138\n",
      "[LOG 20200502-14:28:55] epoch: 9659 train-loss: 0.010551864177816443\n",
      "[LOG 20200502-14:28:55] epoch: 9660 train-loss: 0.010551869455311034\n",
      "[LOG 20200502-14:28:55] epoch: 9661 train-loss: 0.01055187483628591\n",
      "[LOG 20200502-14:28:55] epoch: 9662 train-loss: 0.010551880424221357\n",
      "[LOG 20200502-14:28:56] epoch: 9663 train-loss: 0.010551885908676518\n",
      "[LOG 20200502-14:28:56] epoch: 9664 train-loss: 0.010551890979210535\n",
      "[LOG 20200502-14:28:56] epoch: 9665 train-loss: 0.010551896774106555\n",
      "[LOG 20200502-14:28:56] epoch: 9666 train-loss: 0.010551902775963148\n",
      "[LOG 20200502-14:28:57] epoch: 9667 train-loss: 0.01055190846737888\n",
      "[LOG 20200502-14:28:57] epoch: 9668 train-loss: 0.010551913641393185\n",
      "[LOG 20200502-14:28:57] epoch: 9669 train-loss: 0.01055191933280892\n",
      "[LOG 20200502-14:28:57] epoch: 9670 train-loss: 0.010551924196382364\n",
      "[LOG 20200502-14:28:58] epoch: 9671 train-loss: 0.010551930198238956\n",
      "[LOG 20200502-14:28:58] epoch: 9672 train-loss: 0.010551935889654689\n",
      "[LOG 20200502-14:28:58] epoch: 9673 train-loss: 0.010551941477590136\n",
      "[LOG 20200502-14:28:58] epoch: 9674 train-loss: 0.010551946962045299\n",
      "[LOG 20200502-14:28:59] epoch: 9675 train-loss: 0.010551952032579316\n",
      "[LOG 20200502-14:28:59] epoch: 9676 train-loss: 0.010551957827475335\n",
      "[LOG 20200502-14:28:59] epoch: 9677 train-loss: 0.01055196351889107\n",
      "[LOG 20200502-14:28:59] epoch: 9678 train-loss: 0.01055196931378709\n",
      "[LOG 20200502-14:29:00] epoch: 9679 train-loss: 0.010551974591281679\n",
      "[LOG 20200502-14:29:00] epoch: 9680 train-loss: 0.010551979765295982\n",
      "[LOG 20200502-14:29:00] epoch: 9681 train-loss: 0.010551985456711717\n",
      "[LOG 20200502-14:29:00] epoch: 9682 train-loss: 0.010551991044647165\n",
      "[LOG 20200502-14:29:01] epoch: 9683 train-loss: 0.0105519974604249\n",
      "[LOG 20200502-14:29:01] epoch: 9684 train-loss: 0.01055200222051806\n",
      "[LOG 20200502-14:29:01] epoch: 9685 train-loss: 0.010552008118894365\n",
      "[LOG 20200502-14:29:01] epoch: 9686 train-loss: 0.01055201349986924\n",
      "[LOG 20200502-14:29:02] epoch: 9687 train-loss: 0.010552019191284975\n",
      "[LOG 20200502-14:29:02] epoch: 9688 train-loss: 0.010552025193141567\n",
      "[LOG 20200502-14:29:02] epoch: 9689 train-loss: 0.010552030160195298\n",
      "[LOG 20200502-14:29:02] epoch: 9690 train-loss: 0.010552036265532175\n",
      "[LOG 20200502-14:29:03] epoch: 9691 train-loss: 0.010552042163908482\n",
      "[LOG 20200502-14:29:03] epoch: 9692 train-loss: 0.010552046924001642\n",
      "[LOG 20200502-14:29:03] epoch: 9693 train-loss: 0.010552053339779377\n",
      "[LOG 20200502-14:29:03] epoch: 9694 train-loss: 0.010552058927714825\n",
      "[LOG 20200502-14:29:04] epoch: 9695 train-loss: 0.010552064722610844\n",
      "[LOG 20200502-14:29:04] epoch: 9696 train-loss: 0.010552070414026579\n",
      "[LOG 20200502-14:29:04] epoch: 9697 train-loss: 0.010552076208922598\n",
      "[LOG 20200502-14:29:04] epoch: 9698 train-loss: 0.01055208169337776\n",
      "[LOG 20200502-14:29:05] epoch: 9699 train-loss: 0.010552087591754066\n",
      "[LOG 20200502-14:29:05] epoch: 9700 train-loss: 0.010552093179689514\n",
      "[LOG 20200502-14:29:05] epoch: 9701 train-loss: 0.010552099285026392\n",
      "[LOG 20200502-14:29:05] epoch: 9702 train-loss: 0.01055210487296184\n",
      "[LOG 20200502-14:29:06] epoch: 9703 train-loss: 0.010552110357417\n",
      "[LOG 20200502-14:29:06] epoch: 9704 train-loss: 0.010552116255793307\n",
      "[LOG 20200502-14:29:06] epoch: 9705 train-loss: 0.010552122361130185\n",
      "[LOG 20200502-14:29:06] epoch: 9706 train-loss: 0.010552128052545918\n",
      "[LOG 20200502-14:29:07] epoch: 9707 train-loss: 0.01055213405440251\n",
      "[LOG 20200502-14:29:07] epoch: 9708 train-loss: 0.010552139952778816\n",
      "[LOG 20200502-14:29:07] epoch: 9709 train-loss: 0.010552145437233977\n",
      "[LOG 20200502-14:29:07] epoch: 9710 train-loss: 0.010552151646051142\n",
      "[LOG 20200502-14:29:07] epoch: 9711 train-loss: 0.010552157647907734\n",
      "[LOG 20200502-14:29:08] epoch: 9712 train-loss: 0.010552163442803754\n",
      "[LOG 20200502-14:29:08] epoch: 9713 train-loss: 0.01055216934118006\n",
      "[LOG 20200502-14:29:08] epoch: 9714 train-loss: 0.010552175239556365\n",
      "[LOG 20200502-14:29:08] epoch: 9715 train-loss: 0.010552181344893243\n",
      "[LOG 20200502-14:29:09] epoch: 9716 train-loss: 0.010552187139789263\n",
      "[LOG 20200502-14:29:09] epoch: 9717 train-loss: 0.010552193245126141\n",
      "[LOG 20200502-14:29:09] epoch: 9718 train-loss: 0.010552199143502448\n",
      "[LOG 20200502-14:29:09] epoch: 9719 train-loss: 0.01055220514535904\n",
      "[LOG 20200502-14:29:10] epoch: 9720 train-loss: 0.010552211354176203\n",
      "[LOG 20200502-14:29:10] epoch: 9721 train-loss: 0.010552217356032796\n",
      "[LOG 20200502-14:29:10] epoch: 9722 train-loss: 0.010552223150928816\n",
      "[LOG 20200502-14:29:10] epoch: 9723 train-loss: 0.010552229463226266\n",
      "[LOG 20200502-14:29:10] epoch: 9724 train-loss: 0.010552235775523715\n",
      "[LOG 20200502-14:29:11] epoch: 9725 train-loss: 0.010552242191301452\n",
      "[LOG 20200502-14:29:11] epoch: 9726 train-loss: 0.010552248193158044\n",
      "[LOG 20200502-14:29:11] epoch: 9727 train-loss: 0.010552253884573778\n",
      "[LOG 20200502-14:29:11] epoch: 9728 train-loss: 0.01055225988643037\n",
      "[LOG 20200502-14:29:12] epoch: 9729 train-loss: 0.01055226619872782\n",
      "[LOG 20200502-14:29:12] epoch: 9730 train-loss: 0.010552272304064698\n",
      "[LOG 20200502-14:29:12] epoch: 9731 train-loss: 0.010552278926803006\n",
      "[LOG 20200502-14:29:12] epoch: 9732 train-loss: 0.01055228461821874\n",
      "[LOG 20200502-14:29:13] epoch: 9733 train-loss: 0.01055229093051619\n",
      "[LOG 20200502-14:29:13] epoch: 9734 train-loss: 0.01055229672541221\n",
      "[LOG 20200502-14:29:13] epoch: 9735 train-loss: 0.010552303244670233\n",
      "[LOG 20200502-14:29:13] epoch: 9736 train-loss: 0.010552309763928255\n",
      "[LOG 20200502-14:29:14] epoch: 9737 train-loss: 0.010552315558824275\n",
      "[LOG 20200502-14:29:14] epoch: 9738 train-loss: 0.010552321767641438\n",
      "[LOG 20200502-14:29:14] epoch: 9739 train-loss: 0.01055232828689946\n",
      "[LOG 20200502-14:29:14] epoch: 9740 train-loss: 0.010552334392236339\n",
      "[LOG 20200502-14:29:14] epoch: 9741 train-loss: 0.010552340497573217\n",
      "[LOG 20200502-14:29:15] epoch: 9742 train-loss: 0.01055234701683124\n",
      "[LOG 20200502-14:29:15] epoch: 9743 train-loss: 0.010552353018687831\n",
      "[LOG 20200502-14:29:15] epoch: 9744 train-loss: 0.010552359330985282\n",
      "[LOG 20200502-14:29:15] epoch: 9745 train-loss: 0.010552365643282732\n",
      "[LOG 20200502-14:29:16] epoch: 9746 train-loss: 0.010552371852099895\n",
      "[LOG 20200502-14:29:16] epoch: 9747 train-loss: 0.01055237857831849\n",
      "[LOG 20200502-14:29:16] epoch: 9748 train-loss: 0.010552384476694796\n",
      "[LOG 20200502-14:29:16] epoch: 9749 train-loss: 0.010552390995952819\n",
      "[LOG 20200502-14:29:17] epoch: 9750 train-loss: 0.010552396894329123\n",
      "[LOG 20200502-14:29:17] epoch: 9751 train-loss: 0.010552403517067432\n",
      "[LOG 20200502-14:29:17] epoch: 9752 train-loss: 0.010552410139805742\n",
      "[LOG 20200502-14:29:17] epoch: 9753 train-loss: 0.010552416038182046\n",
      "[LOG 20200502-14:29:18] epoch: 9754 train-loss: 0.010552422350479497\n",
      "[LOG 20200502-14:29:18] epoch: 9755 train-loss: 0.010552428352336088\n",
      "[LOG 20200502-14:29:18] epoch: 9756 train-loss: 0.010552434975074397\n",
      "[LOG 20200502-14:29:18] epoch: 9757 train-loss: 0.010552441287371848\n",
      "[LOG 20200502-14:29:19] epoch: 9758 train-loss: 0.010552447910110155\n",
      "[LOG 20200502-14:29:19] epoch: 9759 train-loss: 0.010552453808486462\n",
      "[LOG 20200502-14:29:19] epoch: 9760 train-loss: 0.010552459499902196\n",
      "[LOG 20200502-14:29:19] epoch: 9761 train-loss: 0.010552466433081362\n",
      "[LOG 20200502-14:29:19] epoch: 9762 train-loss: 0.01055247305581967\n",
      "[LOG 20200502-14:29:20] epoch: 9763 train-loss: 0.010552479471597407\n",
      "[LOG 20200502-14:29:20] epoch: 9764 train-loss: 0.010552485369973712\n",
      "[LOG 20200502-14:29:20] epoch: 9765 train-loss: 0.010552491578790877\n",
      "[LOG 20200502-14:29:20] epoch: 9766 train-loss: 0.010552497994568612\n",
      "[LOG 20200502-14:29:21] epoch: 9767 train-loss: 0.010552504513826635\n",
      "[LOG 20200502-14:29:21] epoch: 9768 train-loss: 0.010552510826124085\n",
      "[LOG 20200502-14:29:21] epoch: 9769 train-loss: 0.010552516827980677\n",
      "[LOG 20200502-14:29:21] epoch: 9770 train-loss: 0.010552522933317555\n",
      "[LOG 20200502-14:29:21] epoch: 9771 train-loss: 0.010552529349095292\n",
      "[LOG 20200502-14:29:22] epoch: 9772 train-loss: 0.010552535454432169\n",
      "[LOG 20200502-14:29:22] epoch: 9773 train-loss: 0.010552541663249334\n",
      "[LOG 20200502-14:29:22] epoch: 9774 train-loss: 0.010552547975546785\n",
      "[LOG 20200502-14:29:22] epoch: 9775 train-loss: 0.010552554494804807\n",
      "[LOG 20200502-14:29:23] epoch: 9776 train-loss: 0.010552560807102256\n",
      "[LOG 20200502-14:29:23] epoch: 9777 train-loss: 0.010552566808958849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:29:23] epoch: 9778 train-loss: 0.010552573431697156\n",
      "[LOG 20200502-14:29:23] epoch: 9779 train-loss: 0.010552579743994607\n",
      "[LOG 20200502-14:29:24] epoch: 9780 train-loss: 0.010552586263252629\n",
      "[LOG 20200502-14:29:24] epoch: 9781 train-loss: 0.010552592472069793\n",
      "[LOG 20200502-14:29:24] epoch: 9782 train-loss: 0.010552598473926386\n",
      "[LOG 20200502-14:29:24] epoch: 9783 train-loss: 0.010552604993184408\n",
      "[LOG 20200502-14:29:24] epoch: 9784 train-loss: 0.010552611305481858\n",
      "[LOG 20200502-14:29:25] epoch: 9785 train-loss: 0.010552617721259594\n",
      "[LOG 20200502-14:29:25] epoch: 9786 train-loss: 0.0105526236196359\n",
      "[LOG 20200502-14:29:25] epoch: 9787 train-loss: 0.010552629621492492\n",
      "[LOG 20200502-14:29:25] epoch: 9788 train-loss: 0.0105526362442308\n",
      "[LOG 20200502-14:29:26] epoch: 9789 train-loss: 0.010552642453047965\n",
      "[LOG 20200502-14:29:26] epoch: 9790 train-loss: 0.010552648661865128\n",
      "[LOG 20200502-14:29:26] epoch: 9791 train-loss: 0.01055265518112315\n",
      "[LOG 20200502-14:29:26] epoch: 9792 train-loss: 0.010552661182979742\n",
      "[LOG 20200502-14:29:27] epoch: 9793 train-loss: 0.01055266728831662\n",
      "[LOG 20200502-14:29:27] epoch: 9794 train-loss: 0.010552673393653499\n",
      "[LOG 20200502-14:29:27] epoch: 9795 train-loss: 0.010552679912911521\n",
      "[LOG 20200502-14:29:27] epoch: 9796 train-loss: 0.010552686225208972\n",
      "[LOG 20200502-14:29:27] epoch: 9797 train-loss: 0.010552692744466994\n",
      "[LOG 20200502-14:29:28] epoch: 9798 train-loss: 0.010552698849803872\n",
      "[LOG 20200502-14:29:28] epoch: 9799 train-loss: 0.010552704851660464\n",
      "[LOG 20200502-14:29:28] epoch: 9800 train-loss: 0.010552711060477627\n",
      "[LOG 20200502-14:29:28] epoch: 9801 train-loss: 0.010552717062334219\n",
      "[LOG 20200502-14:29:29] epoch: 9802 train-loss: 0.010552723478111956\n",
      "[LOG 20200502-14:29:29] epoch: 9803 train-loss: 0.010552729997369979\n",
      "[LOG 20200502-14:29:29] epoch: 9804 train-loss: 0.010552736309667429\n",
      "[LOG 20200502-14:29:29] epoch: 9805 train-loss: 0.01055274231152402\n",
      "[LOG 20200502-14:29:29] epoch: 9806 train-loss: 0.010552748520341184\n",
      "[LOG 20200502-14:29:30] epoch: 9807 train-loss: 0.01055275472915835\n",
      "[LOG 20200502-14:29:30] epoch: 9808 train-loss: 0.010552761248416372\n",
      "[LOG 20200502-14:29:30] epoch: 9809 train-loss: 0.010552767250272963\n",
      "[LOG 20200502-14:29:30] epoch: 9810 train-loss: 0.010552774079971843\n",
      "[LOG 20200502-14:29:31] epoch: 9811 train-loss: 0.010552780081828436\n",
      "[LOG 20200502-14:29:31] epoch: 9812 train-loss: 0.010552786394125886\n",
      "[LOG 20200502-14:29:31] epoch: 9813 train-loss: 0.010552792706423335\n",
      "[LOG 20200502-14:29:31] epoch: 9814 train-loss: 0.010552799122201072\n",
      "[LOG 20200502-14:29:31] epoch: 9815 train-loss: 0.01055280522753795\n",
      "[LOG 20200502-14:29:32] epoch: 9816 train-loss: 0.010552811746795973\n",
      "[LOG 20200502-14:29:32] epoch: 9817 train-loss: 0.010552817748652564\n",
      "[LOG 20200502-14:29:32] epoch: 9818 train-loss: 0.010552823957469728\n",
      "[LOG 20200502-14:29:32] epoch: 9819 train-loss: 0.010552830373247465\n",
      "[LOG 20200502-14:29:33] epoch: 9820 train-loss: 0.0105528367890252\n",
      "[LOG 20200502-14:29:33] epoch: 9821 train-loss: 0.010552843204802938\n",
      "[LOG 20200502-14:29:33] epoch: 9822 train-loss: 0.01055284920665953\n",
      "[LOG 20200502-14:29:33] epoch: 9823 train-loss: 0.010552855622437265\n",
      "[LOG 20200502-14:29:33] epoch: 9824 train-loss: 0.010552861934734715\n",
      "[LOG 20200502-14:29:34] epoch: 9825 train-loss: 0.010552868453992737\n",
      "[LOG 20200502-14:29:34] epoch: 9826 train-loss: 0.010552874559329616\n",
      "[LOG 20200502-14:29:34] epoch: 9827 train-loss: 0.010552881078587638\n",
      "[LOG 20200502-14:29:34] epoch: 9828 train-loss: 0.010552887287404802\n",
      "[LOG 20200502-14:29:35] epoch: 9829 train-loss: 0.010552894013623396\n",
      "[LOG 20200502-14:29:35] epoch: 9830 train-loss: 0.010552899808519416\n",
      "[LOG 20200502-14:29:35] epoch: 9831 train-loss: 0.010552906741698584\n",
      "[LOG 20200502-14:29:35] epoch: 9832 train-loss: 0.010552913157476319\n",
      "[LOG 20200502-14:29:35] epoch: 9833 train-loss: 0.010552919883694913\n",
      "[LOG 20200502-14:29:36] epoch: 9834 train-loss: 0.010552926092512079\n",
      "[LOG 20200502-14:29:36] epoch: 9835 train-loss: 0.010552932922210958\n",
      "[LOG 20200502-14:29:36] epoch: 9836 train-loss: 0.01055293944146898\n",
      "[LOG 20200502-14:29:36] epoch: 9837 train-loss: 0.01055294575376643\n",
      "[LOG 20200502-14:29:37] epoch: 9838 train-loss: 0.010552951859103309\n",
      "[LOG 20200502-14:29:37] epoch: 9839 train-loss: 0.010552958481841616\n",
      "[LOG 20200502-14:29:37] epoch: 9840 train-loss: 0.010552965208060212\n",
      "[LOG 20200502-14:29:37] epoch: 9841 train-loss: 0.010552971520357661\n",
      "[LOG 20200502-14:29:38] epoch: 9842 train-loss: 0.010552978246576257\n",
      "[LOG 20200502-14:29:38] epoch: 9843 train-loss: 0.01055298476583428\n",
      "[LOG 20200502-14:29:38] epoch: 9844 train-loss: 0.010552991388572587\n",
      "[LOG 20200502-14:29:38] epoch: 9845 train-loss: 0.010552998114791181\n",
      "[LOG 20200502-14:29:38] epoch: 9846 train-loss: 0.010553004944490062\n",
      "[LOG 20200502-14:29:39] epoch: 9847 train-loss: 0.010553011153307226\n",
      "[LOG 20200502-14:29:39] epoch: 9848 train-loss: 0.010553018086486392\n",
      "[LOG 20200502-14:29:39] epoch: 9849 train-loss: 0.010553024916185273\n",
      "[LOG 20200502-14:29:39] epoch: 9850 train-loss: 0.01055303133196301\n",
      "[LOG 20200502-14:29:39] epoch: 9851 train-loss: 0.010553037954701317\n",
      "[LOG 20200502-14:29:40] epoch: 9852 train-loss: 0.01055304447395934\n",
      "[LOG 20200502-14:29:40] epoch: 9853 train-loss: 0.010553051510618793\n",
      "[LOG 20200502-14:29:40] epoch: 9854 train-loss: 0.010553057719435956\n",
      "[LOG 20200502-14:29:40] epoch: 9855 train-loss: 0.01055306475609541\n",
      "[LOG 20200502-14:29:41] epoch: 9856 train-loss: 0.010553071482314004\n",
      "[LOG 20200502-14:29:41] epoch: 9857 train-loss: 0.010553078105052313\n",
      "[LOG 20200502-14:29:41] epoch: 9858 train-loss: 0.010553085038231479\n",
      "[LOG 20200502-14:29:41] epoch: 9859 train-loss: 0.010553091660969786\n",
      "[LOG 20200502-14:29:41] epoch: 9860 train-loss: 0.010553098180227809\n",
      "[LOG 20200502-14:29:42] epoch: 9861 train-loss: 0.010553104596005546\n",
      "[LOG 20200502-14:29:42] epoch: 9862 train-loss: 0.010553111011783281\n",
      "[LOG 20200502-14:29:42] epoch: 9863 train-loss: 0.010553118462363878\n",
      "[LOG 20200502-14:29:42] epoch: 9864 train-loss: 0.01055312529206276\n",
      "[LOG 20200502-14:29:43] epoch: 9865 train-loss: 0.010553131811320782\n",
      "[LOG 20200502-14:29:43] epoch: 9866 train-loss: 0.010553138641019663\n",
      "[LOG 20200502-14:29:43] epoch: 9867 train-loss: 0.010553145470718542\n",
      "[LOG 20200502-14:29:43] epoch: 9868 train-loss: 0.010553152507377995\n",
      "[LOG 20200502-14:29:44] epoch: 9869 train-loss: 0.010553159026636018\n",
      "[LOG 20200502-14:29:44] epoch: 9870 train-loss: 0.010553165442413755\n",
      "[LOG 20200502-14:29:44] epoch: 9871 train-loss: 0.010553172582553493\n",
      "[LOG 20200502-14:29:44] epoch: 9872 train-loss: 0.010553179412252374\n",
      "[LOG 20200502-14:29:44] epoch: 9873 train-loss: 0.010553186241951253\n",
      "[LOG 20200502-14:29:45] epoch: 9874 train-loss: 0.010553193071650134\n",
      "[LOG 20200502-14:29:45] epoch: 9875 train-loss: 0.010553199383947585\n",
      "[LOG 20200502-14:29:45] epoch: 9876 train-loss: 0.01055320662756761\n",
      "[LOG 20200502-14:29:45] epoch: 9877 train-loss: 0.010553213353786204\n",
      "[LOG 20200502-14:29:46] epoch: 9878 train-loss: 0.010553219666083654\n",
      "[LOG 20200502-14:29:46] epoch: 9879 train-loss: 0.010553226806223392\n",
      "[LOG 20200502-14:29:46] epoch: 9880 train-loss: 0.010553233118520843\n",
      "[LOG 20200502-14:29:46] epoch: 9881 train-loss: 0.01055324056910144\n",
      "[LOG 20200502-14:29:46] epoch: 9882 train-loss: 0.010553247088359462\n",
      "[LOG 20200502-14:29:47] epoch: 9883 train-loss: 0.010553253814578056\n",
      "[LOG 20200502-14:29:47] epoch: 9884 train-loss: 0.010553260747757223\n",
      "[LOG 20200502-14:29:47] epoch: 9885 train-loss: 0.010553267680936389\n",
      "[LOG 20200502-14:29:47] epoch: 9886 train-loss: 0.010553274096714126\n",
      "[LOG 20200502-14:29:48] epoch: 9887 train-loss: 0.010553280926413007\n",
      "[LOG 20200502-14:29:48] epoch: 9888 train-loss: 0.010553287859592173\n",
      "[LOG 20200502-14:29:48] epoch: 9889 train-loss: 0.010553294171889624\n",
      "[LOG 20200502-14:29:48] epoch: 9890 train-loss: 0.010553301725950506\n",
      "[LOG 20200502-14:29:48] epoch: 9891 train-loss: 0.010553308038247956\n",
      "[LOG 20200502-14:29:49] epoch: 9892 train-loss: 0.010553314557505978\n",
      "[LOG 20200502-14:29:49] epoch: 9893 train-loss: 0.01055332138720486\n",
      "[LOG 20200502-14:29:49] epoch: 9894 train-loss: 0.010553328216903739\n",
      "[LOG 20200502-14:29:49] epoch: 9895 train-loss: 0.010553334632681476\n",
      "[LOG 20200502-14:29:49] epoch: 9896 train-loss: 0.010553341669340929\n",
      "[LOG 20200502-14:29:50] epoch: 9897 train-loss: 0.010553348292079236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:29:50] epoch: 9898 train-loss: 0.010553354811337259\n",
      "[LOG 20200502-14:29:50] epoch: 9899 train-loss: 0.010553361537555853\n",
      "[LOG 20200502-14:29:50] epoch: 9900 train-loss: 0.010553368263774447\n",
      "[LOG 20200502-14:29:51] epoch: 9901 train-loss: 0.010553374886512756\n",
      "[LOG 20200502-14:29:51] epoch: 9902 train-loss: 0.010553381819691923\n",
      "[LOG 20200502-14:29:51] epoch: 9903 train-loss: 0.01055338823546966\n",
      "[LOG 20200502-14:29:51] epoch: 9904 train-loss: 0.010553394961688254\n",
      "[LOG 20200502-14:29:51] epoch: 9905 train-loss: 0.010553401791387133\n",
      "[LOG 20200502-14:29:52] epoch: 9906 train-loss: 0.010553408621086014\n",
      "[LOG 20200502-14:29:52] epoch: 9907 train-loss: 0.010553415657745467\n",
      "[LOG 20200502-14:29:52] epoch: 9908 train-loss: 0.010553421866562631\n",
      "[LOG 20200502-14:29:52] epoch: 9909 train-loss: 0.010553429110182656\n",
      "[LOG 20200502-14:29:53] epoch: 9910 train-loss: 0.010553435525960393\n",
      "[LOG 20200502-14:29:53] epoch: 9911 train-loss: 0.010553442252178987\n",
      "[LOG 20200502-14:29:53] epoch: 9912 train-loss: 0.010553448564476438\n",
      "[LOG 20200502-14:29:53] epoch: 9913 train-loss: 0.010553455497655604\n",
      "[LOG 20200502-14:29:53] epoch: 9914 train-loss: 0.010553462223874198\n",
      "[LOG 20200502-14:29:54] epoch: 9915 train-loss: 0.01055346874313222\n",
      "[LOG 20200502-14:29:54] epoch: 9916 train-loss: 0.010553475158909956\n",
      "[LOG 20200502-14:29:54] epoch: 9917 train-loss: 0.010553482195569409\n",
      "[LOG 20200502-14:29:54] epoch: 9918 train-loss: 0.010553488714827431\n",
      "[LOG 20200502-14:29:55] epoch: 9919 train-loss: 0.010553495441046026\n",
      "[LOG 20200502-14:29:55] epoch: 9920 train-loss: 0.010553502477705479\n",
      "[LOG 20200502-14:29:55] epoch: 9921 train-loss: 0.010553508790002929\n",
      "[LOG 20200502-14:29:55] epoch: 9922 train-loss: 0.01055351510230038\n",
      "[LOG 20200502-14:29:56] epoch: 9923 train-loss: 0.010553521621558402\n",
      "[LOG 20200502-14:29:56] epoch: 9924 train-loss: 0.010553528244296709\n",
      "[LOG 20200502-14:29:56] epoch: 9925 train-loss: 0.010553535487916734\n",
      "[LOG 20200502-14:29:56] epoch: 9926 train-loss: 0.010553542110655043\n",
      "[LOG 20200502-14:29:56] epoch: 9927 train-loss: 0.010553548526432779\n",
      "[LOG 20200502-14:29:57] epoch: 9928 train-loss: 0.010553555252651373\n",
      "[LOG 20200502-14:29:57] epoch: 9929 train-loss: 0.010553562082350254\n",
      "[LOG 20200502-14:29:57] epoch: 9930 train-loss: 0.010553568601608276\n",
      "[LOG 20200502-14:29:57] epoch: 9931 train-loss: 0.010553575431307157\n",
      "[LOG 20200502-14:29:58] epoch: 9932 train-loss: 0.010553582054045465\n",
      "[LOG 20200502-14:29:58] epoch: 9933 train-loss: 0.010553588676783774\n",
      "[LOG 20200502-14:29:58] epoch: 9934 train-loss: 0.010553595506482653\n",
      "[LOG 20200502-14:29:58] epoch: 9935 train-loss: 0.010553601818780104\n",
      "[LOG 20200502-14:29:58] epoch: 9936 train-loss: 0.010553608648478985\n",
      "[LOG 20200502-14:29:59] epoch: 9937 train-loss: 0.010553615374697579\n",
      "[LOG 20200502-14:29:59] epoch: 9938 train-loss: 0.010553621997435888\n",
      "[LOG 20200502-14:29:59] epoch: 9939 train-loss: 0.010553628102772765\n",
      "[LOG 20200502-14:29:59] epoch: 9940 train-loss: 0.010553635035951933\n",
      "[LOG 20200502-14:30:00] epoch: 9941 train-loss: 0.010553641865650812\n",
      "[LOG 20200502-14:30:00] epoch: 9942 train-loss: 0.010553648695349693\n",
      "[LOG 20200502-14:30:00] epoch: 9943 train-loss: 0.010553655111127429\n",
      "[LOG 20200502-14:30:00] epoch: 9944 train-loss: 0.010553661733865738\n",
      "[LOG 20200502-14:30:00] epoch: 9945 train-loss: 0.010553668460084332\n",
      "[LOG 20200502-14:30:01] epoch: 9946 train-loss: 0.010553675289783213\n",
      "[LOG 20200502-14:30:01] epoch: 9947 train-loss: 0.010553681705560949\n",
      "[LOG 20200502-14:30:01] epoch: 9948 train-loss: 0.010553688845700689\n",
      "[LOG 20200502-14:30:01] epoch: 9949 train-loss: 0.010553695571919283\n",
      "[LOG 20200502-14:30:02] epoch: 9950 train-loss: 0.010553702091177305\n",
      "[LOG 20200502-14:30:02] epoch: 9951 train-loss: 0.01055370933479733\n",
      "[LOG 20200502-14:30:02] epoch: 9952 train-loss: 0.010553715440134207\n",
      "[LOG 20200502-14:30:02] epoch: 9953 train-loss: 0.010553722683754232\n",
      "[LOG 20200502-14:30:02] epoch: 9954 train-loss: 0.010553729306492541\n",
      "[LOG 20200502-14:30:03] epoch: 9955 train-loss: 0.010553736032711135\n",
      "[LOG 20200502-14:30:03] epoch: 9956 train-loss: 0.01055374327633116\n",
      "[LOG 20200502-14:30:03] epoch: 9957 train-loss: 0.010553749485148324\n",
      "[LOG 20200502-14:30:03] epoch: 9958 train-loss: 0.010553756521807777\n",
      "[LOG 20200502-14:30:04] epoch: 9959 train-loss: 0.010553763661947515\n",
      "[LOG 20200502-14:30:04] epoch: 9960 train-loss: 0.010553770284685824\n",
      "[LOG 20200502-14:30:04] epoch: 9961 train-loss: 0.010553777424825562\n",
      "[LOG 20200502-14:30:04] epoch: 9962 train-loss: 0.010553784047563871\n",
      "[LOG 20200502-14:30:04] epoch: 9963 train-loss: 0.010553790773782466\n",
      "[LOG 20200502-14:30:05] epoch: 9964 train-loss: 0.010553797706961632\n",
      "[LOG 20200502-14:30:05] epoch: 9965 train-loss: 0.010553804329699941\n",
      "[LOG 20200502-14:30:05] epoch: 9966 train-loss: 0.010553810952438248\n",
      "[LOG 20200502-14:30:05] epoch: 9967 train-loss: 0.010553818920420276\n",
      "[LOG 20200502-14:30:06] epoch: 9968 train-loss: 0.01055382564663887\n",
      "[LOG 20200502-14:30:06] epoch: 9969 train-loss: 0.010553832683298323\n",
      "[LOG 20200502-14:30:06] epoch: 9970 train-loss: 0.010553838995595774\n",
      "[LOG 20200502-14:30:06] epoch: 9971 train-loss: 0.010553846135735512\n",
      "[LOG 20200502-14:30:06] epoch: 9972 train-loss: 0.010553852758473821\n",
      "[LOG 20200502-14:30:07] epoch: 9973 train-loss: 0.010553860002093844\n",
      "[LOG 20200502-14:30:07] epoch: 9974 train-loss: 0.010553866831792725\n",
      "[LOG 20200502-14:30:07] epoch: 9975 train-loss: 0.010553873764971891\n",
      "[LOG 20200502-14:30:07] epoch: 9976 train-loss: 0.010553880594670773\n",
      "[LOG 20200502-14:30:08] epoch: 9977 train-loss: 0.010553887631330226\n",
      "[LOG 20200502-14:30:08] epoch: 9978 train-loss: 0.010553894150588248\n",
      "[LOG 20200502-14:30:08] epoch: 9979 train-loss: 0.010553901808129417\n",
      "[LOG 20200502-14:30:08] epoch: 9980 train-loss: 0.01055390884478887\n",
      "[LOG 20200502-14:30:09] epoch: 9981 train-loss: 0.010553915260566605\n",
      "[LOG 20200502-14:30:09] epoch: 9982 train-loss: 0.010553922711147202\n",
      "[LOG 20200502-14:30:09] epoch: 9983 train-loss: 0.010553929437365796\n",
      "[LOG 20200502-14:30:09] epoch: 9984 train-loss: 0.010553936784466108\n",
      "[LOG 20200502-14:30:09] epoch: 9985 train-loss: 0.010553942889802985\n",
      "[LOG 20200502-14:30:10] epoch: 9986 train-loss: 0.010553949719501866\n",
      "[LOG 20200502-14:30:10] epoch: 9987 train-loss: 0.01055395727356275\n",
      "[LOG 20200502-14:30:10] epoch: 9988 train-loss: 0.010553964206741916\n",
      "[LOG 20200502-14:30:10] epoch: 9989 train-loss: 0.010553971036440797\n",
      "[LOG 20200502-14:30:10] epoch: 9990 train-loss: 0.010553977452218533\n",
      "[LOG 20200502-14:30:11] epoch: 9991 train-loss: 0.01055398490279913\n",
      "[LOG 20200502-14:30:11] epoch: 9992 train-loss: 0.010553991422057152\n",
      "[LOG 20200502-14:30:11] epoch: 9993 train-loss: 0.010553998458716605\n",
      "[LOG 20200502-14:30:11] epoch: 9994 train-loss: 0.010554005391895771\n",
      "[LOG 20200502-14:30:12] epoch: 9995 train-loss: 0.010554012325074937\n",
      "[LOG 20200502-14:30:12] epoch: 9996 train-loss: 0.010554019568694962\n",
      "[LOG 20200502-14:30:12] epoch: 9997 train-loss: 0.010554026191433271\n",
      "[LOG 20200502-14:30:12] epoch: 9998 train-loss: 0.01055403333157301\n",
      "[LOG 20200502-14:30:12] epoch: 9999 train-loss: 0.010554039747350745\n",
      "[LOG 20200502-14:30:13] epoch: 10000 train-loss: 0.010554046370089054\n",
      "[LOG 20200502-14:30:13] epoch: 10001 train-loss: 0.010554053613709079\n",
      "[LOG 20200502-14:30:13] epoch: 10002 train-loss: 0.010554060857329104\n",
      "[LOG 20200502-14:30:13] epoch: 10003 train-loss: 0.010554067376587126\n",
      "[LOG 20200502-14:30:14] epoch: 10004 train-loss: 0.01055407441324658\n",
      "[LOG 20200502-14:30:14] epoch: 10005 train-loss: 0.010554080932504602\n",
      "[LOG 20200502-14:30:14] epoch: 10006 train-loss: 0.010554087762203481\n",
      "[LOG 20200502-14:30:14] epoch: 10007 train-loss: 0.010554094695382647\n",
      "[LOG 20200502-14:30:14] epoch: 10008 train-loss: 0.010554101628561815\n",
      "[LOG 20200502-14:30:15] epoch: 10009 train-loss: 0.010554108458260695\n",
      "[LOG 20200502-14:30:15] epoch: 10010 train-loss: 0.01055411466707786\n",
      "[LOG 20200502-14:30:15] epoch: 10011 train-loss: 0.01055412201417817\n",
      "[LOG 20200502-14:30:15] epoch: 10012 train-loss: 0.010554129257798195\n",
      "[LOG 20200502-14:30:16] epoch: 10013 train-loss: 0.01055413567357593\n",
      "[LOG 20200502-14:30:16] epoch: 10014 train-loss: 0.010554142399794526\n",
      "[LOG 20200502-14:30:16] epoch: 10015 train-loss: 0.010554149539934264\n",
      "[LOG 20200502-14:30:16] epoch: 10016 train-loss: 0.010554155852231715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:30:16] epoch: 10017 train-loss: 0.01055416226800945\n",
      "[LOG 20200502-14:30:17] epoch: 10018 train-loss: 0.010554168994228045\n",
      "[LOG 20200502-14:30:17] epoch: 10019 train-loss: 0.010554175823926926\n",
      "[LOG 20200502-14:30:17] epoch: 10020 train-loss: 0.010554182757106092\n",
      "[LOG 20200502-14:30:17] epoch: 10021 train-loss: 0.010554189483324686\n",
      "[LOG 20200502-14:30:18] epoch: 10022 train-loss: 0.01055419651998414\n",
      "[LOG 20200502-14:30:18] epoch: 10023 train-loss: 0.010554202418360446\n",
      "[LOG 20200502-14:30:18] epoch: 10024 train-loss: 0.010554209248059325\n",
      "[LOG 20200502-14:30:18] epoch: 10025 train-loss: 0.010554215974277921\n",
      "[LOG 20200502-14:30:18] epoch: 10026 train-loss: 0.010554222907457087\n",
      "[LOG 20200502-14:30:19] epoch: 10027 train-loss: 0.010554229219754538\n",
      "[LOG 20200502-14:30:19] epoch: 10028 train-loss: 0.010554236256413989\n",
      "[LOG 20200502-14:30:19] epoch: 10029 train-loss: 0.010554242879152298\n",
      "[LOG 20200502-14:30:19] epoch: 10030 train-loss: 0.010554249191449748\n",
      "[LOG 20200502-14:30:19] epoch: 10031 train-loss: 0.010554255607227484\n",
      "[LOG 20200502-14:30:20] epoch: 10032 train-loss: 0.010554262643886937\n",
      "[LOG 20200502-14:30:20] epoch: 10033 train-loss: 0.010554268542263243\n",
      "[LOG 20200502-14:30:20] epoch: 10034 train-loss: 0.010554275785883268\n",
      "[LOG 20200502-14:30:20] epoch: 10035 train-loss: 0.010554281684259573\n",
      "[LOG 20200502-14:30:21] epoch: 10036 train-loss: 0.010554288513958454\n",
      "[LOG 20200502-14:30:21] epoch: 10037 train-loss: 0.01055429544713762\n",
      "[LOG 20200502-14:30:21] epoch: 10038 train-loss: 0.01055430175943507\n",
      "[LOG 20200502-14:30:21] epoch: 10039 train-loss: 0.01055430838217338\n",
      "[LOG 20200502-14:30:21] epoch: 10040 train-loss: 0.010554314797951115\n",
      "[LOG 20200502-14:30:22] epoch: 10041 train-loss: 0.01055432152416971\n",
      "[LOG 20200502-14:30:22] epoch: 10042 train-loss: 0.01055432783646716\n",
      "[LOG 20200502-14:30:22] epoch: 10043 train-loss: 0.010554334355725182\n",
      "[LOG 20200502-14:30:22] epoch: 10044 train-loss: 0.010554340978463491\n",
      "[LOG 20200502-14:30:22] epoch: 10045 train-loss: 0.010554347911642658\n",
      "[LOG 20200502-14:30:23] epoch: 10046 train-loss: 0.010554353706538677\n",
      "[LOG 20200502-14:30:23] epoch: 10047 train-loss: 0.010554360536237558\n",
      "[LOG 20200502-14:30:23] epoch: 10048 train-loss: 0.01055436705549558\n",
      "[LOG 20200502-14:30:23] epoch: 10049 train-loss: 0.010554373471273316\n",
      "[LOG 20200502-14:30:24] epoch: 10050 train-loss: 0.010554379783570766\n",
      "[LOG 20200502-14:30:24] epoch: 10051 train-loss: 0.010554386923710505\n",
      "[LOG 20200502-14:30:24] epoch: 10052 train-loss: 0.010554393339488242\n",
      "[LOG 20200502-14:30:24] epoch: 10053 train-loss: 0.010554399755265977\n",
      "[LOG 20200502-14:30:25] epoch: 10054 train-loss: 0.010554406274524\n",
      "[LOG 20200502-14:30:25] epoch: 10055 train-loss: 0.010554413000742594\n",
      "[LOG 20200502-14:30:25] epoch: 10056 train-loss: 0.010554419416520331\n",
      "[LOG 20200502-14:30:25] epoch: 10057 train-loss: 0.010554425935778353\n",
      "[LOG 20200502-14:30:26] epoch: 10058 train-loss: 0.010554432455036376\n",
      "[LOG 20200502-14:30:26] epoch: 10059 train-loss: 0.010554438870814111\n",
      "[LOG 20200502-14:30:26] epoch: 10060 train-loss: 0.010554445803993277\n",
      "[LOG 20200502-14:30:26] epoch: 10061 train-loss: 0.010554452219771015\n",
      "[LOG 20200502-14:30:26] epoch: 10062 train-loss: 0.01055445863554875\n",
      "[LOG 20200502-14:30:27] epoch: 10063 train-loss: 0.010554465258287059\n",
      "[LOG 20200502-14:30:27] epoch: 10064 train-loss: 0.010554471984505653\n",
      "[LOG 20200502-14:30:27] epoch: 10065 train-loss: 0.010554478400283389\n",
      "[LOG 20200502-14:30:27] epoch: 10066 train-loss: 0.010554484919541411\n",
      "[LOG 20200502-14:30:28] epoch: 10067 train-loss: 0.010554491749240292\n",
      "[LOG 20200502-14:30:28] epoch: 10068 train-loss: 0.010554498268498315\n",
      "[LOG 20200502-14:30:28] epoch: 10069 train-loss: 0.010554504787756337\n",
      "[LOG 20200502-14:30:28] epoch: 10070 train-loss: 0.010554511513974931\n",
      "[LOG 20200502-14:30:28] epoch: 10071 train-loss: 0.010554517929752668\n",
      "[LOG 20200502-14:30:29] epoch: 10072 train-loss: 0.01055452444901069\n",
      "[LOG 20200502-14:30:29] epoch: 10073 train-loss: 0.010554531071748998\n",
      "[LOG 20200502-14:30:29] epoch: 10074 train-loss: 0.010554537694487307\n",
      "[LOG 20200502-14:30:29] epoch: 10075 train-loss: 0.010554544524186187\n",
      "[LOG 20200502-14:30:30] epoch: 10076 train-loss: 0.010554550733003352\n",
      "[LOG 20200502-14:30:30] epoch: 10077 train-loss: 0.010554557666182518\n",
      "[LOG 20200502-14:30:30] epoch: 10078 train-loss: 0.010554564288920827\n",
      "[LOG 20200502-14:30:30] epoch: 10079 train-loss: 0.01055457080817885\n",
      "[LOG 20200502-14:30:31] epoch: 10080 train-loss: 0.010554577430917157\n",
      "[LOG 20200502-14:30:31] epoch: 10081 train-loss: 0.010554584053655466\n",
      "[LOG 20200502-14:30:31] epoch: 10082 train-loss: 0.01055459077987406\n",
      "[LOG 20200502-14:30:31] epoch: 10083 train-loss: 0.010554597195651796\n",
      "[LOG 20200502-14:30:31] epoch: 10084 train-loss: 0.010554604025350677\n",
      "[LOG 20200502-14:30:32] epoch: 10085 train-loss: 0.010554610441128412\n",
      "[LOG 20200502-14:30:32] epoch: 10086 train-loss: 0.010554617167347007\n",
      "[LOG 20200502-14:30:32] epoch: 10087 train-loss: 0.010554624100526174\n",
      "[LOG 20200502-14:30:32] epoch: 10088 train-loss: 0.010554630412823625\n",
      "[LOG 20200502-14:30:33] epoch: 10089 train-loss: 0.010554636518160502\n",
      "[LOG 20200502-14:30:33] epoch: 10090 train-loss: 0.010554643865260813\n",
      "[LOG 20200502-14:30:33] epoch: 10091 train-loss: 0.010554650074077977\n",
      "[LOG 20200502-14:30:33] epoch: 10092 train-loss: 0.010554656800296571\n",
      "[LOG 20200502-14:30:33] epoch: 10093 train-loss: 0.010554663940436311\n",
      "[LOG 20200502-14:30:34] epoch: 10094 train-loss: 0.010554670149253475\n",
      "[LOG 20200502-14:30:34] epoch: 10095 train-loss: 0.010554676461550925\n",
      "[LOG 20200502-14:30:34] epoch: 10096 train-loss: 0.010554683291249804\n",
      "[LOG 20200502-14:30:34] epoch: 10097 train-loss: 0.0105546900174684\n",
      "[LOG 20200502-14:30:35] epoch: 10098 train-loss: 0.010554696640206708\n",
      "[LOG 20200502-14:30:35] epoch: 10099 train-loss: 0.010554702849023871\n",
      "[LOG 20200502-14:30:35] epoch: 10100 train-loss: 0.010554709368281893\n",
      "[LOG 20200502-14:30:35] epoch: 10101 train-loss: 0.010554716301461061\n",
      "[LOG 20200502-14:30:35] epoch: 10102 train-loss: 0.010554722924199369\n",
      "[LOG 20200502-14:30:36] epoch: 10103 train-loss: 0.010554729650417963\n",
      "[LOG 20200502-14:30:36] epoch: 10104 train-loss: 0.010554735755754842\n",
      "[LOG 20200502-14:30:36] epoch: 10105 train-loss: 0.010554742275012864\n",
      "[LOG 20200502-14:30:36] epoch: 10106 train-loss: 0.010554749311672317\n",
      "[LOG 20200502-14:30:37] epoch: 10107 train-loss: 0.010554755313528908\n",
      "[LOG 20200502-14:30:37] epoch: 10108 train-loss: 0.010554762246708075\n",
      "[LOG 20200502-14:30:37] epoch: 10109 train-loss: 0.01055476845552524\n",
      "[LOG 20200502-14:30:37] epoch: 10110 train-loss: 0.01055477476782269\n",
      "[LOG 20200502-14:30:37] epoch: 10111 train-loss: 0.010554781080120139\n",
      "[LOG 20200502-14:30:38] epoch: 10112 train-loss: 0.010554787495897876\n",
      "[LOG 20200502-14:30:38] epoch: 10113 train-loss: 0.010554793808195326\n",
      "[LOG 20200502-14:30:38] epoch: 10114 train-loss: 0.010554800430933634\n",
      "[LOG 20200502-14:30:38] epoch: 10115 train-loss: 0.010554806950191656\n",
      "[LOG 20200502-14:30:38] epoch: 10116 train-loss: 0.010554813262489107\n",
      "[LOG 20200502-14:30:39] epoch: 10117 train-loss: 0.010554819678266844\n",
      "[LOG 20200502-14:30:39] epoch: 10118 train-loss: 0.01055482609404458\n",
      "[LOG 20200502-14:30:39] epoch: 10119 train-loss: 0.010554832302861743\n",
      "[LOG 20200502-14:30:39] epoch: 10120 train-loss: 0.01055483871863948\n",
      "[LOG 20200502-14:30:40] epoch: 10121 train-loss: 0.010554844927456643\n",
      "[LOG 20200502-14:30:40] epoch: 10122 train-loss: 0.010554850722352663\n",
      "[LOG 20200502-14:30:40] epoch: 10123 train-loss: 0.010554857034650113\n",
      "[LOG 20200502-14:30:40] epoch: 10124 train-loss: 0.010554863346947564\n",
      "[LOG 20200502-14:30:40] epoch: 10125 train-loss: 0.010554869866205586\n",
      "[LOG 20200502-14:30:41] epoch: 10126 train-loss: 0.010554875971542465\n",
      "[LOG 20200502-14:30:41] epoch: 10127 train-loss: 0.0105548823873202\n",
      "[LOG 20200502-14:30:41] epoch: 10128 train-loss: 0.010554888285696507\n",
      "[LOG 20200502-14:30:41] epoch: 10129 train-loss: 0.010554894908434816\n",
      "[LOG 20200502-14:30:42] epoch: 10130 train-loss: 0.010554901013771692\n",
      "[LOG 20200502-14:30:42] epoch: 10131 train-loss: 0.010554907222588858\n",
      "[LOG 20200502-14:30:42] epoch: 10132 train-loss: 0.010554913327925734\n",
      "[LOG 20200502-14:30:42] epoch: 10133 train-loss: 0.010554919019341469\n",
      "[LOG 20200502-14:30:42] epoch: 10134 train-loss: 0.01055492533163892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:30:43] epoch: 10135 train-loss: 0.010554931540456083\n",
      "[LOG 20200502-14:30:43] epoch: 10136 train-loss: 0.01055493743883239\n",
      "[LOG 20200502-14:30:43] epoch: 10137 train-loss: 0.01055494375112984\n",
      "[LOG 20200502-14:30:43] epoch: 10138 train-loss: 0.010554949959947003\n",
      "[LOG 20200502-14:30:43] epoch: 10139 train-loss: 0.010554955754843023\n",
      "[LOG 20200502-14:30:44] epoch: 10140 train-loss: 0.010554961860179901\n",
      "[LOG 20200502-14:30:44] epoch: 10141 train-loss: 0.010554967448115349\n",
      "[LOG 20200502-14:30:44] epoch: 10142 train-loss: 0.010554973967373371\n",
      "[LOG 20200502-14:30:44] epoch: 10143 train-loss: 0.010554979348348247\n",
      "[LOG 20200502-14:30:45] epoch: 10144 train-loss: 0.01055498555716541\n",
      "[LOG 20200502-14:30:45] epoch: 10145 train-loss: 0.010554991765982576\n",
      "[LOG 20200502-14:30:45] epoch: 10146 train-loss: 0.01055499797479974\n",
      "[LOG 20200502-14:30:45] epoch: 10147 train-loss: 0.010555003666215472\n",
      "[LOG 20200502-14:30:46] epoch: 10148 train-loss: 0.01055501008199321\n",
      "[LOG 20200502-14:30:46] epoch: 10149 train-loss: 0.010555015669928657\n",
      "[LOG 20200502-14:30:46] epoch: 10150 train-loss: 0.010555021568304963\n",
      "[LOG 20200502-14:30:46] epoch: 10151 train-loss: 0.010555028087562986\n",
      "[LOG 20200502-14:30:46] epoch: 10152 train-loss: 0.010555033572018147\n",
      "[LOG 20200502-14:30:47] epoch: 10153 train-loss: 0.010555039573874738\n",
      "[LOG 20200502-14:30:47] epoch: 10154 train-loss: 0.010555045161810186\n",
      "[LOG 20200502-14:30:47] epoch: 10155 train-loss: 0.010555051267147064\n",
      "[LOG 20200502-14:30:47] epoch: 10156 train-loss: 0.01055505716552337\n",
      "[LOG 20200502-14:30:48] epoch: 10157 train-loss: 0.010555062753458818\n",
      "[LOG 20200502-14:30:48] epoch: 10158 train-loss: 0.010555069065756269\n",
      "[LOG 20200502-14:30:48] epoch: 10159 train-loss: 0.010555074964132573\n",
      "[LOG 20200502-14:30:48] epoch: 10160 train-loss: 0.010555081172949739\n",
      "[LOG 20200502-14:30:48] epoch: 10161 train-loss: 0.010555086450444328\n",
      "[LOG 20200502-14:30:49] epoch: 10162 train-loss: 0.010555093280143209\n",
      "[LOG 20200502-14:30:49] epoch: 10163 train-loss: 0.010555098868078656\n",
      "[LOG 20200502-14:30:49] epoch: 10164 train-loss: 0.01055510455949439\n",
      "[LOG 20200502-14:30:49] epoch: 10165 train-loss: 0.010555110664831268\n",
      "[LOG 20200502-14:30:49] epoch: 10166 train-loss: 0.010555116356247\n",
      "[LOG 20200502-14:30:50] epoch: 10167 train-loss: 0.010555122461583879\n",
      "[LOG 20200502-14:30:50] epoch: 10168 train-loss: 0.010555128670401044\n",
      "[LOG 20200502-14:30:50] epoch: 10169 train-loss: 0.010555134465297064\n",
      "[LOG 20200502-14:30:50] epoch: 10170 train-loss: 0.010555140260193083\n",
      "[LOG 20200502-14:30:51] epoch: 10171 train-loss: 0.010555146262049675\n",
      "[LOG 20200502-14:30:51] epoch: 10172 train-loss: 0.010555152263906267\n",
      "[LOG 20200502-14:30:51] epoch: 10173 train-loss: 0.010555157851841714\n",
      "[LOG 20200502-14:30:51] epoch: 10174 train-loss: 0.010555164164139165\n",
      "[LOG 20200502-14:30:51] epoch: 10175 train-loss: 0.010555170476436615\n",
      "[LOG 20200502-14:30:52] epoch: 10176 train-loss: 0.010555175960891776\n",
      "[LOG 20200502-14:30:52] epoch: 10177 train-loss: 0.010555181548827224\n",
      "[LOG 20200502-14:30:52] epoch: 10178 train-loss: 0.01055518796460496\n",
      "[LOG 20200502-14:30:52] epoch: 10179 train-loss: 0.010555193966461552\n",
      "[LOG 20200502-14:30:53] epoch: 10180 train-loss: 0.01055520007179843\n",
      "[LOG 20200502-14:30:53] epoch: 10181 train-loss: 0.010555206073655022\n",
      "[LOG 20200502-14:30:53] epoch: 10182 train-loss: 0.0105552121789919\n",
      "[LOG 20200502-14:30:53] epoch: 10183 train-loss: 0.010555217559966776\n",
      "[LOG 20200502-14:30:53] epoch: 10184 train-loss: 0.010555223665303655\n",
      "[LOG 20200502-14:30:54] epoch: 10185 train-loss: 0.010555229874120818\n",
      "[LOG 20200502-14:30:54] epoch: 10186 train-loss: 0.010555236082937982\n",
      "[LOG 20200502-14:30:54] epoch: 10187 train-loss: 0.01055524218827486\n",
      "[LOG 20200502-14:30:54] epoch: 10188 train-loss: 0.010555248293611739\n",
      "[LOG 20200502-14:30:55] epoch: 10189 train-loss: 0.010555253985027472\n",
      "[LOG 20200502-14:30:55] epoch: 10190 train-loss: 0.01055526009036435\n",
      "[LOG 20200502-14:30:55] epoch: 10191 train-loss: 0.0105552664026618\n",
      "[LOG 20200502-14:30:55] epoch: 10192 train-loss: 0.010555272094077535\n",
      "[LOG 20200502-14:30:55] epoch: 10193 train-loss: 0.010555278095934126\n",
      "[LOG 20200502-14:30:56] epoch: 10194 train-loss: 0.010555283683869574\n",
      "[LOG 20200502-14:30:56] epoch: 10195 train-loss: 0.01055529009964731\n",
      "[LOG 20200502-14:30:56] epoch: 10196 train-loss: 0.010555295998023616\n",
      "[LOG 20200502-14:30:56] epoch: 10197 train-loss: 0.010555301999880208\n",
      "[LOG 20200502-14:30:56] epoch: 10198 train-loss: 0.010555307898256514\n",
      "[LOG 20200502-14:30:57] epoch: 10199 train-loss: 0.010555313796632819\n",
      "[LOG 20200502-14:30:57] epoch: 10200 train-loss: 0.01055531907412741\n",
      "[LOG 20200502-14:30:57] epoch: 10201 train-loss: 0.010555325800346004\n",
      "[LOG 20200502-14:30:57] epoch: 10202 train-loss: 0.010555331905682882\n",
      "[LOG 20200502-14:30:58] epoch: 10203 train-loss: 0.010555337700578902\n",
      "[LOG 20200502-14:30:58] epoch: 10204 train-loss: 0.010555343495474922\n",
      "[LOG 20200502-14:30:58] epoch: 10205 train-loss: 0.010555349704292085\n",
      "[LOG 20200502-14:30:58] epoch: 10206 train-loss: 0.01055535539570782\n",
      "[LOG 20200502-14:30:59] epoch: 10207 train-loss: 0.010555361087123552\n",
      "[LOG 20200502-14:30:59] epoch: 10208 train-loss: 0.01055536719246043\n",
      "[LOG 20200502-14:30:59] epoch: 10209 train-loss: 0.010555373194317022\n",
      "[LOG 20200502-14:30:59] epoch: 10210 train-loss: 0.010555378989213042\n",
      "[LOG 20200502-14:30:59] epoch: 10211 train-loss: 0.010555384266707633\n",
      "[LOG 20200502-14:31:00] epoch: 10212 train-loss: 0.010555390372044511\n",
      "[LOG 20200502-14:31:00] epoch: 10213 train-loss: 0.010555396373901103\n",
      "[LOG 20200502-14:31:00] epoch: 10214 train-loss: 0.01055540196183655\n",
      "[LOG 20200502-14:31:00] epoch: 10215 train-loss: 0.010555407549771998\n",
      "[LOG 20200502-14:31:01] epoch: 10216 train-loss: 0.01055541355162859\n",
      "[LOG 20200502-14:31:01] epoch: 10217 train-loss: 0.010555419243044324\n",
      "[LOG 20200502-14:31:01] epoch: 10218 train-loss: 0.010555424830979772\n",
      "[LOG 20200502-14:31:01] epoch: 10219 train-loss: 0.01055543041891522\n",
      "[LOG 20200502-14:31:01] epoch: 10220 train-loss: 0.010555436006850667\n",
      "[LOG 20200502-14:31:02] epoch: 10221 train-loss: 0.010555441594786115\n",
      "[LOG 20200502-14:31:02] epoch: 10222 train-loss: 0.010555447182721563\n",
      "[LOG 20200502-14:31:02] epoch: 10223 train-loss: 0.010555452874137295\n",
      "[LOG 20200502-14:31:02] epoch: 10224 train-loss: 0.010555458875993887\n",
      "[LOG 20200502-14:31:02] epoch: 10225 train-loss: 0.01055546436044905\n",
      "[LOG 20200502-14:31:03] epoch: 10226 train-loss: 0.010555469637943639\n",
      "[LOG 20200502-14:31:03] epoch: 10227 train-loss: 0.010555475122398801\n",
      "[LOG 20200502-14:31:03] epoch: 10228 train-loss: 0.010555480710334249\n",
      "[LOG 20200502-14:31:03] epoch: 10229 train-loss: 0.010555486091309123\n",
      "[LOG 20200502-14:31:04] epoch: 10230 train-loss: 0.010555491575764285\n",
      "[LOG 20200502-14:31:04] epoch: 10231 train-loss: 0.010555497370660305\n",
      "[LOG 20200502-14:31:04] epoch: 10232 train-loss: 0.010555502337714037\n",
      "[LOG 20200502-14:31:04] epoch: 10233 train-loss: 0.010555508339570628\n",
      "[LOG 20200502-14:31:04] epoch: 10234 train-loss: 0.010555513720545504\n",
      "[LOG 20200502-14:31:05] epoch: 10235 train-loss: 0.010555518894559808\n",
      "[LOG 20200502-14:31:05] epoch: 10236 train-loss: 0.010555524275534682\n",
      "[LOG 20200502-14:31:05] epoch: 10237 train-loss: 0.010555529139108129\n",
      "[LOG 20200502-14:31:05] epoch: 10238 train-loss: 0.01055553514096472\n",
      "[LOG 20200502-14:31:06] epoch: 10239 train-loss: 0.01055554010801845\n",
      "[LOG 20200502-14:31:06] epoch: 10240 train-loss: 0.010555545902914472\n",
      "[LOG 20200502-14:31:06] epoch: 10241 train-loss: 0.010555550869968202\n",
      "[LOG 20200502-14:31:06] epoch: 10242 train-loss: 0.010555555837021934\n",
      "[LOG 20200502-14:31:06] epoch: 10243 train-loss: 0.010555561942358812\n",
      "[LOG 20200502-14:31:07] epoch: 10244 train-loss: 0.01055556701289283\n",
      "[LOG 20200502-14:31:07] epoch: 10245 train-loss: 0.010555571979946561\n",
      "[LOG 20200502-14:31:07] epoch: 10246 train-loss: 0.010555576947000291\n",
      "[LOG 20200502-14:31:07] epoch: 10247 train-loss: 0.01055558274189631\n",
      "[LOG 20200502-14:31:07] epoch: 10248 train-loss: 0.010555587708950043\n",
      "[LOG 20200502-14:31:08] epoch: 10249 train-loss: 0.010555592572523488\n",
      "[LOG 20200502-14:31:08] epoch: 10250 train-loss: 0.010555598367419507\n",
      "[LOG 20200502-14:31:08] epoch: 10251 train-loss: 0.010555603127512667\n",
      "[LOG 20200502-14:31:08] epoch: 10252 train-loss: 0.010555608611967828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:31:09] epoch: 10253 train-loss: 0.010555613372060988\n",
      "[LOG 20200502-14:31:09] epoch: 10254 train-loss: 0.010555618442595005\n",
      "[LOG 20200502-14:31:09] epoch: 10255 train-loss: 0.010555624444451597\n",
      "[LOG 20200502-14:31:09] epoch: 10256 train-loss: 0.010555629204544757\n",
      "[LOG 20200502-14:31:09] epoch: 10257 train-loss: 0.010555634482039345\n",
      "[LOG 20200502-14:31:10] epoch: 10258 train-loss: 0.010555639138652219\n",
      "[LOG 20200502-14:31:10] epoch: 10259 train-loss: 0.010555644726587666\n",
      "[LOG 20200502-14:31:10] epoch: 10260 train-loss: 0.010555649797121683\n",
      "[LOG 20200502-14:31:10] epoch: 10261 train-loss: 0.010555655178096559\n",
      "[LOG 20200502-14:31:11] epoch: 10262 train-loss: 0.010555660248630576\n",
      "[LOG 20200502-14:31:11] epoch: 10263 train-loss: 0.010555665526125167\n",
      "[LOG 20200502-14:31:11] epoch: 10264 train-loss: 0.010555671010580327\n",
      "[LOG 20200502-14:31:11] epoch: 10265 train-loss: 0.010555675770673487\n",
      "[LOG 20200502-14:31:12] epoch: 10266 train-loss: 0.010555681255128648\n",
      "[LOG 20200502-14:31:12] epoch: 10267 train-loss: 0.010555686325662665\n",
      "[LOG 20200502-14:31:12] epoch: 10268 train-loss: 0.010555691189236112\n",
      "[LOG 20200502-14:31:12] epoch: 10269 train-loss: 0.010555696259770129\n",
      "[LOG 20200502-14:31:12] epoch: 10270 train-loss: 0.01055570226162672\n",
      "[LOG 20200502-14:31:13] epoch: 10271 train-loss: 0.010555707125200165\n",
      "[LOG 20200502-14:31:13] epoch: 10272 train-loss: 0.010555712402694754\n",
      "[LOG 20200502-14:31:13] epoch: 10273 train-loss: 0.010555717576709058\n",
      "[LOG 20200502-14:31:13] epoch: 10274 train-loss: 0.010555722957683934\n",
      "[LOG 20200502-14:31:14] epoch: 10275 train-loss: 0.010555727924737666\n",
      "[LOG 20200502-14:31:14] epoch: 10276 train-loss: 0.010555733512673113\n",
      "[LOG 20200502-14:31:14] epoch: 10277 train-loss: 0.010555739100608561\n",
      "[LOG 20200502-14:31:14] epoch: 10278 train-loss: 0.010555744067662291\n",
      "[LOG 20200502-14:31:15] epoch: 10279 train-loss: 0.010555749345156882\n",
      "[LOG 20200502-14:31:15] epoch: 10280 train-loss: 0.010555754415690899\n",
      "[LOG 20200502-14:31:15] epoch: 10281 train-loss: 0.010555760107106633\n",
      "[LOG 20200502-14:31:15] epoch: 10282 train-loss: 0.010555765074160364\n",
      "[LOG 20200502-14:31:16] epoch: 10283 train-loss: 0.010555770351654954\n",
      "[LOG 20200502-14:31:16] epoch: 10284 train-loss: 0.010555775629149543\n",
      "[LOG 20200502-14:31:16] epoch: 10285 train-loss: 0.010555781320565276\n",
      "[LOG 20200502-14:31:16] epoch: 10286 train-loss: 0.010555786184138723\n",
      "[LOG 20200502-14:31:16] epoch: 10287 train-loss: 0.010555791668593884\n",
      "[LOG 20200502-14:31:17] epoch: 10288 train-loss: 0.010555796946088472\n",
      "[LOG 20200502-14:31:17] epoch: 10289 train-loss: 0.010555801809661917\n",
      "[LOG 20200502-14:31:17] epoch: 10290 train-loss: 0.010555808018479083\n",
      "[LOG 20200502-14:31:17] epoch: 10291 train-loss: 0.010555812468131384\n",
      "[LOG 20200502-14:31:18] epoch: 10292 train-loss: 0.010555818263027403\n",
      "[LOG 20200502-14:31:18] epoch: 10293 train-loss: 0.010555823230081134\n",
      "[LOG 20200502-14:31:18] epoch: 10294 train-loss: 0.01055582861105601\n",
      "[LOG 20200502-14:31:18] epoch: 10295 train-loss: 0.010555834198991457\n",
      "[LOG 20200502-14:31:18] epoch: 10296 train-loss: 0.010555839166045189\n",
      "[LOG 20200502-14:31:19] epoch: 10297 train-loss: 0.01055584413309892\n",
      "[LOG 20200502-14:31:19] epoch: 10298 train-loss: 0.010555849824514654\n",
      "[LOG 20200502-14:31:19] epoch: 10299 train-loss: 0.010555854688088099\n",
      "[LOG 20200502-14:31:19] epoch: 10300 train-loss: 0.010555860172543261\n",
      "[LOG 20200502-14:31:20] epoch: 10301 train-loss: 0.010555865863958994\n",
      "[LOG 20200502-14:31:20] epoch: 10302 train-loss: 0.010555870313611295\n",
      "[LOG 20200502-14:31:20] epoch: 10303 train-loss: 0.010555876315467887\n",
      "[LOG 20200502-14:31:20] epoch: 10304 train-loss: 0.01055588097208076\n",
      "[LOG 20200502-14:31:20] epoch: 10305 train-loss: 0.010555886042614778\n",
      "[LOG 20200502-14:31:21] epoch: 10306 train-loss: 0.010555891837510798\n",
      "[LOG 20200502-14:31:21] epoch: 10307 train-loss: 0.010555896494123671\n",
      "[LOG 20200502-14:31:21] epoch: 10308 train-loss: 0.010555901875098547\n",
      "[LOG 20200502-14:31:21] epoch: 10309 train-loss: 0.010555906738671992\n",
      "[LOG 20200502-14:31:21] epoch: 10310 train-loss: 0.01055591201616658\n",
      "[LOG 20200502-14:31:22] epoch: 10311 train-loss: 0.010555917500621743\n",
      "[LOG 20200502-14:31:22] epoch: 10312 train-loss: 0.01055592205375433\n",
      "[LOG 20200502-14:31:22] epoch: 10313 train-loss: 0.010555927227768633\n",
      "[LOG 20200502-14:31:22] epoch: 10314 train-loss: 0.010555931884381507\n",
      "[LOG 20200502-14:31:23] epoch: 10315 train-loss: 0.01055593705839581\n",
      "[LOG 20200502-14:31:23] epoch: 10316 train-loss: 0.010555941921969255\n",
      "[LOG 20200502-14:31:23] epoch: 10317 train-loss: 0.010555946578582128\n",
      "[LOG 20200502-14:31:23] epoch: 10318 train-loss: 0.010555952373478148\n",
      "[LOG 20200502-14:31:23] epoch: 10319 train-loss: 0.01055595682313045\n",
      "[LOG 20200502-14:31:24] epoch: 10320 train-loss: 0.010555961376263035\n",
      "[LOG 20200502-14:31:24] epoch: 10321 train-loss: 0.010555966343316767\n",
      "[LOG 20200502-14:31:24] epoch: 10322 train-loss: 0.010555971103409926\n",
      "[LOG 20200502-14:31:24] epoch: 10323 train-loss: 0.01055597627742423\n",
      "[LOG 20200502-14:31:24] epoch: 10324 train-loss: 0.010555980934037102\n",
      "[LOG 20200502-14:31:25] epoch: 10325 train-loss: 0.010555985694130262\n",
      "[LOG 20200502-14:31:25] epoch: 10326 train-loss: 0.010555990661183992\n",
      "[LOG 20200502-14:31:25] epoch: 10327 train-loss: 0.010555995110836294\n",
      "[LOG 20200502-14:31:25] epoch: 10328 train-loss: 0.010555999870929453\n",
      "[LOG 20200502-14:31:26] epoch: 10329 train-loss: 0.010556004631022612\n",
      "[LOG 20200502-14:31:26] epoch: 10330 train-loss: 0.01055600887371434\n",
      "[LOG 20200502-14:31:26] epoch: 10331 train-loss: 0.0105560136338075\n",
      "[LOG 20200502-14:31:26] epoch: 10332 train-loss: 0.010556018497380946\n",
      "[LOG 20200502-14:31:26] epoch: 10333 train-loss: 0.01055602284355296\n",
      "[LOG 20200502-14:31:27] epoch: 10334 train-loss: 0.010556027293205261\n",
      "[LOG 20200502-14:31:27] epoch: 10335 train-loss: 0.010556031949818134\n",
      "[LOG 20200502-14:31:27] epoch: 10336 train-loss: 0.01055603681339158\n",
      "[LOG 20200502-14:31:27] epoch: 10337 train-loss: 0.010556041366524167\n",
      "[LOG 20200502-14:31:28] epoch: 10338 train-loss: 0.010556045816176467\n",
      "[LOG 20200502-14:31:28] epoch: 10339 train-loss: 0.010556050162348483\n",
      "[LOG 20200502-14:31:28] epoch: 10340 train-loss: 0.010556054612000784\n",
      "[LOG 20200502-14:31:28] epoch: 10341 train-loss: 0.010556058854692511\n",
      "[LOG 20200502-14:31:28] epoch: 10342 train-loss: 0.010556063304344812\n",
      "[LOG 20200502-14:31:29] epoch: 10343 train-loss: 0.01055606734007597\n",
      "[LOG 20200502-14:31:29] epoch: 10344 train-loss: 0.010556071996688843\n",
      "[LOG 20200502-14:31:29] epoch: 10345 train-loss: 0.010556076446341144\n",
      "[LOG 20200502-14:31:29] epoch: 10346 train-loss: 0.010556081102954017\n",
      "[LOG 20200502-14:31:30] epoch: 10347 train-loss: 0.010556085138685174\n",
      "[LOG 20200502-14:31:30] epoch: 10348 train-loss: 0.01055609000225862\n",
      "[LOG 20200502-14:31:30] epoch: 10349 train-loss: 0.010556094141470062\n",
      "[LOG 20200502-14:31:30] epoch: 10350 train-loss: 0.010556098694602648\n",
      "[LOG 20200502-14:31:30] epoch: 10351 train-loss: 0.010556102730333805\n",
      "[LOG 20200502-14:31:31] epoch: 10352 train-loss: 0.01055610707650582\n",
      "[LOG 20200502-14:31:31] epoch: 10353 train-loss: 0.01055611183659898\n",
      "[LOG 20200502-14:31:31] epoch: 10354 train-loss: 0.010556115872330137\n",
      "[LOG 20200502-14:31:31] epoch: 10355 train-loss: 0.010556120425462723\n",
      "[LOG 20200502-14:31:32] epoch: 10356 train-loss: 0.010556125082075596\n",
      "[LOG 20200502-14:31:32] epoch: 10357 train-loss: 0.010556129221287038\n",
      "[LOG 20200502-14:31:32] epoch: 10358 train-loss: 0.01055613418834077\n",
      "[LOG 20200502-14:31:32] epoch: 10359 train-loss: 0.010556138327552212\n",
      "[LOG 20200502-14:31:32] epoch: 10360 train-loss: 0.0105561428806848\n",
      "[LOG 20200502-14:31:33] epoch: 10361 train-loss: 0.010556147123376528\n",
      "[LOG 20200502-14:31:33] epoch: 10362 train-loss: 0.010556151159107685\n",
      "[LOG 20200502-14:31:33] epoch: 10363 train-loss: 0.010556155712240271\n",
      "[LOG 20200502-14:31:33] epoch: 10364 train-loss: 0.01055616078277429\n",
      "[LOG 20200502-14:31:34] epoch: 10365 train-loss: 0.010556164818505446\n",
      "[LOG 20200502-14:31:34] epoch: 10366 train-loss: 0.010556169785559177\n",
      "[LOG 20200502-14:31:34] epoch: 10367 train-loss: 0.010556174028250907\n",
      "[LOG 20200502-14:31:34] epoch: 10368 train-loss: 0.010556178374422921\n",
      "[LOG 20200502-14:31:34] epoch: 10369 train-loss: 0.010556183237996366\n",
      "[LOG 20200502-14:31:35] epoch: 10370 train-loss: 0.010556187377207808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:31:35] epoch: 10371 train-loss: 0.010556192137300968\n",
      "[LOG 20200502-14:31:35] epoch: 10372 train-loss: 0.01055619658695327\n",
      "[LOG 20200502-14:31:35] epoch: 10373 train-loss: 0.01055620103660557\n",
      "[LOG 20200502-14:31:36] epoch: 10374 train-loss: 0.010556205900179015\n",
      "[LOG 20200502-14:31:36] epoch: 10375 train-loss: 0.010556210556791889\n",
      "[LOG 20200502-14:31:36] epoch: 10376 train-loss: 0.01055621500644419\n",
      "[LOG 20200502-14:31:36] epoch: 10377 train-loss: 0.010556219042175345\n",
      "[LOG 20200502-14:31:36] epoch: 10378 train-loss: 0.010556224319669936\n",
      "[LOG 20200502-14:31:37] epoch: 10379 train-loss: 0.01055622866584195\n",
      "[LOG 20200502-14:31:37] epoch: 10380 train-loss: 0.010556233322454823\n",
      "[LOG 20200502-14:31:37] epoch: 10381 train-loss: 0.010556238082547983\n",
      "[LOG 20200502-14:31:37] epoch: 10382 train-loss: 0.010556242739160856\n",
      "[LOG 20200502-14:31:37] epoch: 10383 train-loss: 0.010556247188813157\n",
      "[LOG 20200502-14:31:38] epoch: 10384 train-loss: 0.01055625236282746\n",
      "[LOG 20200502-14:31:38] epoch: 10385 train-loss: 0.010556256398558617\n",
      "[LOG 20200502-14:31:38] epoch: 10386 train-loss: 0.010556261365612348\n",
      "[LOG 20200502-14:31:38] epoch: 10387 train-loss: 0.01055626581526465\n",
      "[LOG 20200502-14:31:39] epoch: 10388 train-loss: 0.010556270368397236\n",
      "[LOG 20200502-14:31:39] epoch: 10389 train-loss: 0.010556275025010109\n",
      "[LOG 20200502-14:31:39] epoch: 10390 train-loss: 0.010556279681622982\n",
      "[LOG 20200502-14:31:39] epoch: 10391 train-loss: 0.010556284545196427\n",
      "[LOG 20200502-14:31:39] epoch: 10392 train-loss: 0.010556288270486726\n",
      "[LOG 20200502-14:31:40] epoch: 10393 train-loss: 0.010556293651461601\n",
      "[LOG 20200502-14:31:40] epoch: 10394 train-loss: 0.010556297480232187\n",
      "[LOG 20200502-14:31:40] epoch: 10395 train-loss: 0.010556302033364773\n",
      "[LOG 20200502-14:31:40] epoch: 10396 train-loss: 0.010556306793457933\n",
      "[LOG 20200502-14:31:40] epoch: 10397 train-loss: 0.010556311243110232\n",
      "[LOG 20200502-14:31:41] epoch: 10398 train-loss: 0.010556315692762533\n",
      "[LOG 20200502-14:31:41] epoch: 10399 train-loss: 0.010556320556335978\n",
      "[LOG 20200502-14:31:41] epoch: 10400 train-loss: 0.01055632448858685\n",
      "[LOG 20200502-14:31:41] epoch: 10401 train-loss: 0.010556328731278578\n",
      "[LOG 20200502-14:31:42] epoch: 10402 train-loss: 0.010556333491371738\n",
      "[LOG 20200502-14:31:42] epoch: 10403 train-loss: 0.010556337527102895\n",
      "[LOG 20200502-14:31:42] epoch: 10404 train-loss: 0.01055634239067634\n",
      "[LOG 20200502-14:31:42] epoch: 10405 train-loss: 0.010556346529887782\n",
      "[LOG 20200502-14:31:42] epoch: 10406 train-loss: 0.010556350669099225\n",
      "[LOG 20200502-14:31:43] epoch: 10407 train-loss: 0.010556355118751526\n",
      "[LOG 20200502-14:31:43] epoch: 10408 train-loss: 0.010556359361443255\n",
      "[LOG 20200502-14:31:43] epoch: 10409 train-loss: 0.010556363190213839\n",
      "[LOG 20200502-14:31:43] epoch: 10410 train-loss: 0.01055636763986614\n",
      "[LOG 20200502-14:31:44] epoch: 10411 train-loss: 0.010556371675597297\n",
      "[LOG 20200502-14:31:44] epoch: 10412 train-loss: 0.010556376228729883\n",
      "[LOG 20200502-14:31:44] epoch: 10413 train-loss: 0.010556380160980754\n",
      "[LOG 20200502-14:31:44] epoch: 10414 train-loss: 0.010556383782790767\n",
      "[LOG 20200502-14:31:44] epoch: 10415 train-loss: 0.010556388025482496\n",
      "[LOG 20200502-14:31:45] epoch: 10416 train-loss: 0.010556392268174224\n",
      "[LOG 20200502-14:31:45] epoch: 10417 train-loss: 0.010556395993464522\n",
      "[LOG 20200502-14:31:45] epoch: 10418 train-loss: 0.01055640002919568\n",
      "[LOG 20200502-14:31:45] epoch: 10419 train-loss: 0.01055640396144655\n",
      "[LOG 20200502-14:31:46] epoch: 10420 train-loss: 0.010556407790217135\n",
      "[LOG 20200502-14:31:46] epoch: 10421 train-loss: 0.010556411929428577\n",
      "[LOG 20200502-14:31:46] epoch: 10422 train-loss: 0.010556415654718876\n",
      "[LOG 20200502-14:31:46] epoch: 10423 train-loss: 0.010556419380009174\n",
      "[LOG 20200502-14:31:46] epoch: 10424 train-loss: 0.01055642320877976\n",
      "[LOG 20200502-14:31:47] epoch: 10425 train-loss: 0.010556427037550343\n",
      "[LOG 20200502-14:31:47] epoch: 10426 train-loss: 0.010556430762840642\n",
      "[LOG 20200502-14:31:47] epoch: 10427 train-loss: 0.01055643397072951\n",
      "[LOG 20200502-14:31:47] epoch: 10428 train-loss: 0.010556438109940953\n",
      "[LOG 20200502-14:31:48] epoch: 10429 train-loss: 0.010556441731750965\n",
      "[LOG 20200502-14:31:48] epoch: 10430 train-loss: 0.01055644504312012\n",
      "[LOG 20200502-14:31:48] epoch: 10431 train-loss: 0.010556449078851275\n",
      "[LOG 20200502-14:31:48] epoch: 10432 train-loss: 0.010556452597181002\n",
      "[LOG 20200502-14:31:48] epoch: 10433 train-loss: 0.0105564563224713\n",
      "[LOG 20200502-14:31:49] epoch: 10434 train-loss: 0.010556460151241885\n",
      "[LOG 20200502-14:31:49] epoch: 10435 train-loss: 0.01055646346261104\n",
      "[LOG 20200502-14:31:49] epoch: 10436 train-loss: 0.010556467291381624\n",
      "[LOG 20200502-14:31:49] epoch: 10437 train-loss: 0.01055647080971135\n",
      "[LOG 20200502-14:31:50] epoch: 10438 train-loss: 0.010556474638481935\n",
      "[LOG 20200502-14:31:50] epoch: 10439 train-loss: 0.010556478156811662\n",
      "[LOG 20200502-14:31:50] epoch: 10440 train-loss: 0.010556481571661102\n",
      "[LOG 20200502-14:31:50] epoch: 10441 train-loss: 0.010556484986510541\n",
      "[LOG 20200502-14:31:51] epoch: 10442 train-loss: 0.010556488608320555\n",
      "[LOG 20200502-14:31:51] epoch: 10443 train-loss: 0.010556492023169994\n",
      "[LOG 20200502-14:31:51] epoch: 10444 train-loss: 0.010556495748460293\n",
      "[LOG 20200502-14:31:51] epoch: 10445 train-loss: 0.010556499577230878\n",
      "[LOG 20200502-14:31:52] epoch: 10446 train-loss: 0.010556503509481749\n",
      "[LOG 20200502-14:31:52] epoch: 10447 train-loss: 0.010556506510410044\n",
      "[LOG 20200502-14:31:52] epoch: 10448 train-loss: 0.010556510132220056\n",
      "[LOG 20200502-14:31:52] epoch: 10449 train-loss: 0.010556513857510354\n",
      "[LOG 20200502-14:31:53] epoch: 10450 train-loss: 0.010556517168879509\n",
      "[LOG 20200502-14:31:53] epoch: 10451 train-loss: 0.010556520894169807\n",
      "[LOG 20200502-14:31:53] epoch: 10452 train-loss: 0.010556524515979819\n",
      "[LOG 20200502-14:31:53] epoch: 10453 train-loss: 0.010556528137789832\n",
      "[LOG 20200502-14:31:53] epoch: 10454 train-loss: 0.010556532380481562\n",
      "[LOG 20200502-14:31:54] epoch: 10455 train-loss: 0.010556535381409857\n",
      "[LOG 20200502-14:31:54] epoch: 10456 train-loss: 0.010556539210180441\n",
      "[LOG 20200502-14:31:54] epoch: 10457 train-loss: 0.010556543038951026\n",
      "[LOG 20200502-14:31:54] epoch: 10458 train-loss: 0.01055654686772161\n",
      "[LOG 20200502-14:31:55] epoch: 10459 train-loss: 0.010556550489531623\n",
      "[LOG 20200502-14:31:55] epoch: 10460 train-loss: 0.010556554214821922\n",
      "[LOG 20200502-14:31:55] epoch: 10461 train-loss: 0.01055655845751365\n",
      "[LOG 20200502-14:31:55] epoch: 10462 train-loss: 0.010556561975843377\n",
      "[LOG 20200502-14:31:56] epoch: 10463 train-loss: 0.010556565908094248\n",
      "[LOG 20200502-14:31:56] epoch: 10464 train-loss: 0.0105565692194634\n",
      "[LOG 20200502-14:31:56] epoch: 10465 train-loss: 0.010556573669115702\n",
      "[LOG 20200502-14:31:56] epoch: 10466 train-loss: 0.010556577394406\n",
      "[LOG 20200502-14:31:57] epoch: 10467 train-loss: 0.010556581119696299\n",
      "[LOG 20200502-14:31:57] epoch: 10468 train-loss: 0.010556585258907743\n",
      "[LOG 20200502-14:31:57] epoch: 10469 train-loss: 0.010556589087678326\n",
      "[LOG 20200502-14:31:57] epoch: 10470 train-loss: 0.010556593123409484\n",
      "[LOG 20200502-14:31:58] epoch: 10471 train-loss: 0.010556597159140639\n",
      "[LOG 20200502-14:31:58] epoch: 10472 train-loss: 0.010556600677470366\n",
      "[LOG 20200502-14:31:58] epoch: 10473 train-loss: 0.010556604920162095\n",
      "[LOG 20200502-14:31:58] epoch: 10474 train-loss: 0.010556608955893252\n",
      "[LOG 20200502-14:31:59] epoch: 10475 train-loss: 0.010556613095104694\n",
      "[LOG 20200502-14:31:59] epoch: 10476 train-loss: 0.010556616509954134\n",
      "[LOG 20200502-14:31:59] epoch: 10477 train-loss: 0.010556620442205004\n",
      "[LOG 20200502-14:31:59] epoch: 10478 train-loss: 0.01055662478837702\n",
      "[LOG 20200502-14:31:59] epoch: 10479 train-loss: 0.01055662820322646\n",
      "[LOG 20200502-14:32:00] epoch: 10480 train-loss: 0.01055663244591819\n",
      "[LOG 20200502-14:32:00] epoch: 10481 train-loss: 0.01055663637816906\n",
      "[LOG 20200502-14:32:00] epoch: 10482 train-loss: 0.010556639689538214\n",
      "[LOG 20200502-14:32:00] epoch: 10483 train-loss: 0.010556644449631373\n",
      "[LOG 20200502-14:32:01] epoch: 10484 train-loss: 0.010556647554039955\n",
      "[LOG 20200502-14:32:01] epoch: 10485 train-loss: 0.010556651796731684\n",
      "[LOG 20200502-14:32:01] epoch: 10486 train-loss: 0.01055665531506141\n",
      "[LOG 20200502-14:32:01] epoch: 10487 train-loss: 0.010556659454272853\n",
      "[LOG 20200502-14:32:02] epoch: 10488 train-loss: 0.010556663076082865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:32:02] epoch: 10489 train-loss: 0.010556666801373163\n",
      "[LOG 20200502-14:32:02] epoch: 10490 train-loss: 0.010556670630143749\n",
      "[LOG 20200502-14:32:02] epoch: 10491 train-loss: 0.010556674355434047\n",
      "[LOG 20200502-14:32:03] epoch: 10492 train-loss: 0.010556678598125776\n",
      "[LOG 20200502-14:32:03] epoch: 10493 train-loss: 0.01055668087469207\n",
      "[LOG 20200502-14:32:03] epoch: 10494 train-loss: 0.010556685531304942\n",
      "[LOG 20200502-14:32:03] epoch: 10495 train-loss: 0.010556689153114954\n",
      "[LOG 20200502-14:32:04] epoch: 10496 train-loss: 0.01055669215404325\n",
      "[LOG 20200502-14:32:04] epoch: 10497 train-loss: 0.010556695879333548\n",
      "[LOG 20200502-14:32:04] epoch: 10498 train-loss: 0.010556699087222418\n",
      "[LOG 20200502-14:32:04] epoch: 10499 train-loss: 0.010556702812512716\n",
      "[LOG 20200502-14:32:05] epoch: 10500 train-loss: 0.010556706537803015\n",
      "[LOG 20200502-14:32:05] epoch: 10501 train-loss: 0.010556709952652454\n",
      "[LOG 20200502-14:32:05] epoch: 10502 train-loss: 0.010556713574462466\n",
      "[LOG 20200502-14:32:05] epoch: 10503 train-loss: 0.01055671688583162\n",
      "[LOG 20200502-14:32:06] epoch: 10504 train-loss: 0.010556719886759916\n",
      "[LOG 20200502-14:32:06] epoch: 10505 train-loss: 0.010556722991168499\n",
      "[LOG 20200502-14:32:06] epoch: 10506 train-loss: 0.010556726302537654\n",
      "[LOG 20200502-14:32:06] epoch: 10507 train-loss: 0.010556729613906808\n",
      "[LOG 20200502-14:32:07] epoch: 10508 train-loss: 0.010556732614835104\n",
      "[LOG 20200502-14:32:07] epoch: 10509 train-loss: 0.01055673613316483\n",
      "[LOG 20200502-14:32:07] epoch: 10510 train-loss: 0.010556739134093126\n",
      "[LOG 20200502-14:32:07] epoch: 10511 train-loss: 0.01055674192806085\n",
      "[LOG 20200502-14:32:08] epoch: 10512 train-loss: 0.010556745032469431\n",
      "[LOG 20200502-14:32:08] epoch: 10513 train-loss: 0.010556747929917442\n",
      "[LOG 20200502-14:32:08] epoch: 10514 train-loss: 0.01055675082736545\n",
      "[LOG 20200502-14:32:08] epoch: 10515 train-loss: 0.010556753724813461\n",
      "[LOG 20200502-14:32:09] epoch: 10516 train-loss: 0.010556756622261472\n",
      "[LOG 20200502-14:32:09] epoch: 10517 train-loss: 0.01055675983015034\n",
      "[LOG 20200502-14:32:09] epoch: 10518 train-loss: 0.010556762624118064\n",
      "[LOG 20200502-14:32:09] epoch: 10519 train-loss: 0.010556765107644929\n",
      "[LOG 20200502-14:32:10] epoch: 10520 train-loss: 0.010556768419014083\n",
      "[LOG 20200502-14:32:10] epoch: 10521 train-loss: 0.010556771626902951\n",
      "[LOG 20200502-14:32:10] epoch: 10522 train-loss: 0.01055677431739039\n",
      "[LOG 20200502-14:32:10] epoch: 10523 train-loss: 0.010556776283515824\n",
      "[LOG 20200502-14:32:11] epoch: 10524 train-loss: 0.010556779698365264\n",
      "[LOG 20200502-14:32:11] epoch: 10525 train-loss: 0.010556781767970987\n",
      "[LOG 20200502-14:32:11] epoch: 10526 train-loss: 0.010556785182820426\n",
      "[LOG 20200502-14:32:11] epoch: 10527 train-loss: 0.010556788080268435\n",
      "[LOG 20200502-14:32:12] epoch: 10528 train-loss: 0.010556790563795302\n",
      "[LOG 20200502-14:32:12] epoch: 10529 train-loss: 0.010556793150802454\n",
      "[LOG 20200502-14:32:12] epoch: 10530 train-loss: 0.010556795841289891\n",
      "[LOG 20200502-14:32:12] epoch: 10531 train-loss: 0.010556799049178759\n",
      "[LOG 20200502-14:32:13] epoch: 10532 train-loss: 0.010556801843146483\n",
      "[LOG 20200502-14:32:13] epoch: 10533 train-loss: 0.010556804533633921\n",
      "[LOG 20200502-14:32:13] epoch: 10534 train-loss: 0.010556807017160786\n",
      "[LOG 20200502-14:32:13] epoch: 10535 train-loss: 0.010556809914608797\n",
      "[LOG 20200502-14:32:14] epoch: 10536 train-loss: 0.010556812915537093\n",
      "[LOG 20200502-14:32:14] epoch: 10537 train-loss: 0.010556815399063958\n",
      "[LOG 20200502-14:32:14] epoch: 10538 train-loss: 0.010556818399992254\n",
      "[LOG 20200502-14:32:14] epoch: 10539 train-loss: 0.010556821193959978\n",
      "[LOG 20200502-14:32:15] epoch: 10540 train-loss: 0.010556823987927701\n",
      "[LOG 20200502-14:32:15] epoch: 10541 train-loss: 0.010556826885375712\n",
      "[LOG 20200502-14:32:15] epoch: 10542 train-loss: 0.010556829679343436\n",
      "[LOG 20200502-14:32:15] epoch: 10543 train-loss: 0.010556832887232304\n",
      "[LOG 20200502-14:32:15] epoch: 10544 train-loss: 0.010556835681200027\n",
      "[LOG 20200502-14:32:16] epoch: 10545 train-loss: 0.010556838889088895\n",
      "[LOG 20200502-14:32:16] epoch: 10546 train-loss: 0.010556841786536906\n",
      "[LOG 20200502-14:32:16] epoch: 10547 train-loss: 0.010556845201386346\n",
      "[LOG 20200502-14:32:16] epoch: 10548 train-loss: 0.010556847891873784\n",
      "[LOG 20200502-14:32:17] epoch: 10549 train-loss: 0.010556851099762652\n",
      "[LOG 20200502-14:32:17] epoch: 10550 train-loss: 0.010556853893730376\n",
      "[LOG 20200502-14:32:17] epoch: 10551 train-loss: 0.010556857515540388\n",
      "[LOG 20200502-14:32:17] epoch: 10552 train-loss: 0.010556860309508111\n",
      "[LOG 20200502-14:32:18] epoch: 10553 train-loss: 0.010556863310436407\n",
      "[LOG 20200502-14:32:18] epoch: 10554 train-loss: 0.010556866725285849\n",
      "[LOG 20200502-14:32:18] epoch: 10555 train-loss: 0.010556870036655001\n",
      "[LOG 20200502-14:32:18] epoch: 10556 train-loss: 0.010556873348024156\n",
      "[LOG 20200502-14:32:19] epoch: 10557 train-loss: 0.01055687665939331\n",
      "[LOG 20200502-14:32:19] epoch: 10558 train-loss: 0.010556879556841321\n",
      "[LOG 20200502-14:32:19] epoch: 10559 train-loss: 0.010556883489092192\n",
      "[LOG 20200502-14:32:19] epoch: 10560 train-loss: 0.010556885662178198\n",
      "[LOG 20200502-14:32:20] epoch: 10561 train-loss: 0.010556889594429068\n",
      "[LOG 20200502-14:32:20] epoch: 10562 train-loss: 0.010556892905798223\n",
      "[LOG 20200502-14:32:20] epoch: 10563 train-loss: 0.01055689611368709\n",
      "[LOG 20200502-14:32:20] epoch: 10564 train-loss: 0.010556899838977389\n",
      "[LOG 20200502-14:32:21] epoch: 10565 train-loss: 0.010556902632945113\n",
      "[LOG 20200502-14:32:21] epoch: 10566 train-loss: 0.01055690615127484\n",
      "[LOG 20200502-14:32:21] epoch: 10567 train-loss: 0.01055690904872285\n",
      "[LOG 20200502-14:32:21] epoch: 10568 train-loss: 0.010556912774013149\n",
      "[LOG 20200502-14:32:22] epoch: 10569 train-loss: 0.010556916188862588\n",
      "[LOG 20200502-14:32:22] epoch: 10570 train-loss: 0.010556918982830312\n",
      "[LOG 20200502-14:32:22] epoch: 10571 train-loss: 0.01055692219071918\n",
      "[LOG 20200502-14:32:23] epoch: 10572 train-loss: 0.01055692508816719\n",
      "[LOG 20200502-14:32:23] epoch: 10573 train-loss: 0.010556928296056058\n",
      "[LOG 20200502-14:32:23] epoch: 10574 train-loss: 0.010556931917866072\n",
      "[LOG 20200502-14:32:23] epoch: 10575 train-loss: 0.010556934711833795\n",
      "[LOG 20200502-14:32:24] epoch: 10576 train-loss: 0.01055693750580152\n",
      "[LOG 20200502-14:32:24] epoch: 10577 train-loss: 0.010556941127611531\n",
      "[LOG 20200502-14:32:24] epoch: 10578 train-loss: 0.010556944025059542\n",
      "[LOG 20200502-14:32:24] epoch: 10579 train-loss: 0.01055694723294841\n",
      "[LOG 20200502-14:32:25] epoch: 10580 train-loss: 0.010556949509514702\n",
      "[LOG 20200502-14:32:25] epoch: 10581 train-loss: 0.010556952924364142\n",
      "[LOG 20200502-14:32:25] epoch: 10582 train-loss: 0.010556955718331866\n",
      "[LOG 20200502-14:32:25] epoch: 10583 train-loss: 0.01055695851229959\n",
      "[LOG 20200502-14:32:25] epoch: 10584 train-loss: 0.010556961513227887\n",
      "[LOG 20200502-14:32:26] epoch: 10585 train-loss: 0.010556964203715324\n",
      "[LOG 20200502-14:32:26] epoch: 10586 train-loss: 0.010556966790722476\n",
      "[LOG 20200502-14:32:26] epoch: 10587 train-loss: 0.010556969481209913\n",
      "[LOG 20200502-14:32:26] epoch: 10588 train-loss: 0.010556972171697352\n",
      "[LOG 20200502-14:32:27] epoch: 10589 train-loss: 0.010556974965665076\n",
      "[LOG 20200502-14:32:27] epoch: 10590 train-loss: 0.010556977242231369\n",
      "[LOG 20200502-14:32:27] epoch: 10591 train-loss: 0.010556979932718806\n",
      "[LOG 20200502-14:32:27] epoch: 10592 train-loss: 0.010556982312765386\n",
      "[LOG 20200502-14:32:28] epoch: 10593 train-loss: 0.010556985003252825\n",
      "[LOG 20200502-14:32:28] epoch: 10594 train-loss: 0.01055698748677969\n",
      "[LOG 20200502-14:32:28] epoch: 10595 train-loss: 0.010556989659865698\n",
      "[LOG 20200502-14:32:28] epoch: 10596 train-loss: 0.010556991625991132\n",
      "[LOG 20200502-14:32:28] epoch: 10597 train-loss: 0.010556994316478571\n",
      "[LOG 20200502-14:32:29] epoch: 10598 train-loss: 0.010556996696525149\n",
      "[LOG 20200502-14:32:29] epoch: 10599 train-loss: 0.010556999076571729\n",
      "[LOG 20200502-14:32:29] epoch: 10600 train-loss: 0.010557000732256306\n",
      "[LOG 20200502-14:32:29] epoch: 10601 train-loss: 0.010557002801862028\n",
      "[LOG 20200502-14:32:30] epoch: 10602 train-loss: 0.010557005285388894\n",
      "[LOG 20200502-14:32:30] epoch: 10603 train-loss: 0.010557006837593185\n",
      "[LOG 20200502-14:32:30] epoch: 10604 train-loss: 0.010557009217639765\n",
      "[LOG 20200502-14:32:30] epoch: 10605 train-loss: 0.010557010873324342\n",
      "[LOG 20200502-14:32:31] epoch: 10606 train-loss: 0.010557012942930063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:32:31] epoch: 10607 train-loss: 0.010557015529937215\n",
      "[LOG 20200502-14:32:31] epoch: 10608 train-loss: 0.010557017082141506\n",
      "[LOG 20200502-14:32:31] epoch: 10609 train-loss: 0.010557018737826083\n",
      "[LOG 20200502-14:32:32] epoch: 10610 train-loss: 0.010557021117872663\n",
      "[LOG 20200502-14:32:32] epoch: 10611 train-loss: 0.010557022980517812\n",
      "[LOG 20200502-14:32:32] epoch: 10612 train-loss: 0.010557024636202388\n",
      "[LOG 20200502-14:32:32] epoch: 10613 train-loss: 0.010557026291886965\n",
      "[LOG 20200502-14:32:33] epoch: 10614 train-loss: 0.010557027947571542\n",
      "[LOG 20200502-14:32:33] epoch: 10615 train-loss: 0.010557030017177263\n",
      "[LOG 20200502-14:32:33] epoch: 10616 train-loss: 0.010557032190263271\n",
      "[LOG 20200502-14:32:33] epoch: 10617 train-loss: 0.010557033949428134\n",
      "[LOG 20200502-14:32:34] epoch: 10618 train-loss: 0.01055703591555357\n",
      "[LOG 20200502-14:32:34] epoch: 10619 train-loss: 0.010557037881679006\n",
      "[LOG 20200502-14:32:34] epoch: 10620 train-loss: 0.010557039019962152\n",
      "[LOG 20200502-14:32:35] epoch: 10621 train-loss: 0.010557041813929876\n",
      "[LOG 20200502-14:32:35] epoch: 10622 train-loss: 0.010557042952213023\n",
      "[LOG 20200502-14:32:35] epoch: 10623 train-loss: 0.010557045021818744\n",
      "[LOG 20200502-14:32:35] epoch: 10624 train-loss: 0.010557046987944178\n",
      "[LOG 20200502-14:32:36] epoch: 10625 train-loss: 0.010557048954069614\n",
      "[LOG 20200502-14:32:36] epoch: 10626 train-loss: 0.010557050713234477\n",
      "[LOG 20200502-14:32:36] epoch: 10627 train-loss: 0.010557053093281057\n",
      "[LOG 20200502-14:32:36] epoch: 10628 train-loss: 0.010557055576807924\n",
      "[LOG 20200502-14:32:37] epoch: 10629 train-loss: 0.01055705774989393\n",
      "[LOG 20200502-14:32:37] epoch: 10630 train-loss: 0.010557059612539079\n",
      "[LOG 20200502-14:32:37] epoch: 10631 train-loss: 0.010557061889105372\n",
      "[LOG 20200502-14:32:37] epoch: 10632 train-loss: 0.010557064165671667\n",
      "[LOG 20200502-14:32:37] epoch: 10633 train-loss: 0.010557066338757673\n",
      "[LOG 20200502-14:32:38] epoch: 10634 train-loss: 0.010557068408363394\n",
      "[LOG 20200502-14:32:38] epoch: 10635 train-loss: 0.01055707037448883\n",
      "[LOG 20200502-14:32:38] epoch: 10636 train-loss: 0.010557073168456554\n",
      "[LOG 20200502-14:32:38] epoch: 10637 train-loss: 0.010557075548503134\n",
      "[LOG 20200502-14:32:39] epoch: 10638 train-loss: 0.01055707803203\n",
      "[LOG 20200502-14:32:39] epoch: 10639 train-loss: 0.010557080308596293\n",
      "[LOG 20200502-14:32:39] epoch: 10640 train-loss: 0.010557082585162587\n",
      "[LOG 20200502-14:32:39] epoch: 10641 train-loss: 0.010557085689571168\n",
      "[LOG 20200502-14:32:40] epoch: 10642 train-loss: 0.010557088483538892\n",
      "[LOG 20200502-14:32:40] epoch: 10643 train-loss: 0.010557090760105185\n",
      "[LOG 20200502-14:32:40] epoch: 10644 train-loss: 0.010557093243632052\n",
      "[LOG 20200502-14:32:40] epoch: 10645 train-loss: 0.010557096037599776\n",
      "[LOG 20200502-14:32:40] epoch: 10646 train-loss: 0.01055709800372521\n",
      "[LOG 20200502-14:32:41] epoch: 10647 train-loss: 0.010557100797692934\n",
      "[LOG 20200502-14:32:41] epoch: 10648 train-loss: 0.010557103384700086\n",
      "[LOG 20200502-14:32:41] epoch: 10649 train-loss: 0.010557105971707238\n",
      "[LOG 20200502-14:32:41] epoch: 10650 train-loss: 0.010557108455234103\n",
      "[LOG 20200502-14:32:42] epoch: 10651 train-loss: 0.0105571114561624\n",
      "[LOG 20200502-14:32:42] epoch: 10652 train-loss: 0.010557113939689266\n",
      "[LOG 20200502-14:32:42] epoch: 10653 train-loss: 0.010557116526696417\n",
      "[LOG 20200502-14:32:42] epoch: 10654 train-loss: 0.010557119217183854\n",
      "[LOG 20200502-14:32:43] epoch: 10655 train-loss: 0.010557121700710721\n",
      "[LOG 20200502-14:32:43] epoch: 10656 train-loss: 0.010557124287717871\n",
      "[LOG 20200502-14:32:43] epoch: 10657 train-loss: 0.010557127081685595\n",
      "[LOG 20200502-14:32:43] epoch: 10658 train-loss: 0.010557129565212462\n",
      "[LOG 20200502-14:32:44] epoch: 10659 train-loss: 0.010557132152219614\n",
      "[LOG 20200502-14:32:44] epoch: 10660 train-loss: 0.010557133807904191\n",
      "[LOG 20200502-14:32:44] epoch: 10661 train-loss: 0.0105571367053522\n",
      "[LOG 20200502-14:32:44] epoch: 10662 train-loss: 0.010557138774957921\n",
      "[LOG 20200502-14:32:45] epoch: 10663 train-loss: 0.010557141568925645\n",
      "[LOG 20200502-14:32:45] epoch: 10664 train-loss: 0.010557143638531366\n",
      "[LOG 20200502-14:32:45] epoch: 10665 train-loss: 0.010557146122058233\n",
      "[LOG 20200502-14:32:45] epoch: 10666 train-loss: 0.010557148502104811\n",
      "[LOG 20200502-14:32:45] epoch: 10667 train-loss: 0.01055715067519082\n",
      "[LOG 20200502-14:32:46] epoch: 10668 train-loss: 0.01055715274479654\n",
      "[LOG 20200502-14:32:46] epoch: 10669 train-loss: 0.010557154917882549\n",
      "[LOG 20200502-14:32:46] epoch: 10670 train-loss: 0.010557156573567126\n",
      "[LOG 20200502-14:32:46] epoch: 10671 train-loss: 0.010557158643172847\n",
      "[LOG 20200502-14:32:47] epoch: 10672 train-loss: 0.010557161333660284\n",
      "[LOG 20200502-14:32:47] epoch: 10673 train-loss: 0.010557162885864576\n",
      "[LOG 20200502-14:32:47] epoch: 10674 train-loss: 0.010557164541549154\n",
      "[LOG 20200502-14:32:47] epoch: 10675 train-loss: 0.010557166611154875\n",
      "[LOG 20200502-14:32:47] epoch: 10676 train-loss: 0.010557168577280309\n",
      "[LOG 20200502-14:32:48] epoch: 10677 train-loss: 0.010557170232964886\n",
      "[LOG 20200502-14:32:48] epoch: 10678 train-loss: 0.010557171681688892\n",
      "[LOG 20200502-14:32:48] epoch: 10679 train-loss: 0.010557172819972038\n",
      "[LOG 20200502-14:32:48] epoch: 10680 train-loss: 0.010557174475656616\n",
      "[LOG 20200502-14:32:49] epoch: 10681 train-loss: 0.010557176855703196\n",
      "[LOG 20200502-14:32:49] epoch: 10682 train-loss: 0.010557178097466627\n",
      "[LOG 20200502-14:32:49] epoch: 10683 train-loss: 0.010557179856631491\n",
      "[LOG 20200502-14:32:49] epoch: 10684 train-loss: 0.010557180580993494\n",
      "[LOG 20200502-14:32:50] epoch: 10685 train-loss: 0.010557181512316069\n",
      "[LOG 20200502-14:32:50] epoch: 10686 train-loss: 0.010557183788882362\n",
      "[LOG 20200502-14:32:50] epoch: 10687 train-loss: 0.010557184513244364\n",
      "[LOG 20200502-14:32:50] epoch: 10688 train-loss: 0.010557185651527511\n",
      "[LOG 20200502-14:32:50] epoch: 10689 train-loss: 0.010557187100251516\n",
      "[LOG 20200502-14:32:51] epoch: 10690 train-loss: 0.010557188755936094\n",
      "[LOG 20200502-14:32:51] epoch: 10691 train-loss: 0.010557189583778381\n",
      "[LOG 20200502-14:32:51] epoch: 10692 train-loss: 0.010557190618581243\n",
      "[LOG 20200502-14:32:51] epoch: 10693 train-loss: 0.010557191653384103\n",
      "[LOG 20200502-14:32:52] epoch: 10694 train-loss: 0.010557192170785533\n",
      "[LOG 20200502-14:32:52] epoch: 10695 train-loss: 0.01055719330906868\n",
      "[LOG 20200502-14:32:52] epoch: 10696 train-loss: 0.010557194033430682\n",
      "[LOG 20200502-14:32:52] epoch: 10697 train-loss: 0.010557195482154688\n",
      "[LOG 20200502-14:32:52] epoch: 10698 train-loss: 0.01055719672391812\n",
      "[LOG 20200502-14:32:53] epoch: 10699 train-loss: 0.010557197758720981\n",
      "[LOG 20200502-14:32:53] epoch: 10700 train-loss: 0.010557198483082984\n",
      "[LOG 20200502-14:32:53] epoch: 10701 train-loss: 0.010557199517885843\n",
      "[LOG 20200502-14:32:53] epoch: 10702 train-loss: 0.010557200138767561\n",
      "[LOG 20200502-14:32:54] epoch: 10703 train-loss: 0.010557201690971851\n",
      "[LOG 20200502-14:32:54] epoch: 10704 train-loss: 0.010557202518814139\n",
      "[LOG 20200502-14:32:54] epoch: 10705 train-loss: 0.010557204071018431\n",
      "[LOG 20200502-14:32:54] epoch: 10706 train-loss: 0.010557204484939575\n",
      "[LOG 20200502-14:32:55] epoch: 10707 train-loss: 0.010557206347584724\n",
      "[LOG 20200502-14:32:55] epoch: 10708 train-loss: 0.01055720696846644\n",
      "[LOG 20200502-14:32:55] epoch: 10709 train-loss: 0.010557208417190446\n",
      "[LOG 20200502-14:32:55] epoch: 10710 train-loss: 0.010557209245032735\n",
      "[LOG 20200502-14:32:55] epoch: 10711 train-loss: 0.010557210486796167\n",
      "[LOG 20200502-14:32:56] epoch: 10712 train-loss: 0.010557212142480744\n",
      "[LOG 20200502-14:32:56] epoch: 10713 train-loss: 0.01055721328076389\n",
      "[LOG 20200502-14:32:56] epoch: 10714 train-loss: 0.010557214729487896\n",
      "[LOG 20200502-14:32:56] epoch: 10715 train-loss: 0.010557215764290757\n",
      "[LOG 20200502-14:32:57] epoch: 10716 train-loss: 0.010557217937376764\n",
      "[LOG 20200502-14:32:57] epoch: 10717 train-loss: 0.010557219386100769\n",
      "[LOG 20200502-14:32:57] epoch: 10718 train-loss: 0.01055722093830506\n",
      "[LOG 20200502-14:32:57] epoch: 10719 train-loss: 0.010557222180068493\n",
      "[LOG 20200502-14:32:58] epoch: 10720 train-loss: 0.010557224249674214\n",
      "[LOG 20200502-14:32:58] epoch: 10721 train-loss: 0.01055722621579965\n",
      "[LOG 20200502-14:32:58] epoch: 10722 train-loss: 0.010557227664523654\n",
      "[LOG 20200502-14:32:58] epoch: 10723 train-loss: 0.01055722963064909\n",
      "[LOG 20200502-14:32:58] epoch: 10724 train-loss: 0.010557231907215383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:32:59] epoch: 10725 train-loss: 0.010557233976821104\n",
      "[LOG 20200502-14:32:59] epoch: 10726 train-loss: 0.01055723594294654\n",
      "[LOG 20200502-14:32:59] epoch: 10727 train-loss: 0.01055723780559169\n",
      "[LOG 20200502-14:32:59] epoch: 10728 train-loss: 0.010557239461276267\n",
      "[LOG 20200502-14:33:00] epoch: 10729 train-loss: 0.010557242048283419\n",
      "[LOG 20200502-14:33:00] epoch: 10730 train-loss: 0.010557244221369425\n",
      "[LOG 20200502-14:33:00] epoch: 10731 train-loss: 0.01055724649793572\n",
      "[LOG 20200502-14:33:00] epoch: 10732 train-loss: 0.010557248360580869\n",
      "[LOG 20200502-14:33:00] epoch: 10733 train-loss: 0.010557250119745731\n",
      "[LOG 20200502-14:33:01] epoch: 10734 train-loss: 0.010557252706752883\n",
      "[LOG 20200502-14:33:01] epoch: 10735 train-loss: 0.010557254672878318\n",
      "[LOG 20200502-14:33:01] epoch: 10736 train-loss: 0.010557257156405184\n",
      "[LOG 20200502-14:33:01] epoch: 10737 train-loss: 0.01055725963993205\n",
      "[LOG 20200502-14:33:02] epoch: 10738 train-loss: 0.010557261606057486\n",
      "[LOG 20200502-14:33:02] epoch: 10739 train-loss: 0.01055726357218292\n",
      "[LOG 20200502-14:33:02] epoch: 10740 train-loss: 0.010557266055709787\n",
      "[LOG 20200502-14:33:02] epoch: 10741 train-loss: 0.010557267814874649\n",
      "[LOG 20200502-14:33:02] epoch: 10742 train-loss: 0.01055726988448037\n",
      "[LOG 20200502-14:33:03] epoch: 10743 train-loss: 0.010557272471487522\n",
      "[LOG 20200502-14:33:03] epoch: 10744 train-loss: 0.010557274023691813\n",
      "[LOG 20200502-14:33:03] epoch: 10745 train-loss: 0.010557276093297534\n",
      "[LOG 20200502-14:33:03] epoch: 10746 train-loss: 0.010557278473344114\n",
      "[LOG 20200502-14:33:04] epoch: 10747 train-loss: 0.010557280232508978\n",
      "[LOG 20200502-14:33:04] epoch: 10748 train-loss: 0.010557282302114699\n",
      "[LOG 20200502-14:33:04] epoch: 10749 train-loss: 0.010557283750838704\n",
      "[LOG 20200502-14:33:04] epoch: 10750 train-loss: 0.010557285303042995\n",
      "[LOG 20200502-14:33:04] epoch: 10751 train-loss: 0.010557287476129003\n",
      "[LOG 20200502-14:33:05] epoch: 10752 train-loss: 0.010557289131813578\n",
      "[LOG 20200502-14:33:05] epoch: 10753 train-loss: 0.010557290580537584\n",
      "[LOG 20200502-14:33:05] epoch: 10754 train-loss: 0.01055729254666302\n",
      "[LOG 20200502-14:33:05] epoch: 10755 train-loss: 0.010557293374505308\n",
      "[LOG 20200502-14:33:06] epoch: 10756 train-loss: 0.0105572949267096\n",
      "[LOG 20200502-14:33:06] epoch: 10757 train-loss: 0.010557296892835034\n",
      "[LOG 20200502-14:33:06] epoch: 10758 train-loss: 0.010557298238078753\n",
      "[LOG 20200502-14:33:06] epoch: 10759 train-loss: 0.010557299479842186\n",
      "[LOG 20200502-14:33:06] epoch: 10760 train-loss: 0.01055730072160562\n",
      "[LOG 20200502-14:33:07] epoch: 10761 train-loss: 0.010557301859888766\n",
      "[LOG 20200502-14:33:07] epoch: 10762 train-loss: 0.010557303101652198\n",
      "[LOG 20200502-14:33:07] epoch: 10763 train-loss: 0.010557303722533915\n",
      "[LOG 20200502-14:33:07] epoch: 10764 train-loss: 0.010557305067777634\n",
      "[LOG 20200502-14:33:08] epoch: 10765 train-loss: 0.010557306309541067\n",
      "[LOG 20200502-14:33:08] epoch: 10766 train-loss: 0.010557307240863642\n",
      "[LOG 20200502-14:33:08] epoch: 10767 train-loss: 0.010557307965225644\n",
      "[LOG 20200502-14:33:08] epoch: 10768 train-loss: 0.01055730858610736\n",
      "[LOG 20200502-14:33:08] epoch: 10769 train-loss: 0.010557309413949648\n",
      "[LOG 20200502-14:33:09] epoch: 10770 train-loss: 0.010557310241791937\n",
      "[LOG 20200502-14:33:09] epoch: 10771 train-loss: 0.010557310655713081\n",
      "[LOG 20200502-14:33:09] epoch: 10772 train-loss: 0.010557311587035656\n",
      "[LOG 20200502-14:33:09] epoch: 10773 train-loss: 0.01055731251835823\n",
      "[LOG 20200502-14:33:10] epoch: 10774 train-loss: 0.01055731282879909\n",
      "[LOG 20200502-14:33:10] epoch: 10775 train-loss: 0.010557312932279374\n",
      "[LOG 20200502-14:33:10] epoch: 10776 train-loss: 0.010557313760121664\n",
      "[LOG 20200502-14:33:10] epoch: 10777 train-loss: 0.010557313967082236\n",
      "[LOG 20200502-14:33:10] epoch: 10778 train-loss: 0.010557314174042808\n",
      "[LOG 20200502-14:33:11] epoch: 10779 train-loss: 0.010557314794924524\n",
      "[LOG 20200502-14:33:11] epoch: 10780 train-loss: 0.010557315312325954\n",
      "[LOG 20200502-14:33:11] epoch: 10781 train-loss: 0.010557315726247098\n",
      "[LOG 20200502-14:33:11] epoch: 10782 train-loss: 0.010557315415806241\n",
      "[LOG 20200502-14:33:12] epoch: 10783 train-loss: 0.010557316140168242\n",
      "[LOG 20200502-14:33:12] epoch: 10784 train-loss: 0.010557316036687957\n",
      "[LOG 20200502-14:33:12] epoch: 10785 train-loss: 0.01055731676104996\n",
      "[LOG 20200502-14:33:12] epoch: 10786 train-loss: 0.01055731727845139\n",
      "[LOG 20200502-14:33:12] epoch: 10787 train-loss: 0.010557317381931676\n",
      "[LOG 20200502-14:33:13] epoch: 10788 train-loss: 0.010557317899333106\n",
      "[LOG 20200502-14:33:13] epoch: 10789 train-loss: 0.010557318106293678\n",
      "[LOG 20200502-14:33:13] epoch: 10790 train-loss: 0.010557318934135966\n",
      "[LOG 20200502-14:33:13] epoch: 10791 train-loss: 0.010557319451537397\n",
      "[LOG 20200502-14:33:14] epoch: 10792 train-loss: 0.010557319968938828\n",
      "[LOG 20200502-14:33:14] epoch: 10793 train-loss: 0.010557320796781115\n",
      "[LOG 20200502-14:33:14] epoch: 10794 train-loss: 0.010557321314182546\n",
      "[LOG 20200502-14:33:14] epoch: 10795 train-loss: 0.010557322452465693\n",
      "[LOG 20200502-14:33:14] epoch: 10796 train-loss: 0.010557322659426265\n",
      "[LOG 20200502-14:33:15] epoch: 10797 train-loss: 0.010557323901189698\n",
      "[LOG 20200502-14:33:15] epoch: 10798 train-loss: 0.0105573246255517\n",
      "[LOG 20200502-14:33:15] epoch: 10799 train-loss: 0.010557325763834847\n",
      "[LOG 20200502-14:33:15] epoch: 10800 train-loss: 0.010557327212558852\n",
      "[LOG 20200502-14:33:16] epoch: 10801 train-loss: 0.010557328764763143\n",
      "[LOG 20200502-14:33:16] epoch: 10802 train-loss: 0.010557329592605432\n",
      "[LOG 20200502-14:33:16] epoch: 10803 train-loss: 0.010557330937849151\n",
      "[LOG 20200502-14:33:16] epoch: 10804 train-loss: 0.010557332179612584\n",
      "[LOG 20200502-14:33:16] epoch: 10805 train-loss: 0.010557333524856303\n",
      "[LOG 20200502-14:33:17] epoch: 10806 train-loss: 0.010557335284021165\n",
      "[LOG 20200502-14:33:17] epoch: 10807 train-loss: 0.010557336629264884\n",
      "[LOG 20200502-14:33:17] epoch: 10808 train-loss: 0.010557338905831179\n",
      "[LOG 20200502-14:33:17] epoch: 10809 train-loss: 0.010557339940634038\n",
      "[LOG 20200502-14:33:18] epoch: 10810 train-loss: 0.010557341906759474\n",
      "[LOG 20200502-14:33:18] epoch: 10811 train-loss: 0.010557343562444052\n",
      "[LOG 20200502-14:33:18] epoch: 10812 train-loss: 0.010557345735530058\n",
      "[LOG 20200502-14:33:18] epoch: 10813 train-loss: 0.010557348219056925\n",
      "[LOG 20200502-14:33:18] epoch: 10814 train-loss: 0.010557349253859784\n",
      "[LOG 20200502-14:33:19] epoch: 10815 train-loss: 0.010557351944347223\n",
      "[LOG 20200502-14:33:19] epoch: 10816 train-loss: 0.0105573536000318\n",
      "[LOG 20200502-14:33:19] epoch: 10817 train-loss: 0.01055735546267695\n",
      "[LOG 20200502-14:33:19] epoch: 10818 train-loss: 0.010557357428802384\n",
      "[LOG 20200502-14:33:20] epoch: 10819 train-loss: 0.01055735939492782\n",
      "[LOG 20200502-14:33:20] epoch: 10820 train-loss: 0.010557361568013826\n",
      "[LOG 20200502-14:33:20] epoch: 10821 train-loss: 0.010557363534139262\n",
      "[LOG 20200502-14:33:20] epoch: 10822 train-loss: 0.010557365293304125\n",
      "[LOG 20200502-14:33:21] epoch: 10823 train-loss: 0.010557367259429561\n",
      "[LOG 20200502-14:33:21] epoch: 10824 train-loss: 0.01055736912207471\n",
      "[LOG 20200502-14:33:21] epoch: 10825 train-loss: 0.010557371191680431\n",
      "[LOG 20200502-14:33:21] epoch: 10826 train-loss: 0.010557373261286153\n",
      "[LOG 20200502-14:33:21] epoch: 10827 train-loss: 0.01055737491697073\n",
      "[LOG 20200502-14:33:22] epoch: 10828 train-loss: 0.01055737646917502\n",
      "[LOG 20200502-14:33:22] epoch: 10829 train-loss: 0.01055737833182017\n",
      "[LOG 20200502-14:33:22] epoch: 10830 train-loss: 0.010557380297945606\n",
      "[LOG 20200502-14:33:22] epoch: 10831 train-loss: 0.010557381746669611\n",
      "[LOG 20200502-14:33:23] epoch: 10832 train-loss: 0.010557383402354188\n",
      "[LOG 20200502-14:33:23] epoch: 10833 train-loss: 0.010557384747597907\n",
      "[LOG 20200502-14:33:23] epoch: 10834 train-loss: 0.010557386506762769\n",
      "[LOG 20200502-14:33:23] epoch: 10835 train-loss: 0.010557387645045916\n",
      "[LOG 20200502-14:33:24] epoch: 10836 train-loss: 0.01055738940421078\n",
      "[LOG 20200502-14:33:24] epoch: 10837 train-loss: 0.01055739043901364\n",
      "[LOG 20200502-14:33:24] epoch: 10838 train-loss: 0.010557391680777073\n",
      "[LOG 20200502-14:33:24] epoch: 10839 train-loss: 0.010557392715579934\n",
      "[LOG 20200502-14:33:24] epoch: 10840 train-loss: 0.010557393853863081\n",
      "[LOG 20200502-14:33:25] epoch: 10841 train-loss: 0.010557394992146227\n",
      "[LOG 20200502-14:33:25] epoch: 10842 train-loss: 0.010557395613027943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:33:25] epoch: 10843 train-loss: 0.010557396854791377\n",
      "[LOG 20200502-14:33:25] epoch: 10844 train-loss: 0.010557397475673093\n",
      "[LOG 20200502-14:33:26] epoch: 10845 train-loss: 0.010557398406995667\n",
      "[LOG 20200502-14:33:26] epoch: 10846 train-loss: 0.010557399545278814\n",
      "[LOG 20200502-14:33:26] epoch: 10847 train-loss: 0.010557399959199958\n",
      "[LOG 20200502-14:33:26] epoch: 10848 train-loss: 0.010557400373121103\n",
      "[LOG 20200502-14:33:26] epoch: 10849 train-loss: 0.010557401304443678\n",
      "[LOG 20200502-14:33:27] epoch: 10850 train-loss: 0.010557401718364822\n",
      "[LOG 20200502-14:33:27] epoch: 10851 train-loss: 0.010557401614884535\n",
      "[LOG 20200502-14:33:27] epoch: 10852 train-loss: 0.010557402339246538\n",
      "[LOG 20200502-14:33:27] epoch: 10853 train-loss: 0.010557402132285966\n",
      "[LOG 20200502-14:33:28] epoch: 10854 train-loss: 0.010557402339246538\n",
      "[LOG 20200502-14:33:28] epoch: 10855 train-loss: 0.01055740254620711\n",
      "[LOG 20200502-14:33:28] epoch: 10856 train-loss: 0.010557402339246538\n",
      "[LOG 20200502-14:33:28] epoch: 10857 train-loss: 0.010557402649687396\n",
      "[LOG 20200502-14:33:29] epoch: 10858 train-loss: 0.01055740254620711\n",
      "[LOG 20200502-14:33:29] epoch: 10859 train-loss: 0.010557402960128255\n",
      "[LOG 20200502-14:33:29] epoch: 10860 train-loss: 0.01055740254620711\n",
      "[LOG 20200502-14:33:29] epoch: 10861 train-loss: 0.010557401718364822\n",
      "[LOG 20200502-14:33:30] epoch: 10862 train-loss: 0.01055740202880568\n",
      "[LOG 20200502-14:33:30] epoch: 10863 train-loss: 0.010557401925325394\n",
      "[LOG 20200502-14:33:30] epoch: 10864 train-loss: 0.010557401718364822\n",
      "[LOG 20200502-14:33:30] epoch: 10865 train-loss: 0.010557401097483106\n",
      "[LOG 20200502-14:33:30] epoch: 10866 train-loss: 0.010557401200963391\n",
      "[LOG 20200502-14:33:31] epoch: 10867 train-loss: 0.010557400890522532\n",
      "[LOG 20200502-14:33:31] epoch: 10868 train-loss: 0.010557400062680244\n",
      "[LOG 20200502-14:33:31] epoch: 10869 train-loss: 0.010557400166160531\n",
      "[LOG 20200502-14:33:31] epoch: 10870 train-loss: 0.010557400269640816\n",
      "[LOG 20200502-14:33:32] epoch: 10871 train-loss: 0.010557399545278814\n",
      "[LOG 20200502-14:33:32] epoch: 10872 train-loss: 0.010557399752239386\n",
      "[LOG 20200502-14:33:32] epoch: 10873 train-loss: 0.010557399441798529\n",
      "[LOG 20200502-14:33:32] epoch: 10874 train-loss: 0.010557399338318242\n",
      "[LOG 20200502-14:33:32] epoch: 10875 train-loss: 0.010557399545278814\n",
      "[LOG 20200502-14:33:33] epoch: 10876 train-loss: 0.010557399752239386\n",
      "[LOG 20200502-14:33:33] epoch: 10877 train-loss: 0.010557399441798529\n",
      "[LOG 20200502-14:33:33] epoch: 10878 train-loss: 0.010557400062680244\n",
      "[LOG 20200502-14:33:33] epoch: 10879 train-loss: 0.010557400062680244\n",
      "[LOG 20200502-14:33:34] epoch: 10880 train-loss: 0.01055740068356196\n",
      "[LOG 20200502-14:33:34] epoch: 10881 train-loss: 0.010557401304443678\n",
      "[LOG 20200502-14:33:34] epoch: 10882 train-loss: 0.010557401614884535\n",
      "[LOG 20200502-14:33:34] epoch: 10883 train-loss: 0.010557402649687396\n",
      "[LOG 20200502-14:33:35] epoch: 10884 train-loss: 0.010557403581009971\n",
      "[LOG 20200502-14:33:35] epoch: 10885 train-loss: 0.010557403994931115\n",
      "[LOG 20200502-14:33:35] epoch: 10886 train-loss: 0.010557405754095979\n",
      "[LOG 20200502-14:33:35] epoch: 10887 train-loss: 0.010557406374977695\n",
      "[LOG 20200502-14:33:36] epoch: 10888 train-loss: 0.010557407616741128\n",
      "[LOG 20200502-14:33:36] epoch: 10889 train-loss: 0.01055740885850456\n",
      "[LOG 20200502-14:33:36] epoch: 10890 train-loss: 0.010557409893307421\n",
      "[LOG 20200502-14:33:36] epoch: 10891 train-loss: 0.010557411548991999\n",
      "[LOG 20200502-14:33:36] epoch: 10892 train-loss: 0.010557413204676576\n",
      "[LOG 20200502-14:33:37] epoch: 10893 train-loss: 0.010557414963841438\n",
      "[LOG 20200502-14:33:37] epoch: 10894 train-loss: 0.010557416619526016\n",
      "[LOG 20200502-14:33:37] epoch: 10895 train-loss: 0.01055741858565145\n",
      "[LOG 20200502-14:33:37] epoch: 10896 train-loss: 0.01055741993089517\n",
      "[LOG 20200502-14:33:38] epoch: 10897 train-loss: 0.010557422207461463\n",
      "[LOG 20200502-14:33:38] epoch: 10898 train-loss: 0.010557423552705182\n",
      "[LOG 20200502-14:33:38] epoch: 10899 train-loss: 0.01055742572579119\n",
      "[LOG 20200502-14:33:38] epoch: 10900 train-loss: 0.010557427381475767\n",
      "[LOG 20200502-14:33:38] epoch: 10901 train-loss: 0.01055742914064063\n",
      "[LOG 20200502-14:33:39] epoch: 10902 train-loss: 0.01055743152068721\n",
      "[LOG 20200502-14:33:39] epoch: 10903 train-loss: 0.010557433176371787\n",
      "[LOG 20200502-14:33:39] epoch: 10904 train-loss: 0.010557435142497221\n",
      "[LOG 20200502-14:33:39] epoch: 10905 train-loss: 0.010557437315583229\n",
      "[LOG 20200502-14:33:40] epoch: 10906 train-loss: 0.010557438971267806\n",
      "[LOG 20200502-14:33:40] epoch: 10907 train-loss: 0.010557440730432669\n",
      "[LOG 20200502-14:33:40] epoch: 10908 train-loss: 0.010557442386117246\n",
      "[LOG 20200502-14:33:40] epoch: 10909 train-loss: 0.010557444041801823\n",
      "[LOG 20200502-14:33:41] epoch: 10910 train-loss: 0.010557445800966687\n",
      "[LOG 20200502-14:33:41] epoch: 10911 train-loss: 0.01055744756013155\n",
      "[LOG 20200502-14:33:41] epoch: 10912 train-loss: 0.010557448905375268\n",
      "[LOG 20200502-14:33:41] epoch: 10913 train-loss: 0.010557450354099274\n",
      "[LOG 20200502-14:33:41] epoch: 10914 train-loss: 0.010557452423704995\n",
      "[LOG 20200502-14:33:42] epoch: 10915 train-loss: 0.010557453458507856\n",
      "[LOG 20200502-14:33:42] epoch: 10916 train-loss: 0.010557455010712147\n",
      "[LOG 20200502-14:33:42] epoch: 10917 train-loss: 0.010557456355955865\n",
      "[LOG 20200502-14:33:42] epoch: 10918 train-loss: 0.010557457080317868\n",
      "[LOG 20200502-14:33:43] epoch: 10919 train-loss: 0.010557458736002445\n",
      "[LOG 20200502-14:33:43] epoch: 10920 train-loss: 0.010557459770805307\n",
      "[LOG 20200502-14:33:43] epoch: 10921 train-loss: 0.010557460805608166\n",
      "[LOG 20200502-14:33:43] epoch: 10922 train-loss: 0.010557460909088453\n",
      "[LOG 20200502-14:33:44] epoch: 10923 train-loss: 0.010557461943891313\n",
      "[LOG 20200502-14:33:44] epoch: 10924 train-loss: 0.010557463392615318\n",
      "[LOG 20200502-14:33:44] epoch: 10925 train-loss: 0.010557463703056177\n",
      "[LOG 20200502-14:33:44] epoch: 10926 train-loss: 0.010557464116977321\n",
      "[LOG 20200502-14:33:44] epoch: 10927 train-loss: 0.010557463910016749\n",
      "[LOG 20200502-14:33:45] epoch: 10928 train-loss: 0.010557464841339324\n",
      "[LOG 20200502-14:33:45] epoch: 10929 train-loss: 0.010557465255260468\n",
      "[LOG 20200502-14:33:45] epoch: 10930 train-loss: 0.010557465358740754\n",
      "[LOG 20200502-14:33:45] epoch: 10931 train-loss: 0.010557465565701326\n",
      "[LOG 20200502-14:33:46] epoch: 10932 train-loss: 0.010557465876142183\n",
      "[LOG 20200502-14:33:46] epoch: 10933 train-loss: 0.010557466186583042\n",
      "[LOG 20200502-14:33:46] epoch: 10934 train-loss: 0.010557465565701326\n",
      "[LOG 20200502-14:33:46] epoch: 10935 train-loss: 0.01055746546222104\n",
      "[LOG 20200502-14:33:46] epoch: 10936 train-loss: 0.01055746515178018\n",
      "[LOG 20200502-14:33:47] epoch: 10937 train-loss: 0.010557464944819609\n",
      "[LOG 20200502-14:33:47] epoch: 10938 train-loss: 0.010557464737859037\n",
      "[LOG 20200502-14:33:47] epoch: 10939 train-loss: 0.010557463806536462\n",
      "[LOG 20200502-14:33:47] epoch: 10940 train-loss: 0.010557463703056177\n",
      "[LOG 20200502-14:33:48] epoch: 10941 train-loss: 0.010557463392615318\n",
      "[LOG 20200502-14:33:48] epoch: 10942 train-loss: 0.010557462150851885\n",
      "[LOG 20200502-14:33:48] epoch: 10943 train-loss: 0.010557461529970169\n",
      "[LOG 20200502-14:33:48] epoch: 10944 train-loss: 0.010557460598647594\n",
      "[LOG 20200502-14:33:49] epoch: 10945 train-loss: 0.010557459563844733\n",
      "[LOG 20200502-14:33:49] epoch: 10946 train-loss: 0.010557459460364448\n",
      "[LOG 20200502-14:33:49] epoch: 10947 train-loss: 0.010557458425561586\n",
      "[LOG 20200502-14:33:49] epoch: 10948 train-loss: 0.010557458011640443\n",
      "[LOG 20200502-14:33:50] epoch: 10949 train-loss: 0.01055745625247558\n",
      "[LOG 20200502-14:33:50] epoch: 10950 train-loss: 0.01055745573507415\n",
      "[LOG 20200502-14:33:50] epoch: 10951 train-loss: 0.010557454596791003\n",
      "[LOG 20200502-14:33:50] epoch: 10952 train-loss: 0.010557453665468428\n",
      "[LOG 20200502-14:33:50] epoch: 10953 train-loss: 0.010557452630665567\n",
      "[LOG 20200502-14:33:51] epoch: 10954 train-loss: 0.010557452009783851\n",
      "[LOG 20200502-14:33:51] epoch: 10955 train-loss: 0.010557451388902135\n",
      "[LOG 20200502-14:33:51] epoch: 10956 train-loss: 0.010557450664540132\n",
      "[LOG 20200502-14:33:51] epoch: 10957 train-loss: 0.01055744994017813\n",
      "[LOG 20200502-14:33:52] epoch: 10958 train-loss: 0.010557448801894983\n",
      "[LOG 20200502-14:33:52] epoch: 10959 train-loss: 0.010557448491454124\n",
      "[LOG 20200502-14:33:52] epoch: 10960 train-loss: 0.010557448284493553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:33:52] epoch: 10961 train-loss: 0.01055744807753298\n",
      "[LOG 20200502-14:33:52] epoch: 10962 train-loss: 0.010557447663611837\n",
      "[LOG 20200502-14:33:53] epoch: 10963 train-loss: 0.010557447870572409\n",
      "[LOG 20200502-14:33:53] epoch: 10964 train-loss: 0.010557447870572409\n",
      "[LOG 20200502-14:33:53] epoch: 10965 train-loss: 0.010557448387973838\n",
      "[LOG 20200502-14:33:53] epoch: 10966 train-loss: 0.010557448491454124\n",
      "[LOG 20200502-14:33:54] epoch: 10967 train-loss: 0.01055744911233584\n",
      "[LOG 20200502-14:33:54] epoch: 10968 train-loss: 0.010557448905375268\n",
      "[LOG 20200502-14:33:54] epoch: 10969 train-loss: 0.010557450147138702\n",
      "[LOG 20200502-14:33:54] epoch: 10970 train-loss: 0.010557451285421848\n",
      "[LOG 20200502-14:33:55] epoch: 10971 train-loss: 0.010557452113264136\n",
      "[LOG 20200502-14:33:55] epoch: 10972 train-loss: 0.010557453975909285\n",
      "[LOG 20200502-14:33:55] epoch: 10973 train-loss: 0.010557454700271288\n",
      "[LOG 20200502-14:33:55] epoch: 10974 train-loss: 0.010557456045515008\n",
      "[LOG 20200502-14:33:55] epoch: 10975 train-loss: 0.010557457390758727\n",
      "[LOG 20200502-14:33:56] epoch: 10976 train-loss: 0.010557458839482732\n",
      "[LOG 20200502-14:33:56] epoch: 10977 train-loss: 0.010557460391687023\n",
      "[LOG 20200502-14:33:56] epoch: 10978 train-loss: 0.0105574620473716\n",
      "[LOG 20200502-14:33:56] epoch: 10979 train-loss: 0.010557464013497034\n",
      "[LOG 20200502-14:33:57] epoch: 10980 train-loss: 0.010557465876142183\n",
      "[LOG 20200502-14:33:57] epoch: 10981 train-loss: 0.010557467738787333\n",
      "[LOG 20200502-14:33:57] epoch: 10982 train-loss: 0.010557469497952197\n",
      "[LOG 20200502-14:33:57] epoch: 10983 train-loss: 0.010557471464077631\n",
      "[LOG 20200502-14:33:58] epoch: 10984 train-loss: 0.010557473016281923\n",
      "[LOG 20200502-14:33:58] epoch: 10985 train-loss: 0.010557476120690504\n",
      "[LOG 20200502-14:33:58] epoch: 10986 train-loss: 0.010557477672894796\n",
      "[LOG 20200502-14:33:58] epoch: 10987 train-loss: 0.010557479535539946\n",
      "[LOG 20200502-14:33:58] epoch: 10988 train-loss: 0.010557481191224523\n",
      "[LOG 20200502-14:33:59] epoch: 10989 train-loss: 0.010557483260830244\n",
      "[LOG 20200502-14:33:59] epoch: 10990 train-loss: 0.010557485330435965\n",
      "[LOG 20200502-14:33:59] epoch: 10991 train-loss: 0.010557486468719112\n",
      "[LOG 20200502-14:33:59] epoch: 10992 train-loss: 0.010557488124403689\n",
      "[LOG 20200502-14:34:00] epoch: 10993 train-loss: 0.01055749019400941\n",
      "[LOG 20200502-14:34:00] epoch: 10994 train-loss: 0.010557491642733416\n",
      "[LOG 20200502-14:34:00] epoch: 10995 train-loss: 0.010557492677536275\n",
      "[LOG 20200502-14:34:00] epoch: 10996 train-loss: 0.010557494643661711\n",
      "[LOG 20200502-14:34:01] epoch: 10997 train-loss: 0.010557496092385717\n",
      "[LOG 20200502-14:34:01] epoch: 10998 train-loss: 0.010557497127188576\n",
      "[LOG 20200502-14:34:01] epoch: 10999 train-loss: 0.010557498472432295\n",
      "[LOG 20200502-14:34:01] epoch: 11000 train-loss: 0.010557499817676015\n",
      "[LOG 20200502-14:34:01] epoch: 11001 train-loss: 0.010557500645518303\n",
      "[LOG 20200502-14:34:02] epoch: 11002 train-loss: 0.01055750178380145\n",
      "[LOG 20200502-14:34:02] epoch: 11003 train-loss: 0.010557502508163452\n",
      "[LOG 20200502-14:34:02] epoch: 11004 train-loss: 0.010557503025564883\n",
      "[LOG 20200502-14:34:02] epoch: 11005 train-loss: 0.01055750385340717\n",
      "[LOG 20200502-14:34:02] epoch: 11006 train-loss: 0.010557504474288888\n",
      "[LOG 20200502-14:34:03] epoch: 11007 train-loss: 0.01055750519865089\n",
      "[LOG 20200502-14:34:03] epoch: 11008 train-loss: 0.010557505819532607\n",
      "[LOG 20200502-14:34:03] epoch: 11009 train-loss: 0.010557505923012892\n",
      "[LOG 20200502-14:34:03] epoch: 11010 train-loss: 0.01055750571605232\n",
      "[LOG 20200502-14:34:04] epoch: 11011 train-loss: 0.010557506026493179\n",
      "[LOG 20200502-14:34:04] epoch: 11012 train-loss: 0.010557506026493179\n",
      "[LOG 20200502-14:34:04] epoch: 11013 train-loss: 0.010557506026493179\n",
      "[LOG 20200502-14:34:04] epoch: 11014 train-loss: 0.010557505923012892\n",
      "[LOG 20200502-14:34:04] epoch: 11015 train-loss: 0.010557505612572035\n",
      "[LOG 20200502-14:34:05] epoch: 11016 train-loss: 0.01055750571605232\n",
      "[LOG 20200502-14:34:05] epoch: 11017 train-loss: 0.01055750519865089\n",
      "[LOG 20200502-14:34:05] epoch: 11018 train-loss: 0.01055750416384803\n",
      "[LOG 20200502-14:34:05] epoch: 11019 train-loss: 0.01055750468124946\n",
      "[LOG 20200502-14:34:05] epoch: 11020 train-loss: 0.01055750385340717\n",
      "[LOG 20200502-14:34:06] epoch: 11021 train-loss: 0.010557502611643739\n",
      "[LOG 20200502-14:34:06] epoch: 11022 train-loss: 0.010557501990762021\n",
      "[LOG 20200502-14:34:06] epoch: 11023 train-loss: 0.010557500852478875\n",
      "[LOG 20200502-14:34:06] epoch: 11024 train-loss: 0.010557499507235156\n",
      "[LOG 20200502-14:34:07] epoch: 11025 train-loss: 0.01055749888635344\n",
      "[LOG 20200502-14:34:07] epoch: 11026 train-loss: 0.010557497644590007\n",
      "[LOG 20200502-14:34:07] epoch: 11027 train-loss: 0.010557496609787146\n",
      "[LOG 20200502-14:34:07] epoch: 11028 train-loss: 0.010557494954102568\n",
      "[LOG 20200502-14:34:08] epoch: 11029 train-loss: 0.010557494022779994\n",
      "[LOG 20200502-14:34:08] epoch: 11030 train-loss: 0.010557492263615131\n",
      "[LOG 20200502-14:34:08] epoch: 11031 train-loss: 0.01055749122881227\n",
      "[LOG 20200502-14:34:08] epoch: 11032 train-loss: 0.010557489883568551\n",
      "[LOG 20200502-14:34:08] epoch: 11033 train-loss: 0.010557487917443117\n",
      "[LOG 20200502-14:34:09] epoch: 11034 train-loss: 0.010557486675679684\n",
      "[LOG 20200502-14:34:09] epoch: 11035 train-loss: 0.010557485019995106\n",
      "[LOG 20200502-14:34:09] epoch: 11036 train-loss: 0.010557484088672532\n",
      "[LOG 20200502-14:34:09] epoch: 11037 train-loss: 0.010557482226027383\n",
      "[LOG 20200502-14:34:10] epoch: 11038 train-loss: 0.010557481087744236\n",
      "[LOG 20200502-14:34:10] epoch: 11039 train-loss: 0.010557479742500517\n",
      "[LOG 20200502-14:34:10] epoch: 11040 train-loss: 0.010557478397256799\n",
      "[LOG 20200502-14:34:10] epoch: 11041 train-loss: 0.010557477776375081\n",
      "[LOG 20200502-14:34:10] epoch: 11042 train-loss: 0.010557476224170791\n",
      "[LOG 20200502-14:34:11] epoch: 11043 train-loss: 0.010557475603289075\n",
      "[LOG 20200502-14:34:11] epoch: 11044 train-loss: 0.010557475085887644\n",
      "[LOG 20200502-14:34:11] epoch: 11045 train-loss: 0.010557474258045355\n",
      "[LOG 20200502-14:34:11] epoch: 11046 train-loss: 0.010557473947604498\n",
      "[LOG 20200502-14:34:12] epoch: 11047 train-loss: 0.010557473740643926\n",
      "[LOG 20200502-14:34:12] epoch: 11048 train-loss: 0.010557473533683352\n",
      "[LOG 20200502-14:34:12] epoch: 11049 train-loss: 0.010557473947604498\n",
      "[LOG 20200502-14:34:12] epoch: 11050 train-loss: 0.010557473947604498\n",
      "[LOG 20200502-14:34:12] epoch: 11051 train-loss: 0.010557474568486214\n",
      "[LOG 20200502-14:34:13] epoch: 11052 train-loss: 0.01055747518936793\n",
      "[LOG 20200502-14:34:13] epoch: 11053 train-loss: 0.010557475913729932\n",
      "[LOG 20200502-14:34:13] epoch: 11054 train-loss: 0.010557477155493366\n",
      "[LOG 20200502-14:34:13] epoch: 11055 train-loss: 0.010557478190296225\n",
      "[LOG 20200502-14:34:14] epoch: 11056 train-loss: 0.010557479328579374\n",
      "[LOG 20200502-14:34:14] epoch: 11057 train-loss: 0.010557481087744236\n",
      "[LOG 20200502-14:34:14] epoch: 11058 train-loss: 0.010557482226027383\n",
      "[LOG 20200502-14:34:14] epoch: 11059 train-loss: 0.01055748388171196\n",
      "[LOG 20200502-14:34:14] epoch: 11060 train-loss: 0.010557485744357109\n",
      "[LOG 20200502-14:34:15] epoch: 11061 train-loss: 0.01055748781396283\n",
      "[LOG 20200502-14:34:15] epoch: 11062 train-loss: 0.010557489780088266\n",
      "[LOG 20200502-14:34:15] epoch: 11063 train-loss: 0.0105574917462137\n",
      "[LOG 20200502-14:34:15] epoch: 11064 train-loss: 0.01055749412626028\n",
      "[LOG 20200502-14:34:16] epoch: 11065 train-loss: 0.01055749598890543\n",
      "[LOG 20200502-14:34:16] epoch: 11066 train-loss: 0.010557497955030866\n",
      "[LOG 20200502-14:34:16] epoch: 11067 train-loss: 0.010557500542038016\n",
      "[LOG 20200502-14:34:16] epoch: 11068 train-loss: 0.010557502922084596\n",
      "[LOG 20200502-14:34:16] epoch: 11069 train-loss: 0.010557505095170604\n",
      "[LOG 20200502-14:34:17] epoch: 11070 train-loss: 0.010557506750855181\n",
      "[LOG 20200502-14:34:17] epoch: 11071 train-loss: 0.010557509027421474\n",
      "[LOG 20200502-14:34:17] epoch: 11072 train-loss: 0.010557511407468054\n",
      "[LOG 20200502-14:34:17] epoch: 11073 train-loss: 0.010557513684034348\n",
      "[LOG 20200502-14:34:18] epoch: 11074 train-loss: 0.010557515236238638\n",
      "[LOG 20200502-14:34:18] epoch: 11075 train-loss: 0.01055751782324579\n",
      "[LOG 20200502-14:34:18] epoch: 11076 train-loss: 0.010557519271969795\n",
      "[LOG 20200502-14:34:18] epoch: 11077 train-loss: 0.010557521031134658\n",
      "[LOG 20200502-14:34:19] epoch: 11078 train-loss: 0.010557522790299522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:34:19] epoch: 11079 train-loss: 0.010557524342503812\n",
      "[LOG 20200502-14:34:19] epoch: 11080 train-loss: 0.010557525791227818\n",
      "[LOG 20200502-14:34:19] epoch: 11081 train-loss: 0.010557527446912395\n",
      "[LOG 20200502-14:34:19] epoch: 11082 train-loss: 0.0105575288956364\n",
      "[LOG 20200502-14:34:20] epoch: 11083 train-loss: 0.010557530033919547\n",
      "[LOG 20200502-14:34:20] epoch: 11084 train-loss: 0.010557530861761835\n",
      "[LOG 20200502-14:34:20] epoch: 11085 train-loss: 0.010557531896564696\n",
      "[LOG 20200502-14:34:20] epoch: 11086 train-loss: 0.010557533034847843\n",
      "[LOG 20200502-14:34:21] epoch: 11087 train-loss: 0.010557533966170417\n",
      "[LOG 20200502-14:34:21] epoch: 11088 train-loss: 0.010557534587052133\n",
      "[LOG 20200502-14:34:21] epoch: 11089 train-loss: 0.010557535104453564\n",
      "[LOG 20200502-14:34:21] epoch: 11090 train-loss: 0.010557535311414136\n",
      "[LOG 20200502-14:34:21] epoch: 11091 train-loss: 0.010557536139256425\n",
      "[LOG 20200502-14:34:22] epoch: 11092 train-loss: 0.010557536967098713\n",
      "[LOG 20200502-14:34:22] epoch: 11093 train-loss: 0.010557536760138141\n",
      "[LOG 20200502-14:34:22] epoch: 11094 train-loss: 0.010557537174059285\n",
      "[LOG 20200502-14:34:22] epoch: 11095 train-loss: 0.010557537277539572\n",
      "[LOG 20200502-14:34:23] epoch: 11096 train-loss: 0.010557537174059285\n",
      "[LOG 20200502-14:34:23] epoch: 11097 train-loss: 0.010557536967098713\n",
      "[LOG 20200502-14:34:23] epoch: 11098 train-loss: 0.010557536656657854\n",
      "[LOG 20200502-14:34:23] epoch: 11099 train-loss: 0.01055753624273671\n",
      "[LOG 20200502-14:34:24] epoch: 11100 train-loss: 0.010557536139256425\n",
      "[LOG 20200502-14:34:24] epoch: 11101 train-loss: 0.010557535518374708\n",
      "[LOG 20200502-14:34:24] epoch: 11102 train-loss: 0.010557535000973277\n",
      "[LOG 20200502-14:34:24] epoch: 11103 train-loss: 0.010557534380091561\n",
      "[LOG 20200502-14:34:25] epoch: 11104 train-loss: 0.01055753386269013\n",
      "[LOG 20200502-14:34:25] epoch: 11105 train-loss: 0.010557532620926699\n",
      "[LOG 20200502-14:34:25] epoch: 11106 train-loss: 0.010557531689604124\n",
      "[LOG 20200502-14:34:25] epoch: 11107 train-loss: 0.010557530344360404\n",
      "[LOG 20200502-14:34:26] epoch: 11108 train-loss: 0.010557529619998403\n",
      "[LOG 20200502-14:34:26] epoch: 11109 train-loss: 0.010557527653872967\n",
      "[LOG 20200502-14:34:26] epoch: 11110 train-loss: 0.01055752651558982\n",
      "[LOG 20200502-14:34:26] epoch: 11111 train-loss: 0.010557525066865815\n",
      "[LOG 20200502-14:34:27] epoch: 11112 train-loss: 0.010557523514661524\n",
      "[LOG 20200502-14:34:27] epoch: 11113 train-loss: 0.010557522169417806\n",
      "[LOG 20200502-14:34:27] epoch: 11114 train-loss: 0.01055752020329237\n",
      "[LOG 20200502-14:34:27] epoch: 11115 train-loss: 0.010557518030206362\n",
      "[LOG 20200502-14:34:27] epoch: 11116 train-loss: 0.010557516581482358\n",
      "[LOG 20200502-14:34:28] epoch: 11117 train-loss: 0.010557514615356922\n",
      "[LOG 20200502-14:34:28] epoch: 11118 train-loss: 0.010557512752711773\n",
      "[LOG 20200502-14:34:28] epoch: 11119 train-loss: 0.010557510786586337\n",
      "[LOG 20200502-14:34:28] epoch: 11120 train-loss: 0.010557509027421474\n",
      "[LOG 20200502-14:34:29] epoch: 11121 train-loss: 0.010557507164776325\n",
      "[LOG 20200502-14:34:29] epoch: 11122 train-loss: 0.010557505612572035\n",
      "[LOG 20200502-14:34:29] epoch: 11123 train-loss: 0.010557504060367743\n",
      "[LOG 20200502-14:34:29] epoch: 11124 train-loss: 0.01055750230120288\n",
      "[LOG 20200502-14:34:30] epoch: 11125 train-loss: 0.010557500852478875\n",
      "[LOG 20200502-14:34:30] epoch: 11126 train-loss: 0.010557499714195728\n",
      "[LOG 20200502-14:34:30] epoch: 11127 train-loss: 0.010557498472432295\n",
      "[LOG 20200502-14:34:30] epoch: 11128 train-loss: 0.010557497334149148\n",
      "[LOG 20200502-14:34:31] epoch: 11129 train-loss: 0.010557497230668863\n",
      "[LOG 20200502-14:34:31] epoch: 11130 train-loss: 0.010557496402826574\n",
      "[LOG 20200502-14:34:31] epoch: 11131 train-loss: 0.010557495471503999\n",
      "[LOG 20200502-14:34:31] epoch: 11132 train-loss: 0.010557496195866002\n",
      "[LOG 20200502-14:34:32] epoch: 11133 train-loss: 0.010557495574984286\n",
      "[LOG 20200502-14:34:32] epoch: 11134 train-loss: 0.010557496299346289\n",
      "[LOG 20200502-14:34:32] epoch: 11135 train-loss: 0.010557496609787146\n",
      "[LOG 20200502-14:34:32] epoch: 11136 train-loss: 0.010557497748070292\n",
      "[LOG 20200502-14:34:32] epoch: 11137 train-loss: 0.010557498575912582\n",
      "[LOG 20200502-14:34:33] epoch: 11138 train-loss: 0.010557499817676015\n",
      "[LOG 20200502-14:34:33] epoch: 11139 train-loss: 0.010557501369880306\n",
      "[LOG 20200502-14:34:33] epoch: 11140 train-loss: 0.010557503025564883\n",
      "[LOG 20200502-14:34:33] epoch: 11141 train-loss: 0.010557504474288888\n",
      "[LOG 20200502-14:34:34] epoch: 11142 train-loss: 0.010557507164776325\n",
      "[LOG 20200502-14:34:34] epoch: 11143 train-loss: 0.010557508303059472\n",
      "[LOG 20200502-14:34:34] epoch: 11144 train-loss: 0.010557511200507482\n",
      "[LOG 20200502-14:34:34] epoch: 11145 train-loss: 0.010557513373593489\n",
      "[LOG 20200502-14:34:34] epoch: 11146 train-loss: 0.010557515753640069\n",
      "[LOG 20200502-14:34:35] epoch: 11147 train-loss: 0.010557518237166934\n",
      "[LOG 20200502-14:34:35] epoch: 11148 train-loss: 0.010557521445055803\n",
      "[LOG 20200502-14:34:35] epoch: 11149 train-loss: 0.010557523411181238\n",
      "[LOG 20200502-14:34:35] epoch: 11150 train-loss: 0.01055752651558982\n",
      "[LOG 20200502-14:34:36] epoch: 11151 train-loss: 0.010557529206077257\n",
      "[LOG 20200502-14:34:36] epoch: 11152 train-loss: 0.010557532000044981\n",
      "[LOG 20200502-14:34:36] epoch: 11153 train-loss: 0.010557534794012705\n",
      "[LOG 20200502-14:34:36] epoch: 11154 train-loss: 0.010557537691460716\n",
      "[LOG 20200502-14:34:36] epoch: 11155 train-loss: 0.01055754017498758\n",
      "[LOG 20200502-14:34:37] epoch: 11156 train-loss: 0.010557543175915876\n",
      "[LOG 20200502-14:34:37] epoch: 11157 train-loss: 0.010557546176844172\n",
      "[LOG 20200502-14:34:37] epoch: 11158 train-loss: 0.010557548039489321\n",
      "[LOG 20200502-14:34:37] epoch: 11159 train-loss: 0.010557550419535901\n",
      "[LOG 20200502-14:34:38] epoch: 11160 train-loss: 0.010557552696102195\n",
      "[LOG 20200502-14:34:38] epoch: 11161 train-loss: 0.010557554765707917\n",
      "[LOG 20200502-14:34:38] epoch: 11162 train-loss: 0.010557557456195354\n",
      "[LOG 20200502-14:34:38] epoch: 11163 train-loss: 0.01055755942232079\n",
      "[LOG 20200502-14:34:38] epoch: 11164 train-loss: 0.010557560871044794\n",
      "[LOG 20200502-14:34:39] epoch: 11165 train-loss: 0.010557562940650515\n",
      "[LOG 20200502-14:34:39] epoch: 11166 train-loss: 0.01055756469981538\n",
      "[LOG 20200502-14:34:39] epoch: 11167 train-loss: 0.010557566148539385\n",
      "[LOG 20200502-14:34:39] epoch: 11168 train-loss: 0.010557567286822531\n",
      "[LOG 20200502-14:34:40] epoch: 11169 train-loss: 0.01055756914946768\n",
      "[LOG 20200502-14:34:40] epoch: 11170 train-loss: 0.01055757018427054\n",
      "[LOG 20200502-14:34:40] epoch: 11171 train-loss: 0.010557571426033974\n",
      "[LOG 20200502-14:34:40] epoch: 11172 train-loss: 0.010557572460836835\n",
      "[LOG 20200502-14:34:41] epoch: 11173 train-loss: 0.010557573495639695\n",
      "[LOG 20200502-14:34:41] epoch: 11174 train-loss: 0.010557574323481984\n",
      "[LOG 20200502-14:34:41] epoch: 11175 train-loss: 0.01055757442696227\n",
      "[LOG 20200502-14:34:41] epoch: 11176 train-loss: 0.010557575565245416\n",
      "[LOG 20200502-14:34:41] epoch: 11177 train-loss: 0.010557575772205988\n",
      "[LOG 20200502-14:34:42] epoch: 11178 train-loss: 0.010557576289607419\n",
      "[LOG 20200502-14:34:42] epoch: 11179 train-loss: 0.010557576393087706\n",
      "[LOG 20200502-14:34:42] epoch: 11180 train-loss: 0.010557576910489135\n",
      "[LOG 20200502-14:34:42] epoch: 11181 train-loss: 0.010557576393087706\n",
      "[LOG 20200502-14:34:43] epoch: 11182 train-loss: 0.010557577013969421\n",
      "[LOG 20200502-14:34:43] epoch: 11183 train-loss: 0.010557577013969421\n",
      "[LOG 20200502-14:34:43] epoch: 11184 train-loss: 0.010557576600048277\n",
      "[LOG 20200502-14:34:43] epoch: 11185 train-loss: 0.010557576910489135\n",
      "[LOG 20200502-14:34:43] epoch: 11186 train-loss: 0.010557576393087706\n",
      "[LOG 20200502-14:34:44] epoch: 11187 train-loss: 0.01055757597916656\n",
      "[LOG 20200502-14:34:44] epoch: 11188 train-loss: 0.010557575461765131\n",
      "[LOG 20200502-14:34:44] epoch: 11189 train-loss: 0.010557574737403128\n",
      "[LOG 20200502-14:34:44] epoch: 11190 train-loss: 0.01055757411652141\n",
      "[LOG 20200502-14:34:45] epoch: 11191 train-loss: 0.010557573185198836\n",
      "[LOG 20200502-14:34:45] epoch: 11192 train-loss: 0.010557572150395976\n",
      "[LOG 20200502-14:34:45] epoch: 11193 train-loss: 0.010557570701671971\n",
      "[LOG 20200502-14:34:45] epoch: 11194 train-loss: 0.010557570080790255\n",
      "[LOG 20200502-14:34:46] epoch: 11195 train-loss: 0.010557568218145106\n",
      "[LOG 20200502-14:34:46] epoch: 11196 train-loss: 0.010557566665940814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:34:46] epoch: 11197 train-loss: 0.010557564803295665\n",
      "[LOG 20200502-14:34:46] epoch: 11198 train-loss: 0.010557563251091374\n",
      "[LOG 20200502-14:34:46] epoch: 11199 train-loss: 0.010557561388446225\n",
      "[LOG 20200502-14:34:47] epoch: 11200 train-loss: 0.01055755942232079\n",
      "[LOG 20200502-14:34:47] epoch: 11201 train-loss: 0.010557557352715068\n",
      "[LOG 20200502-14:34:47] epoch: 11202 train-loss: 0.010557555386589633\n",
      "[LOG 20200502-14:34:47] epoch: 11203 train-loss: 0.010557553006543053\n",
      "[LOG 20200502-14:34:48] epoch: 11204 train-loss: 0.010557551350858476\n",
      "[LOG 20200502-14:34:48] epoch: 11205 train-loss: 0.010557549074292183\n",
      "[LOG 20200502-14:34:48] epoch: 11206 train-loss: 0.010557547522087892\n",
      "[LOG 20200502-14:34:48] epoch: 11207 train-loss: 0.010557545245521598\n",
      "[LOG 20200502-14:34:48] epoch: 11208 train-loss: 0.010557543796797594\n",
      "[LOG 20200502-14:34:49] epoch: 11209 train-loss: 0.010557542141113017\n",
      "[LOG 20200502-14:34:49] epoch: 11210 train-loss: 0.01055754048542844\n",
      "[LOG 20200502-14:34:49] epoch: 11211 train-loss: 0.010557538933224149\n",
      "[LOG 20200502-14:34:49] epoch: 11212 train-loss: 0.010557537587980429\n",
      "[LOG 20200502-14:34:50] epoch: 11213 train-loss: 0.010557536553177569\n",
      "[LOG 20200502-14:34:50] epoch: 11214 train-loss: 0.010557535621854994\n",
      "[LOG 20200502-14:34:50] epoch: 11215 train-loss: 0.010557535518374708\n",
      "[LOG 20200502-14:34:50] epoch: 11216 train-loss: 0.01055753469053242\n",
      "[LOG 20200502-14:34:51] epoch: 11217 train-loss: 0.010557534483571848\n",
      "[LOG 20200502-14:34:51] epoch: 11218 train-loss: 0.010557535311414136\n",
      "[LOG 20200502-14:34:51] epoch: 11219 train-loss: 0.010557535311414136\n",
      "[LOG 20200502-14:34:51] epoch: 11220 train-loss: 0.010557536553177569\n",
      "[LOG 20200502-14:34:51] epoch: 11221 train-loss: 0.010557537484500144\n",
      "[LOG 20200502-14:34:52] epoch: 11222 train-loss: 0.01055753862278329\n",
      "[LOG 20200502-14:34:52] epoch: 11223 train-loss: 0.010557540588908725\n",
      "[LOG 20200502-14:34:52] epoch: 11224 train-loss: 0.010557542244593302\n",
      "[LOG 20200502-14:34:52] epoch: 11225 train-loss: 0.010557544003758166\n",
      "[LOG 20200502-14:34:53] epoch: 11226 train-loss: 0.010557546487285031\n",
      "[LOG 20200502-14:34:53] epoch: 11227 train-loss: 0.010557548453410467\n",
      "[LOG 20200502-14:34:53] epoch: 11228 train-loss: 0.010557551557819048\n",
      "[LOG 20200502-14:34:53] epoch: 11229 train-loss: 0.010557554351786772\n",
      "[LOG 20200502-14:34:54] epoch: 11230 train-loss: 0.010557557870116498\n",
      "[LOG 20200502-14:34:54] epoch: 11231 train-loss: 0.010557560871044794\n",
      "[LOG 20200502-14:34:54] epoch: 11232 train-loss: 0.010557564078933664\n",
      "[LOG 20200502-14:34:54] epoch: 11233 train-loss: 0.010557567597263388\n",
      "[LOG 20200502-14:34:55] epoch: 11234 train-loss: 0.010557571115593115\n",
      "[LOG 20200502-14:34:55] epoch: 11235 train-loss: 0.0105575749443637\n",
      "[LOG 20200502-14:34:55] epoch: 11236 train-loss: 0.01055757835921314\n",
      "[LOG 20200502-14:34:55] epoch: 11237 train-loss: 0.010557582084503438\n",
      "[LOG 20200502-14:34:56] epoch: 11238 train-loss: 0.010557585913274024\n",
      "[LOG 20200502-14:34:56] epoch: 11239 train-loss: 0.010557589328123463\n",
      "[LOG 20200502-14:34:56] epoch: 11240 train-loss: 0.010557592949933477\n",
      "[LOG 20200502-14:34:56] epoch: 11241 train-loss: 0.010557596364782916\n",
      "[LOG 20200502-14:34:56] epoch: 11242 train-loss: 0.010557599986592928\n",
      "[LOG 20200502-14:34:57] epoch: 11243 train-loss: 0.01055760340144237\n",
      "[LOG 20200502-14:34:57] epoch: 11244 train-loss: 0.010557606402370665\n",
      "[LOG 20200502-14:34:57] epoch: 11245 train-loss: 0.010557609506779246\n",
      "[LOG 20200502-14:34:57] epoch: 11246 train-loss: 0.010557612507707544\n",
      "[LOG 20200502-14:34:58] epoch: 11247 train-loss: 0.010557614991234409\n",
      "[LOG 20200502-14:34:58] epoch: 11248 train-loss: 0.010557618095642991\n",
      "[LOG 20200502-14:34:58] epoch: 11249 train-loss: 0.010557620579169856\n",
      "[LOG 20200502-14:34:58] epoch: 11250 train-loss: 0.010557622648775578\n",
      "[LOG 20200502-14:34:59] epoch: 11251 train-loss: 0.010557625442743301\n",
      "[LOG 20200502-14:34:59] epoch: 11252 train-loss: 0.01055762761582931\n",
      "[LOG 20200502-14:34:59] epoch: 11253 train-loss: 0.01055762968543503\n",
      "[LOG 20200502-14:34:59] epoch: 11254 train-loss: 0.010557631341119608\n",
      "[LOG 20200502-14:35:00] epoch: 11255 train-loss: 0.01055763310028447\n",
      "[LOG 20200502-14:35:00] epoch: 11256 train-loss: 0.010557634549008476\n",
      "[LOG 20200502-14:35:00] epoch: 11257 train-loss: 0.01055763651513391\n",
      "[LOG 20200502-14:35:00] epoch: 11258 train-loss: 0.010557637653417058\n",
      "[LOG 20200502-14:35:01] epoch: 11259 train-loss: 0.010557639205621349\n",
      "[LOG 20200502-14:35:01] epoch: 11260 train-loss: 0.010557640240424208\n",
      "[LOG 20200502-14:35:01] epoch: 11261 train-loss: 0.01055764127522707\n",
      "[LOG 20200502-14:35:01] epoch: 11262 train-loss: 0.010557642516990503\n",
      "[LOG 20200502-14:35:02] epoch: 11263 train-loss: 0.010557643241352506\n",
      "[LOG 20200502-14:35:02] epoch: 11264 train-loss: 0.010557643965714507\n",
      "[LOG 20200502-14:35:02] epoch: 11265 train-loss: 0.010557644793556796\n",
      "[LOG 20200502-14:35:02] epoch: 11266 train-loss: 0.010557646138800515\n",
      "[LOG 20200502-14:35:03] epoch: 11267 train-loss: 0.010557646345761087\n",
      "[LOG 20200502-14:35:03] epoch: 11268 train-loss: 0.01055764707012309\n",
      "[LOG 20200502-14:35:03] epoch: 11269 train-loss: 0.010557647484044233\n",
      "[LOG 20200502-14:35:03] epoch: 11270 train-loss: 0.010557647691004805\n",
      "[LOG 20200502-14:35:04] epoch: 11271 train-loss: 0.010557648001445664\n",
      "[LOG 20200502-14:35:04] epoch: 11272 train-loss: 0.01055764862232738\n",
      "[LOG 20200502-14:35:04] epoch: 11273 train-loss: 0.01055764862232738\n",
      "[LOG 20200502-14:35:04] epoch: 11274 train-loss: 0.010557648311886523\n",
      "[LOG 20200502-14:35:04] epoch: 11275 train-loss: 0.010557648208406236\n",
      "[LOG 20200502-14:35:05] epoch: 11276 train-loss: 0.010557647897965379\n",
      "[LOG 20200502-14:35:05] epoch: 11277 train-loss: 0.010557647484044233\n",
      "[LOG 20200502-14:35:05] epoch: 11278 train-loss: 0.010557647173603376\n",
      "[LOG 20200502-14:35:05] epoch: 11279 train-loss: 0.010557646242280802\n",
      "[LOG 20200502-14:35:06] epoch: 11280 train-loss: 0.010557645517918799\n",
      "[LOG 20200502-14:35:06] epoch: 11281 train-loss: 0.01055764417267508\n",
      "[LOG 20200502-14:35:06] epoch: 11282 train-loss: 0.010557643137872219\n",
      "[LOG 20200502-14:35:06] epoch: 11283 train-loss: 0.010557641482187642\n",
      "[LOG 20200502-14:35:07] epoch: 11284 train-loss: 0.010557640033463636\n",
      "[LOG 20200502-14:35:07] epoch: 11285 train-loss: 0.01055763837777906\n",
      "[LOG 20200502-14:35:07] epoch: 11286 train-loss: 0.010557636204693053\n",
      "[LOG 20200502-14:35:07] epoch: 11287 train-loss: 0.010557634859449334\n",
      "[LOG 20200502-14:35:08] epoch: 11288 train-loss: 0.010557632893323898\n",
      "[LOG 20200502-14:35:08] epoch: 11289 train-loss: 0.010557631134159036\n",
      "[LOG 20200502-14:35:08] epoch: 11290 train-loss: 0.010557628961073028\n",
      "[LOG 20200502-14:35:08] epoch: 11291 train-loss: 0.01055762678798702\n",
      "[LOG 20200502-14:35:08] epoch: 11292 train-loss: 0.010557624511420727\n",
      "[LOG 20200502-14:35:09] epoch: 11293 train-loss: 0.010557623062696721\n",
      "[LOG 20200502-14:35:09] epoch: 11294 train-loss: 0.010557620786130428\n",
      "[LOG 20200502-14:35:09] epoch: 11295 train-loss: 0.010557618923485279\n",
      "[LOG 20200502-14:35:09] epoch: 11296 train-loss: 0.010557616853879558\n",
      "[LOG 20200502-14:35:10] epoch: 11297 train-loss: 0.01055761550863584\n",
      "[LOG 20200502-14:35:10] epoch: 11298 train-loss: 0.010557614059911834\n",
      "[LOG 20200502-14:35:10] epoch: 11299 train-loss: 0.010557613025108973\n",
      "[LOG 20200502-14:35:10] epoch: 11300 train-loss: 0.01055761230074697\n",
      "[LOG 20200502-14:35:11] epoch: 11301 train-loss: 0.010557611990306113\n",
      "[LOG 20200502-14:35:11] epoch: 11302 train-loss: 0.010557611058983538\n",
      "[LOG 20200502-14:35:11] epoch: 11303 train-loss: 0.010557611058983538\n",
      "[LOG 20200502-14:35:11] epoch: 11304 train-loss: 0.01055761126594411\n",
      "[LOG 20200502-14:35:11] epoch: 11305 train-loss: 0.010557611679865254\n",
      "[LOG 20200502-14:35:12] epoch: 11306 train-loss: 0.010557612197266685\n",
      "[LOG 20200502-14:35:12] epoch: 11307 train-loss: 0.010557613439030118\n",
      "[LOG 20200502-14:35:12] epoch: 11308 train-loss: 0.010557615301675267\n",
      "[LOG 20200502-14:35:12] epoch: 11309 train-loss: 0.010557617474761274\n",
      "[LOG 20200502-14:35:13] epoch: 11310 train-loss: 0.010557619337406423\n",
      "[LOG 20200502-14:35:13] epoch: 11311 train-loss: 0.010557621717453003\n",
      "[LOG 20200502-14:35:13] epoch: 11312 train-loss: 0.010557624718381299\n",
      "[LOG 20200502-14:35:13] epoch: 11313 train-loss: 0.010557627408868737\n",
      "[LOG 20200502-14:35:14] epoch: 11314 train-loss: 0.010557630927198462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:35:14] epoch: 11315 train-loss: 0.010557634342047904\n",
      "[LOG 20200502-14:35:14] epoch: 11316 train-loss: 0.010557638481259346\n",
      "[LOG 20200502-14:35:14] epoch: 11317 train-loss: 0.010557642103069358\n",
      "[LOG 20200502-14:35:15] epoch: 11318 train-loss: 0.010557646242280802\n",
      "[LOG 20200502-14:35:15] epoch: 11319 train-loss: 0.010557651209334532\n",
      "[LOG 20200502-14:35:15] epoch: 11320 train-loss: 0.010557655348545976\n",
      "[LOG 20200502-14:35:15] epoch: 11321 train-loss: 0.01055765969471799\n",
      "[LOG 20200502-14:35:15] epoch: 11322 train-loss: 0.010557665075692866\n",
      "[LOG 20200502-14:35:16] epoch: 11323 train-loss: 0.010557669111424021\n",
      "[LOG 20200502-14:35:16] epoch: 11324 train-loss: 0.010557673768036895\n",
      "[LOG 20200502-14:35:16] epoch: 11325 train-loss: 0.010557678424649768\n",
      "[LOG 20200502-14:35:16] epoch: 11326 train-loss: 0.010557683598664071\n",
      "[LOG 20200502-14:35:17] epoch: 11327 train-loss: 0.010557687737875514\n",
      "[LOG 20200502-14:35:17] epoch: 11328 train-loss: 0.010557692497968674\n",
      "[LOG 20200502-14:35:17] epoch: 11329 train-loss: 0.010557696844140688\n",
      "[LOG 20200502-14:35:17] epoch: 11330 train-loss: 0.010557701190312704\n",
      "[LOG 20200502-14:35:18] epoch: 11331 train-loss: 0.010557705433004431\n",
      "[LOG 20200502-14:35:18] epoch: 11332 train-loss: 0.01055770915829473\n",
      "[LOG 20200502-14:35:18] epoch: 11333 train-loss: 0.010557713504466746\n",
      "[LOG 20200502-14:35:18] epoch: 11334 train-loss: 0.0105577168158359\n",
      "[LOG 20200502-14:35:19] epoch: 11335 train-loss: 0.010557720851567056\n",
      "[LOG 20200502-14:35:19] epoch: 11336 train-loss: 0.010557723955975639\n",
      "[LOG 20200502-14:35:19] epoch: 11337 train-loss: 0.010557727163864506\n",
      "[LOG 20200502-14:35:19] epoch: 11338 train-loss: 0.010557729854351945\n",
      "[LOG 20200502-14:35:19] epoch: 11339 train-loss: 0.010557732855280241\n",
      "[LOG 20200502-14:35:20] epoch: 11340 train-loss: 0.010557735442287393\n",
      "[LOG 20200502-14:35:20] epoch: 11341 train-loss: 0.010557738546695974\n",
      "[LOG 20200502-14:35:20] epoch: 11342 train-loss: 0.010557740409341123\n",
      "[LOG 20200502-14:35:20] epoch: 11343 train-loss: 0.010557742789387703\n",
      "[LOG 20200502-14:35:21] epoch: 11344 train-loss: 0.010557745272914568\n",
      "[LOG 20200502-14:35:21] epoch: 11345 train-loss: 0.010557747239040004\n",
      "[LOG 20200502-14:35:21] epoch: 11346 train-loss: 0.010557748894724581\n",
      "[LOG 20200502-14:35:21] epoch: 11347 train-loss: 0.01055775075736973\n",
      "[LOG 20200502-14:35:22] epoch: 11348 train-loss: 0.010557752516534593\n",
      "[LOG 20200502-14:35:22] epoch: 11349 train-loss: 0.010557754482660029\n",
      "[LOG 20200502-14:35:22] epoch: 11350 train-loss: 0.010557755620943176\n",
      "[LOG 20200502-14:35:22] epoch: 11351 train-loss: 0.010557757276627753\n",
      "[LOG 20200502-14:35:22] epoch: 11352 train-loss: 0.010557758828832043\n",
      "[LOG 20200502-14:35:23] epoch: 11353 train-loss: 0.010557760381036334\n",
      "[LOG 20200502-14:35:23] epoch: 11354 train-loss: 0.010557761726280054\n",
      "[LOG 20200502-14:35:23] epoch: 11355 train-loss: 0.010557763071523773\n",
      "[LOG 20200502-14:35:23] epoch: 11356 train-loss: 0.010557764002846347\n",
      "[LOG 20200502-14:35:24] epoch: 11357 train-loss: 0.010557765451570352\n",
      "[LOG 20200502-14:35:24] epoch: 11358 train-loss: 0.010557766382892927\n",
      "[LOG 20200502-14:35:24] epoch: 11359 train-loss: 0.01055776783161693\n",
      "[LOG 20200502-14:35:24] epoch: 11360 train-loss: 0.010557768245538076\n",
      "[LOG 20200502-14:35:25] epoch: 11361 train-loss: 0.010557769383821223\n",
      "[LOG 20200502-14:35:25] epoch: 11362 train-loss: 0.010557770108183226\n",
      "[LOG 20200502-14:35:25] epoch: 11363 train-loss: 0.010557770108183226\n",
      "[LOG 20200502-14:35:25] epoch: 11364 train-loss: 0.010557770832545228\n",
      "[LOG 20200502-14:35:26] epoch: 11365 train-loss: 0.010557771246466372\n",
      "[LOG 20200502-14:35:26] epoch: 11366 train-loss: 0.010557771349946657\n",
      "[LOG 20200502-14:35:26] epoch: 11367 train-loss: 0.010557771453426944\n",
      "[LOG 20200502-14:35:26] epoch: 11368 train-loss: 0.010557771142986085\n",
      "[LOG 20200502-14:35:27] epoch: 11369 train-loss: 0.0105577710395058\n",
      "[LOG 20200502-14:35:27] epoch: 11370 train-loss: 0.010557770315143798\n",
      "[LOG 20200502-14:35:27] epoch: 11371 train-loss: 0.010557769487301508\n",
      "[LOG 20200502-14:35:27] epoch: 11372 train-loss: 0.010557768245538076\n",
      "[LOG 20200502-14:35:28] epoch: 11373 train-loss: 0.010557767728136646\n",
      "[LOG 20200502-14:35:28] epoch: 11374 train-loss: 0.010557765865491496\n",
      "[LOG 20200502-14:35:28] epoch: 11375 train-loss: 0.010557764623728063\n",
      "[LOG 20200502-14:35:28] epoch: 11376 train-loss: 0.010557762761082914\n",
      "[LOG 20200502-14:35:29] epoch: 11377 train-loss: 0.010557761312358908\n",
      "[LOG 20200502-14:35:29] epoch: 11378 train-loss: 0.01055775944971376\n",
      "[LOG 20200502-14:35:29] epoch: 11379 train-loss: 0.010557757897509469\n",
      "[LOG 20200502-14:35:29] epoch: 11380 train-loss: 0.010557755827903748\n",
      "[LOG 20200502-14:35:30] epoch: 11381 train-loss: 0.010557753447857168\n",
      "[LOG 20200502-14:35:30] epoch: 11382 train-loss: 0.010557751067810588\n",
      "[LOG 20200502-14:35:30] epoch: 11383 train-loss: 0.010557749619086584\n",
      "[LOG 20200502-14:35:30] epoch: 11384 train-loss: 0.01055774734252029\n",
      "[LOG 20200502-14:35:30] epoch: 11385 train-loss: 0.01055774599727657\n",
      "[LOG 20200502-14:35:31] epoch: 11386 train-loss: 0.010557744548552565\n",
      "[LOG 20200502-14:35:31] epoch: 11387 train-loss: 0.010557742996348275\n",
      "[LOG 20200502-14:35:31] epoch: 11388 train-loss: 0.010557741858065128\n",
      "[LOG 20200502-14:35:31] epoch: 11389 train-loss: 0.010557741133703126\n",
      "[LOG 20200502-14:35:32] epoch: 11390 train-loss: 0.010557740202380551\n",
      "[LOG 20200502-14:35:32] epoch: 11391 train-loss: 0.010557739891939692\n",
      "[LOG 20200502-14:35:32] epoch: 11392 train-loss: 0.010557740305860838\n",
      "[LOG 20200502-14:35:32] epoch: 11393 train-loss: 0.010557740098900266\n",
      "[LOG 20200502-14:35:33] epoch: 11394 train-loss: 0.010557740823262267\n",
      "[LOG 20200502-14:35:33] epoch: 11395 train-loss: 0.010557742582427131\n",
      "[LOG 20200502-14:35:33] epoch: 11396 train-loss: 0.010557743824190564\n",
      "[LOG 20200502-14:35:33] epoch: 11397 train-loss: 0.010557745790315999\n",
      "[LOG 20200502-14:35:33] epoch: 11398 train-loss: 0.010557748066882292\n",
      "[LOG 20200502-14:35:34] epoch: 11399 train-loss: 0.010557751274771161\n",
      "[LOG 20200502-14:35:34] epoch: 11400 train-loss: 0.010557753861778311\n",
      "[LOG 20200502-14:35:34] epoch: 11401 train-loss: 0.010557757069667181\n",
      "[LOG 20200502-14:35:34] epoch: 11402 train-loss: 0.010557760587996907\n",
      "[LOG 20200502-14:35:35] epoch: 11403 train-loss: 0.01055776472720835\n",
      "[LOG 20200502-14:35:35] epoch: 11404 train-loss: 0.010557769073380364\n",
      "[LOG 20200502-14:35:35] epoch: 11405 train-loss: 0.010557773833473524\n",
      "[LOG 20200502-14:35:35] epoch: 11406 train-loss: 0.010557779110968113\n",
      "[LOG 20200502-14:35:35] epoch: 11407 train-loss: 0.010557783767580986\n",
      "[LOG 20200502-14:35:36] epoch: 11408 train-loss: 0.010557789045075575\n",
      "[LOG 20200502-14:35:36] epoch: 11409 train-loss: 0.010557794322570166\n",
      "[LOG 20200502-14:35:36] epoch: 11410 train-loss: 0.010557800531387329\n",
      "[LOG 20200502-14:35:36] epoch: 11411 train-loss: 0.010557805808881918\n",
      "[LOG 20200502-14:35:37] epoch: 11412 train-loss: 0.010557811810738511\n",
      "[LOG 20200502-14:35:37] epoch: 11413 train-loss: 0.010557817709114816\n",
      "[LOG 20200502-14:35:37] epoch: 11414 train-loss: 0.01055782288312912\n",
      "[LOG 20200502-14:35:37] epoch: 11415 train-loss: 0.010557828988465998\n",
      "[LOG 20200502-14:35:38] epoch: 11416 train-loss: 0.010557834886842303\n",
      "[LOG 20200502-14:35:38] epoch: 11417 train-loss: 0.010557840681738324\n",
      "[LOG 20200502-14:35:38] epoch: 11418 train-loss: 0.010557846062713198\n",
      "[LOG 20200502-14:35:38] epoch: 11419 train-loss: 0.010557851650648646\n",
      "[LOG 20200502-14:35:38] epoch: 11420 train-loss: 0.010557857549024953\n",
      "[LOG 20200502-14:35:39] epoch: 11421 train-loss: 0.010557862412598398\n",
      "[LOG 20200502-14:35:39] epoch: 11422 train-loss: 0.010557866965730986\n",
      "[LOG 20200502-14:35:39] epoch: 11423 train-loss: 0.010557872139745288\n",
      "[LOG 20200502-14:35:39] epoch: 11424 train-loss: 0.01055787679635816\n",
      "[LOG 20200502-14:35:40] epoch: 11425 train-loss: 0.010557880935569605\n",
      "[LOG 20200502-14:35:40] epoch: 11426 train-loss: 0.01055788497130076\n",
      "[LOG 20200502-14:35:40] epoch: 11427 train-loss: 0.010557888800071346\n",
      "[LOG 20200502-14:35:40] epoch: 11428 train-loss: 0.010557892835802503\n",
      "[LOG 20200502-14:35:40] epoch: 11429 train-loss: 0.010557896147171656\n",
      "[LOG 20200502-14:35:41] epoch: 11430 train-loss: 0.010557899872461954\n",
      "[LOG 20200502-14:35:41] epoch: 11431 train-loss: 0.010557902459469106\n",
      "[LOG 20200502-14:35:41] epoch: 11432 train-loss: 0.010557905977798833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:35:41] epoch: 11433 train-loss: 0.010557908978727128\n",
      "[LOG 20200502-14:35:42] epoch: 11434 train-loss: 0.01055791156573428\n",
      "[LOG 20200502-14:35:42] epoch: 11435 train-loss: 0.010557914256221719\n",
      "[LOG 20200502-14:35:42] epoch: 11436 train-loss: 0.010557916636268297\n",
      "[LOG 20200502-14:35:42] epoch: 11437 train-loss: 0.010557918705874018\n",
      "[LOG 20200502-14:35:42] epoch: 11438 train-loss: 0.010557920982440313\n",
      "[LOG 20200502-14:35:43] epoch: 11439 train-loss: 0.010557923569447465\n",
      "[LOG 20200502-14:35:43] epoch: 11440 train-loss: 0.010557925328612328\n",
      "[LOG 20200502-14:35:43] epoch: 11441 train-loss: 0.010557927398218049\n",
      "[LOG 20200502-14:35:43] epoch: 11442 train-loss: 0.010557929571304057\n",
      "[LOG 20200502-14:35:44] epoch: 11443 train-loss: 0.01055793184787035\n",
      "[LOG 20200502-14:35:44] epoch: 11444 train-loss: 0.010557933710515499\n",
      "[LOG 20200502-14:35:44] epoch: 11445 train-loss: 0.01055793578012122\n",
      "[LOG 20200502-14:35:44] epoch: 11446 train-loss: 0.010557937953207228\n",
      "[LOG 20200502-14:35:45] epoch: 11447 train-loss: 0.010557939608891806\n",
      "[LOG 20200502-14:35:45] epoch: 11448 train-loss: 0.010557941678497527\n",
      "[LOG 20200502-14:35:45] epoch: 11449 train-loss: 0.01055794395506382\n",
      "[LOG 20200502-14:35:45] epoch: 11450 train-loss: 0.010557945403787825\n",
      "[LOG 20200502-14:35:45] epoch: 11451 train-loss: 0.01055794736991326\n",
      "[LOG 20200502-14:35:46] epoch: 11452 train-loss: 0.010557949129078124\n",
      "[LOG 20200502-14:35:46] epoch: 11453 train-loss: 0.01055795026736127\n",
      "[LOG 20200502-14:35:46] epoch: 11454 train-loss: 0.01055795181956556\n",
      "[LOG 20200502-14:35:46] epoch: 11455 train-loss: 0.01055795316480928\n",
      "[LOG 20200502-14:35:47] epoch: 11456 train-loss: 0.010557954717013571\n",
      "[LOG 20200502-14:35:47] epoch: 11457 train-loss: 0.010557955234415002\n",
      "[LOG 20200502-14:35:47] epoch: 11458 train-loss: 0.010557956165737577\n",
      "[LOG 20200502-14:35:47] epoch: 11459 train-loss: 0.010557956993579865\n",
      "[LOG 20200502-14:35:48] epoch: 11460 train-loss: 0.010557957200540436\n",
      "[LOG 20200502-14:35:48] epoch: 11461 train-loss: 0.010557957510981295\n",
      "[LOG 20200502-14:35:48] epoch: 11462 train-loss: 0.010557957304020723\n",
      "[LOG 20200502-14:35:48] epoch: 11463 train-loss: 0.010557957510981295\n",
      "[LOG 20200502-14:35:49] epoch: 11464 train-loss: 0.010557956993579865\n",
      "[LOG 20200502-14:35:49] epoch: 11465 train-loss: 0.010557956269217862\n",
      "[LOG 20200502-14:35:49] epoch: 11466 train-loss: 0.010557955130934715\n",
      "[LOG 20200502-14:35:49] epoch: 11467 train-loss: 0.010557954406572713\n",
      "[LOG 20200502-14:35:50] epoch: 11468 train-loss: 0.010557952957848707\n",
      "[LOG 20200502-14:35:50] epoch: 11469 train-loss: 0.010557951509124704\n",
      "[LOG 20200502-14:35:50] epoch: 11470 train-loss: 0.010557950163880983\n",
      "[LOG 20200502-14:35:50] epoch: 11471 train-loss: 0.010557948404716121\n",
      "[LOG 20200502-14:35:50] epoch: 11472 train-loss: 0.010557946438590685\n",
      "[LOG 20200502-14:35:51] epoch: 11473 train-loss: 0.010557944782906108\n",
      "[LOG 20200502-14:35:51] epoch: 11474 train-loss: 0.01055794343766239\n",
      "[LOG 20200502-14:35:51] epoch: 11475 train-loss: 0.01055794105761581\n",
      "[LOG 20200502-14:35:51] epoch: 11476 train-loss: 0.010557939815852378\n",
      "[LOG 20200502-14:35:52] epoch: 11477 train-loss: 0.010557938263648085\n",
      "[LOG 20200502-14:35:52] epoch: 11478 train-loss: 0.010557936401002936\n",
      "[LOG 20200502-14:35:52] epoch: 11479 train-loss: 0.010557934848798646\n",
      "[LOG 20200502-14:35:52] epoch: 11480 train-loss: 0.010557933503554927\n",
      "[LOG 20200502-14:35:53] epoch: 11481 train-loss: 0.010557932986153496\n",
      "[LOG 20200502-14:35:53] epoch: 11482 train-loss: 0.010557932779192924\n",
      "[LOG 20200502-14:35:53] epoch: 11483 train-loss: 0.010557932261791494\n",
      "[LOG 20200502-14:35:53] epoch: 11484 train-loss: 0.010557932468752066\n",
      "[LOG 20200502-14:35:53] epoch: 11485 train-loss: 0.010557932779192924\n",
      "[LOG 20200502-14:35:54] epoch: 11486 train-loss: 0.010557933607035212\n",
      "[LOG 20200502-14:35:54] epoch: 11487 train-loss: 0.010557935159239504\n",
      "[LOG 20200502-14:35:54] epoch: 11488 train-loss: 0.010557936401002936\n",
      "[LOG 20200502-14:35:54] epoch: 11489 train-loss: 0.010557938574088944\n",
      "[LOG 20200502-14:35:55] epoch: 11490 train-loss: 0.010557941471536955\n",
      "[LOG 20200502-14:35:55] epoch: 11491 train-loss: 0.01055794395506382\n",
      "[LOG 20200502-14:35:55] epoch: 11492 train-loss: 0.01055794736991326\n",
      "[LOG 20200502-14:35:55] epoch: 11493 train-loss: 0.01055795130216413\n",
      "[LOG 20200502-14:35:55] epoch: 11494 train-loss: 0.01055795554485586\n",
      "[LOG 20200502-14:35:56] epoch: 11495 train-loss: 0.010557960201468732\n",
      "[LOG 20200502-14:35:56] epoch: 11496 train-loss: 0.010557965065042177\n",
      "[LOG 20200502-14:35:56] epoch: 11497 train-loss: 0.010557969825135337\n",
      "[LOG 20200502-14:35:56] epoch: 11498 train-loss: 0.010557975826991929\n",
      "[LOG 20200502-14:35:57] epoch: 11499 train-loss: 0.01055798182884852\n",
      "[LOG 20200502-14:35:57] epoch: 11500 train-loss: 0.010557987934185399\n",
      "[LOG 20200502-14:35:57] epoch: 11501 train-loss: 0.010557994039522277\n",
      "[LOG 20200502-14:35:57] epoch: 11502 train-loss: 0.0105580005587803\n",
      "[LOG 20200502-14:35:58] epoch: 11503 train-loss: 0.010558007181518607\n",
      "[LOG 20200502-14:35:58] epoch: 11504 train-loss: 0.01055801370077663\n",
      "[LOG 20200502-14:35:58] epoch: 11505 train-loss: 0.010558020944396654\n",
      "[LOG 20200502-14:35:58] epoch: 11506 train-loss: 0.010558027877575822\n",
      "[LOG 20200502-14:35:59] epoch: 11507 train-loss: 0.010558034707274701\n",
      "[LOG 20200502-14:35:59] epoch: 11508 train-loss: 0.010558041743934155\n",
      "[LOG 20200502-14:35:59] epoch: 11509 train-loss: 0.01055804867711332\n",
      "[LOG 20200502-14:35:59] epoch: 11510 train-loss: 0.010558055092891058\n",
      "[LOG 20200502-14:35:59] epoch: 11511 train-loss: 0.01055806212955051\n",
      "[LOG 20200502-14:36:00] epoch: 11512 train-loss: 0.010558068545328246\n",
      "[LOG 20200502-14:36:00] epoch: 11513 train-loss: 0.01055807475414541\n",
      "[LOG 20200502-14:36:00] epoch: 11514 train-loss: 0.010558080962962575\n",
      "[LOG 20200502-14:36:00] epoch: 11515 train-loss: 0.010558087068299452\n",
      "[LOG 20200502-14:36:01] epoch: 11516 train-loss: 0.010558092759715186\n",
      "[LOG 20200502-14:36:01] epoch: 11517 train-loss: 0.010558097830249203\n",
      "[LOG 20200502-14:36:01] epoch: 11518 train-loss: 0.010558103418184651\n",
      "[LOG 20200502-14:36:01] epoch: 11519 train-loss: 0.010558108385238383\n",
      "[LOG 20200502-14:36:02] epoch: 11520 train-loss: 0.0105581134557724\n",
      "[LOG 20200502-14:36:02] epoch: 11521 train-loss: 0.010558118112385273\n",
      "[LOG 20200502-14:36:02] epoch: 11522 train-loss: 0.010558122251596715\n",
      "[LOG 20200502-14:36:02] epoch: 11523 train-loss: 0.010558126183847586\n",
      "[LOG 20200502-14:36:03] epoch: 11524 train-loss: 0.010558130219578743\n",
      "[LOG 20200502-14:36:03] epoch: 11525 train-loss: 0.010558133841388755\n",
      "[LOG 20200502-14:36:03] epoch: 11526 train-loss: 0.01055813715275791\n",
      "[LOG 20200502-14:36:03] epoch: 11527 train-loss: 0.010558140360646777\n",
      "[LOG 20200502-14:36:04] epoch: 11528 train-loss: 0.010558143775496218\n",
      "[LOG 20200502-14:36:04] epoch: 11529 train-loss: 0.010558146672944227\n",
      "[LOG 20200502-14:36:04] epoch: 11530 train-loss: 0.010558149984313382\n",
      "[LOG 20200502-14:36:04] epoch: 11531 train-loss: 0.010558152571320534\n",
      "[LOG 20200502-14:36:05] epoch: 11532 train-loss: 0.010558154951367114\n",
      "[LOG 20200502-14:36:05] epoch: 11533 train-loss: 0.01055815764185455\n",
      "[LOG 20200502-14:36:05] epoch: 11534 train-loss: 0.010558160125381418\n",
      "[LOG 20200502-14:36:05] epoch: 11535 train-loss: 0.010558163229789998\n",
      "[LOG 20200502-14:36:06] epoch: 11536 train-loss: 0.010558166023757722\n",
      "[LOG 20200502-14:36:06] epoch: 11537 train-loss: 0.010558168507284589\n",
      "[LOG 20200502-14:36:06] epoch: 11538 train-loss: 0.01055817109429174\n",
      "[LOG 20200502-14:36:06] epoch: 11539 train-loss: 0.010558173577818606\n",
      "[LOG 20200502-14:36:07] epoch: 11540 train-loss: 0.010558176785707474\n",
      "[LOG 20200502-14:36:07] epoch: 11541 train-loss: 0.010558179269234339\n",
      "[LOG 20200502-14:36:07] epoch: 11542 train-loss: 0.01055818185624149\n",
      "[LOG 20200502-14:36:07] epoch: 11543 train-loss: 0.010558184339768358\n",
      "[LOG 20200502-14:36:08] epoch: 11544 train-loss: 0.010558186823295223\n",
      "[LOG 20200502-14:36:08] epoch: 11545 train-loss: 0.010558189927703805\n",
      "[LOG 20200502-14:36:08] epoch: 11546 train-loss: 0.01055819241123067\n",
      "[LOG 20200502-14:36:08] epoch: 11547 train-loss: 0.010558194894757535\n",
      "[LOG 20200502-14:36:08] epoch: 11548 train-loss: 0.010558197378284402\n",
      "[LOG 20200502-14:36:09] epoch: 11549 train-loss: 0.010558199654850695\n",
      "[LOG 20200502-14:36:09] epoch: 11550 train-loss: 0.010558201827936702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:36:09] epoch: 11551 train-loss: 0.010558203897542424\n",
      "[LOG 20200502-14:36:09] epoch: 11552 train-loss: 0.010558205242786143\n",
      "[LOG 20200502-14:36:10] epoch: 11553 train-loss: 0.010558207519352436\n",
      "[LOG 20200502-14:36:10] epoch: 11554 train-loss: 0.010558209071556727\n",
      "[LOG 20200502-14:36:10] epoch: 11555 train-loss: 0.010558210002879301\n",
      "[LOG 20200502-14:36:10] epoch: 11556 train-loss: 0.010558211244642735\n",
      "[LOG 20200502-14:36:11] epoch: 11557 train-loss: 0.010558211969004737\n",
      "[LOG 20200502-14:36:11] epoch: 11558 train-loss: 0.01055821269336674\n",
      "[LOG 20200502-14:36:11] epoch: 11559 train-loss: 0.010558212900327312\n",
      "[LOG 20200502-14:36:11] epoch: 11560 train-loss: 0.010558213831649886\n",
      "[LOG 20200502-14:36:11] epoch: 11561 train-loss: 0.010558213314248456\n",
      "[LOG 20200502-14:36:12] epoch: 11562 train-loss: 0.01055821321076817\n",
      "[LOG 20200502-14:36:12] epoch: 11563 train-loss: 0.010558212279445596\n",
      "[LOG 20200502-14:36:12] epoch: 11564 train-loss: 0.010558211555083593\n",
      "[LOG 20200502-14:36:12] epoch: 11565 train-loss: 0.01055821083072159\n",
      "[LOG 20200502-14:36:13] epoch: 11566 train-loss: 0.010558209381997585\n",
      "[LOG 20200502-14:36:13] epoch: 11567 train-loss: 0.010558208347194724\n",
      "[LOG 20200502-14:36:13] epoch: 11568 train-loss: 0.010558207208911577\n",
      "[LOG 20200502-14:36:13] epoch: 11569 train-loss: 0.010558205242786143\n",
      "[LOG 20200502-14:36:14] epoch: 11570 train-loss: 0.01055820369058185\n",
      "[LOG 20200502-14:36:14] epoch: 11571 train-loss: 0.010558202241857847\n",
      "[LOG 20200502-14:36:14] epoch: 11572 train-loss: 0.01055820058617327\n",
      "[LOG 20200502-14:36:14] epoch: 11573 train-loss: 0.010558199240929551\n",
      "[LOG 20200502-14:36:15] epoch: 11574 train-loss: 0.01055819768872526\n",
      "[LOG 20200502-14:36:15] epoch: 11575 train-loss: 0.0105581966539224\n",
      "[LOG 20200502-14:36:15] epoch: 11576 train-loss: 0.010558195929560397\n",
      "[LOG 20200502-14:36:15] epoch: 11577 train-loss: 0.010558194894757535\n",
      "[LOG 20200502-14:36:16] epoch: 11578 train-loss: 0.010558194066915248\n",
      "[LOG 20200502-14:36:16] epoch: 11579 train-loss: 0.010558194584316678\n",
      "[LOG 20200502-14:36:16] epoch: 11580 train-loss: 0.01055819479127725\n",
      "[LOG 20200502-14:36:16] epoch: 11581 train-loss: 0.010558194894757535\n",
      "[LOG 20200502-14:36:16] epoch: 11582 train-loss: 0.010558196136520969\n",
      "[LOG 20200502-14:36:17] epoch: 11583 train-loss: 0.010558197378284402\n",
      "[LOG 20200502-14:36:17] epoch: 11584 train-loss: 0.010558199033968978\n",
      "[LOG 20200502-14:36:17] epoch: 11585 train-loss: 0.010558201827936702\n",
      "[LOG 20200502-14:36:17] epoch: 11586 train-loss: 0.010558203897542424\n",
      "[LOG 20200502-14:36:18] epoch: 11587 train-loss: 0.010558207622832723\n",
      "[LOG 20200502-14:36:18] epoch: 11588 train-loss: 0.010558210727241304\n",
      "[LOG 20200502-14:36:18] epoch: 11589 train-loss: 0.010558214969933033\n",
      "[LOG 20200502-14:36:18] epoch: 11590 train-loss: 0.010558219316105047\n",
      "[LOG 20200502-14:36:19] epoch: 11591 train-loss: 0.01055822428315878\n",
      "[LOG 20200502-14:36:19] epoch: 11592 train-loss: 0.01055822976761394\n",
      "[LOG 20200502-14:36:19] epoch: 11593 train-loss: 0.010558235252069103\n",
      "[LOG 20200502-14:36:19] epoch: 11594 train-loss: 0.010558241357405981\n",
      "[LOG 20200502-14:36:19] epoch: 11595 train-loss: 0.010558248083624575\n",
      "[LOG 20200502-14:36:20] epoch: 11596 train-loss: 0.010558254602882598\n",
      "[LOG 20200502-14:36:20] epoch: 11597 train-loss: 0.010558261846502623\n",
      "[LOG 20200502-14:36:20] epoch: 11598 train-loss: 0.01055826898664236\n",
      "[LOG 20200502-14:36:21] epoch: 11599 train-loss: 0.010558276540703244\n",
      "[LOG 20200502-14:36:21] epoch: 11600 train-loss: 0.010558284198244413\n",
      "[LOG 20200502-14:36:21] epoch: 11601 train-loss: 0.010558291959265867\n",
      "[LOG 20200502-14:36:21] epoch: 11602 train-loss: 0.010558299409846464\n",
      "[LOG 20200502-14:36:22] epoch: 11603 train-loss: 0.010558307584789064\n",
      "[LOG 20200502-14:36:22] epoch: 11604 train-loss: 0.010558315552771091\n",
      "[LOG 20200502-14:36:22] epoch: 11605 train-loss: 0.010558323313792547\n",
      "[LOG 20200502-14:36:22] epoch: 11606 train-loss: 0.010558331281774573\n",
      "[LOG 20200502-14:36:23] epoch: 11607 train-loss: 0.010558339042796029\n",
      "[LOG 20200502-14:36:23] epoch: 11608 train-loss: 0.010558346803817484\n",
      "[LOG 20200502-14:36:23] epoch: 11609 train-loss: 0.010558354254398081\n",
      "[LOG 20200502-14:36:23] epoch: 11610 train-loss: 0.010558361291057534\n",
      "[LOG 20200502-14:36:23] epoch: 11611 train-loss: 0.010558368431197273\n",
      "[LOG 20200502-14:36:24] epoch: 11612 train-loss: 0.010558375985258155\n",
      "[LOG 20200502-14:36:24] epoch: 11613 train-loss: 0.01055838219407532\n",
      "[LOG 20200502-14:36:24] epoch: 11614 train-loss: 0.010558388195931911\n",
      "[LOG 20200502-14:36:24] epoch: 11615 train-loss: 0.010558394508229362\n",
      "[LOG 20200502-14:36:25] epoch: 11616 train-loss: 0.010558400820526812\n",
      "[LOG 20200502-14:36:25] epoch: 11617 train-loss: 0.010558405994541116\n",
      "[LOG 20200502-14:36:25] epoch: 11618 train-loss: 0.010558411685956849\n",
      "[LOG 20200502-14:36:25] epoch: 11619 train-loss: 0.010558416549530294\n",
      "[LOG 20200502-14:36:25] epoch: 11620 train-loss: 0.010558421309623454\n",
      "[LOG 20200502-14:36:26] epoch: 11621 train-loss: 0.010558426173196899\n",
      "[LOG 20200502-14:36:26] epoch: 11622 train-loss: 0.010558430001967482\n",
      "[LOG 20200502-14:36:26] epoch: 11623 train-loss: 0.01055843403769864\n",
      "[LOG 20200502-14:36:26] epoch: 11624 train-loss: 0.01055843796994951\n",
      "[LOG 20200502-14:36:27] epoch: 11625 train-loss: 0.010558441798720095\n",
      "[LOG 20200502-14:36:27] epoch: 11626 train-loss: 0.010558445317049822\n",
      "[LOG 20200502-14:36:27] epoch: 11627 train-loss: 0.01055844852493869\n",
      "[LOG 20200502-14:36:27] epoch: 11628 train-loss: 0.010558451836307844\n",
      "[LOG 20200502-14:36:28] epoch: 11629 train-loss: 0.010558455561598143\n",
      "[LOG 20200502-14:36:28] epoch: 11630 train-loss: 0.01055845876948701\n",
      "[LOG 20200502-14:36:28] epoch: 11631 train-loss: 0.010558461770415306\n",
      "[LOG 20200502-14:36:28] epoch: 11632 train-loss: 0.010558464874823889\n",
      "[LOG 20200502-14:36:29] epoch: 11633 train-loss: 0.010558467565311326\n",
      "[LOG 20200502-14:36:29] epoch: 11634 train-loss: 0.01055847087668048\n",
      "[LOG 20200502-14:36:29] epoch: 11635 train-loss: 0.010558473360207345\n",
      "[LOG 20200502-14:36:29] epoch: 11636 train-loss: 0.010558477085497644\n",
      "[LOG 20200502-14:36:30] epoch: 11637 train-loss: 0.010558480500347085\n",
      "[LOG 20200502-14:36:30] epoch: 11638 train-loss: 0.010558483190834522\n",
      "[LOG 20200502-14:36:30] epoch: 11639 train-loss: 0.010558487123085393\n",
      "[LOG 20200502-14:36:30] epoch: 11640 train-loss: 0.01055849033097426\n",
      "[LOG 20200502-14:36:30] epoch: 11641 train-loss: 0.01055849426322513\n",
      "[LOG 20200502-14:36:31] epoch: 11642 train-loss: 0.010558496850232283\n",
      "[LOG 20200502-14:36:31] epoch: 11643 train-loss: 0.01055850036856201\n",
      "[LOG 20200502-14:36:31] epoch: 11644 train-loss: 0.010558504093852308\n",
      "[LOG 20200502-14:36:31] epoch: 11645 train-loss: 0.010558507301741175\n",
      "[LOG 20200502-14:36:32] epoch: 11646 train-loss: 0.01055851061311033\n",
      "[LOG 20200502-14:36:32] epoch: 11647 train-loss: 0.010558513924479485\n",
      "[LOG 20200502-14:36:32] epoch: 11648 train-loss: 0.010558517546289496\n",
      "[LOG 20200502-14:36:32] epoch: 11649 train-loss: 0.010558520443737507\n",
      "[LOG 20200502-14:36:33] epoch: 11650 train-loss: 0.010558523134224944\n",
      "[LOG 20200502-14:36:33] epoch: 11651 train-loss: 0.010558526342113813\n",
      "[LOG 20200502-14:36:33] epoch: 11652 train-loss: 0.010558529136081537\n",
      "[LOG 20200502-14:36:33] epoch: 11653 train-loss: 0.010558531516128115\n",
      "[LOG 20200502-14:36:34] epoch: 11654 train-loss: 0.010558533999654982\n",
      "[LOG 20200502-14:36:34] epoch: 11655 train-loss: 0.010558535862300131\n",
      "[LOG 20200502-14:36:34] epoch: 11656 train-loss: 0.010558538035386138\n",
      "[LOG 20200502-14:36:34] epoch: 11657 train-loss: 0.010558539794551002\n",
      "[LOG 20200502-14:36:35] epoch: 11658 train-loss: 0.010558541036314435\n",
      "[LOG 20200502-14:36:35] epoch: 11659 train-loss: 0.010558542381558154\n",
      "[LOG 20200502-14:36:35] epoch: 11660 train-loss: 0.010558543312880728\n",
      "[LOG 20200502-14:36:35] epoch: 11661 train-loss: 0.010558543312880728\n",
      "[LOG 20200502-14:36:35] epoch: 11662 train-loss: 0.010558544140723016\n",
      "[LOG 20200502-14:36:36] epoch: 11663 train-loss: 0.010558544451163875\n",
      "[LOG 20200502-14:36:36] epoch: 11664 train-loss: 0.010558544451163875\n",
      "[LOG 20200502-14:36:36] epoch: 11665 train-loss: 0.010558543623321585\n",
      "[LOG 20200502-14:36:36] epoch: 11666 train-loss: 0.010558543209400442\n",
      "[LOG 20200502-14:36:37] epoch: 11667 train-loss: 0.010558542381558154\n",
      "[LOG 20200502-14:36:37] epoch: 11668 train-loss: 0.01055854145023558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:36:37] epoch: 11669 train-loss: 0.010558539794551002\n",
      "[LOG 20200502-14:36:37] epoch: 11670 train-loss: 0.010558539173669286\n",
      "[LOG 20200502-14:36:37] epoch: 11671 train-loss: 0.010558537311024137\n",
      "[LOG 20200502-14:36:38] epoch: 11672 train-loss: 0.010558536276221275\n",
      "[LOG 20200502-14:36:38] epoch: 11673 train-loss: 0.01055853482749727\n",
      "[LOG 20200502-14:36:38] epoch: 11674 train-loss: 0.010558533689214123\n",
      "[LOG 20200502-14:36:38] epoch: 11675 train-loss: 0.010558532757891549\n",
      "[LOG 20200502-14:36:39] epoch: 11676 train-loss: 0.010558531309167543\n",
      "[LOG 20200502-14:36:39] epoch: 11677 train-loss: 0.010558531309167543\n",
      "[LOG 20200502-14:36:39] epoch: 11678 train-loss: 0.010558530170884397\n",
      "[LOG 20200502-14:36:39] epoch: 11679 train-loss: 0.010558530274364684\n",
      "[LOG 20200502-14:36:40] epoch: 11680 train-loss: 0.010558530067404112\n",
      "[LOG 20200502-14:36:40] epoch: 11681 train-loss: 0.01055853058480554\n",
      "[LOG 20200502-14:36:40] epoch: 11682 train-loss: 0.010558531309167543\n",
      "[LOG 20200502-14:36:40] epoch: 11683 train-loss: 0.010558532757891549\n",
      "[LOG 20200502-14:36:40] epoch: 11684 train-loss: 0.010558534413576126\n",
      "[LOG 20200502-14:36:41] epoch: 11685 train-loss: 0.010558536069260703\n",
      "[LOG 20200502-14:36:41] epoch: 11686 train-loss: 0.010558538656267855\n",
      "[LOG 20200502-14:36:41] epoch: 11687 train-loss: 0.010558542071117295\n",
      "[LOG 20200502-14:36:41] epoch: 11688 train-loss: 0.010558545485966735\n",
      "[LOG 20200502-14:36:42] epoch: 11689 train-loss: 0.01055854931473732\n",
      "[LOG 20200502-14:36:42] epoch: 11690 train-loss: 0.010558553867869906\n",
      "[LOG 20200502-14:36:42] epoch: 11691 train-loss: 0.010558558834923638\n",
      "[LOG 20200502-14:36:42] epoch: 11692 train-loss: 0.010558564422859086\n",
      "[LOG 20200502-14:36:43] epoch: 11693 train-loss: 0.01055857011427482\n",
      "[LOG 20200502-14:36:43] epoch: 11694 train-loss: 0.010558576219611697\n",
      "[LOG 20200502-14:36:43] epoch: 11695 train-loss: 0.010558582635389434\n",
      "[LOG 20200502-14:36:43] epoch: 11696 train-loss: 0.010558590189450316\n",
      "[LOG 20200502-14:36:44] epoch: 11697 train-loss: 0.010558597019149197\n",
      "[LOG 20200502-14:36:44] epoch: 11698 train-loss: 0.010558604676690366\n",
      "[LOG 20200502-14:36:44] epoch: 11699 train-loss: 0.01055861274815268\n",
      "[LOG 20200502-14:36:44] epoch: 11700 train-loss: 0.010558620819614993\n",
      "[LOG 20200502-14:36:45] epoch: 11701 train-loss: 0.010558628994557593\n",
      "[LOG 20200502-14:36:45] epoch: 11702 train-loss: 0.01055863830778334\n",
      "[LOG 20200502-14:36:45] epoch: 11703 train-loss: 0.01055864617228508\n",
      "[LOG 20200502-14:36:45] epoch: 11704 train-loss: 0.01055865486462911\n",
      "[LOG 20200502-14:36:45] epoch: 11705 train-loss: 0.01055866376393371\n",
      "[LOG 20200502-14:36:46] epoch: 11706 train-loss: 0.010558672559758028\n",
      "[LOG 20200502-14:36:46] epoch: 11707 train-loss: 0.010558681459062629\n",
      "[LOG 20200502-14:36:46] epoch: 11708 train-loss: 0.0105586898409658\n",
      "[LOG 20200502-14:36:46] epoch: 11709 train-loss: 0.01055869853330983\n",
      "[LOG 20200502-14:36:47] epoch: 11710 train-loss: 0.01055870722565386\n",
      "[LOG 20200502-14:36:47] epoch: 11711 train-loss: 0.010558715297116173\n",
      "[LOG 20200502-14:36:47] epoch: 11712 train-loss: 0.010558723161617914\n",
      "[LOG 20200502-14:36:47] epoch: 11713 train-loss: 0.010558731129599942\n",
      "[LOG 20200502-14:36:48] epoch: 11714 train-loss: 0.010558738994101683\n",
      "[LOG 20200502-14:36:48] epoch: 11715 train-loss: 0.010558746651642852\n",
      "[LOG 20200502-14:36:48] epoch: 11716 train-loss: 0.010558753895262877\n",
      "[LOG 20200502-14:36:48] epoch: 11717 train-loss: 0.01055876062148147\n",
      "[LOG 20200502-14:36:48] epoch: 11718 train-loss: 0.01055876672681835\n",
      "[LOG 20200502-14:36:49] epoch: 11719 train-loss: 0.010558773142596086\n",
      "[LOG 20200502-14:36:49] epoch: 11720 train-loss: 0.010558779247932963\n",
      "[LOG 20200502-14:36:49] epoch: 11721 train-loss: 0.010558785042828985\n",
      "[LOG 20200502-14:36:49] epoch: 11722 train-loss: 0.010558790527284145\n",
      "[LOG 20200502-14:36:50] epoch: 11723 train-loss: 0.010558795183897018\n",
      "[LOG 20200502-14:36:50] epoch: 11724 train-loss: 0.010558800461391607\n",
      "[LOG 20200502-14:36:50] epoch: 11725 train-loss: 0.010558804807563623\n",
      "[LOG 20200502-14:36:50] epoch: 11726 train-loss: 0.01055880936069621\n",
      "[LOG 20200502-14:36:51] epoch: 11727 train-loss: 0.010558813085986508\n",
      "[LOG 20200502-14:36:51] epoch: 11728 train-loss: 0.010558817742599381\n",
      "[LOG 20200502-14:36:51] epoch: 11729 train-loss: 0.010558821053968536\n",
      "[LOG 20200502-14:36:51] epoch: 11730 train-loss: 0.01055882488273912\n",
      "[LOG 20200502-14:36:51] epoch: 11731 train-loss: 0.010558828090627989\n",
      "[LOG 20200502-14:36:52] epoch: 11732 train-loss: 0.010558831401997142\n",
      "[LOG 20200502-14:36:52] epoch: 11733 train-loss: 0.010558835334248014\n",
      "[LOG 20200502-14:36:52] epoch: 11734 train-loss: 0.010558838645617167\n",
      "[LOG 20200502-14:36:52] epoch: 11735 train-loss: 0.010558841853506036\n",
      "[LOG 20200502-14:36:53] epoch: 11736 train-loss: 0.010558845578796335\n",
      "[LOG 20200502-14:36:53] epoch: 11737 train-loss: 0.01055884857972463\n",
      "[LOG 20200502-14:36:53] epoch: 11738 train-loss: 0.010558852305014929\n",
      "[LOG 20200502-14:36:53] epoch: 11739 train-loss: 0.010558856133785512\n",
      "[LOG 20200502-14:36:54] epoch: 11740 train-loss: 0.010558859755595526\n",
      "[LOG 20200502-14:36:54] epoch: 11741 train-loss: 0.01055886358436611\n",
      "[LOG 20200502-14:36:54] epoch: 11742 train-loss: 0.010558867723577552\n",
      "[LOG 20200502-14:36:54] epoch: 11743 train-loss: 0.010558870931466421\n",
      "[LOG 20200502-14:36:55] epoch: 11744 train-loss: 0.010558874863717291\n",
      "[LOG 20200502-14:36:55] epoch: 11745 train-loss: 0.010558878899448447\n",
      "[LOG 20200502-14:36:55] epoch: 11746 train-loss: 0.010558883038659891\n",
      "[LOG 20200502-14:36:55] epoch: 11747 train-loss: 0.01055888676395019\n",
      "[LOG 20200502-14:36:56] epoch: 11748 train-loss: 0.01055889121360249\n",
      "[LOG 20200502-14:36:56] epoch: 11749 train-loss: 0.010558895766735077\n",
      "[LOG 20200502-14:36:56] epoch: 11750 train-loss: 0.010558899388545089\n",
      "[LOG 20200502-14:36:56] epoch: 11751 train-loss: 0.01055890332079596\n",
      "[LOG 20200502-14:36:56] epoch: 11752 train-loss: 0.010558906942605972\n",
      "[LOG 20200502-14:36:57] epoch: 11753 train-loss: 0.010558911702699132\n",
      "[LOG 20200502-14:36:57] epoch: 11754 train-loss: 0.010558915117548572\n",
      "[LOG 20200502-14:36:57] epoch: 11755 train-loss: 0.010558919360240301\n",
      "[LOG 20200502-14:36:57] epoch: 11756 train-loss: 0.010558922464648882\n",
      "[LOG 20200502-14:36:58] epoch: 11757 train-loss: 0.010558926396899752\n",
      "[LOG 20200502-14:36:58] epoch: 11758 train-loss: 0.010558929294347763\n",
      "[LOG 20200502-14:36:58] epoch: 11759 train-loss: 0.010558932088315487\n",
      "[LOG 20200502-14:36:58] epoch: 11760 train-loss: 0.010558935606645213\n",
      "[LOG 20200502-14:36:58] epoch: 11761 train-loss: 0.010558937986691793\n",
      "[LOG 20200502-14:36:59] epoch: 11762 train-loss: 0.0105589401597778\n",
      "[LOG 20200502-14:36:59] epoch: 11763 train-loss: 0.01055894222938352\n",
      "[LOG 20200502-14:36:59] epoch: 11764 train-loss: 0.010558944195508957\n",
      "[LOG 20200502-14:36:59] epoch: 11765 train-loss: 0.01055894543727239\n",
      "[LOG 20200502-14:37:00] epoch: 11766 train-loss: 0.01055894647207525\n",
      "[LOG 20200502-14:37:00] epoch: 11767 train-loss: 0.010558947817318969\n",
      "[LOG 20200502-14:37:00] epoch: 11768 train-loss: 0.0105589483347204\n",
      "[LOG 20200502-14:37:00] epoch: 11769 train-loss: 0.010558948231240114\n",
      "[LOG 20200502-14:37:01] epoch: 11770 train-loss: 0.01055894885212183\n",
      "[LOG 20200502-14:37:01] epoch: 11771 train-loss: 0.010558948748641543\n",
      "[LOG 20200502-14:37:01] epoch: 11772 train-loss: 0.010558948231240114\n",
      "[LOG 20200502-14:37:01] epoch: 11773 train-loss: 0.010558947713838683\n",
      "[LOG 20200502-14:37:02] epoch: 11774 train-loss: 0.010558947092956968\n",
      "[LOG 20200502-14:37:02] epoch: 11775 train-loss: 0.01055894595467382\n",
      "[LOG 20200502-14:37:02] epoch: 11776 train-loss: 0.010558945230311818\n",
      "[LOG 20200502-14:37:02] epoch: 11777 train-loss: 0.010558944195508957\n",
      "[LOG 20200502-14:37:02] epoch: 11778 train-loss: 0.010558942953745524\n",
      "[LOG 20200502-14:37:03] epoch: 11779 train-loss: 0.010558941711982092\n",
      "[LOG 20200502-14:37:03] epoch: 11780 train-loss: 0.010558940366738372\n",
      "[LOG 20200502-14:37:03] epoch: 11781 train-loss: 0.010558939331935512\n",
      "[LOG 20200502-14:37:03] epoch: 11782 train-loss: 0.010558939021494653\n",
      "[LOG 20200502-14:37:04] epoch: 11783 train-loss: 0.010558938711053796\n",
      "[LOG 20200502-14:37:04] epoch: 11784 train-loss: 0.01055893829713265\n",
      "[LOG 20200502-14:37:04] epoch: 11785 train-loss: 0.010558938193652365\n",
      "[LOG 20200502-14:37:04] epoch: 11786 train-loss: 0.010558938400612937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:37:05] epoch: 11787 train-loss: 0.010558939021494653\n",
      "[LOG 20200502-14:37:05] epoch: 11788 train-loss: 0.010558940263258087\n",
      "[LOG 20200502-14:37:05] epoch: 11789 train-loss: 0.010558941091100374\n",
      "[LOG 20200502-14:37:05] epoch: 11790 train-loss: 0.010558942850265238\n",
      "[LOG 20200502-14:37:05] epoch: 11791 train-loss: 0.010558945333792103\n",
      "[LOG 20200502-14:37:06] epoch: 11792 train-loss: 0.010558948127759827\n",
      "[LOG 20200502-14:37:06] epoch: 11793 train-loss: 0.010558951542609267\n",
      "[LOG 20200502-14:37:06] epoch: 11794 train-loss: 0.010558955267899565\n",
      "[LOG 20200502-14:37:06] epoch: 11795 train-loss: 0.010558959200150438\n",
      "[LOG 20200502-14:37:07] epoch: 11796 train-loss: 0.010558964270684455\n",
      "[LOG 20200502-14:37:07] epoch: 11797 train-loss: 0.01055896965165933\n",
      "[LOG 20200502-14:37:07] epoch: 11798 train-loss: 0.010558975136114491\n",
      "[LOG 20200502-14:37:07] epoch: 11799 train-loss: 0.010558981137971083\n",
      "[LOG 20200502-14:37:08] epoch: 11800 train-loss: 0.010558987346788248\n",
      "[LOG 20200502-14:37:08] epoch: 11801 train-loss: 0.010558994590408273\n",
      "[LOG 20200502-14:37:08] epoch: 11802 train-loss: 0.010559001834028296\n",
      "[LOG 20200502-14:37:08] epoch: 11803 train-loss: 0.01055900938808918\n",
      "[LOG 20200502-14:37:09] epoch: 11804 train-loss: 0.010559016838669777\n",
      "[LOG 20200502-14:37:09] epoch: 11805 train-loss: 0.01055902573797438\n",
      "[LOG 20200502-14:37:09] epoch: 11806 train-loss: 0.01055903411987755\n",
      "[LOG 20200502-14:37:09] epoch: 11807 train-loss: 0.010559042915701866\n",
      "[LOG 20200502-14:37:09] epoch: 11808 train-loss: 0.010559052125447325\n",
      "[LOG 20200502-14:37:10] epoch: 11809 train-loss: 0.010559061024751928\n",
      "[LOG 20200502-14:37:10] epoch: 11810 train-loss: 0.010559070337977674\n",
      "[LOG 20200502-14:37:10] epoch: 11811 train-loss: 0.010559079547723135\n",
      "[LOG 20200502-14:37:10] epoch: 11812 train-loss: 0.01055908865398831\n",
      "[LOG 20200502-14:37:11] epoch: 11813 train-loss: 0.0105590983811352\n",
      "[LOG 20200502-14:37:11] epoch: 11814 train-loss: 0.010559108004801802\n",
      "[LOG 20200502-14:37:11] epoch: 11815 train-loss: 0.010559117318027549\n",
      "[LOG 20200502-14:37:11] epoch: 11816 train-loss: 0.010559125803411007\n",
      "[LOG 20200502-14:37:11] epoch: 11817 train-loss: 0.01055913522011704\n",
      "[LOG 20200502-14:37:12] epoch: 11818 train-loss: 0.010559143808980783\n",
      "[LOG 20200502-14:37:12] epoch: 11819 train-loss: 0.010559152604805099\n",
      "[LOG 20200502-14:37:12] epoch: 11820 train-loss: 0.010559161400629414\n",
      "[LOG 20200502-14:37:12] epoch: 11821 train-loss: 0.0105591696790523\n",
      "[LOG 20200502-14:37:12] epoch: 11822 train-loss: 0.010559177543554042\n",
      "[LOG 20200502-14:37:13] epoch: 11823 train-loss: 0.010559185304575495\n",
      "[LOG 20200502-14:37:13] epoch: 11824 train-loss: 0.010559192341234949\n",
      "[LOG 20200502-14:37:13] epoch: 11825 train-loss: 0.010559199067453543\n",
      "[LOG 20200502-14:37:13] epoch: 11826 train-loss: 0.010559206414553855\n",
      "[LOG 20200502-14:37:14] epoch: 11827 train-loss: 0.010559212623371018\n",
      "[LOG 20200502-14:37:14] epoch: 11828 train-loss: 0.010559218935668468\n",
      "[LOG 20200502-14:37:14] epoch: 11829 train-loss: 0.010559224109682772\n",
      "[LOG 20200502-14:37:14] epoch: 11830 train-loss: 0.010559230111539364\n",
      "[LOG 20200502-14:37:14] epoch: 11831 train-loss: 0.01055923518207338\n",
      "[LOG 20200502-14:37:15] epoch: 11832 train-loss: 0.010559239631725682\n",
      "[LOG 20200502-14:37:15] epoch: 11833 train-loss: 0.010559244805739986\n",
      "[LOG 20200502-14:37:15] epoch: 11834 train-loss: 0.010559248841471143\n",
      "[LOG 20200502-14:37:15] epoch: 11835 train-loss: 0.010559253291123442\n",
      "[LOG 20200502-14:37:16] epoch: 11836 train-loss: 0.010559256912933456\n",
      "[LOG 20200502-14:37:16] epoch: 11837 train-loss: 0.010559261052144898\n",
      "[LOG 20200502-14:37:16] epoch: 11838 train-loss: 0.010559264466994338\n",
      "[LOG 20200502-14:37:16] epoch: 11839 train-loss: 0.010559268192284636\n",
      "[LOG 20200502-14:37:17] epoch: 11840 train-loss: 0.010559271607134078\n",
      "[LOG 20200502-14:37:17] epoch: 11841 train-loss: 0.010559275021983517\n",
      "[LOG 20200502-14:37:17] epoch: 11842 train-loss: 0.01055927864379353\n",
      "[LOG 20200502-14:37:17] epoch: 11843 train-loss: 0.010559281644721826\n",
      "[LOG 20200502-14:37:17] epoch: 11844 train-loss: 0.010559285576972697\n",
      "[LOG 20200502-14:37:18] epoch: 11845 train-loss: 0.010559288577900993\n",
      "[LOG 20200502-14:37:18] epoch: 11846 train-loss: 0.010559293441474438\n",
      "[LOG 20200502-14:37:18] epoch: 11847 train-loss: 0.010559296752843592\n",
      "[LOG 20200502-14:37:18] epoch: 11848 train-loss: 0.010559300374653604\n",
      "[LOG 20200502-14:37:19] epoch: 11849 train-loss: 0.010559304099943902\n",
      "[LOG 20200502-14:37:19] epoch: 11850 train-loss: 0.010559308342635632\n",
      "[LOG 20200502-14:37:19] epoch: 11851 train-loss: 0.010559312274886502\n",
      "[LOG 20200502-14:37:19] epoch: 11852 train-loss: 0.010559316931499375\n",
      "[LOG 20200502-14:37:19] epoch: 11853 train-loss: 0.010559320346348815\n",
      "[LOG 20200502-14:37:20] epoch: 11854 train-loss: 0.010559325106441975\n",
      "[LOG 20200502-14:37:20] epoch: 11855 train-loss: 0.01055932997001542\n",
      "[LOG 20200502-14:37:20] epoch: 11856 train-loss: 0.01055933441966772\n",
      "[LOG 20200502-14:37:20] epoch: 11857 train-loss: 0.010559338972800307\n",
      "[LOG 20200502-14:37:21] epoch: 11858 train-loss: 0.010559343215492036\n",
      "[LOG 20200502-14:37:21] epoch: 11859 train-loss: 0.01055934787210491\n",
      "[LOG 20200502-14:37:21] epoch: 11860 train-loss: 0.010559352425237497\n",
      "[LOG 20200502-14:37:21] epoch: 11861 train-loss: 0.01055935708185037\n",
      "[LOG 20200502-14:37:21] epoch: 11862 train-loss: 0.010559362255864672\n",
      "[LOG 20200502-14:37:22] epoch: 11863 train-loss: 0.010559365981154971\n",
      "[LOG 20200502-14:37:22] epoch: 11864 train-loss: 0.010559370948208703\n",
      "[LOG 20200502-14:37:22] epoch: 11865 train-loss: 0.01055937498393986\n",
      "[LOG 20200502-14:37:22] epoch: 11866 train-loss: 0.01055937891619073\n",
      "[LOG 20200502-14:37:23] epoch: 11867 train-loss: 0.010559382744961314\n",
      "[LOG 20200502-14:37:23] epoch: 11868 train-loss: 0.010559386470251612\n",
      "[LOG 20200502-14:37:23] epoch: 11869 train-loss: 0.010559389885101054\n",
      "[LOG 20200502-14:37:23] epoch: 11870 train-loss: 0.010559393713871637\n",
      "[LOG 20200502-14:37:23] epoch: 11871 train-loss: 0.010559396507839361\n",
      "[LOG 20200502-14:37:24] epoch: 11872 train-loss: 0.010559399612247944\n",
      "[LOG 20200502-14:37:24] epoch: 11873 train-loss: 0.010559401371412806\n",
      "[LOG 20200502-14:37:24] epoch: 11874 train-loss: 0.010559403854939673\n",
      "[LOG 20200502-14:37:24] epoch: 11875 train-loss: 0.010559405821065107\n",
      "[LOG 20200502-14:37:25] epoch: 11876 train-loss: 0.010559407269789113\n",
      "[LOG 20200502-14:37:25] epoch: 11877 train-loss: 0.01055940892547369\n",
      "[LOG 20200502-14:37:25] epoch: 11878 train-loss: 0.010559409960276551\n",
      "[LOG 20200502-14:37:25] epoch: 11879 train-loss: 0.01055941047767798\n",
      "[LOG 20200502-14:37:25] epoch: 11880 train-loss: 0.010559411409000555\n",
      "[LOG 20200502-14:37:26] epoch: 11881 train-loss: 0.010559411409000555\n",
      "[LOG 20200502-14:37:26] epoch: 11882 train-loss: 0.010559410995079411\n",
      "[LOG 20200502-14:37:26] epoch: 11883 train-loss: 0.010559410167237123\n",
      "[LOG 20200502-14:37:26] epoch: 11884 train-loss: 0.010559410270717409\n",
      "[LOG 20200502-14:37:26] epoch: 11885 train-loss: 0.010559409339394834\n",
      "[LOG 20200502-14:37:27] epoch: 11886 train-loss: 0.010559408304591974\n",
      "[LOG 20200502-14:37:27] epoch: 11887 train-loss: 0.010559407166308828\n",
      "[LOG 20200502-14:37:27] epoch: 11888 train-loss: 0.010559406338466538\n",
      "[LOG 20200502-14:37:27] epoch: 11889 train-loss: 0.010559405200183392\n",
      "[LOG 20200502-14:37:28] epoch: 11890 train-loss: 0.010559404268860817\n",
      "[LOG 20200502-14:37:28] epoch: 11891 train-loss: 0.010559403234057956\n",
      "[LOG 20200502-14:37:28] epoch: 11892 train-loss: 0.010559402095774809\n",
      "[LOG 20200502-14:37:28] epoch: 11893 train-loss: 0.010559401371412806\n",
      "[LOG 20200502-14:37:28] epoch: 11894 train-loss: 0.010559400440090232\n",
      "[LOG 20200502-14:37:29] epoch: 11895 train-loss: 0.010559399819208516\n",
      "[LOG 20200502-14:37:29] epoch: 11896 train-loss: 0.010559399922688803\n",
      "[LOG 20200502-14:37:29] epoch: 11897 train-loss: 0.010559400336609947\n",
      "[LOG 20200502-14:37:29] epoch: 11898 train-loss: 0.010559400440090232\n",
      "[LOG 20200502-14:37:30] epoch: 11899 train-loss: 0.010559401474893093\n",
      "[LOG 20200502-14:37:30] epoch: 11900 train-loss: 0.010559402923617098\n",
      "[LOG 20200502-14:37:30] epoch: 11901 train-loss: 0.01055940468278196\n",
      "[LOG 20200502-14:37:30] epoch: 11902 train-loss: 0.010559407269789113\n",
      "[LOG 20200502-14:37:30] epoch: 11903 train-loss: 0.010559409649835693\n",
      "[LOG 20200502-14:37:31] epoch: 11904 train-loss: 0.010559413271645704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:37:31] epoch: 11905 train-loss: 0.010559416065613428\n",
      "[LOG 20200502-14:37:31] epoch: 11906 train-loss: 0.010559420204824872\n",
      "[LOG 20200502-14:37:31] epoch: 11907 train-loss: 0.010559425171878602\n",
      "[LOG 20200502-14:37:32] epoch: 11908 train-loss: 0.010559430345892906\n",
      "[LOG 20200502-14:37:32] epoch: 11909 train-loss: 0.010559435623387495\n",
      "[LOG 20200502-14:37:32] epoch: 11910 train-loss: 0.010559441728724374\n",
      "[LOG 20200502-14:37:32] epoch: 11911 train-loss: 0.010559448041021824\n",
      "[LOG 20200502-14:37:32] epoch: 11912 train-loss: 0.01055945549160242\n",
      "[LOG 20200502-14:37:33] epoch: 11913 train-loss: 0.01055946211434073\n",
      "[LOG 20200502-14:37:33] epoch: 11914 train-loss: 0.010559470185803043\n",
      "[LOG 20200502-14:37:33] epoch: 11915 train-loss: 0.010559478464225927\n",
      "[LOG 20200502-14:37:33] epoch: 11916 train-loss: 0.010559485914806524\n",
      "[LOG 20200502-14:37:34] epoch: 11917 train-loss: 0.010559495124551985\n",
      "[LOG 20200502-14:37:34] epoch: 11918 train-loss: 0.010559503816896014\n",
      "[LOG 20200502-14:37:34] epoch: 11919 train-loss: 0.010559512612720331\n",
      "[LOG 20200502-14:37:34] epoch: 11920 train-loss: 0.010559522029426362\n",
      "[LOG 20200502-14:37:34] epoch: 11921 train-loss: 0.01055953186005354\n",
      "[LOG 20200502-14:37:35] epoch: 11922 train-loss: 0.010559540759358142\n",
      "[LOG 20200502-14:37:35] epoch: 11923 train-loss: 0.010559550589985318\n",
      "[LOG 20200502-14:37:35] epoch: 11924 train-loss: 0.010559559799730778\n",
      "[LOG 20200502-14:37:35] epoch: 11925 train-loss: 0.010559569940798812\n",
      "[LOG 20200502-14:37:36] epoch: 11926 train-loss: 0.010559579150544273\n",
      "[LOG 20200502-14:37:36] epoch: 11927 train-loss: 0.010559589395092593\n",
      "[LOG 20200502-14:37:36] epoch: 11928 train-loss: 0.010559598190916909\n",
      "[LOG 20200502-14:37:36] epoch: 11929 train-loss: 0.010559607607622942\n",
      "[LOG 20200502-14:37:37] epoch: 11930 train-loss: 0.010559617024328973\n",
      "[LOG 20200502-14:37:37] epoch: 11931 train-loss: 0.010559626027113862\n",
      "[LOG 20200502-14:37:37] epoch: 11932 train-loss: 0.010559634409017034\n",
      "[LOG 20200502-14:37:37] epoch: 11933 train-loss: 0.010559642583959632\n",
      "[LOG 20200502-14:37:37] epoch: 11934 train-loss: 0.010559651379783949\n",
      "[LOG 20200502-14:37:38] epoch: 11935 train-loss: 0.010559659037325118\n",
      "[LOG 20200502-14:37:38] epoch: 11936 train-loss: 0.010559666694866287\n",
      "[LOG 20200502-14:37:38] epoch: 11937 train-loss: 0.010559674662848314\n",
      "[LOG 20200502-14:37:38] epoch: 11938 train-loss: 0.01055968107862605\n",
      "[LOG 20200502-14:37:39] epoch: 11939 train-loss: 0.010559687494403787\n",
      "[LOG 20200502-14:37:39] epoch: 11940 train-loss: 0.01055969370322095\n",
      "[LOG 20200502-14:37:39] epoch: 11941 train-loss: 0.0105597000155184\n",
      "[LOG 20200502-14:37:39] epoch: 11942 train-loss: 0.01055970581041442\n",
      "[LOG 20200502-14:37:39] epoch: 11943 train-loss: 0.010559711294869581\n",
      "[LOG 20200502-14:37:40] epoch: 11944 train-loss: 0.010559716261923313\n",
      "[LOG 20200502-14:37:40] epoch: 11945 train-loss: 0.010559720608095327\n",
      "[LOG 20200502-14:37:40] epoch: 11946 train-loss: 0.010559725368188487\n",
      "[LOG 20200502-14:37:40] epoch: 11947 train-loss: 0.010559729714360502\n",
      "[LOG 20200502-14:37:41] epoch: 11948 train-loss: 0.010559733025729656\n",
      "[LOG 20200502-14:37:41] epoch: 11949 train-loss: 0.010559737061460813\n",
      "[LOG 20200502-14:37:41] epoch: 11950 train-loss: 0.01055974057979054\n",
      "[LOG 20200502-14:37:41] epoch: 11951 train-loss: 0.01055974399463998\n",
      "[LOG 20200502-14:37:41] epoch: 11952 train-loss: 0.010559747616449991\n",
      "[LOG 20200502-14:37:42] epoch: 11953 train-loss: 0.01055975134174029\n",
      "[LOG 20200502-14:37:42] epoch: 11954 train-loss: 0.010559754135708014\n",
      "[LOG 20200502-14:37:42] epoch: 11955 train-loss: 0.010559757240116596\n",
      "[LOG 20200502-14:37:43] epoch: 11956 train-loss: 0.010559760861926608\n",
      "[LOG 20200502-14:37:43] epoch: 11957 train-loss: 0.01055976396633519\n",
      "[LOG 20200502-14:37:43] epoch: 11958 train-loss: 0.010559767174224058\n",
      "[LOG 20200502-14:37:43] epoch: 11959 train-loss: 0.010559770692553785\n",
      "[LOG 20200502-14:37:43] epoch: 11960 train-loss: 0.010559774107403226\n",
      "[LOG 20200502-14:37:44] epoch: 11961 train-loss: 0.010559777522252666\n",
      "[LOG 20200502-14:37:44] epoch: 11962 train-loss: 0.01055978135102325\n",
      "[LOG 20200502-14:37:44] epoch: 11963 train-loss: 0.010559785179793835\n",
      "[LOG 20200502-14:37:44] epoch: 11964 train-loss: 0.010559788698123561\n",
      "[LOG 20200502-14:37:45] epoch: 11965 train-loss: 0.01055979345821672\n",
      "[LOG 20200502-14:37:45] epoch: 11966 train-loss: 0.010559797080026733\n",
      "[LOG 20200502-14:37:45] epoch: 11967 train-loss: 0.010559801840119891\n",
      "[LOG 20200502-14:37:45] epoch: 11968 train-loss: 0.010559806186291907\n",
      "[LOG 20200502-14:37:45] epoch: 11969 train-loss: 0.010559810532463921\n",
      "[LOG 20200502-14:37:46] epoch: 11970 train-loss: 0.010559814878635936\n",
      "[LOG 20200502-14:37:46] epoch: 11971 train-loss: 0.010559819949169954\n",
      "[LOG 20200502-14:37:46] epoch: 11972 train-loss: 0.010559824398822255\n",
      "[LOG 20200502-14:37:46] epoch: 11973 train-loss: 0.0105598292623957\n",
      "[LOG 20200502-14:37:47] epoch: 11974 train-loss: 0.01055983422944943\n",
      "[LOG 20200502-14:37:47] epoch: 11975 train-loss: 0.010559839403463734\n",
      "[LOG 20200502-14:37:47] epoch: 11976 train-loss: 0.010559843956596322\n",
      "[LOG 20200502-14:37:47] epoch: 11977 train-loss: 0.01055984871668948\n",
      "[LOG 20200502-14:37:48] epoch: 11978 train-loss: 0.010559853166341782\n",
      "[LOG 20200502-14:37:48] epoch: 11979 train-loss: 0.010559857926434942\n",
      "[LOG 20200502-14:37:48] epoch: 11980 train-loss: 0.010559862790008387\n",
      "[LOG 20200502-14:37:48] epoch: 11981 train-loss: 0.010559867032700114\n",
      "[LOG 20200502-14:37:48] epoch: 11982 train-loss: 0.010559870757990412\n",
      "[LOG 20200502-14:37:49] epoch: 11983 train-loss: 0.010559874897201857\n",
      "[LOG 20200502-14:37:49] epoch: 11984 train-loss: 0.010559879450334443\n",
      "[LOG 20200502-14:37:49] epoch: 11985 train-loss: 0.010559882347782453\n",
      "[LOG 20200502-14:37:49] epoch: 11986 train-loss: 0.010559885659151606\n",
      "[LOG 20200502-14:37:50] epoch: 11987 train-loss: 0.010559888142678473\n",
      "[LOG 20200502-14:37:50] epoch: 11988 train-loss: 0.010559891040126482\n",
      "[LOG 20200502-14:37:50] epoch: 11989 train-loss: 0.01055989373061392\n",
      "[LOG 20200502-14:37:50] epoch: 11990 train-loss: 0.010559895903699927\n",
      "[LOG 20200502-14:37:50] epoch: 11991 train-loss: 0.010559897662864791\n",
      "[LOG 20200502-14:37:51] epoch: 11992 train-loss: 0.01055989900810851\n",
      "[LOG 20200502-14:37:51] epoch: 11993 train-loss: 0.010559899628990226\n",
      "[LOG 20200502-14:37:51] epoch: 11994 train-loss: 0.010559900870753659\n",
      "[LOG 20200502-14:37:51] epoch: 11995 train-loss: 0.010559901284674803\n",
      "[LOG 20200502-14:37:52] epoch: 11996 train-loss: 0.010559901595115662\n",
      "[LOG 20200502-14:37:52] epoch: 11997 train-loss: 0.010559901595115662\n",
      "[LOG 20200502-14:37:52] epoch: 11998 train-loss: 0.010559901181194518\n",
      "[LOG 20200502-14:37:52] epoch: 11999 train-loss: 0.0105599005603128\n",
      "[LOG 20200502-14:37:52] epoch: 12000 train-loss: 0.010559899318549369\n",
      "[LOG 20200502-14:37:53] epoch: 12001 train-loss: 0.010559898697667651\n",
      "[LOG 20200502-14:37:53] epoch: 12002 train-loss: 0.010559897352423932\n",
      "[LOG 20200502-14:37:53] epoch: 12003 train-loss: 0.010559896421101358\n",
      "[LOG 20200502-14:37:53] epoch: 12004 train-loss: 0.01055989476541678\n",
      "[LOG 20200502-14:37:53] epoch: 12005 train-loss: 0.010559893109732203\n",
      "[LOG 20200502-14:37:54] epoch: 12006 train-loss: 0.010559891867968772\n",
      "[LOG 20200502-14:37:54] epoch: 12007 train-loss: 0.010559890005323622\n",
      "[LOG 20200502-14:37:54] epoch: 12008 train-loss: 0.010559889177481333\n",
      "[LOG 20200502-14:37:54] epoch: 12009 train-loss: 0.010559887935717901\n",
      "[LOG 20200502-14:37:55] epoch: 12010 train-loss: 0.01055988741831647\n",
      "[LOG 20200502-14:37:55] epoch: 12011 train-loss: 0.01055988586611218\n",
      "[LOG 20200502-14:37:55] epoch: 12012 train-loss: 0.010559885762631893\n",
      "[LOG 20200502-14:37:55] epoch: 12013 train-loss: 0.010559885141750177\n",
      "[LOG 20200502-14:37:56] epoch: 12014 train-loss: 0.010559885969592465\n",
      "[LOG 20200502-14:37:56] epoch: 12015 train-loss: 0.01055988586611218\n",
      "[LOG 20200502-14:37:56] epoch: 12016 train-loss: 0.010559887107875612\n",
      "[LOG 20200502-14:37:56] epoch: 12017 train-loss: 0.010559888349639045\n",
      "[LOG 20200502-14:37:56] epoch: 12018 train-loss: 0.01055989031576448\n",
      "[LOG 20200502-14:37:57] epoch: 12019 train-loss: 0.010559891971449057\n",
      "[LOG 20200502-14:37:57] epoch: 12020 train-loss: 0.010559895179337926\n",
      "[LOG 20200502-14:37:57] epoch: 12021 train-loss: 0.010559898076785935\n",
      "[LOG 20200502-14:37:57] epoch: 12022 train-loss: 0.010559902215997377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:37:57] epoch: 12023 train-loss: 0.010559905941287676\n",
      "[LOG 20200502-14:37:58] epoch: 12024 train-loss: 0.010559910908341408\n",
      "[LOG 20200502-14:37:58] epoch: 12025 train-loss: 0.01055991587539514\n",
      "[LOG 20200502-14:37:58] epoch: 12026 train-loss: 0.010559921152889729\n",
      "[LOG 20200502-14:37:58] epoch: 12027 train-loss: 0.010559927465187179\n",
      "[LOG 20200502-14:37:59] epoch: 12028 train-loss: 0.010559934087925486\n",
      "[LOG 20200502-14:37:59] epoch: 12029 train-loss: 0.010559940503703224\n",
      "[LOG 20200502-14:37:59] epoch: 12030 train-loss: 0.010559948471685251\n",
      "[LOG 20200502-14:37:59] epoch: 12031 train-loss: 0.010559955818785561\n",
      "[LOG 20200502-14:37:59] epoch: 12032 train-loss: 0.010559963786767589\n",
      "[LOG 20200502-14:38:00] epoch: 12033 train-loss: 0.01055997165126933\n",
      "[LOG 20200502-14:38:00] epoch: 12034 train-loss: 0.01055998086101479\n",
      "[LOG 20200502-14:38:00] epoch: 12035 train-loss: 0.010559988828996817\n",
      "[LOG 20200502-14:38:00] epoch: 12036 train-loss: 0.010559998349183135\n",
      "[LOG 20200502-14:38:01] epoch: 12037 train-loss: 0.010560007455448309\n",
      "[LOG 20200502-14:38:01] epoch: 12038 train-loss: 0.010560016975634627\n",
      "[LOG 20200502-14:38:01] epoch: 12039 train-loss: 0.010560026081899801\n",
      "[LOG 20200502-14:38:01] epoch: 12040 train-loss: 0.010560035498605834\n",
      "[LOG 20200502-14:38:02] epoch: 12041 train-loss: 0.010560045018792152\n",
      "[LOG 20200502-14:38:02] epoch: 12042 train-loss: 0.010560054642458757\n",
      "[LOG 20200502-14:38:02] epoch: 12043 train-loss: 0.01056006426612536\n",
      "[LOG 20200502-14:38:02] epoch: 12044 train-loss: 0.010560073682831394\n",
      "[LOG 20200502-14:38:02] epoch: 12045 train-loss: 0.010560083203017712\n",
      "[LOG 20200502-14:38:03] epoch: 12046 train-loss: 0.010560092102322314\n",
      "[LOG 20200502-14:38:03] epoch: 12047 train-loss: 0.01056010141554806\n",
      "[LOG 20200502-14:38:03] epoch: 12048 train-loss: 0.010560110314852662\n",
      "[LOG 20200502-14:38:03] epoch: 12049 train-loss: 0.010560119007196691\n",
      "[LOG 20200502-14:38:04] epoch: 12050 train-loss: 0.01056012749258015\n",
      "[LOG 20200502-14:38:04] epoch: 12051 train-loss: 0.01056013587448332\n",
      "[LOG 20200502-14:38:04] epoch: 12052 train-loss: 0.010560143428544203\n",
      "[LOG 20200502-14:38:04] epoch: 12053 train-loss: 0.01056015139652623\n",
      "[LOG 20200502-14:38:05] epoch: 12054 train-loss: 0.010560158640146255\n",
      "[LOG 20200502-14:38:05] epoch: 12055 train-loss: 0.010560165573325422\n",
      "[LOG 20200502-14:38:05] epoch: 12056 train-loss: 0.010560171368221441\n",
      "[LOG 20200502-14:38:05] epoch: 12057 train-loss: 0.010560178404880894\n",
      "[LOG 20200502-14:38:05] epoch: 12058 train-loss: 0.010560183992816342\n",
      "[LOG 20200502-14:38:06] epoch: 12059 train-loss: 0.010560189477271505\n",
      "[LOG 20200502-14:38:06] epoch: 12060 train-loss: 0.010560194547805522\n",
      "[LOG 20200502-14:38:06] epoch: 12061 train-loss: 0.010560199721819825\n",
      "[LOG 20200502-14:38:06] epoch: 12062 train-loss: 0.010560204481912984\n",
      "[LOG 20200502-14:38:07] epoch: 12063 train-loss: 0.010560208414163854\n",
      "[LOG 20200502-14:38:07] epoch: 12064 train-loss: 0.010560212656855583\n",
      "[LOG 20200502-14:38:07] epoch: 12065 train-loss: 0.010560215864744451\n",
      "[LOG 20200502-14:38:07] epoch: 12066 train-loss: 0.010560219693515036\n",
      "[LOG 20200502-14:38:08] epoch: 12067 train-loss: 0.01056022300488419\n",
      "[LOG 20200502-14:38:08] epoch: 12068 train-loss: 0.010560226005812487\n",
      "[LOG 20200502-14:38:08] epoch: 12069 train-loss: 0.01056022983458307\n",
      "[LOG 20200502-14:38:08] epoch: 12070 train-loss: 0.010560232111149363\n",
      "[LOG 20200502-14:38:09] epoch: 12071 train-loss: 0.010560235008597374\n",
      "[LOG 20200502-14:38:09] epoch: 12072 train-loss: 0.010560237492124239\n",
      "[LOG 20200502-14:38:09] epoch: 12073 train-loss: 0.010560240596532822\n",
      "[LOG 20200502-14:38:09] epoch: 12074 train-loss: 0.01056024276961883\n",
      "[LOG 20200502-14:38:09] epoch: 12075 train-loss: 0.010560245356625982\n",
      "[LOG 20200502-14:38:10] epoch: 12076 train-loss: 0.01056024825407399\n",
      "[LOG 20200502-14:38:10] epoch: 12077 train-loss: 0.010560251255002286\n",
      "[LOG 20200502-14:38:10] epoch: 12078 train-loss: 0.010560253945489725\n",
      "[LOG 20200502-14:38:10] epoch: 12079 train-loss: 0.010560257049898306\n",
      "[LOG 20200502-14:38:11] epoch: 12080 train-loss: 0.010560260464747747\n",
      "[LOG 20200502-14:38:11] epoch: 12081 train-loss: 0.010560263672636615\n",
      "[LOG 20200502-14:38:11] epoch: 12082 train-loss: 0.010560266673564911\n",
      "[LOG 20200502-14:38:11] epoch: 12083 train-loss: 0.010560270605815781\n",
      "[LOG 20200502-14:38:11] epoch: 12084 train-loss: 0.010560274434586367\n",
      "[LOG 20200502-14:38:12] epoch: 12085 train-loss: 0.010560278056396378\n",
      "[LOG 20200502-14:38:12] epoch: 12086 train-loss: 0.010560282092127535\n",
      "[LOG 20200502-14:38:12] epoch: 12087 train-loss: 0.010560286645260122\n",
      "[LOG 20200502-14:38:12] epoch: 12088 train-loss: 0.010560290267070135\n",
      "[LOG 20200502-14:38:13] epoch: 12089 train-loss: 0.010560295027163293\n",
      "[LOG 20200502-14:38:13] epoch: 12090 train-loss: 0.010560299787256453\n",
      "[LOG 20200502-14:38:13] epoch: 12091 train-loss: 0.010560304133428467\n",
      "[LOG 20200502-14:38:13] epoch: 12092 train-loss: 0.01056030879004134\n",
      "[LOG 20200502-14:38:13] epoch: 12093 train-loss: 0.010560313446654214\n",
      "[LOG 20200502-14:38:14] epoch: 12094 train-loss: 0.010560317999786802\n",
      "[LOG 20200502-14:38:14] epoch: 12095 train-loss: 0.01056032327728139\n",
      "[LOG 20200502-14:38:14] epoch: 12096 train-loss: 0.01056032751997312\n",
      "[LOG 20200502-14:38:14] epoch: 12097 train-loss: 0.010560332176585993\n",
      "[LOG 20200502-14:38:15] epoch: 12098 train-loss: 0.010560336833198866\n",
      "[LOG 20200502-14:38:15] epoch: 12099 train-loss: 0.010560341075890593\n",
      "[LOG 20200502-14:38:15] epoch: 12100 train-loss: 0.010560345732503466\n",
      "[LOG 20200502-14:38:15] epoch: 12101 train-loss: 0.010560349664754339\n",
      "[LOG 20200502-14:38:15] epoch: 12102 train-loss: 0.010560353907446066\n",
      "[LOG 20200502-14:38:16] epoch: 12103 train-loss: 0.010560357839696936\n",
      "[LOG 20200502-14:38:16] epoch: 12104 train-loss: 0.010560361564987235\n",
      "[LOG 20200502-14:38:16] epoch: 12105 train-loss: 0.010560364669395817\n",
      "[LOG 20200502-14:38:16] epoch: 12106 train-loss: 0.010560367463363541\n",
      "[LOG 20200502-14:38:16] epoch: 12107 train-loss: 0.010560370774732696\n",
      "[LOG 20200502-14:38:17] epoch: 12108 train-loss: 0.010560373672180705\n",
      "[LOG 20200502-14:38:17] epoch: 12109 train-loss: 0.010560375948747\n",
      "[LOG 20200502-14:38:17] epoch: 12110 train-loss: 0.010560377811392149\n",
      "[LOG 20200502-14:38:17] epoch: 12111 train-loss: 0.010560379674037298\n",
      "[LOG 20200502-14:38:18] epoch: 12112 train-loss: 0.010560381019281017\n",
      "[LOG 20200502-14:38:18] epoch: 12113 train-loss: 0.010560381536682447\n",
      "[LOG 20200502-14:38:18] epoch: 12114 train-loss: 0.010560382468005022\n",
      "[LOG 20200502-14:38:18] epoch: 12115 train-loss: 0.010560382571485307\n",
      "[LOG 20200502-14:38:18] epoch: 12116 train-loss: 0.010560382985406451\n",
      "[LOG 20200502-14:38:19] epoch: 12117 train-loss: 0.010560382157564163\n",
      "[LOG 20200502-14:38:19] epoch: 12118 train-loss: 0.010560381536682447\n",
      "[LOG 20200502-14:38:19] epoch: 12119 train-loss: 0.010560381122761302\n",
      "[LOG 20200502-14:38:19] epoch: 12120 train-loss: 0.010560379777517583\n",
      "[LOG 20200502-14:38:19] epoch: 12121 train-loss: 0.010560378432273865\n",
      "[LOG 20200502-14:38:20] epoch: 12122 train-loss: 0.010560376362668144\n",
      "[LOG 20200502-14:38:20] epoch: 12123 train-loss: 0.010560375224384997\n",
      "[LOG 20200502-14:38:20] epoch: 12124 train-loss: 0.010560373465220133\n",
      "[LOG 20200502-14:38:20] epoch: 12125 train-loss: 0.010560370567772124\n",
      "[LOG 20200502-14:38:21] epoch: 12126 train-loss: 0.010560368912087547\n",
      "[LOG 20200502-14:38:21] epoch: 12127 train-loss: 0.010560366428560682\n",
      "[LOG 20200502-14:38:21] epoch: 12128 train-loss: 0.010560364358954959\n",
      "[LOG 20200502-14:38:21] epoch: 12129 train-loss: 0.010560362392829524\n",
      "[LOG 20200502-14:38:21] epoch: 12130 train-loss: 0.01056036063366466\n",
      "[LOG 20200502-14:38:22] epoch: 12131 train-loss: 0.01056035908146037\n",
      "[LOG 20200502-14:38:22] epoch: 12132 train-loss: 0.010560357632736364\n",
      "[LOG 20200502-14:38:22] epoch: 12133 train-loss: 0.010560356494453218\n",
      "[LOG 20200502-14:38:22] epoch: 12134 train-loss: 0.010560355252689786\n",
      "[LOG 20200502-14:38:23] epoch: 12135 train-loss: 0.010560354735288355\n",
      "[LOG 20200502-14:38:23] epoch: 12136 train-loss: 0.010560354528327784\n",
      "[LOG 20200502-14:38:23] epoch: 12137 train-loss: 0.0105603551492095\n",
      "[LOG 20200502-14:38:23] epoch: 12138 train-loss: 0.010560355356170071\n",
      "[LOG 20200502-14:38:23] epoch: 12139 train-loss: 0.010560356287492646\n",
      "[LOG 20200502-14:38:24] epoch: 12140 train-loss: 0.01056035804665751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:38:24] epoch: 12141 train-loss: 0.010560360012782944\n",
      "[LOG 20200502-14:38:24] epoch: 12142 train-loss: 0.010560362185868952\n",
      "[LOG 20200502-14:38:24] epoch: 12143 train-loss: 0.01056036591115925\n",
      "[LOG 20200502-14:38:25] epoch: 12144 train-loss: 0.010560368394686116\n",
      "[LOG 20200502-14:38:25] epoch: 12145 train-loss: 0.010560372533897558\n",
      "[LOG 20200502-14:38:25] epoch: 12146 train-loss: 0.01056037698354986\n",
      "[LOG 20200502-14:38:25] epoch: 12147 train-loss: 0.010560381122761302\n",
      "[LOG 20200502-14:38:26] epoch: 12148 train-loss: 0.010560387021137608\n",
      "[LOG 20200502-14:38:26] epoch: 12149 train-loss: 0.010560392712553343\n",
      "[LOG 20200502-14:38:26] epoch: 12150 train-loss: 0.010560398093528219\n",
      "[LOG 20200502-14:38:26] epoch: 12151 train-loss: 0.010560405440628529\n",
      "[LOG 20200502-14:38:26] epoch: 12152 train-loss: 0.01056041144248512\n",
      "[LOG 20200502-14:38:27] epoch: 12153 train-loss: 0.010560419410467148\n",
      "[LOG 20200502-14:38:27] epoch: 12154 train-loss: 0.010560426550606886\n",
      "[LOG 20200502-14:38:27] epoch: 12155 train-loss: 0.010560434415108629\n",
      "[LOG 20200502-14:38:27] epoch: 12156 train-loss: 0.010560442590051226\n",
      "[LOG 20200502-14:38:28] epoch: 12157 train-loss: 0.010560451178914972\n",
      "[LOG 20200502-14:38:28] epoch: 12158 train-loss: 0.010560459146896998\n",
      "[LOG 20200502-14:38:28] epoch: 12159 train-loss: 0.0105604680462016\n",
      "[LOG 20200502-14:38:28] epoch: 12160 train-loss: 0.010560477152466774\n",
      "[LOG 20200502-14:38:28] epoch: 12161 train-loss: 0.010560486155251661\n",
      "[LOG 20200502-14:38:29] epoch: 12162 train-loss: 0.010560494951075979\n",
      "[LOG 20200502-14:38:29] epoch: 12163 train-loss: 0.010560504574742582\n",
      "[LOG 20200502-14:38:29] epoch: 12164 train-loss: 0.010560512956645753\n",
      "[LOG 20200502-14:38:29] epoch: 12165 train-loss: 0.0105605222698715\n",
      "[LOG 20200502-14:38:30] epoch: 12166 train-loss: 0.010560530651774671\n",
      "[LOG 20200502-14:38:30] epoch: 12167 train-loss: 0.010560540275441276\n",
      "[LOG 20200502-14:38:30] epoch: 12168 train-loss: 0.01056054886430502\n",
      "[LOG 20200502-14:38:30] epoch: 12169 train-loss: 0.01056055755664905\n",
      "[LOG 20200502-14:38:30] epoch: 12170 train-loss: 0.010560565835071934\n",
      "[LOG 20200502-14:38:31] epoch: 12171 train-loss: 0.010560574216975106\n",
      "[LOG 20200502-14:38:31] epoch: 12172 train-loss: 0.010560582702358564\n",
      "[LOG 20200502-14:38:31] epoch: 12173 train-loss: 0.01056058963553773\n",
      "[LOG 20200502-14:38:31] epoch: 12174 train-loss: 0.010560597707000043\n",
      "[LOG 20200502-14:38:32] epoch: 12175 train-loss: 0.010560604743659496\n",
      "[LOG 20200502-14:38:32] epoch: 12176 train-loss: 0.010560611262917519\n",
      "[LOG 20200502-14:38:32] epoch: 12177 train-loss: 0.010560617885655828\n",
      "[LOG 20200502-14:38:32] epoch: 12178 train-loss: 0.010560624301433563\n",
      "[LOG 20200502-14:38:32] epoch: 12179 train-loss: 0.010560629992849298\n",
      "[LOG 20200502-14:38:33] epoch: 12180 train-loss: 0.010560635580784745\n",
      "[LOG 20200502-14:38:33] epoch: 12181 train-loss: 0.01056064096175962\n",
      "[LOG 20200502-14:38:33] epoch: 12182 train-loss: 0.010560645618372493\n",
      "[LOG 20200502-14:38:33] epoch: 12183 train-loss: 0.010560650274985366\n",
      "[LOG 20200502-14:38:34] epoch: 12184 train-loss: 0.010560654207236238\n",
      "[LOG 20200502-14:38:34] epoch: 12185 train-loss: 0.01056065834644768\n",
      "[LOG 20200502-14:38:34] epoch: 12186 train-loss: 0.010560661657816835\n",
      "[LOG 20200502-14:38:34] epoch: 12187 train-loss: 0.010560665072666274\n",
      "[LOG 20200502-14:38:35] epoch: 12188 train-loss: 0.010560668280555142\n",
      "[LOG 20200502-14:38:35] epoch: 12189 train-loss: 0.010560670764082007\n",
      "[LOG 20200502-14:38:35] epoch: 12190 train-loss: 0.010560672937168015\n",
      "[LOG 20200502-14:38:35] epoch: 12191 train-loss: 0.010560675627655454\n",
      "[LOG 20200502-14:38:35] epoch: 12192 train-loss: 0.010560677697261175\n",
      "[LOG 20200502-14:38:36] epoch: 12193 train-loss: 0.010560679870347181\n",
      "[LOG 20200502-14:38:36] epoch: 12194 train-loss: 0.010560681939952903\n",
      "[LOG 20200502-14:38:36] epoch: 12195 train-loss: 0.010560683388676908\n",
      "[LOG 20200502-14:38:36] epoch: 12196 train-loss: 0.010560684837400913\n",
      "[LOG 20200502-14:38:36] epoch: 12197 train-loss: 0.010560687217447493\n",
      "[LOG 20200502-14:38:37] epoch: 12198 train-loss: 0.010560688976612356\n",
      "[LOG 20200502-14:38:37] epoch: 12199 train-loss: 0.010560690321856074\n",
      "[LOG 20200502-14:38:37] epoch: 12200 train-loss: 0.01056069177058008\n",
      "[LOG 20200502-14:38:37] epoch: 12201 train-loss: 0.010560693736705516\n",
      "[LOG 20200502-14:38:38] epoch: 12202 train-loss: 0.010560696634153524\n",
      "[LOG 20200502-14:38:38] epoch: 12203 train-loss: 0.010560698186357817\n",
      "[LOG 20200502-14:38:38] epoch: 12204 train-loss: 0.010560700773364969\n",
      "[LOG 20200502-14:38:38] epoch: 12205 train-loss: 0.010560703567332692\n",
      "[LOG 20200502-14:38:39] epoch: 12206 train-loss: 0.010560705429977842\n",
      "[LOG 20200502-14:38:39] epoch: 12207 train-loss: 0.010560708741346994\n",
      "[LOG 20200502-14:38:39] epoch: 12208 train-loss: 0.010560711535314718\n",
      "[LOG 20200502-14:38:39] epoch: 12209 train-loss: 0.010560714846683873\n",
      "[LOG 20200502-14:38:39] epoch: 12210 train-loss: 0.01056071805457274\n",
      "[LOG 20200502-14:38:40] epoch: 12211 train-loss: 0.010560721572902467\n",
      "[LOG 20200502-14:38:40] epoch: 12212 train-loss: 0.010560725815594196\n",
      "[LOG 20200502-14:38:40] epoch: 12213 train-loss: 0.010560728920002779\n",
      "[LOG 20200502-14:38:40] epoch: 12214 train-loss: 0.010560732748773363\n",
      "[LOG 20200502-14:38:41] epoch: 12215 train-loss: 0.010560736681024233\n",
      "[LOG 20200502-14:38:41] epoch: 12216 train-loss: 0.010560740820235677\n",
      "[LOG 20200502-14:38:41] epoch: 12217 train-loss: 0.010560744752486547\n",
      "[LOG 20200502-14:38:41] epoch: 12218 train-loss: 0.010560748581257131\n",
      "[LOG 20200502-14:38:41] epoch: 12219 train-loss: 0.010560753548310863\n",
      "[LOG 20200502-14:38:42] epoch: 12220 train-loss: 0.01056075758404202\n",
      "[LOG 20200502-14:38:42] epoch: 12221 train-loss: 0.01056076151629289\n",
      "[LOG 20200502-14:38:42] epoch: 12222 train-loss: 0.010560766276386049\n",
      "[LOG 20200502-14:38:42] epoch: 12223 train-loss: 0.010560770105156634\n",
      "[LOG 20200502-14:38:42] epoch: 12224 train-loss: 0.010560773830446932\n",
      "[LOG 20200502-14:38:43] epoch: 12225 train-loss: 0.010560777866178088\n",
      "[LOG 20200502-14:38:43] epoch: 12226 train-loss: 0.010560781487988101\n",
      "[LOG 20200502-14:38:43] epoch: 12227 train-loss: 0.010560785109798113\n",
      "[LOG 20200502-14:38:43] epoch: 12228 train-loss: 0.01056078862812784\n",
      "[LOG 20200502-14:38:44] epoch: 12229 train-loss: 0.010560791215134991\n",
      "[LOG 20200502-14:38:44] epoch: 12230 train-loss: 0.010560794319543574\n",
      "[LOG 20200502-14:38:44] epoch: 12231 train-loss: 0.010560797010031011\n",
      "[LOG 20200502-14:38:44] epoch: 12232 train-loss: 0.01056079970051845\n",
      "[LOG 20200502-14:38:44] epoch: 12233 train-loss: 0.010560801563163599\n",
      "[LOG 20200502-14:38:45] epoch: 12234 train-loss: 0.010560803218848176\n",
      "[LOG 20200502-14:38:45] epoch: 12235 train-loss: 0.01056080466757218\n",
      "[LOG 20200502-14:38:45] epoch: 12236 train-loss: 0.010560805288453897\n",
      "[LOG 20200502-14:38:45] epoch: 12237 train-loss: 0.010560806426737044\n",
      "[LOG 20200502-14:38:46] epoch: 12238 train-loss: 0.010560806633697616\n",
      "[LOG 20200502-14:38:46] epoch: 12239 train-loss: 0.010560806633697616\n",
      "[LOG 20200502-14:38:46] epoch: 12240 train-loss: 0.010560806737177901\n",
      "[LOG 20200502-14:38:46] epoch: 12241 train-loss: 0.010560805702375041\n",
      "[LOG 20200502-14:38:46] epoch: 12242 train-loss: 0.010560804978013039\n",
      "[LOG 20200502-14:38:47] epoch: 12243 train-loss: 0.010560803322328461\n",
      "[LOG 20200502-14:38:47] epoch: 12244 train-loss: 0.010560801977084743\n",
      "[LOG 20200502-14:38:47] epoch: 12245 train-loss: 0.010560799493557878\n",
      "[LOG 20200502-14:38:47] epoch: 12246 train-loss: 0.01056079732047187\n",
      "[LOG 20200502-14:38:48] epoch: 12247 train-loss: 0.010560794319543574\n",
      "[LOG 20200502-14:38:48] epoch: 12248 train-loss: 0.010560792249937853\n",
      "[LOG 20200502-14:38:48] epoch: 12249 train-loss: 0.01056078914552927\n",
      "[LOG 20200502-14:38:48] epoch: 12250 train-loss: 0.010560786041120688\n",
      "[LOG 20200502-14:38:49] epoch: 12251 train-loss: 0.010560783143672679\n",
      "[LOG 20200502-14:38:49] epoch: 12252 train-loss: 0.010560780246224668\n",
      "[LOG 20200502-14:38:49] epoch: 12253 train-loss: 0.010560776831375228\n",
      "[LOG 20200502-14:38:49] epoch: 12254 train-loss: 0.010560773416525789\n",
      "[LOG 20200502-14:38:49] epoch: 12255 train-loss: 0.010560770105156634\n",
      "[LOG 20200502-14:38:50] epoch: 12256 train-loss: 0.010560767207708623\n",
      "[LOG 20200502-14:38:50] epoch: 12257 train-loss: 0.010560764517221186\n",
      "[LOG 20200502-14:38:50] epoch: 12258 train-loss: 0.010560762344135178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:38:50] epoch: 12259 train-loss: 0.010560759653647741\n",
      "[LOG 20200502-14:38:51] epoch: 12260 train-loss: 0.010560757894482877\n",
      "[LOG 20200502-14:38:51] epoch: 12261 train-loss: 0.010560755928357443\n",
      "[LOG 20200502-14:38:51] epoch: 12262 train-loss: 0.010560754583113723\n",
      "[LOG 20200502-14:38:51] epoch: 12263 train-loss: 0.010560753444830576\n",
      "[LOG 20200502-14:38:52] epoch: 12264 train-loss: 0.010560752927429147\n",
      "[LOG 20200502-14:38:52] epoch: 12265 train-loss: 0.010560753341350291\n",
      "[LOG 20200502-14:38:52] epoch: 12266 train-loss: 0.010560753444830576\n",
      "[LOG 20200502-14:38:52] epoch: 12267 train-loss: 0.01056075468659401\n",
      "[LOG 20200502-14:38:53] epoch: 12268 train-loss: 0.01056075572139687\n",
      "[LOG 20200502-14:38:53] epoch: 12269 train-loss: 0.01056075758404202\n",
      "[LOG 20200502-14:38:53] epoch: 12270 train-loss: 0.01056075944668717\n",
      "[LOG 20200502-14:38:53] epoch: 12271 train-loss: 0.010560761930214034\n",
      "[LOG 20200502-14:38:54] epoch: 12272 train-loss: 0.010560765655504333\n",
      "[LOG 20200502-14:38:54] epoch: 12273 train-loss: 0.010560768966873487\n",
      "[LOG 20200502-14:38:54] epoch: 12274 train-loss: 0.010560773416525789\n",
      "[LOG 20200502-14:38:54] epoch: 12275 train-loss: 0.010560777452256944\n",
      "[LOG 20200502-14:38:55] epoch: 12276 train-loss: 0.010560783143672679\n",
      "[LOG 20200502-14:38:55] epoch: 12277 train-loss: 0.010560787903765837\n",
      "[LOG 20200502-14:38:55] epoch: 12278 train-loss: 0.010560793388221\n",
      "[LOG 20200502-14:38:55] epoch: 12279 train-loss: 0.01056079970051845\n",
      "[LOG 20200502-14:38:55] epoch: 12280 train-loss: 0.010560806219776472\n",
      "[LOG 20200502-14:38:56] epoch: 12281 train-loss: 0.010560812532073922\n",
      "[LOG 20200502-14:38:56] epoch: 12282 train-loss: 0.010560820086134804\n",
      "[LOG 20200502-14:38:56] epoch: 12283 train-loss: 0.010560826708873114\n",
      "[LOG 20200502-14:38:56] epoch: 12284 train-loss: 0.010560834573374854\n",
      "[LOG 20200502-14:38:57] epoch: 12285 train-loss: 0.01056084233439631\n",
      "[LOG 20200502-14:38:57] epoch: 12286 train-loss: 0.010560849784976907\n",
      "[LOG 20200502-14:38:57] epoch: 12287 train-loss: 0.01056085785643922\n",
      "[LOG 20200502-14:38:57] epoch: 12288 train-loss: 0.010560865824421247\n",
      "[LOG 20200502-14:38:57] epoch: 12289 train-loss: 0.010560873999363847\n",
      "[LOG 20200502-14:38:58] epoch: 12290 train-loss: 0.010560881553424729\n",
      "[LOG 20200502-14:38:58] epoch: 12291 train-loss: 0.010560890142288473\n",
      "[LOG 20200502-14:38:58] epoch: 12292 train-loss: 0.010560898006790213\n",
      "[LOG 20200502-14:38:58] epoch: 12293 train-loss: 0.01056090680261453\n",
      "[LOG 20200502-14:38:59] epoch: 12294 train-loss: 0.010560914253195127\n",
      "[LOG 20200502-14:38:59] epoch: 12295 train-loss: 0.010560921910736296\n",
      "[LOG 20200502-14:38:59] epoch: 12296 train-loss: 0.010560929154356321\n",
      "[LOG 20200502-14:38:59] epoch: 12297 train-loss: 0.010560937122338347\n",
      "[LOG 20200502-14:39:00] epoch: 12298 train-loss: 0.010560944365958372\n",
      "[LOG 20200502-14:39:00] epoch: 12299 train-loss: 0.01056095129913754\n",
      "[LOG 20200502-14:39:00] epoch: 12300 train-loss: 0.010560958232316706\n",
      "[LOG 20200502-14:39:00] epoch: 12301 train-loss: 0.010560964855055014\n",
      "[LOG 20200502-14:39:01] epoch: 12302 train-loss: 0.010560971477793323\n",
      "[LOG 20200502-14:39:01] epoch: 12303 train-loss: 0.01056097737616963\n",
      "[LOG 20200502-14:39:01] epoch: 12304 train-loss: 0.010560983171065649\n",
      "[LOG 20200502-14:39:01] epoch: 12305 train-loss: 0.010560988241599666\n",
      "[LOG 20200502-14:39:02] epoch: 12306 train-loss: 0.01056099341561397\n",
      "[LOG 20200502-14:39:02] epoch: 12307 train-loss: 0.0105609983826677\n",
      "[LOG 20200502-14:39:02] epoch: 12308 train-loss: 0.010561002004477713\n",
      "[LOG 20200502-14:39:02] epoch: 12309 train-loss: 0.010561006764570871\n",
      "[LOG 20200502-14:39:02] epoch: 12310 train-loss: 0.010561009765499167\n",
      "[LOG 20200502-14:39:03] epoch: 12311 train-loss: 0.010561012766427465\n",
      "[LOG 20200502-14:39:03] epoch: 12312 train-loss: 0.010561015663875474\n",
      "[LOG 20200502-14:39:03] epoch: 12313 train-loss: 0.01056101866480377\n",
      "[LOG 20200502-14:39:03] epoch: 12314 train-loss: 0.010561020630929206\n",
      "[LOG 20200502-14:39:04] epoch: 12315 train-loss: 0.010561022700534927\n",
      "[LOG 20200502-14:39:04] epoch: 12316 train-loss: 0.010561024459699789\n",
      "[LOG 20200502-14:39:04] epoch: 12317 train-loss: 0.010561026011904081\n",
      "[LOG 20200502-14:39:04] epoch: 12318 train-loss: 0.010561027150187228\n",
      "[LOG 20200502-14:39:04] epoch: 12319 train-loss: 0.01056102787454923\n",
      "[LOG 20200502-14:39:05] epoch: 12320 train-loss: 0.01056102890935209\n",
      "[LOG 20200502-14:39:05] epoch: 12321 train-loss: 0.010561029530233808\n",
      "[LOG 20200502-14:39:05] epoch: 12322 train-loss: 0.010561030358076096\n",
      "[LOG 20200502-14:39:05] epoch: 12323 train-loss: 0.010561030978957811\n",
      "[LOG 20200502-14:39:05] epoch: 12324 train-loss: 0.010561031392878957\n",
      "[LOG 20200502-14:39:06] epoch: 12325 train-loss: 0.01056103128939867\n",
      "[LOG 20200502-14:39:06] epoch: 12326 train-loss: 0.010561031910280386\n",
      "[LOG 20200502-14:39:06] epoch: 12327 train-loss: 0.010561032634642389\n",
      "[LOG 20200502-14:39:06] epoch: 12328 train-loss: 0.01056103284160296\n",
      "[LOG 20200502-14:39:07] epoch: 12329 train-loss: 0.010561033462484678\n",
      "[LOG 20200502-14:39:07] epoch: 12330 train-loss: 0.010561033979886107\n",
      "[LOG 20200502-14:39:07] epoch: 12331 train-loss: 0.010561035118169255\n",
      "[LOG 20200502-14:39:07] epoch: 12332 train-loss: 0.010561035635570684\n",
      "[LOG 20200502-14:39:07] epoch: 12333 train-loss: 0.010561037187774977\n",
      "[LOG 20200502-14:39:08] epoch: 12334 train-loss: 0.01056103863649898\n",
      "[LOG 20200502-14:39:08] epoch: 12335 train-loss: 0.010561039567821555\n",
      "[LOG 20200502-14:39:08] epoch: 12336 train-loss: 0.010561041326986419\n",
      "[LOG 20200502-14:39:08] epoch: 12337 train-loss: 0.010561043293111853\n",
      "[LOG 20200502-14:39:09] epoch: 12338 train-loss: 0.010561045052276717\n",
      "[LOG 20200502-14:39:09] epoch: 12339 train-loss: 0.01056104763928387\n",
      "[LOG 20200502-14:39:09] epoch: 12340 train-loss: 0.010561049812369876\n",
      "[LOG 20200502-14:39:09] epoch: 12341 train-loss: 0.0105610526063376\n",
      "[LOG 20200502-14:39:09] epoch: 12342 train-loss: 0.01056105498638418\n",
      "[LOG 20200502-14:39:10] epoch: 12343 train-loss: 0.010561057469911046\n",
      "[LOG 20200502-14:39:10] epoch: 12344 train-loss: 0.010561060677799914\n",
      "[LOG 20200502-14:39:10] epoch: 12345 train-loss: 0.010561063989169069\n",
      "[LOG 20200502-14:39:10] epoch: 12346 train-loss: 0.010561066679656506\n",
      "[LOG 20200502-14:39:10] epoch: 12347 train-loss: 0.010561069887545373\n",
      "[LOG 20200502-14:39:11] epoch: 12348 train-loss: 0.010561073302394815\n",
      "[LOG 20200502-14:39:11] epoch: 12349 train-loss: 0.010561076717244254\n",
      "[LOG 20200502-14:39:11] epoch: 12350 train-loss: 0.010561079614692263\n",
      "[LOG 20200502-14:39:11] epoch: 12351 train-loss: 0.01056108313302199\n",
      "[LOG 20200502-14:39:12] epoch: 12352 train-loss: 0.010561085926989714\n",
      "[LOG 20200502-14:39:12] epoch: 12353 train-loss: 0.010561089341839155\n",
      "[LOG 20200502-14:39:12] epoch: 12354 train-loss: 0.010561091928846307\n",
      "[LOG 20200502-14:39:12] epoch: 12355 train-loss: 0.01056109524021546\n",
      "[LOG 20200502-14:39:12] epoch: 12356 train-loss: 0.010561097827222612\n",
      "[LOG 20200502-14:39:13] epoch: 12357 train-loss: 0.010561100414229764\n",
      "[LOG 20200502-14:39:13] epoch: 12358 train-loss: 0.0105611023803552\n",
      "[LOG 20200502-14:39:13] epoch: 12359 train-loss: 0.010561104553441206\n",
      "[LOG 20200502-14:39:13] epoch: 12360 train-loss: 0.01056110631260607\n",
      "[LOG 20200502-14:39:14] epoch: 12361 train-loss: 0.01056110786481036\n",
      "[LOG 20200502-14:39:14] epoch: 12362 train-loss: 0.010561109210054079\n",
      "[LOG 20200502-14:39:14] epoch: 12363 train-loss: 0.010561110762258371\n",
      "[LOG 20200502-14:39:14] epoch: 12364 train-loss: 0.010561110969218943\n",
      "[LOG 20200502-14:39:14] epoch: 12365 train-loss: 0.010561110969218943\n",
      "[LOG 20200502-14:39:15] epoch: 12366 train-loss: 0.010561110865738656\n",
      "[LOG 20200502-14:39:15] epoch: 12367 train-loss: 0.010561111072699228\n",
      "[LOG 20200502-14:39:15] epoch: 12368 train-loss: 0.010561109623975225\n",
      "[LOG 20200502-14:39:15] epoch: 12369 train-loss: 0.010561108382211791\n",
      "[LOG 20200502-14:39:16] epoch: 12370 train-loss: 0.010561107036968073\n",
      "[LOG 20200502-14:39:16] epoch: 12371 train-loss: 0.010561105070842637\n",
      "[LOG 20200502-14:39:16] epoch: 12372 train-loss: 0.010561102897756629\n",
      "[LOG 20200502-14:39:16] epoch: 12373 train-loss: 0.010561100310749479\n",
      "[LOG 20200502-14:39:16] epoch: 12374 train-loss: 0.010561097516781755\n",
      "[LOG 20200502-14:39:17] epoch: 12375 train-loss: 0.010561093791491456\n",
      "[LOG 20200502-14:39:17] epoch: 12376 train-loss: 0.010561090066201158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:39:17] epoch: 12377 train-loss: 0.010561085926989714\n",
      "[LOG 20200502-14:39:17] epoch: 12378 train-loss: 0.010561081994738843\n",
      "[LOG 20200502-14:39:18] epoch: 12379 train-loss: 0.010561077648566829\n",
      "[LOG 20200502-14:39:18] epoch: 12380 train-loss: 0.010561073095434241\n",
      "[LOG 20200502-14:39:18] epoch: 12381 train-loss: 0.010561068335341083\n",
      "[LOG 20200502-14:39:18] epoch: 12382 train-loss: 0.010561064092649354\n",
      "[LOG 20200502-14:39:19] epoch: 12383 train-loss: 0.01056105943603648\n",
      "[LOG 20200502-14:39:19] epoch: 12384 train-loss: 0.010561054055061605\n",
      "[LOG 20200502-14:39:19] epoch: 12385 train-loss: 0.010561049501929019\n",
      "[LOG 20200502-14:39:19] epoch: 12386 train-loss: 0.010561044741835859\n",
      "[LOG 20200502-14:39:19] epoch: 12387 train-loss: 0.01056104049914413\n",
      "[LOG 20200502-14:39:20] epoch: 12388 train-loss: 0.010561036256452402\n",
      "[LOG 20200502-14:39:20] epoch: 12389 train-loss: 0.010561031806800101\n",
      "[LOG 20200502-14:39:20] epoch: 12390 train-loss: 0.010561028702391518\n",
      "[LOG 20200502-14:39:20] epoch: 12391 train-loss: 0.010561025080581507\n",
      "[LOG 20200502-14:39:21] epoch: 12392 train-loss: 0.010561021355291208\n",
      "[LOG 20200502-14:39:21] epoch: 12393 train-loss: 0.0105610191822052\n",
      "[LOG 20200502-14:39:21] epoch: 12394 train-loss: 0.010561016181276904\n",
      "[LOG 20200502-14:39:21] epoch: 12395 train-loss: 0.01056101442211204\n",
      "[LOG 20200502-14:39:22] epoch: 12396 train-loss: 0.01056101286990775\n",
      "[LOG 20200502-14:39:22] epoch: 12397 train-loss: 0.010561011524664031\n",
      "[LOG 20200502-14:39:22] epoch: 12398 train-loss: 0.010561011110742887\n",
      "[LOG 20200502-14:39:22] epoch: 12399 train-loss: 0.010561011214223173\n",
      "[LOG 20200502-14:39:22] epoch: 12400 train-loss: 0.01056101183510489\n",
      "[LOG 20200502-14:39:23] epoch: 12401 train-loss: 0.010561012455986606\n",
      "[LOG 20200502-14:39:23] epoch: 12402 train-loss: 0.010561013904710611\n",
      "[LOG 20200502-14:39:23] epoch: 12403 train-loss: 0.010561015353434615\n",
      "[LOG 20200502-14:39:23] epoch: 12404 train-loss: 0.010561017216079764\n",
      "[LOG 20200502-14:39:23] epoch: 12405 train-loss: 0.010561019285685487\n",
      "[LOG 20200502-14:39:24] epoch: 12406 train-loss: 0.010561023010975785\n",
      "[LOG 20200502-14:39:24] epoch: 12407 train-loss: 0.010561026115384366\n",
      "[LOG 20200502-14:39:24] epoch: 12408 train-loss: 0.010561029944154952\n",
      "[LOG 20200502-14:39:24] epoch: 12409 train-loss: 0.010561033876405822\n",
      "[LOG 20200502-14:39:25] epoch: 12410 train-loss: 0.010561038533018695\n",
      "[LOG 20200502-14:39:25] epoch: 12411 train-loss: 0.010561043603552712\n",
      "[LOG 20200502-14:39:25] epoch: 12412 train-loss: 0.010561048881047301\n",
      "[LOG 20200502-14:39:25] epoch: 12413 train-loss: 0.010561054365502464\n",
      "[LOG 20200502-14:39:25] epoch: 12414 train-loss: 0.010561059539516767\n",
      "[LOG 20200502-14:39:26] epoch: 12415 train-loss: 0.01056106605877479\n",
      "[LOG 20200502-14:39:26] epoch: 12416 train-loss: 0.010561072164111666\n",
      "[LOG 20200502-14:39:26] epoch: 12417 train-loss: 0.010561077855527401\n",
      "[LOG 20200502-14:39:26] epoch: 12418 train-loss: 0.01056108447826571\n",
      "[LOG 20200502-14:39:27] epoch: 12419 train-loss: 0.010561090687082874\n",
      "[LOG 20200502-14:39:27] epoch: 12420 train-loss: 0.010561097102860609\n",
      "[LOG 20200502-14:39:27] epoch: 12421 train-loss: 0.01056110393255949\n",
      "[LOG 20200502-14:39:27] epoch: 12422 train-loss: 0.0105611105552978\n",
      "[LOG 20200502-14:39:27] epoch: 12423 train-loss: 0.010561116971075535\n",
      "[LOG 20200502-14:39:28] epoch: 12424 train-loss: 0.010561124007734988\n",
      "[LOG 20200502-14:39:28] epoch: 12425 train-loss: 0.010561130113071866\n",
      "[LOG 20200502-14:39:28] epoch: 12426 train-loss: 0.010561136218408743\n",
      "[LOG 20200502-14:39:28] epoch: 12427 train-loss: 0.010561142427225908\n",
      "[LOG 20200502-14:39:29] epoch: 12428 train-loss: 0.010561148636043072\n",
      "[LOG 20200502-14:39:29] epoch: 12429 train-loss: 0.01056115474137995\n",
      "[LOG 20200502-14:39:29] epoch: 12430 train-loss: 0.010561160432795683\n",
      "[LOG 20200502-14:39:29] epoch: 12431 train-loss: 0.010561165813770559\n",
      "[LOG 20200502-14:39:30] epoch: 12432 train-loss: 0.010561171298225721\n",
      "[LOG 20200502-14:39:30] epoch: 12433 train-loss: 0.010561176368759738\n",
      "[LOG 20200502-14:39:30] epoch: 12434 train-loss: 0.010561180921892325\n",
      "[LOG 20200502-14:39:30] epoch: 12435 train-loss: 0.010561185164584054\n",
      "[LOG 20200502-14:39:30] epoch: 12436 train-loss: 0.010561189717716642\n",
      "[LOG 20200502-14:39:31] epoch: 12437 train-loss: 0.010561193132566081\n",
      "[LOG 20200502-14:39:31] epoch: 12438 train-loss: 0.010561196340454949\n",
      "[LOG 20200502-14:39:31] epoch: 12439 train-loss: 0.010561199651824104\n",
      "[LOG 20200502-14:39:31] epoch: 12440 train-loss: 0.010561202445791828\n",
      "[LOG 20200502-14:39:32] epoch: 12441 train-loss: 0.010561204929318693\n",
      "[LOG 20200502-14:39:32] epoch: 12442 train-loss: 0.010561206791963842\n",
      "[LOG 20200502-14:39:32] epoch: 12443 train-loss: 0.010561208447648419\n",
      "[LOG 20200502-14:39:32] epoch: 12444 train-loss: 0.010561209896372424\n",
      "[LOG 20200502-14:39:33] epoch: 12445 train-loss: 0.010561210931175284\n",
      "[LOG 20200502-14:39:33] epoch: 12446 train-loss: 0.010561211862497859\n",
      "[LOG 20200502-14:39:33] epoch: 12447 train-loss: 0.010561212276419004\n",
      "[LOG 20200502-14:39:33] epoch: 12448 train-loss: 0.010561211862497859\n",
      "[LOG 20200502-14:39:33] epoch: 12449 train-loss: 0.01056121237989929\n",
      "[LOG 20200502-14:39:34] epoch: 12450 train-loss: 0.010561211655537287\n",
      "[LOG 20200502-14:39:34] epoch: 12451 train-loss: 0.010561210620734427\n",
      "[LOG 20200502-14:39:34] epoch: 12452 train-loss: 0.010561210413773855\n",
      "[LOG 20200502-14:39:34] epoch: 12453 train-loss: 0.010561209585931566\n",
      "[LOG 20200502-14:39:35] epoch: 12454 train-loss: 0.010561208551128706\n",
      "[LOG 20200502-14:39:35] epoch: 12455 train-loss: 0.0105612071024047\n",
      "[LOG 20200502-14:39:35] epoch: 12456 train-loss: 0.010561205860641267\n",
      "[LOG 20200502-14:39:35] epoch: 12457 train-loss: 0.010561204825838407\n",
      "[LOG 20200502-14:39:35] epoch: 12458 train-loss: 0.010561203377114402\n",
      "[LOG 20200502-14:39:36] epoch: 12459 train-loss: 0.010561202031870684\n",
      "[LOG 20200502-14:39:36] epoch: 12460 train-loss: 0.01056120079010725\n",
      "[LOG 20200502-14:39:36] epoch: 12461 train-loss: 0.010561198927462101\n",
      "[LOG 20200502-14:39:36] epoch: 12462 train-loss: 0.010561197996139526\n",
      "[LOG 20200502-14:39:36] epoch: 12463 train-loss: 0.010561196754376093\n",
      "[LOG 20200502-14:39:37] epoch: 12464 train-loss: 0.010561195512612661\n",
      "[LOG 20200502-14:39:37] epoch: 12465 train-loss: 0.010561195098691516\n",
      "[LOG 20200502-14:39:37] epoch: 12466 train-loss: 0.010561194063888656\n",
      "[LOG 20200502-14:39:37] epoch: 12467 train-loss: 0.010561193649967512\n",
      "[LOG 20200502-14:39:38] epoch: 12468 train-loss: 0.010561193029085795\n",
      "[LOG 20200502-14:39:38] epoch: 12469 train-loss: 0.01056119292560551\n",
      "[LOG 20200502-14:39:38] epoch: 12470 train-loss: 0.010561193029085795\n",
      "[LOG 20200502-14:39:38] epoch: 12471 train-loss: 0.010561193546487225\n",
      "[LOG 20200502-14:39:38] epoch: 12472 train-loss: 0.010561193546487225\n",
      "[LOG 20200502-14:39:39] epoch: 12473 train-loss: 0.010561193856928084\n",
      "[LOG 20200502-14:39:39] epoch: 12474 train-loss: 0.010561195409132374\n",
      "[LOG 20200502-14:39:39] epoch: 12475 train-loss: 0.01056119603001409\n",
      "[LOG 20200502-14:39:39] epoch: 12476 train-loss: 0.010561197271777524\n",
      "[LOG 20200502-14:39:40] epoch: 12477 train-loss: 0.01056119841006067\n",
      "[LOG 20200502-14:39:40] epoch: 12478 train-loss: 0.01056120027270582\n",
      "[LOG 20200502-14:39:40] epoch: 12479 train-loss: 0.010561201617949538\n",
      "[LOG 20200502-14:39:40] epoch: 12480 train-loss: 0.010561202859712971\n",
      "[LOG 20200502-14:39:40] epoch: 12481 train-loss: 0.01056120472235812\n",
      "[LOG 20200502-14:39:41] epoch: 12482 train-loss: 0.010561206378042698\n",
      "[LOG 20200502-14:39:41] epoch: 12483 train-loss: 0.010561207930246988\n",
      "[LOG 20200502-14:39:41] epoch: 12484 train-loss: 0.010561210206813283\n",
      "[LOG 20200502-14:39:41] epoch: 12485 train-loss: 0.010561211965978146\n",
      "[LOG 20200502-14:39:41] epoch: 12486 train-loss: 0.010561213311221864\n",
      "[LOG 20200502-14:39:42] epoch: 12487 train-loss: 0.010561215173867013\n",
      "[LOG 20200502-14:39:42] epoch: 12488 train-loss: 0.010561216208669875\n",
      "[LOG 20200502-14:39:42] epoch: 12489 train-loss: 0.01056121765739388\n",
      "[LOG 20200502-14:39:42] epoch: 12490 train-loss: 0.010561218899157312\n",
      "[LOG 20200502-14:39:43] epoch: 12491 train-loss: 0.010561220451361604\n",
      "[LOG 20200502-14:39:43] epoch: 12492 train-loss: 0.010561220451361604\n",
      "[LOG 20200502-14:39:43] epoch: 12493 train-loss: 0.010561221796605322\n",
      "[LOG 20200502-14:39:43] epoch: 12494 train-loss: 0.010561222314006753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:39:43] epoch: 12495 train-loss: 0.01056122210704618\n",
      "[LOG 20200502-14:39:44] epoch: 12496 train-loss: 0.01056122158964475\n",
      "[LOG 20200502-14:39:44] epoch: 12497 train-loss: 0.010561221382684179\n",
      "[LOG 20200502-14:39:44] epoch: 12498 train-loss: 0.01056122024440103\n",
      "[LOG 20200502-14:39:44] epoch: 12499 train-loss: 0.010561219002637599\n",
      "[LOG 20200502-14:39:45] epoch: 12500 train-loss: 0.010561217450433306\n",
      "[LOG 20200502-14:39:45] epoch: 12501 train-loss: 0.010561215173867013\n",
      "[LOG 20200502-14:39:45] epoch: 12502 train-loss: 0.010561213104261292\n",
      "[LOG 20200502-14:39:45] epoch: 12503 train-loss: 0.010561210103332996\n",
      "[LOG 20200502-14:39:46] epoch: 12504 train-loss: 0.010561206791963842\n",
      "[LOG 20200502-14:39:46] epoch: 12505 train-loss: 0.010561202963193258\n",
      "[LOG 20200502-14:39:46] epoch: 12506 train-loss: 0.010561199030942388\n",
      "[LOG 20200502-14:39:46] epoch: 12507 train-loss: 0.0105611944778098\n",
      "[LOG 20200502-14:39:46] epoch: 12508 train-loss: 0.010561189821196927\n",
      "[LOG 20200502-14:39:47] epoch: 12509 train-loss: 0.01056118475066291\n",
      "[LOG 20200502-14:39:47] epoch: 12510 train-loss: 0.010561179266207747\n",
      "[LOG 20200502-14:39:47] epoch: 12511 train-loss: 0.010561173264351156\n",
      "[LOG 20200502-14:39:47] epoch: 12512 train-loss: 0.010561167676415708\n",
      "[LOG 20200502-14:39:48] epoch: 12513 train-loss: 0.010561160950197114\n",
      "[LOG 20200502-14:39:48] epoch: 12514 train-loss: 0.010561154534419378\n",
      "[LOG 20200502-14:39:48] epoch: 12515 train-loss: 0.010561147394279638\n",
      "[LOG 20200502-14:39:48] epoch: 12516 train-loss: 0.010561141288942762\n",
      "[LOG 20200502-14:39:49] epoch: 12517 train-loss: 0.010561134252283309\n",
      "[LOG 20200502-14:39:49] epoch: 12518 train-loss: 0.010561127215623856\n",
      "[LOG 20200502-14:39:49] epoch: 12519 train-loss: 0.010561120075484117\n",
      "[LOG 20200502-14:39:49] epoch: 12520 train-loss: 0.010561113970147239\n",
      "[LOG 20200502-14:39:50] epoch: 12521 train-loss: 0.010561106416086355\n",
      "[LOG 20200502-14:39:50] epoch: 12522 train-loss: 0.010561100724670622\n",
      "[LOG 20200502-14:39:50] epoch: 12523 train-loss: 0.010561094515853457\n",
      "[LOG 20200502-14:39:50] epoch: 12524 train-loss: 0.010561088203556009\n",
      "[LOG 20200502-14:39:50] epoch: 12525 train-loss: 0.010561081787778271\n",
      "[LOG 20200502-14:39:51] epoch: 12526 train-loss: 0.010561076613763968\n",
      "[LOG 20200502-14:39:51] epoch: 12527 train-loss: 0.010561071957151094\n",
      "[LOG 20200502-14:39:51] epoch: 12528 train-loss: 0.01056106709357765\n",
      "[LOG 20200502-14:39:51] epoch: 12529 train-loss: 0.010561062643925348\n",
      "[LOG 20200502-14:39:52] epoch: 12530 train-loss: 0.010561058815154765\n",
      "[LOG 20200502-14:39:52] epoch: 12531 train-loss: 0.010561054779423608\n",
      "[LOG 20200502-14:39:52] epoch: 12532 train-loss: 0.01056105208893617\n",
      "[LOG 20200502-14:39:52] epoch: 12533 train-loss: 0.010561049501929019\n",
      "[LOG 20200502-14:39:52] epoch: 12534 train-loss: 0.01056104732884301\n",
      "[LOG 20200502-14:39:53] epoch: 12535 train-loss: 0.01056104577663872\n",
      "[LOG 20200502-14:39:53] epoch: 12536 train-loss: 0.010561043913993571\n",
      "[LOG 20200502-14:39:53] epoch: 12537 train-loss: 0.01056104339659214\n",
      "[LOG 20200502-14:39:53] epoch: 12538 train-loss: 0.01056104339659214\n",
      "[LOG 20200502-14:39:54] epoch: 12539 train-loss: 0.010561043500072427\n",
      "[LOG 20200502-14:39:54] epoch: 12540 train-loss: 0.010561043913993571\n",
      "[LOG 20200502-14:39:54] epoch: 12541 train-loss: 0.010561044638355574\n",
      "[LOG 20200502-14:39:54] epoch: 12542 train-loss: 0.010561045983599292\n",
      "[LOG 20200502-14:39:55] epoch: 12543 train-loss: 0.01056104763928387\n",
      "[LOG 20200502-14:39:55] epoch: 12544 train-loss: 0.01056104970888959\n",
      "[LOG 20200502-14:39:55] epoch: 12545 train-loss: 0.0105610526063376\n",
      "[LOG 20200502-14:39:55] epoch: 12546 train-loss: 0.010561055193344751\n",
      "[LOG 20200502-14:39:55] epoch: 12547 train-loss: 0.010561057987312475\n",
      "[LOG 20200502-14:39:56] epoch: 12548 train-loss: 0.010561061402161917\n",
      "[LOG 20200502-14:39:56] epoch: 12549 train-loss: 0.010561065127452215\n",
      "[LOG 20200502-14:39:56] epoch: 12550 train-loss: 0.010561068956222799\n",
      "[LOG 20200502-14:39:56] epoch: 12551 train-loss: 0.010561072784993384\n",
      "[LOG 20200502-14:39:57] epoch: 12552 train-loss: 0.010561076510283682\n",
      "[LOG 20200502-14:39:57] epoch: 12553 train-loss: 0.01056108127037684\n",
      "[LOG 20200502-14:39:57] epoch: 12554 train-loss: 0.010561085099147426\n",
      "[LOG 20200502-14:39:57] epoch: 12555 train-loss: 0.01056108944531944\n",
      "[LOG 20200502-14:39:57] epoch: 12556 train-loss: 0.01056109368801117\n",
      "[LOG 20200502-14:39:58] epoch: 12557 train-loss: 0.010561098655064901\n",
      "[LOG 20200502-14:39:58] epoch: 12558 train-loss: 0.010561103001236916\n",
      "[LOG 20200502-14:39:58] epoch: 12559 train-loss: 0.010561107657849789\n",
      "[LOG 20200502-14:39:58] epoch: 12560 train-loss: 0.010561112004021803\n",
      "[LOG 20200502-14:39:59] epoch: 12561 train-loss: 0.01056111603975296\n",
      "[LOG 20200502-14:39:59] epoch: 12562 train-loss: 0.010561121006806692\n",
      "[LOG 20200502-14:39:59] epoch: 12563 train-loss: 0.010561124835577276\n",
      "[LOG 20200502-14:39:59] epoch: 12564 train-loss: 0.010561128664347861\n",
      "[LOG 20200502-14:40:00] epoch: 12565 train-loss: 0.010561132803559303\n",
      "[LOG 20200502-14:40:00] epoch: 12566 train-loss: 0.010561136425369315\n",
      "[LOG 20200502-14:40:00] epoch: 12567 train-loss: 0.01056113973673847\n",
      "[LOG 20200502-14:40:00] epoch: 12568 train-loss: 0.010561143151587911\n",
      "[LOG 20200502-14:40:01] epoch: 12569 train-loss: 0.010561146152516207\n",
      "[LOG 20200502-14:40:01] epoch: 12570 train-loss: 0.010561148636043072\n",
      "[LOG 20200502-14:40:01] epoch: 12571 train-loss: 0.010561151223050224\n",
      "[LOG 20200502-14:40:01] epoch: 12572 train-loss: 0.010561153396136232\n",
      "[LOG 20200502-14:40:01] epoch: 12573 train-loss: 0.010561155465741953\n",
      "[LOG 20200502-14:40:02] epoch: 12574 train-loss: 0.010561156707505385\n",
      "[LOG 20200502-14:40:02] epoch: 12575 train-loss: 0.010561158259709677\n",
      "[LOG 20200502-14:40:02] epoch: 12576 train-loss: 0.010561159191032251\n",
      "[LOG 20200502-14:40:02] epoch: 12577 train-loss: 0.010561159915394254\n",
      "[LOG 20200502-14:40:03] epoch: 12578 train-loss: 0.010561160122354826\n",
      "[LOG 20200502-14:40:03] epoch: 12579 train-loss: 0.010561160225835111\n",
      "[LOG 20200502-14:40:03] epoch: 12580 train-loss: 0.010561159604953395\n",
      "[LOG 20200502-14:40:03] epoch: 12581 train-loss: 0.010561158880591393\n",
      "[LOG 20200502-14:40:03] epoch: 12582 train-loss: 0.010561158259709677\n",
      "[LOG 20200502-14:40:04] epoch: 12583 train-loss: 0.010561156810985671\n",
      "[LOG 20200502-14:40:04] epoch: 12584 train-loss: 0.010561155258781381\n",
      "[LOG 20200502-14:40:04] epoch: 12585 train-loss: 0.010561153603096804\n",
      "[LOG 20200502-14:40:04] epoch: 12586 train-loss: 0.010561151016089652\n",
      "[LOG 20200502-14:40:05] epoch: 12587 train-loss: 0.010561148532562785\n",
      "[LOG 20200502-14:40:05] epoch: 12588 train-loss: 0.010561146773397923\n",
      "[LOG 20200502-14:40:05] epoch: 12589 train-loss: 0.010561143462028768\n",
      "[LOG 20200502-14:40:05] epoch: 12590 train-loss: 0.010561140564580759\n",
      "[LOG 20200502-14:40:05] epoch: 12591 train-loss: 0.010561137563652463\n",
      "[LOG 20200502-14:40:06] epoch: 12592 train-loss: 0.010561134562724166\n",
      "[LOG 20200502-14:40:06] epoch: 12593 train-loss: 0.010561130940914154\n",
      "[LOG 20200502-14:40:06] epoch: 12594 train-loss: 0.010561127939985858\n",
      "[LOG 20200502-14:40:06] epoch: 12595 train-loss: 0.010561124318175845\n",
      "[LOG 20200502-14:40:07] epoch: 12596 train-loss: 0.01056112079984612\n",
      "[LOG 20200502-14:40:07] epoch: 12597 train-loss: 0.010561117178036107\n",
      "[LOG 20200502-14:40:07] epoch: 12598 train-loss: 0.01056111365970638\n",
      "[LOG 20200502-14:40:07] epoch: 12599 train-loss: 0.010561110865738656\n",
      "[LOG 20200502-14:40:07] epoch: 12600 train-loss: 0.010561107140448358\n",
      "[LOG 20200502-14:40:08] epoch: 12601 train-loss: 0.010561104449960921\n",
      "[LOG 20200502-14:40:08] epoch: 12602 train-loss: 0.010561101138591766\n",
      "[LOG 20200502-14:40:08] epoch: 12603 train-loss: 0.010561097930702899\n",
      "[LOG 20200502-14:40:08] epoch: 12604 train-loss: 0.010561095343695747\n",
      "[LOG 20200502-14:40:09] epoch: 12605 train-loss: 0.010561092963649167\n",
      "[LOG 20200502-14:40:09] epoch: 12606 train-loss: 0.010561090169681443\n",
      "[LOG 20200502-14:40:09] epoch: 12607 train-loss: 0.01056108841051658\n",
      "[LOG 20200502-14:40:09] epoch: 12608 train-loss: 0.01056108603047\n",
      "[LOG 20200502-14:40:09] epoch: 12609 train-loss: 0.01056108365042342\n",
      "[LOG 20200502-14:40:10] epoch: 12610 train-loss: 0.010561082305179702\n",
      "[LOG 20200502-14:40:10] epoch: 12611 train-loss: 0.010561080752975412\n",
      "[LOG 20200502-14:40:10] epoch: 12612 train-loss: 0.010561080132093694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:40:10] epoch: 12613 train-loss: 0.010561079097290834\n",
      "[LOG 20200502-14:40:10] epoch: 12614 train-loss: 0.010561077752047114\n",
      "[LOG 20200502-14:40:11] epoch: 12615 train-loss: 0.010561077027685113\n",
      "[LOG 20200502-14:40:11] epoch: 12616 train-loss: 0.010561076096362539\n",
      "[LOG 20200502-14:40:11] epoch: 12617 train-loss: 0.01056107578592168\n",
      "[LOG 20200502-14:40:11] epoch: 12618 train-loss: 0.010561075372000536\n",
      "[LOG 20200502-14:40:12] epoch: 12619 train-loss: 0.010561075061559677\n",
      "[LOG 20200502-14:40:12] epoch: 12620 train-loss: 0.01056107495807939\n",
      "[LOG 20200502-14:40:12] epoch: 12621 train-loss: 0.010561075372000536\n",
      "[LOG 20200502-14:40:12] epoch: 12622 train-loss: 0.010561075061559677\n",
      "[LOG 20200502-14:40:12] epoch: 12623 train-loss: 0.01056107495807939\n",
      "[LOG 20200502-14:40:13] epoch: 12624 train-loss: 0.010561074337197674\n",
      "[LOG 20200502-14:40:13] epoch: 12625 train-loss: 0.01056107495807939\n",
      "[LOG 20200502-14:40:13] epoch: 12626 train-loss: 0.010561074026756816\n",
      "[LOG 20200502-14:40:13] epoch: 12627 train-loss: 0.01056107423371739\n",
      "[LOG 20200502-14:40:14] epoch: 12628 train-loss: 0.010561073302394815\n",
      "[LOG 20200502-14:40:14] epoch: 12629 train-loss: 0.010561073095434241\n",
      "[LOG 20200502-14:40:14] epoch: 12630 train-loss: 0.010561072267591953\n",
      "[LOG 20200502-14:40:14] epoch: 12631 train-loss: 0.010561071129308807\n",
      "[LOG 20200502-14:40:14] epoch: 12632 train-loss: 0.010561070197986232\n",
      "[LOG 20200502-14:40:15] epoch: 12633 train-loss: 0.010561068542301655\n",
      "[LOG 20200502-14:40:15] epoch: 12634 train-loss: 0.01056106709357765\n",
      "[LOG 20200502-14:40:15] epoch: 12635 train-loss: 0.010561064920491643\n",
      "[LOG 20200502-14:40:15] epoch: 12636 train-loss: 0.010561062230004204\n",
      "[LOG 20200502-14:40:16] epoch: 12637 train-loss: 0.01056105974647734\n",
      "[LOG 20200502-14:40:16] epoch: 12638 train-loss: 0.010561056435108185\n",
      "[LOG 20200502-14:40:16] epoch: 12639 train-loss: 0.010561053641140461\n",
      "[LOG 20200502-14:40:16] epoch: 12640 train-loss: 0.01056104970888959\n",
      "[LOG 20200502-14:40:16] epoch: 12641 train-loss: 0.010561045155757003\n",
      "[LOG 20200502-14:40:17] epoch: 12642 train-loss: 0.010561040706104703\n",
      "[LOG 20200502-14:40:17] epoch: 12643 train-loss: 0.010561035428610113\n",
      "[LOG 20200502-14:40:17] epoch: 12644 train-loss: 0.010561030047635237\n",
      "[LOG 20200502-14:40:17] epoch: 12645 train-loss: 0.010561024563180076\n",
      "[LOG 20200502-14:40:18] epoch: 12646 train-loss: 0.010561018561323484\n",
      "[LOG 20200502-14:40:18] epoch: 12647 train-loss: 0.010561011524664031\n",
      "[LOG 20200502-14:40:18] epoch: 12648 train-loss: 0.010561005315846868\n",
      "[LOG 20200502-14:40:18] epoch: 12649 train-loss: 0.01056099683046341\n",
      "[LOG 20200502-14:40:18] epoch: 12650 train-loss: 0.010560990000764528\n",
      "[LOG 20200502-14:40:19] epoch: 12651 train-loss: 0.010560981929302216\n",
      "[LOG 20200502-14:40:19] epoch: 12652 train-loss: 0.010560973961320188\n",
      "[LOG 20200502-14:40:19] epoch: 12653 train-loss: 0.010560965682897303\n",
      "[LOG 20200502-14:40:19] epoch: 12654 train-loss: 0.010560956576632129\n",
      "[LOG 20200502-14:40:20] epoch: 12655 train-loss: 0.010560947884288099\n",
      "[LOG 20200502-14:40:20] epoch: 12656 train-loss: 0.01056093919194407\n",
      "[LOG 20200502-14:40:20] epoch: 12657 train-loss: 0.010560930603080325\n",
      "[LOG 20200502-14:40:20] epoch: 12658 train-loss: 0.010560921703775724\n",
      "[LOG 20200502-14:40:20] epoch: 12659 train-loss: 0.01056091259751055\n",
      "[LOG 20200502-14:40:21] epoch: 12660 train-loss: 0.010560904112127092\n",
      "[LOG 20200502-14:40:21] epoch: 12661 train-loss: 0.010560895626743635\n",
      "[LOG 20200502-14:40:21] epoch: 12662 train-loss: 0.010560886416998174\n",
      "[LOG 20200502-14:40:21] epoch: 12663 train-loss: 0.010560878242055574\n",
      "[LOG 20200502-14:40:22] epoch: 12664 train-loss: 0.01056087048103412\n",
      "[LOG 20200502-14:40:22] epoch: 12665 train-loss: 0.010560862823492952\n",
      "[LOG 20200502-14:40:22] epoch: 12666 train-loss: 0.01056085547639264\n",
      "[LOG 20200502-14:40:22] epoch: 12667 train-loss: 0.01056084812929233\n",
      "[LOG 20200502-14:40:23] epoch: 12668 train-loss: 0.010560841403073735\n",
      "[LOG 20200502-14:40:23] epoch: 12669 train-loss: 0.010560834883815713\n",
      "[LOG 20200502-14:40:23] epoch: 12670 train-loss: 0.010560829192399979\n",
      "[LOG 20200502-14:40:23] epoch: 12671 train-loss: 0.010560823397503959\n",
      "[LOG 20200502-14:40:23] epoch: 12672 train-loss: 0.010560818326969942\n",
      "[LOG 20200502-14:40:24] epoch: 12673 train-loss: 0.010560813773837354\n",
      "[LOG 20200502-14:40:24] epoch: 12674 train-loss: 0.010560809220704768\n",
      "[LOG 20200502-14:40:24] epoch: 12675 train-loss: 0.010560805702375041\n",
      "[LOG 20200502-14:40:24] epoch: 12676 train-loss: 0.010560802494486174\n",
      "[LOG 20200502-14:40:24] epoch: 12677 train-loss: 0.010560799079636732\n",
      "[LOG 20200502-14:40:25] epoch: 12678 train-loss: 0.010560796803070439\n",
      "[LOG 20200502-14:40:25] epoch: 12679 train-loss: 0.010560794733464718\n",
      "[LOG 20200502-14:40:25] epoch: 12680 train-loss: 0.010560793388221\n",
      "[LOG 20200502-14:40:25] epoch: 12681 train-loss: 0.010560791318615278\n",
      "[LOG 20200502-14:40:26] epoch: 12682 train-loss: 0.01056079069773356\n",
      "[LOG 20200502-14:40:26] epoch: 12683 train-loss: 0.010560790490772989\n",
      "[LOG 20200502-14:40:26] epoch: 12684 train-loss: 0.010560789973371558\n",
      "[LOG 20200502-14:40:26] epoch: 12685 train-loss: 0.010560789766410986\n",
      "[LOG 20200502-14:40:27] epoch: 12686 train-loss: 0.01056079100817442\n",
      "[LOG 20200502-14:40:27] epoch: 12687 train-loss: 0.01056079100817442\n",
      "[LOG 20200502-14:40:27] epoch: 12688 train-loss: 0.01056079204297728\n",
      "[LOG 20200502-14:40:27] epoch: 12689 train-loss: 0.010560793698661856\n",
      "[LOG 20200502-14:40:28] epoch: 12690 train-loss: 0.010560794319543574\n",
      "[LOG 20200502-14:40:28] epoch: 12691 train-loss: 0.01056079576826758\n",
      "[LOG 20200502-14:40:28] epoch: 12692 train-loss: 0.0105607978378733\n",
      "[LOG 20200502-14:40:28] epoch: 12693 train-loss: 0.010560800010959307\n",
      "[LOG 20200502-14:40:29] epoch: 12694 train-loss: 0.010560802184045315\n",
      "[LOG 20200502-14:40:29] epoch: 12695 train-loss: 0.010560804046690464\n",
      "[LOG 20200502-14:40:29] epoch: 12696 train-loss: 0.010560806530217329\n",
      "[LOG 20200502-14:40:29] epoch: 12697 train-loss: 0.010560808496342765\n",
      "[LOG 20200502-14:40:29] epoch: 12698 train-loss: 0.01056081097986963\n",
      "[LOG 20200502-14:40:30] epoch: 12699 train-loss: 0.01056081284251478\n",
      "[LOG 20200502-14:40:30] epoch: 12700 train-loss: 0.010560815326041646\n",
      "[LOG 20200502-14:40:30] epoch: 12701 train-loss: 0.01056081760260794\n",
      "[LOG 20200502-14:40:30] epoch: 12702 train-loss: 0.010560819775693946\n",
      "[LOG 20200502-14:40:31] epoch: 12703 train-loss: 0.01056082205226024\n",
      "[LOG 20200502-14:40:31] epoch: 12704 train-loss: 0.010560824121865962\n",
      "[LOG 20200502-14:40:31] epoch: 12705 train-loss: 0.010560825363629393\n",
      "[LOG 20200502-14:40:31] epoch: 12706 train-loss: 0.010560827536715401\n",
      "[LOG 20200502-14:40:32] epoch: 12707 train-loss: 0.010560828571518263\n",
      "[LOG 20200502-14:40:32] epoch: 12708 train-loss: 0.01056083022720284\n",
      "[LOG 20200502-14:40:32] epoch: 12709 train-loss: 0.010560830951564841\n",
      "[LOG 20200502-14:40:32] epoch: 12710 train-loss: 0.010560832400288846\n",
      "[LOG 20200502-14:40:32] epoch: 12711 train-loss: 0.01056083281420999\n",
      "[LOG 20200502-14:40:33] epoch: 12712 train-loss: 0.01056083281420999\n",
      "[LOG 20200502-14:40:33] epoch: 12713 train-loss: 0.010560833331611421\n",
      "[LOG 20200502-14:40:33] epoch: 12714 train-loss: 0.010560833331611421\n",
      "[LOG 20200502-14:40:33] epoch: 12715 train-loss: 0.010560832607249418\n",
      "[LOG 20200502-14:40:34] epoch: 12716 train-loss: 0.010560831882887416\n",
      "[LOG 20200502-14:40:34] epoch: 12717 train-loss: 0.010560830848084556\n",
      "[LOG 20200502-14:40:34] epoch: 12718 train-loss: 0.01056082939936055\n",
      "[LOG 20200502-14:40:34] epoch: 12719 train-loss: 0.010560828054116832\n",
      "[LOG 20200502-14:40:35] epoch: 12720 train-loss: 0.010560825777550539\n",
      "[LOG 20200502-14:40:35] epoch: 12721 train-loss: 0.010560823707944818\n",
      "[LOG 20200502-14:40:35] epoch: 12722 train-loss: 0.010560821120937666\n",
      "[LOG 20200502-14:40:35] epoch: 12723 train-loss: 0.010560818533930514\n",
      "[LOG 20200502-14:40:36] epoch: 12724 train-loss: 0.010560815429521931\n",
      "[LOG 20200502-14:40:36] epoch: 12725 train-loss: 0.010560811911192205\n",
      "[LOG 20200502-14:40:36] epoch: 12726 train-loss: 0.010560807978941334\n",
      "[LOG 20200502-14:40:36] epoch: 12727 train-loss: 0.010560804253651036\n",
      "[LOG 20200502-14:40:36] epoch: 12728 train-loss: 0.010560800010959307\n",
      "[LOG 20200502-14:40:37] epoch: 12729 train-loss: 0.01056079545782672\n",
      "[LOG 20200502-14:40:37] epoch: 12730 train-loss: 0.010560790490772989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:40:37] epoch: 12731 train-loss: 0.010560786144600974\n",
      "[LOG 20200502-14:40:37] epoch: 12732 train-loss: 0.010560781177547243\n",
      "[LOG 20200502-14:40:38] epoch: 12733 train-loss: 0.010560776417454084\n",
      "[LOG 20200502-14:40:38] epoch: 12734 train-loss: 0.01056077155388064\n",
      "[LOG 20200502-14:40:38] epoch: 12735 train-loss: 0.010560765965945192\n",
      "[LOG 20200502-14:40:38] epoch: 12736 train-loss: 0.010560760378009744\n",
      "[LOG 20200502-14:40:39] epoch: 12737 train-loss: 0.010560754583113723\n",
      "[LOG 20200502-14:40:39] epoch: 12738 train-loss: 0.010560749305619134\n",
      "[LOG 20200502-14:40:39] epoch: 12739 train-loss: 0.010560743924644258\n",
      "[LOG 20200502-14:40:39] epoch: 12740 train-loss: 0.010560738750629954\n",
      "[LOG 20200502-14:40:40] epoch: 12741 train-loss: 0.010560733162694506\n",
      "[LOG 20200502-14:40:40] epoch: 12742 train-loss: 0.010560727988680204\n",
      "[LOG 20200502-14:40:40] epoch: 12743 train-loss: 0.010560722193784185\n",
      "[LOG 20200502-14:40:40] epoch: 12744 train-loss: 0.010560717330210738\n",
      "[LOG 20200502-14:40:40] epoch: 12745 train-loss: 0.010560712777078152\n",
      "[LOG 20200502-14:40:41] epoch: 12746 train-loss: 0.01056070781002442\n",
      "[LOG 20200502-14:40:41] epoch: 12747 train-loss: 0.010560702946450975\n",
      "[LOG 20200502-14:40:41] epoch: 12748 train-loss: 0.010560698703759246\n",
      "[LOG 20200502-14:40:41] epoch: 12749 train-loss: 0.010560694461067518\n",
      "[LOG 20200502-14:40:42] epoch: 12750 train-loss: 0.010560689597494073\n",
      "[LOG 20200502-14:40:42] epoch: 12751 train-loss: 0.010560686596565776\n",
      "[LOG 20200502-14:40:42] epoch: 12752 train-loss: 0.010560682457354333\n",
      "[LOG 20200502-14:40:42] epoch: 12753 train-loss: 0.01056067862858375\n",
      "[LOG 20200502-14:40:43] epoch: 12754 train-loss: 0.010560675731135739\n",
      "[LOG 20200502-14:40:43] epoch: 12755 train-loss: 0.010560672419766584\n",
      "[LOG 20200502-14:40:43] epoch: 12756 train-loss: 0.010560669418838289\n",
      "[LOG 20200502-14:40:43] epoch: 12757 train-loss: 0.010560666314429708\n",
      "[LOG 20200502-14:40:44] epoch: 12758 train-loss: 0.010560663623942269\n",
      "[LOG 20200502-14:40:44] epoch: 12759 train-loss: 0.010560661657816835\n",
      "[LOG 20200502-14:40:44] epoch: 12760 train-loss: 0.010560658656888537\n",
      "[LOG 20200502-14:40:44] epoch: 12761 train-loss: 0.010560656587282816\n",
      "[LOG 20200502-14:40:45] epoch: 12762 train-loss: 0.010560654517677095\n",
      "[LOG 20200502-14:40:45] epoch: 12763 train-loss: 0.010560652448071374\n",
      "[LOG 20200502-14:40:45] epoch: 12764 train-loss: 0.010560649861064222\n",
      "[LOG 20200502-14:40:46] epoch: 12765 train-loss: 0.010560648515820503\n",
      "[LOG 20200502-14:40:46] epoch: 12766 train-loss: 0.010560646342734495\n",
      "[LOG 20200502-14:40:46] epoch: 12767 train-loss: 0.010560644790530205\n",
      "[LOG 20200502-14:40:46] epoch: 12768 train-loss: 0.010560641893082194\n",
      "[LOG 20200502-14:40:47] epoch: 12769 train-loss: 0.010560640237397619\n",
      "[LOG 20200502-14:40:47] epoch: 12770 train-loss: 0.01056063806431161\n",
      "[LOG 20200502-14:40:47] epoch: 12771 train-loss: 0.010560635580784745\n",
      "[LOG 20200502-14:40:47] epoch: 12772 train-loss: 0.010560633200738165\n",
      "[LOG 20200502-14:40:48] epoch: 12773 train-loss: 0.010560630924171872\n",
      "[LOG 20200502-14:40:48] epoch: 12774 train-loss: 0.010560627923243575\n",
      "[LOG 20200502-14:40:48] epoch: 12775 train-loss: 0.010560624818834994\n",
      "[LOG 20200502-14:40:48] epoch: 12776 train-loss: 0.010560621817906698\n",
      "[LOG 20200502-14:40:49] epoch: 12777 train-loss: 0.010560618713498116\n",
      "[LOG 20200502-14:40:49] epoch: 12778 train-loss: 0.010560614677766958\n",
      "[LOG 20200502-14:40:49] epoch: 12779 train-loss: 0.010560610538555516\n",
      "[LOG 20200502-14:40:49] epoch: 12780 train-loss: 0.010560606502824359\n",
      "[LOG 20200502-14:40:50] epoch: 12781 train-loss: 0.010560601121849485\n",
      "[LOG 20200502-14:40:50] epoch: 12782 train-loss: 0.010560595947835181\n",
      "[LOG 20200502-14:40:50] epoch: 12783 train-loss: 0.01056058994597859\n",
      "[LOG 20200502-14:40:50] epoch: 12784 train-loss: 0.01056058487544457\n",
      "[LOG 20200502-14:40:51] epoch: 12785 train-loss: 0.010560578666627407\n",
      "[LOG 20200502-14:40:51] epoch: 12786 train-loss: 0.010560571526487669\n",
      "[LOG 20200502-14:40:51] epoch: 12787 train-loss: 0.010560564386347929\n",
      "[LOG 20200502-14:40:51] epoch: 12788 train-loss: 0.01056055672880676\n",
      "[LOG 20200502-14:40:52] epoch: 12789 train-loss: 0.010560549485186735\n",
      "[LOG 20200502-14:40:52] epoch: 12790 train-loss: 0.010560540585882135\n",
      "[LOG 20200502-14:40:52] epoch: 12791 train-loss: 0.010560532100498676\n",
      "[LOG 20200502-14:40:52] epoch: 12792 train-loss: 0.01056052278727293\n",
      "[LOG 20200502-14:40:53] epoch: 12793 train-loss: 0.010560514301889472\n",
      "[LOG 20200502-14:40:53] epoch: 12794 train-loss: 0.010560504057341151\n",
      "[LOG 20200502-14:40:53] epoch: 12795 train-loss: 0.010560493916273117\n",
      "[LOG 20200502-14:40:53] epoch: 12796 train-loss: 0.010560484189126227\n",
      "[LOG 20200502-14:40:54] epoch: 12797 train-loss: 0.01056047384109762\n",
      "[LOG 20200502-14:40:54] epoch: 12798 train-loss: 0.010560463182628155\n",
      "[LOG 20200502-14:40:54] epoch: 12799 train-loss: 0.010560452834599547\n",
      "[LOG 20200502-14:40:54] epoch: 12800 train-loss: 0.010560441865689225\n",
      "[LOG 20200502-14:40:55] epoch: 12801 train-loss: 0.010560431000259187\n",
      "[LOG 20200502-14:40:55] epoch: 12802 train-loss: 0.01056042044527001\n",
      "[LOG 20200502-14:40:55] epoch: 12803 train-loss: 0.010560409372879399\n",
      "[LOG 20200502-14:40:55] epoch: 12804 train-loss: 0.010560399128331078\n",
      "[LOG 20200502-14:40:56] epoch: 12805 train-loss: 0.010560388780302472\n",
      "[LOG 20200502-14:40:56] epoch: 12806 train-loss: 0.010560377811392149\n",
      "[LOG 20200502-14:40:56] epoch: 12807 train-loss: 0.010560368187725544\n",
      "[LOG 20200502-14:40:56] epoch: 12808 train-loss: 0.010560358253618082\n",
      "[LOG 20200502-14:40:56] epoch: 12809 train-loss: 0.010560348629951477\n",
      "[LOG 20200502-14:40:57] epoch: 12810 train-loss: 0.010560339006284872\n",
      "[LOG 20200502-14:40:57] epoch: 12811 train-loss: 0.010560329900019698\n",
      "[LOG 20200502-14:40:57] epoch: 12812 train-loss: 0.010560321414636241\n",
      "[LOG 20200502-14:40:57] epoch: 12813 train-loss: 0.01056031406753593\n",
      "[LOG 20200502-14:40:58] epoch: 12814 train-loss: 0.010560305685632758\n",
      "[LOG 20200502-14:40:58] epoch: 12815 train-loss: 0.010560298442012735\n",
      "[LOG 20200502-14:40:58] epoch: 12816 train-loss: 0.010560290887951851\n",
      "[LOG 20200502-14:40:58] epoch: 12817 train-loss: 0.010560284161733257\n",
      "[LOG 20200502-14:40:59] epoch: 12818 train-loss: 0.010560278159876665\n",
      "[LOG 20200502-14:40:59] epoch: 12819 train-loss: 0.010560272882382074\n",
      "[LOG 20200502-14:40:59] epoch: 12820 train-loss: 0.010560266777045198\n",
      "[LOG 20200502-14:40:59] epoch: 12821 train-loss: 0.010560262534353469\n",
      "[LOG 20200502-14:41:00] epoch: 12822 train-loss: 0.010560257877740595\n",
      "[LOG 20200502-14:41:00] epoch: 12823 train-loss: 0.010560253738529153\n",
      "[LOG 20200502-14:41:00] epoch: 12824 train-loss: 0.010560250013238855\n",
      "[LOG 20200502-14:41:00] epoch: 12825 train-loss: 0.0105602467018697\n",
      "[LOG 20200502-14:41:01] epoch: 12826 train-loss: 0.010560243597461117\n",
      "[LOG 20200502-14:41:01] epoch: 12827 train-loss: 0.010560241320894824\n",
      "[LOG 20200502-14:41:01] epoch: 12828 train-loss: 0.010560238940848244\n",
      "[LOG 20200502-14:41:01] epoch: 12829 train-loss: 0.01056023697472281\n",
      "[LOG 20200502-14:41:01] epoch: 12830 train-loss: 0.010560235215557946\n",
      "[LOG 20200502-14:41:02] epoch: 12831 train-loss: 0.010560233973794513\n",
      "[LOG 20200502-14:41:02] epoch: 12832 train-loss: 0.010560233042471938\n",
      "[LOG 20200502-14:41:02] epoch: 12833 train-loss: 0.01056023221462965\n",
      "[LOG 20200502-14:41:03] epoch: 12834 train-loss: 0.010560231800708506\n",
      "[LOG 20200502-14:41:03] epoch: 12835 train-loss: 0.010560230972866217\n",
      "[LOG 20200502-14:41:03] epoch: 12836 train-loss: 0.01056023066242536\n",
      "[LOG 20200502-14:41:03] epoch: 12837 train-loss: 0.01056023066242536\n",
      "[LOG 20200502-14:41:04] epoch: 12838 train-loss: 0.010560230869385932\n",
      "[LOG 20200502-14:41:04] epoch: 12839 train-loss: 0.010560230248504214\n",
      "[LOG 20200502-14:41:04] epoch: 12840 train-loss: 0.010560230765905645\n",
      "[LOG 20200502-14:41:04] epoch: 12841 train-loss: 0.010560230972866217\n",
      "[LOG 20200502-14:41:05] epoch: 12842 train-loss: 0.010560231490267647\n",
      "[LOG 20200502-14:41:05] epoch: 12843 train-loss: 0.010560231490267647\n",
      "[LOG 20200502-14:41:05] epoch: 12844 train-loss: 0.010560231904188791\n",
      "[LOG 20200502-14:41:05] epoch: 12845 train-loss: 0.010560232318109937\n",
      "[LOG 20200502-14:41:06] epoch: 12846 train-loss: 0.010560232421590222\n",
      "[LOG 20200502-14:41:06] epoch: 12847 train-loss: 0.01056023221462965\n",
      "[LOG 20200502-14:41:06] epoch: 12848 train-loss: 0.01056023273203108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:41:06] epoch: 12849 train-loss: 0.010560232318109937\n",
      "[LOG 20200502-14:41:06] epoch: 12850 train-loss: 0.010560232835511366\n",
      "[LOG 20200502-14:41:07] epoch: 12851 train-loss: 0.010560232111149363\n",
      "[LOG 20200502-14:41:07] epoch: 12852 train-loss: 0.010560231283307076\n",
      "[LOG 20200502-14:41:07] epoch: 12853 train-loss: 0.010560230765905645\n",
      "[LOG 20200502-14:41:07] epoch: 12854 train-loss: 0.01056022983458307\n",
      "[LOG 20200502-14:41:08] epoch: 12855 train-loss: 0.01056022879978021\n",
      "[LOG 20200502-14:41:08] epoch: 12856 train-loss: 0.010560227764977349\n",
      "[LOG 20200502-14:41:08] epoch: 12857 train-loss: 0.010560225695371628\n",
      "[LOG 20200502-14:41:08] epoch: 12858 train-loss: 0.010560224246647622\n",
      "[LOG 20200502-14:41:09] epoch: 12859 train-loss: 0.010560221866601043\n",
      "[LOG 20200502-14:41:09] epoch: 12860 train-loss: 0.010560219693515036\n",
      "[LOG 20200502-14:41:09] epoch: 12861 train-loss: 0.01056021720998817\n",
      "[LOG 20200502-14:41:09] epoch: 12862 train-loss: 0.010560214105579589\n",
      "[LOG 20200502-14:41:10] epoch: 12863 train-loss: 0.010560211104651293\n",
      "[LOG 20200502-14:41:10] epoch: 12864 train-loss: 0.010560207379360994\n",
      "[LOG 20200502-14:41:10] epoch: 12865 train-loss: 0.010560203343629837\n",
      "[LOG 20200502-14:41:10] epoch: 12866 train-loss: 0.010560199721819825\n",
      "[LOG 20200502-14:41:11] epoch: 12867 train-loss: 0.010560195479128096\n",
      "[LOG 20200502-14:41:11] epoch: 12868 train-loss: 0.010560190719034936\n",
      "[LOG 20200502-14:41:11] epoch: 12869 train-loss: 0.010560186062422063\n",
      "[LOG 20200502-14:41:11] epoch: 12870 train-loss: 0.010560180681447187\n",
      "[LOG 20200502-14:41:11] epoch: 12871 train-loss: 0.010560175403952599\n",
      "[LOG 20200502-14:41:12] epoch: 12872 train-loss: 0.010560169505576292\n",
      "[LOG 20200502-14:41:12] epoch: 12873 train-loss: 0.010560163917640844\n",
      "[LOG 20200502-14:41:12] epoch: 12874 train-loss: 0.010560157501863109\n",
      "[LOG 20200502-14:41:12] epoch: 12875 train-loss: 0.010560151500006517\n",
      "[LOG 20200502-14:41:13] epoch: 12876 train-loss: 0.010560145705110498\n",
      "[LOG 20200502-14:41:13] epoch: 12877 train-loss: 0.010560138461490473\n",
      "[LOG 20200502-14:41:13] epoch: 12878 train-loss: 0.010560131838752164\n",
      "[LOG 20200502-14:41:13] epoch: 12879 train-loss: 0.01056012480209271\n",
      "[LOG 20200502-14:41:14] epoch: 12880 train-loss: 0.010560118282834688\n",
      "[LOG 20200502-14:41:14] epoch: 12881 train-loss: 0.01056011114269495\n",
      "[LOG 20200502-14:41:14] epoch: 12882 train-loss: 0.010560103485153781\n",
      "[LOG 20200502-14:41:14] epoch: 12883 train-loss: 0.01056009748329719\n",
      "[LOG 20200502-14:41:14] epoch: 12884 train-loss: 0.01056008982575602\n",
      "[LOG 20200502-14:41:15] epoch: 12885 train-loss: 0.010560082582135996\n",
      "[LOG 20200502-14:41:15] epoch: 12886 train-loss: 0.010560075855917402\n",
      "[LOG 20200502-14:41:15] epoch: 12887 train-loss: 0.010560068715777662\n",
      "[LOG 20200502-14:41:15] epoch: 12888 train-loss: 0.010560061575637924\n",
      "[LOG 20200502-14:41:16] epoch: 12889 train-loss: 0.010560054952899614\n",
      "[LOG 20200502-14:41:16] epoch: 12890 train-loss: 0.010560048433641592\n",
      "[LOG 20200502-14:41:16] epoch: 12891 train-loss: 0.010560041810903285\n",
      "[LOG 20200502-14:41:16] epoch: 12892 train-loss: 0.010560035498605834\n",
      "[LOG 20200502-14:41:16] epoch: 12893 train-loss: 0.010560029186308384\n",
      "[LOG 20200502-14:41:17] epoch: 12894 train-loss: 0.01056002297749122\n",
      "[LOG 20200502-14:41:17] epoch: 12895 train-loss: 0.010560017493036058\n",
      "[LOG 20200502-14:41:17] epoch: 12896 train-loss: 0.010560011698140038\n",
      "[LOG 20200502-14:41:17] epoch: 12897 train-loss: 0.01056000611020459\n",
      "[LOG 20200502-14:41:18] epoch: 12898 train-loss: 0.010560000832710002\n",
      "[LOG 20200502-14:41:18] epoch: 12899 train-loss: 0.010559996072616842\n",
      "[LOG 20200502-14:41:18] epoch: 12900 train-loss: 0.010559991209043397\n",
      "[LOG 20200502-14:41:18] epoch: 12901 train-loss: 0.010559986035029093\n",
      "[LOG 20200502-14:41:19] epoch: 12902 train-loss: 0.010559981792337365\n",
      "[LOG 20200502-14:41:19] epoch: 12903 train-loss: 0.010559977135724492\n",
      "[LOG 20200502-14:41:19] epoch: 12904 train-loss: 0.010559973720875051\n",
      "[LOG 20200502-14:41:19] epoch: 12905 train-loss: 0.010559969374703037\n",
      "[LOG 20200502-14:41:20] epoch: 12906 train-loss: 0.010559965235491594\n",
      "[LOG 20200502-14:41:20] epoch: 12907 train-loss: 0.010559961303240724\n",
      "[LOG 20200502-14:41:20] epoch: 12908 train-loss: 0.010559958302312426\n",
      "[LOG 20200502-14:41:20] epoch: 12909 train-loss: 0.010559953852660127\n",
      "[LOG 20200502-14:41:20] epoch: 12910 train-loss: 0.01055995085173183\n",
      "[LOG 20200502-14:41:21] epoch: 12911 train-loss: 0.010559947229921818\n",
      "[LOG 20200502-14:41:21] epoch: 12912 train-loss: 0.010559943711592091\n",
      "[LOG 20200502-14:41:21] epoch: 12913 train-loss: 0.010559940400222937\n",
      "[LOG 20200502-14:41:21] epoch: 12914 train-loss: 0.010559937088853784\n",
      "[LOG 20200502-14:41:22] epoch: 12915 train-loss: 0.010559933260083199\n",
      "[LOG 20200502-14:41:22] epoch: 12916 train-loss: 0.010559929120871756\n",
      "[LOG 20200502-14:41:22] epoch: 12917 train-loss: 0.010559925706022315\n",
      "[LOG 20200502-14:41:22] epoch: 12918 train-loss: 0.01055992218769259\n",
      "[LOG 20200502-14:41:23] epoch: 12919 train-loss: 0.01055991794500086\n",
      "[LOG 20200502-14:41:23] epoch: 12920 train-loss: 0.010559914219710562\n",
      "[LOG 20200502-14:41:23] epoch: 12921 train-loss: 0.010559909356137117\n",
      "[LOG 20200502-14:41:23] epoch: 12922 train-loss: 0.01055990480300453\n",
      "[LOG 20200502-14:41:23] epoch: 12923 train-loss: 0.010559900249871943\n",
      "[LOG 20200502-14:41:24] epoch: 12924 train-loss: 0.01055989507585764\n",
      "[LOG 20200502-14:41:24] epoch: 12925 train-loss: 0.01055988928096162\n",
      "[LOG 20200502-14:41:24] epoch: 12926 train-loss: 0.010559883796506457\n",
      "[LOG 20200502-14:41:24] epoch: 12927 train-loss: 0.010559877587689294\n",
      "[LOG 20200502-14:41:25] epoch: 12928 train-loss: 0.0105598708614707\n",
      "[LOG 20200502-14:41:25] epoch: 12929 train-loss: 0.010559864652653536\n",
      "[LOG 20200502-14:41:25] epoch: 12930 train-loss: 0.010559857512513796\n",
      "[LOG 20200502-14:41:25] epoch: 12931 train-loss: 0.01055984954453177\n",
      "[LOG 20200502-14:41:26] epoch: 12932 train-loss: 0.010559841576549742\n",
      "[LOG 20200502-14:41:26] epoch: 12933 train-loss: 0.010559833815528287\n",
      "[LOG 20200502-14:41:26] epoch: 12934 train-loss: 0.010559824605782827\n",
      "[LOG 20200502-14:41:26] epoch: 12935 train-loss: 0.010559816016919084\n",
      "[LOG 20200502-14:41:26] epoch: 12936 train-loss: 0.010559806703693338\n",
      "[LOG 20200502-14:41:27] epoch: 12937 train-loss: 0.010559796045223871\n",
      "[LOG 20200502-14:41:27] epoch: 12938 train-loss: 0.010559786421557268\n",
      "[LOG 20200502-14:41:27] epoch: 12939 train-loss: 0.010559775659607517\n",
      "[LOG 20200502-14:41:27] epoch: 12940 train-loss: 0.010559765001138052\n",
      "[LOG 20200502-14:41:28] epoch: 12941 train-loss: 0.010559754135708014\n",
      "[LOG 20200502-14:41:28] epoch: 12942 train-loss: 0.010559743477238549\n",
      "[LOG 20200502-14:41:28] epoch: 12943 train-loss: 0.010559731163084507\n",
      "[LOG 20200502-14:41:28] epoch: 12944 train-loss: 0.01055971978025304\n",
      "[LOG 20200502-14:41:29] epoch: 12945 train-loss: 0.010559707776539855\n",
      "[LOG 20200502-14:41:29] epoch: 12946 train-loss: 0.010559695358905528\n",
      "[LOG 20200502-14:41:29] epoch: 12947 train-loss: 0.010559683665633202\n",
      "[LOG 20200502-14:41:29] epoch: 12948 train-loss: 0.010559671765400304\n",
      "[LOG 20200502-14:41:29] epoch: 12949 train-loss: 0.010559659968647692\n",
      "[LOG 20200502-14:41:30] epoch: 12950 train-loss: 0.010559647551013364\n",
      "[LOG 20200502-14:41:30] epoch: 12951 train-loss: 0.010559636271662183\n",
      "[LOG 20200502-14:41:30] epoch: 12952 train-loss: 0.010559624578389857\n",
      "[LOG 20200502-14:41:30] epoch: 12953 train-loss: 0.010559612988597818\n",
      "[LOG 20200502-14:41:31] epoch: 12954 train-loss: 0.010559601916207207\n",
      "[LOG 20200502-14:41:31] epoch: 12955 train-loss: 0.010559590947296884\n",
      "[LOG 20200502-14:41:31] epoch: 12956 train-loss: 0.010559580599268278\n",
      "[LOG 20200502-14:41:31] epoch: 12957 train-loss: 0.010559570044279099\n",
      "[LOG 20200502-14:41:32] epoch: 12958 train-loss: 0.010559560213651922\n",
      "[LOG 20200502-14:41:32] epoch: 12959 train-loss: 0.010559550383024745\n",
      "[LOG 20200502-14:41:32] epoch: 12960 train-loss: 0.010559541069798999\n",
      "[LOG 20200502-14:41:32] epoch: 12961 train-loss: 0.010559532067014111\n",
      "[LOG 20200502-14:41:33] epoch: 12962 train-loss: 0.010559524305992656\n",
      "[LOG 20200502-14:41:33] epoch: 12963 train-loss: 0.010559515820609199\n",
      "[LOG 20200502-14:41:33] epoch: 12964 train-loss: 0.010559507956107458\n",
      "[LOG 20200502-14:41:33] epoch: 12965 train-loss: 0.01055950102292829\n",
      "[LOG 20200502-14:41:34] epoch: 12966 train-loss: 0.010559493882788552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:41:34] epoch: 12967 train-loss: 0.010559488087892532\n",
      "[LOG 20200502-14:41:34] epoch: 12968 train-loss: 0.01055948208603594\n",
      "[LOG 20200502-14:41:34] epoch: 12969 train-loss: 0.01055947680854135\n",
      "[LOG 20200502-14:41:35] epoch: 12970 train-loss: 0.010559471220605902\n",
      "[LOG 20200502-14:41:35] epoch: 12971 train-loss: 0.010559466874433888\n",
      "[LOG 20200502-14:41:35] epoch: 12972 train-loss: 0.010559462321301302\n",
      "[LOG 20200502-14:41:35] epoch: 12973 train-loss: 0.010559458285570145\n",
      "[LOG 20200502-14:41:35] epoch: 12974 train-loss: 0.010559454767240418\n",
      "[LOG 20200502-14:41:36] epoch: 12975 train-loss: 0.010559451662831836\n",
      "[LOG 20200502-14:41:36] epoch: 12976 train-loss: 0.010559448765383827\n",
      "[LOG 20200502-14:41:36] epoch: 12977 train-loss: 0.010559445971416103\n",
      "[LOG 20200502-14:41:36] epoch: 12978 train-loss: 0.010559443591369523\n",
      "[LOG 20200502-14:41:37] epoch: 12979 train-loss: 0.010559441418283515\n",
      "[LOG 20200502-14:41:37] epoch: 12980 train-loss: 0.010559439555638366\n",
      "[LOG 20200502-14:41:37] epoch: 12981 train-loss: 0.010559437796473503\n",
      "[LOG 20200502-14:41:37] epoch: 12982 train-loss: 0.010559436761670642\n",
      "[LOG 20200502-14:41:38] epoch: 12983 train-loss: 0.010559435209466351\n",
      "[LOG 20200502-14:41:38] epoch: 12984 train-loss: 0.010559433967702918\n",
      "[LOG 20200502-14:41:38] epoch: 12985 train-loss: 0.01055943313986063\n",
      "[LOG 20200502-14:41:38] epoch: 12986 train-loss: 0.010559432105057769\n",
      "[LOG 20200502-14:41:39] epoch: 12987 train-loss: 0.010559431173735194\n",
      "[LOG 20200502-14:41:39] epoch: 12988 train-loss: 0.010559430345892906\n",
      "[LOG 20200502-14:41:39] epoch: 12989 train-loss: 0.010559430035452047\n",
      "[LOG 20200502-14:41:39] epoch: 12990 train-loss: 0.010559429000649188\n",
      "[LOG 20200502-14:41:39] epoch: 12991 train-loss: 0.010559428172806898\n",
      "[LOG 20200502-14:41:40] epoch: 12992 train-loss: 0.010559427551925182\n",
      "[LOG 20200502-14:41:40] epoch: 12993 train-loss: 0.010559426620602608\n",
      "[LOG 20200502-14:41:40] epoch: 12994 train-loss: 0.010559425999720892\n",
      "[LOG 20200502-14:41:40] epoch: 12995 train-loss: 0.010559424757957458\n",
      "[LOG 20200502-14:41:41] epoch: 12996 train-loss: 0.010559424033595456\n",
      "[LOG 20200502-14:41:41] epoch: 12997 train-loss: 0.010559423205753168\n",
      "[LOG 20200502-14:41:41] epoch: 12998 train-loss: 0.010559421860509448\n",
      "[LOG 20200502-14:41:41] epoch: 12999 train-loss: 0.010559420618746016\n",
      "[LOG 20200502-14:41:42] epoch: 13000 train-loss: 0.01055941865262058\n",
      "[LOG 20200502-14:41:42] epoch: 13001 train-loss: 0.01055941710041629\n",
      "[LOG 20200502-14:41:42] epoch: 13002 train-loss: 0.01055941523777114\n",
      "[LOG 20200502-14:41:42] epoch: 13003 train-loss: 0.010559412650763988\n",
      "[LOG 20200502-14:41:42] epoch: 13004 train-loss: 0.010559410167237123\n",
      "[LOG 20200502-14:41:43] epoch: 13005 train-loss: 0.010559407269789113\n",
      "[LOG 20200502-14:41:43] epoch: 13006 train-loss: 0.010559404372341104\n",
      "[LOG 20200502-14:41:43] epoch: 13007 train-loss: 0.010559401474893093\n",
      "[LOG 20200502-14:41:43] epoch: 13008 train-loss: 0.010559398163523939\n",
      "[LOG 20200502-14:41:44] epoch: 13009 train-loss: 0.010559394334753355\n",
      "[LOG 20200502-14:41:44] epoch: 13010 train-loss: 0.010559390402502485\n",
      "[LOG 20200502-14:41:44] epoch: 13011 train-loss: 0.010559386470251612\n",
      "[LOG 20200502-14:41:44] epoch: 13012 train-loss: 0.010559381399717595\n",
      "[LOG 20200502-14:41:45] epoch: 13013 train-loss: 0.010559376639624437\n",
      "[LOG 20200502-14:41:45] epoch: 13014 train-loss: 0.010559371362129847\n",
      "[LOG 20200502-14:41:45] epoch: 13015 train-loss: 0.010559366188115545\n",
      "[LOG 20200502-14:41:45] epoch: 13016 train-loss: 0.010559360600180097\n",
      "[LOG 20200502-14:41:45] epoch: 13017 train-loss: 0.010559354494843218\n",
      "[LOG 20200502-14:41:46] epoch: 13018 train-loss: 0.010559348596466912\n",
      "[LOG 20200502-14:41:46] epoch: 13019 train-loss: 0.01055934207720889\n",
      "[LOG 20200502-14:41:46] epoch: 13020 train-loss: 0.010559335454470582\n",
      "[LOG 20200502-14:41:46] epoch: 13021 train-loss: 0.010559328521291414\n",
      "[LOG 20200502-14:41:47] epoch: 13022 train-loss: 0.010559322002033392\n",
      "[LOG 20200502-14:41:47] epoch: 13023 train-loss: 0.010559314654933082\n",
      "[LOG 20200502-14:41:47] epoch: 13024 train-loss: 0.010559307100872198\n",
      "[LOG 20200502-14:41:47] epoch: 13025 train-loss: 0.010559299650291601\n",
      "[LOG 20200502-14:41:48] epoch: 13026 train-loss: 0.010559292199711004\n",
      "[LOG 20200502-14:41:48] epoch: 13027 train-loss: 0.010559284231728978\n",
      "[LOG 20200502-14:41:48] epoch: 13028 train-loss: 0.010559276884628667\n",
      "[LOG 20200502-14:41:48] epoch: 13029 train-loss: 0.010559269020126926\n",
      "[LOG 20200502-14:41:49] epoch: 13030 train-loss: 0.010559260845184326\n",
      "[LOG 20200502-14:41:49] epoch: 13031 train-loss: 0.01055925339460373\n",
      "[LOG 20200502-14:41:49] epoch: 13032 train-loss: 0.010559245633582274\n",
      "[LOG 20200502-14:41:49] epoch: 13033 train-loss: 0.01055923756211996\n",
      "[LOG 20200502-14:41:50] epoch: 13034 train-loss: 0.010559230111539364\n",
      "[LOG 20200502-14:41:50] epoch: 13035 train-loss: 0.010559222867919339\n",
      "[LOG 20200502-14:41:50] epoch: 13036 train-loss: 0.010559215106897883\n",
      "[LOG 20200502-14:41:50] epoch: 13037 train-loss: 0.010559207449356714\n",
      "[LOG 20200502-14:41:50] epoch: 13038 train-loss: 0.010559200619657835\n",
      "[LOG 20200502-14:41:51] epoch: 13039 train-loss: 0.010559193479518095\n",
      "[LOG 20200502-14:41:51] epoch: 13040 train-loss: 0.01055918623589807\n",
      "[LOG 20200502-14:41:51] epoch: 13041 train-loss: 0.010559179923600621\n",
      "[LOG 20200502-14:41:51] epoch: 13042 train-loss: 0.0105591734043426\n",
      "[LOG 20200502-14:41:52] epoch: 13043 train-loss: 0.010559167402486006\n",
      "[LOG 20200502-14:41:52] epoch: 13044 train-loss: 0.010559160572787126\n",
      "[LOG 20200502-14:41:52] epoch: 13045 train-loss: 0.010559154777891107\n",
      "[LOG 20200502-14:41:52] epoch: 13046 train-loss: 0.010559149603876803\n",
      "[LOG 20200502-14:41:53] epoch: 13047 train-loss: 0.010559143705500497\n",
      "[LOG 20200502-14:41:53] epoch: 13048 train-loss: 0.010559138117565049\n",
      "[LOG 20200502-14:41:53] epoch: 13049 train-loss: 0.010559132943550745\n",
      "[LOG 20200502-14:41:53] epoch: 13050 train-loss: 0.010559127873016728\n",
      "[LOG 20200502-14:41:53] epoch: 13051 train-loss: 0.010559123526844714\n",
      "[LOG 20200502-14:41:54] epoch: 13052 train-loss: 0.010559118456310697\n",
      "[LOG 20200502-14:41:54] epoch: 13053 train-loss: 0.010559114524059825\n",
      "[LOG 20200502-14:41:54] epoch: 13054 train-loss: 0.010559110074407525\n",
      "[LOG 20200502-14:41:54] epoch: 13055 train-loss: 0.010559105521274937\n",
      "[LOG 20200502-14:41:55] epoch: 13056 train-loss: 0.010559101175102923\n",
      "[LOG 20200502-14:41:55] epoch: 13057 train-loss: 0.01055909703589148\n",
      "[LOG 20200502-14:41:55] epoch: 13058 train-loss: 0.010559093207120895\n",
      "[LOG 20200502-14:41:55] epoch: 13059 train-loss: 0.010559089481830597\n",
      "[LOG 20200502-14:41:55] epoch: 13060 train-loss: 0.010559085032178296\n",
      "[LOG 20200502-14:41:56] epoch: 13061 train-loss: 0.010559080686006281\n",
      "[LOG 20200502-14:41:56] epoch: 13062 train-loss: 0.01055907727115684\n",
      "[LOG 20200502-14:41:56] epoch: 13063 train-loss: 0.010559072924984826\n",
      "[LOG 20200502-14:41:56] epoch: 13064 train-loss: 0.010559069096214242\n",
      "[LOG 20200502-14:41:57] epoch: 13065 train-loss: 0.01055906443960137\n",
      "[LOG 20200502-14:41:57] epoch: 13066 train-loss: 0.010559060403870212\n",
      "[LOG 20200502-14:41:57] epoch: 13067 train-loss: 0.010559055540296767\n",
      "[LOG 20200502-14:41:57] epoch: 13068 train-loss: 0.010559050987164179\n",
      "[LOG 20200502-14:41:57] epoch: 13069 train-loss: 0.010559046020110449\n",
      "[LOG 20200502-14:41:58] epoch: 13070 train-loss: 0.010559040742615858\n",
      "[LOG 20200502-14:41:58] epoch: 13071 train-loss: 0.010559035258160697\n",
      "[LOG 20200502-14:41:58] epoch: 13072 train-loss: 0.010559029463264678\n",
      "[LOG 20200502-14:41:58] epoch: 13073 train-loss: 0.010559023254447512\n",
      "[LOG 20200502-14:41:59] epoch: 13074 train-loss: 0.01055901725259092\n",
      "[LOG 20200502-14:41:59] epoch: 13075 train-loss: 0.01055900990549061\n",
      "[LOG 20200502-14:41:59] epoch: 13076 train-loss: 0.010559003179272016\n",
      "[LOG 20200502-14:41:59] epoch: 13077 train-loss: 0.010558995625211133\n",
      "[LOG 20200502-14:41:59] epoch: 13078 train-loss: 0.010558988381591108\n",
      "[LOG 20200502-14:42:00] epoch: 13079 train-loss: 0.010558980620569654\n",
      "[LOG 20200502-14:42:00] epoch: 13080 train-loss: 0.01055897151430448\n",
      "[LOG 20200502-14:42:00] epoch: 13081 train-loss: 0.010558963235881593\n",
      "[LOG 20200502-14:42:00] epoch: 13082 train-loss: 0.010558953508734703\n",
      "[LOG 20200502-14:42:01] epoch: 13083 train-loss: 0.01055894336766667\n",
      "[LOG 20200502-14:42:01] epoch: 13084 train-loss: 0.01055893384748035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:42:01] epoch: 13085 train-loss: 0.010558923706412315\n",
      "[LOG 20200502-14:42:01] epoch: 13086 train-loss: 0.010558912840982279\n",
      "[LOG 20200502-14:42:01] epoch: 13087 train-loss: 0.010558901768591668\n",
      "[LOG 20200502-14:42:02] epoch: 13088 train-loss: 0.010558890799681345\n",
      "[LOG 20200502-14:42:02] epoch: 13089 train-loss: 0.010558879209889306\n",
      "[LOG 20200502-14:42:02] epoch: 13090 train-loss: 0.010558867620097266\n",
      "[LOG 20200502-14:42:02] epoch: 13091 train-loss: 0.010558855202462938\n",
      "[LOG 20200502-14:42:03] epoch: 13092 train-loss: 0.01055884330223004\n",
      "[LOG 20200502-14:42:03] epoch: 13093 train-loss: 0.010558830263713995\n",
      "[LOG 20200502-14:42:03] epoch: 13094 train-loss: 0.010558818466961384\n",
      "[LOG 20200502-14:42:03] epoch: 13095 train-loss: 0.010558806152807342\n",
      "[LOG 20200502-14:42:03] epoch: 13096 train-loss: 0.010558793114291297\n",
      "[LOG 20200502-14:42:04] epoch: 13097 train-loss: 0.010558781110578112\n",
      "[LOG 20200502-14:42:04] epoch: 13098 train-loss: 0.010558768382502927\n",
      "[LOG 20200502-14:42:04] epoch: 13099 train-loss: 0.010558756482270028\n",
      "[LOG 20200502-14:42:04] epoch: 13100 train-loss: 0.010558744375076558\n",
      "[LOG 20200502-14:42:05] epoch: 13101 train-loss: 0.01055873247484366\n",
      "[LOG 20200502-14:42:05] epoch: 13102 train-loss: 0.010558720574610762\n",
      "[LOG 20200502-14:42:05] epoch: 13103 train-loss: 0.010558708984818723\n",
      "[LOG 20200502-14:42:05] epoch: 13104 train-loss: 0.0105586980159084\n",
      "[LOG 20200502-14:42:05] epoch: 13105 train-loss: 0.01055868797832065\n",
      "[LOG 20200502-14:42:06] epoch: 13106 train-loss: 0.010558676492008898\n",
      "[LOG 20200502-14:42:06] epoch: 13107 train-loss: 0.01055866697182258\n",
      "[LOG 20200502-14:42:06] epoch: 13108 train-loss: 0.010558657141195403\n",
      "[LOG 20200502-14:42:06] epoch: 13109 train-loss: 0.010558647414048513\n",
      "[LOG 20200502-14:42:07] epoch: 13110 train-loss: 0.010558638514743911\n",
      "[LOG 20200502-14:42:07] epoch: 13111 train-loss: 0.010558629718919596\n",
      "[LOG 20200502-14:42:07] epoch: 13112 train-loss: 0.010558621854417853\n",
      "[LOG 20200502-14:42:07] epoch: 13113 train-loss: 0.010558613886435827\n",
      "[LOG 20200502-14:42:07] epoch: 13114 train-loss: 0.010558606746296087\n",
      "[LOG 20200502-14:42:08] epoch: 13115 train-loss: 0.010558599813116921\n",
      "[LOG 20200502-14:42:08] epoch: 13116 train-loss: 0.01055859381126033\n",
      "[LOG 20200502-14:42:08] epoch: 13117 train-loss: 0.010558587395482592\n",
      "[LOG 20200502-14:42:08] epoch: 13118 train-loss: 0.010558582014507718\n",
      "[LOG 20200502-14:42:09] epoch: 13119 train-loss: 0.0105585769439737\n",
      "[LOG 20200502-14:42:09] epoch: 13120 train-loss: 0.010558572287360827\n",
      "[LOG 20200502-14:42:09] epoch: 13121 train-loss: 0.01055856773422824\n",
      "[LOG 20200502-14:42:09] epoch: 13122 train-loss: 0.010558563388056226\n",
      "[LOG 20200502-14:42:09] epoch: 13123 train-loss: 0.010558559973206785\n",
      "[LOG 20200502-14:42:10] epoch: 13124 train-loss: 0.01055855666183763\n",
      "[LOG 20200502-14:42:10] epoch: 13125 train-loss: 0.01055855324698819\n",
      "[LOG 20200502-14:42:10] epoch: 13126 train-loss: 0.010558550763461325\n",
      "[LOG 20200502-14:42:10] epoch: 13127 train-loss: 0.010558548176454173\n",
      "[LOG 20200502-14:42:11] epoch: 13128 train-loss: 0.01055854589988788\n",
      "[LOG 20200502-14:42:11] epoch: 13129 train-loss: 0.010558543830282159\n",
      "[LOG 20200502-14:42:11] epoch: 13130 train-loss: 0.010558541760676436\n",
      "[LOG 20200502-14:42:11] epoch: 13131 train-loss: 0.010558540415432718\n",
      "[LOG 20200502-14:42:12] epoch: 13132 train-loss: 0.01055853875974814\n",
      "[LOG 20200502-14:42:12] epoch: 13133 train-loss: 0.010558537621464994\n",
      "[LOG 20200502-14:42:12] epoch: 13134 train-loss: 0.010558536172740988\n",
      "[LOG 20200502-14:42:12] epoch: 13135 train-loss: 0.01055853565533956\n",
      "[LOG 20200502-14:42:13] epoch: 13136 train-loss: 0.010558534206615554\n",
      "[LOG 20200502-14:42:13] epoch: 13137 train-loss: 0.010558533896174695\n",
      "[LOG 20200502-14:42:13] epoch: 13138 train-loss: 0.010558532240490118\n",
      "[LOG 20200502-14:42:13] epoch: 13139 train-loss: 0.010558531723088689\n",
      "[LOG 20200502-14:42:13] epoch: 13140 train-loss: 0.01055853058480554\n",
      "[LOG 20200502-14:42:14] epoch: 13141 train-loss: 0.010558529550002681\n",
      "[LOG 20200502-14:42:14] epoch: 13142 train-loss: 0.010558528929120965\n",
      "[LOG 20200502-14:42:14] epoch: 13143 train-loss: 0.01055852696299553\n",
      "[LOG 20200502-14:42:14] epoch: 13144 train-loss: 0.010558526135153241\n",
      "[LOG 20200502-14:42:15] epoch: 13145 train-loss: 0.01055852510035038\n",
      "[LOG 20200502-14:42:15] epoch: 13146 train-loss: 0.010558523341185518\n",
      "[LOG 20200502-14:42:15] epoch: 13147 train-loss: 0.010558521995941797\n",
      "[LOG 20200502-14:42:15] epoch: 13148 train-loss: 0.010558520443737507\n",
      "[LOG 20200502-14:42:15] epoch: 13149 train-loss: 0.010558518167171214\n",
      "[LOG 20200502-14:42:16] epoch: 13150 train-loss: 0.010558516201045778\n",
      "[LOG 20200502-14:42:16] epoch: 13151 train-loss: 0.010558513200117482\n",
      "[LOG 20200502-14:42:16] epoch: 13152 train-loss: 0.010558510923551189\n",
      "[LOG 20200502-14:42:16] epoch: 13153 train-loss: 0.010558508026103178\n",
      "[LOG 20200502-14:42:17] epoch: 13154 train-loss: 0.010558504507773452\n",
      "[LOG 20200502-14:42:17] epoch: 13155 train-loss: 0.01055850140336487\n",
      "[LOG 20200502-14:42:17] epoch: 13156 train-loss: 0.010558497574594285\n",
      "[LOG 20200502-14:42:17] epoch: 13157 train-loss: 0.010558493952784274\n",
      "[LOG 20200502-14:42:17] epoch: 13158 train-loss: 0.010558489917053116\n",
      "[LOG 20200502-14:42:18] epoch: 13159 train-loss: 0.010558485053479671\n",
      "[LOG 20200502-14:42:18] epoch: 13160 train-loss: 0.010558480500347085\n",
      "[LOG 20200502-14:42:18] epoch: 13161 train-loss: 0.010558475429813067\n",
      "[LOG 20200502-14:42:18] epoch: 13162 train-loss: 0.010558469738397334\n",
      "[LOG 20200502-14:42:19] epoch: 13163 train-loss: 0.010558464253942171\n",
      "[LOG 20200502-14:42:19] epoch: 13164 train-loss: 0.010558458355565866\n",
      "[LOG 20200502-14:42:19] epoch: 13165 train-loss: 0.010558452353709273\n",
      "[LOG 20200502-14:42:19] epoch: 13166 train-loss: 0.010558445937931538\n",
      "[LOG 20200502-14:42:19] epoch: 13167 train-loss: 0.010558439108232657\n",
      "[LOG 20200502-14:42:20] epoch: 13168 train-loss: 0.01055843217505349\n",
      "[LOG 20200502-14:42:20] epoch: 13169 train-loss: 0.010558425034913752\n",
      "[LOG 20200502-14:42:20] epoch: 13170 train-loss: 0.010558417480852868\n",
      "[LOG 20200502-14:42:20] epoch: 13171 train-loss: 0.010558410237232843\n",
      "[LOG 20200502-14:42:20] epoch: 13172 train-loss: 0.010558402683171961\n",
      "[LOG 20200502-14:42:21] epoch: 13173 train-loss: 0.010558395439551936\n",
      "[LOG 20200502-14:42:21] epoch: 13174 train-loss: 0.010558386850688193\n",
      "[LOG 20200502-14:42:21] epoch: 13175 train-loss: 0.010558378675745593\n",
      "[LOG 20200502-14:42:21] epoch: 13176 train-loss: 0.010558370914724138\n",
      "[LOG 20200502-14:42:22] epoch: 13177 train-loss: 0.010558363050222397\n",
      "[LOG 20200502-14:42:22] epoch: 13178 train-loss: 0.010558354875279797\n",
      "[LOG 20200502-14:42:22] epoch: 13179 train-loss: 0.010558346700337198\n",
      "[LOG 20200502-14:42:22] epoch: 13180 train-loss: 0.01055833873235517\n",
      "[LOG 20200502-14:42:22] epoch: 13181 train-loss: 0.01055833086785343\n",
      "[LOG 20200502-14:42:23] epoch: 13182 train-loss: 0.010558323003351688\n",
      "[LOG 20200502-14:42:23] epoch: 13183 train-loss: 0.01055831503536966\n",
      "[LOG 20200502-14:42:23] epoch: 13184 train-loss: 0.010558307274348207\n",
      "[LOG 20200502-14:42:23] epoch: 13185 train-loss: 0.010558299202885892\n",
      "[LOG 20200502-14:42:24] epoch: 13186 train-loss: 0.010558292062746154\n",
      "[LOG 20200502-14:42:24] epoch: 13187 train-loss: 0.010558284612165557\n",
      "[LOG 20200502-14:42:24] epoch: 13188 train-loss: 0.010558277368545532\n",
      "[LOG 20200502-14:42:24] epoch: 13189 train-loss: 0.010558270435366366\n",
      "[LOG 20200502-14:42:25] epoch: 13190 train-loss: 0.010558263605667485\n",
      "[LOG 20200502-14:42:25] epoch: 13191 train-loss: 0.010558257293370035\n",
      "[LOG 20200502-14:42:25] epoch: 13192 train-loss: 0.01055825056715144\n",
      "[LOG 20200502-14:42:25] epoch: 13193 train-loss: 0.010558244875735708\n",
      "[LOG 20200502-14:42:25] epoch: 13194 train-loss: 0.010558238149517112\n",
      "[LOG 20200502-14:42:26] epoch: 13195 train-loss: 0.010558232768542238\n",
      "[LOG 20200502-14:42:26] epoch: 13196 train-loss: 0.010558227077126503\n",
      "[LOG 20200502-14:42:26] epoch: 13197 train-loss: 0.010558222006592486\n",
      "[LOG 20200502-14:42:26] epoch: 13198 train-loss: 0.010558216522137323\n",
      "[LOG 20200502-14:42:26] epoch: 13199 train-loss: 0.010558211348123021\n",
      "[LOG 20200502-14:42:27] epoch: 13200 train-loss: 0.01055820638106929\n",
      "[LOG 20200502-14:42:27] epoch: 13201 train-loss: 0.010558202034897275\n",
      "[LOG 20200502-14:42:27] epoch: 13202 train-loss: 0.010558197481764687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:42:27] epoch: 13203 train-loss: 0.010558193239072958\n",
      "[LOG 20200502-14:42:28] epoch: 13204 train-loss: 0.01055818899638123\n",
      "[LOG 20200502-14:42:28] epoch: 13205 train-loss: 0.01055818423628807\n",
      "[LOG 20200502-14:42:28] epoch: 13206 train-loss: 0.010558179890116056\n",
      "[LOG 20200502-14:42:28] epoch: 13207 train-loss: 0.010558176578746902\n",
      "[LOG 20200502-14:42:28] epoch: 13208 train-loss: 0.010558172336055173\n",
      "[LOG 20200502-14:42:29] epoch: 13209 train-loss: 0.010558167886402871\n",
      "[LOG 20200502-14:42:29] epoch: 13210 train-loss: 0.010558164057632288\n",
      "[LOG 20200502-14:42:29] epoch: 13211 train-loss: 0.010558159194058843\n",
      "[LOG 20200502-14:42:29] epoch: 13212 train-loss: 0.010558155054847399\n",
      "[LOG 20200502-14:42:30] epoch: 13213 train-loss: 0.010558150605195098\n",
      "[LOG 20200502-14:42:30] epoch: 13214 train-loss: 0.01055814636250337\n",
      "[LOG 20200502-14:42:30] epoch: 13215 train-loss: 0.010558141498929925\n",
      "[LOG 20200502-14:42:30] epoch: 13216 train-loss: 0.01055813715275791\n",
      "[LOG 20200502-14:42:30] epoch: 13217 train-loss: 0.01055813187526332\n",
      "[LOG 20200502-14:42:31] epoch: 13218 train-loss: 0.010558126597768731\n",
      "[LOG 20200502-14:42:31] epoch: 13219 train-loss: 0.01055812132027414\n",
      "[LOG 20200502-14:42:31] epoch: 13220 train-loss: 0.010558114801016118\n",
      "[LOG 20200502-14:42:31] epoch: 13221 train-loss: 0.010558109109600386\n",
      "[LOG 20200502-14:42:31] epoch: 13222 train-loss: 0.010558102486862076\n",
      "[LOG 20200502-14:42:32] epoch: 13223 train-loss: 0.010558096071084341\n",
      "[LOG 20200502-14:42:32] epoch: 13224 train-loss: 0.010558089137905173\n",
      "[LOG 20200502-14:42:32] epoch: 13225 train-loss: 0.010558081273403432\n",
      "[LOG 20200502-14:42:32] epoch: 13226 train-loss: 0.010558074029783407\n",
      "[LOG 20200502-14:42:33] epoch: 13227 train-loss: 0.010558065440919664\n",
      "[LOG 20200502-14:42:33] epoch: 13228 train-loss: 0.010558057265977064\n",
      "[LOG 20200502-14:42:33] epoch: 13229 train-loss: 0.010558048573633036\n",
      "[LOG 20200502-14:42:33] epoch: 13230 train-loss: 0.010558039053446718\n",
      "[LOG 20200502-14:42:34] epoch: 13231 train-loss: 0.010558029326299826\n",
      "[LOG 20200502-14:42:34] epoch: 13232 train-loss: 0.01055801949567265\n",
      "[LOG 20200502-14:42:34] epoch: 13233 train-loss: 0.010558009044163756\n",
      "[LOG 20200502-14:42:34] epoch: 13234 train-loss: 0.01055799786829286\n",
      "[LOG 20200502-14:42:34] epoch: 13235 train-loss: 0.010557987313303683\n",
      "[LOG 20200502-14:42:35] epoch: 13236 train-loss: 0.010557975620031357\n",
      "[LOG 20200502-14:42:35] epoch: 13237 train-loss: 0.010557964030239318\n",
      "[LOG 20200502-14:42:35] epoch: 13238 train-loss: 0.010557952440447278\n",
      "[LOG 20200502-14:42:35] epoch: 13239 train-loss: 0.010557940333253808\n",
      "[LOG 20200502-14:42:35] epoch: 13240 train-loss: 0.010557927501698336\n",
      "[LOG 20200502-14:42:36] epoch: 13241 train-loss: 0.010557915084064007\n",
      "[LOG 20200502-14:42:36] epoch: 13242 train-loss: 0.010557902769909965\n",
      "[LOG 20200502-14:42:36] epoch: 13243 train-loss: 0.010557890041834779\n",
      "[LOG 20200502-14:42:36] epoch: 13244 train-loss: 0.01055787710679902\n",
      "[LOG 20200502-14:42:37] epoch: 13245 train-loss: 0.010557864585684406\n",
      "[LOG 20200502-14:42:37] epoch: 13246 train-loss: 0.010557851961089505\n",
      "[LOG 20200502-14:42:37] epoch: 13247 train-loss: 0.01055783892257346\n",
      "[LOG 20200502-14:42:37] epoch: 13248 train-loss: 0.010557826711899705\n",
      "[LOG 20200502-14:42:37] epoch: 13249 train-loss: 0.010557814087304804\n",
      "[LOG 20200502-14:42:38] epoch: 13250 train-loss: 0.010557801980111334\n",
      "[LOG 20200502-14:42:38] epoch: 13251 train-loss: 0.010557789872917864\n",
      "[LOG 20200502-14:42:38] epoch: 13252 train-loss: 0.01055777838660611\n",
      "[LOG 20200502-14:42:38] epoch: 13253 train-loss: 0.010557766796814071\n",
      "[LOG 20200502-14:42:39] epoch: 13254 train-loss: 0.010557755931384034\n",
      "[LOG 20200502-14:42:39] epoch: 13255 train-loss: 0.010557745169434283\n",
      "[LOG 20200502-14:42:39] epoch: 13256 train-loss: 0.010557735338807106\n",
      "[LOG 20200502-14:42:39] epoch: 13257 train-loss: 0.010557725197739072\n",
      "[LOG 20200502-14:42:39] epoch: 13258 train-loss: 0.010557715884513326\n",
      "[LOG 20200502-14:42:40] epoch: 13259 train-loss: 0.010557706467807293\n",
      "[LOG 20200502-14:42:40] epoch: 13260 train-loss: 0.010557697361542119\n",
      "[LOG 20200502-14:42:40] epoch: 13261 train-loss: 0.010557689290079806\n",
      "[LOG 20200502-14:42:40] epoch: 13262 train-loss: 0.010557681218617491\n",
      "[LOG 20200502-14:42:41] epoch: 13263 train-loss: 0.010557673974997468\n",
      "[LOG 20200502-14:42:41] epoch: 13264 train-loss: 0.010557666938338015\n",
      "[LOG 20200502-14:42:41] epoch: 13265 train-loss: 0.010557660626040565\n",
      "[LOG 20200502-14:42:41] epoch: 13266 train-loss: 0.010557654417223401\n",
      "[LOG 20200502-14:42:42] epoch: 13267 train-loss: 0.010557648829287954\n",
      "[LOG 20200502-14:42:42] epoch: 13268 train-loss: 0.010557643241352506\n",
      "[LOG 20200502-14:42:42] epoch: 13269 train-loss: 0.010557638688219918\n",
      "[LOG 20200502-14:42:42] epoch: 13270 train-loss: 0.010557634135087332\n",
      "[LOG 20200502-14:42:42] epoch: 13271 train-loss: 0.010557630202836461\n",
      "[LOG 20200502-14:42:43] epoch: 13272 train-loss: 0.010557626063625017\n",
      "[LOG 20200502-14:42:43] epoch: 13273 train-loss: 0.010557622648775578\n",
      "[LOG 20200502-14:42:43] epoch: 13274 train-loss: 0.010557620061768426\n",
      "[LOG 20200502-14:42:43] epoch: 13275 train-loss: 0.010557617164320417\n",
      "[LOG 20200502-14:42:44] epoch: 13276 train-loss: 0.010557614784273837\n",
      "[LOG 20200502-14:42:44] epoch: 13277 train-loss: 0.010557612611187829\n",
      "[LOG 20200502-14:42:44] epoch: 13278 train-loss: 0.010557610541582108\n",
      "[LOG 20200502-14:42:44] epoch: 13279 train-loss: 0.010557608678936958\n",
      "[LOG 20200502-14:42:45] epoch: 13280 train-loss: 0.010557606712811522\n",
      "[LOG 20200502-14:42:45] epoch: 13281 train-loss: 0.010557605781488948\n",
      "[LOG 20200502-14:42:45] epoch: 13282 train-loss: 0.010557604539725516\n",
      "[LOG 20200502-14:42:45] epoch: 13283 train-loss: 0.010557603297962083\n",
      "[LOG 20200502-14:42:46] epoch: 13284 train-loss: 0.010557602366639508\n",
      "[LOG 20200502-14:42:46] epoch: 13285 train-loss: 0.010557601228356361\n",
      "[LOG 20200502-14:42:46] epoch: 13286 train-loss: 0.010557600607474646\n",
      "[LOG 20200502-14:42:46] epoch: 13287 train-loss: 0.010557599262230925\n",
      "[LOG 20200502-14:42:46] epoch: 13288 train-loss: 0.010557598537868924\n",
      "[LOG 20200502-14:42:47] epoch: 13289 train-loss: 0.010557597916987207\n",
      "[LOG 20200502-14:42:47] epoch: 13290 train-loss: 0.010557596882184347\n",
      "[LOG 20200502-14:42:47] epoch: 13291 train-loss: 0.010557596157822344\n",
      "[LOG 20200502-14:42:47] epoch: 13292 train-loss: 0.010557594916058911\n",
      "[LOG 20200502-14:42:48] epoch: 13293 train-loss: 0.010557593777775764\n",
      "[LOG 20200502-14:42:48] epoch: 13294 train-loss: 0.010557592432532046\n",
      "[LOG 20200502-14:42:48] epoch: 13295 train-loss: 0.010557590880327754\n",
      "[LOG 20200502-14:42:48] epoch: 13296 train-loss: 0.01055758943160375\n",
      "[LOG 20200502-14:42:48] epoch: 13297 train-loss: 0.010557587361998029\n",
      "[LOG 20200502-14:42:49] epoch: 13298 train-loss: 0.010557585292392306\n",
      "[LOG 20200502-14:42:49] epoch: 13299 train-loss: 0.010557582808865441\n",
      "[LOG 20200502-14:42:49] epoch: 13300 train-loss: 0.010557580635779433\n",
      "[LOG 20200502-14:42:49] epoch: 13301 train-loss: 0.010557577738331424\n",
      "[LOG 20200502-14:42:50] epoch: 13302 train-loss: 0.010557574840883413\n",
      "[LOG 20200502-14:42:50] epoch: 13303 train-loss: 0.010557571632994546\n",
      "[LOG 20200502-14:42:50] epoch: 13304 train-loss: 0.010557567700743675\n",
      "[LOG 20200502-14:42:50] epoch: 13305 train-loss: 0.010557564596335093\n",
      "[LOG 20200502-14:42:50] epoch: 13306 train-loss: 0.010557560250163078\n",
      "[LOG 20200502-14:42:51] epoch: 13307 train-loss: 0.010557555386589633\n",
      "[LOG 20200502-14:42:51] epoch: 13308 train-loss: 0.010557550833457045\n",
      "[LOG 20200502-14:42:51] epoch: 13309 train-loss: 0.010557545762923028\n",
      "[LOG 20200502-14:42:51] epoch: 13310 train-loss: 0.01055754048542844\n",
      "[LOG 20200502-14:42:52] epoch: 13311 train-loss: 0.010557534587052133\n",
      "[LOG 20200502-14:42:52] epoch: 13312 train-loss: 0.0105575288956364\n",
      "[LOG 20200502-14:42:52] epoch: 13313 train-loss: 0.010557522376378378\n",
      "[LOG 20200502-14:42:52] epoch: 13314 train-loss: 0.010557515650159784\n",
      "[LOG 20200502-14:42:52] epoch: 13315 train-loss: 0.010557509337862333\n",
      "[LOG 20200502-14:42:53] epoch: 13316 train-loss: 0.010557502094242308\n",
      "[LOG 20200502-14:42:53] epoch: 13317 train-loss: 0.010557495057582855\n",
      "[LOG 20200502-14:42:53] epoch: 13318 train-loss: 0.010557487607002258\n",
      "[LOG 20200502-14:42:53] epoch: 13319 train-loss: 0.01055747963902023\n",
      "[LOG 20200502-14:42:54] epoch: 13320 train-loss: 0.01055747229191992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:42:54] epoch: 13321 train-loss: 0.010557464220457606\n",
      "[LOG 20200502-14:42:54] epoch: 13322 train-loss: 0.01055745573507415\n",
      "[LOG 20200502-14:42:54] epoch: 13323 train-loss: 0.010557447663611837\n",
      "[LOG 20200502-14:42:55] epoch: 13324 train-loss: 0.010557439178228378\n",
      "[LOG 20200502-14:42:55] epoch: 13325 train-loss: 0.010557430485884348\n",
      "[LOG 20200502-14:42:55] epoch: 13326 train-loss: 0.010557422828343179\n",
      "[LOG 20200502-14:42:55] epoch: 13327 train-loss: 0.010557413929038577\n",
      "[LOG 20200502-14:42:55] epoch: 13328 train-loss: 0.010557405340174833\n",
      "[LOG 20200502-14:42:56] epoch: 13329 train-loss: 0.010557397372192807\n",
      "[LOG 20200502-14:42:56] epoch: 13330 train-loss: 0.010557388783329062\n",
      "[LOG 20200502-14:42:56] epoch: 13331 train-loss: 0.010557380504906178\n",
      "[LOG 20200502-14:42:56] epoch: 13332 train-loss: 0.010557372329963578\n",
      "[LOG 20200502-14:42:56] epoch: 13333 train-loss: 0.010557364258501265\n",
      "[LOG 20200502-14:42:57] epoch: 13334 train-loss: 0.01055735649747981\n",
      "[LOG 20200502-14:42:57] epoch: 13335 train-loss: 0.010557349253859784\n",
      "[LOG 20200502-14:42:57] epoch: 13336 train-loss: 0.010557341699798902\n",
      "[LOG 20200502-14:42:57] epoch: 13337 train-loss: 0.010557334249218306\n",
      "[LOG 20200502-14:42:58] epoch: 13338 train-loss: 0.010557327212558852\n",
      "[LOG 20200502-14:42:58] epoch: 13339 train-loss: 0.01055732069330083\n",
      "[LOG 20200502-14:42:58] epoch: 13340 train-loss: 0.010557313553161092\n",
      "[LOG 20200502-14:42:58] epoch: 13341 train-loss: 0.010557307447824214\n",
      "[LOG 20200502-14:42:58] epoch: 13342 train-loss: 0.01055730072160562\n",
      "[LOG 20200502-14:42:59] epoch: 13343 train-loss: 0.0105572949267096\n",
      "[LOG 20200502-14:42:59] epoch: 13344 train-loss: 0.010557289442254437\n",
      "[LOG 20200502-14:42:59] epoch: 13345 train-loss: 0.01055728354387813\n",
      "[LOG 20200502-14:42:59] epoch: 13346 train-loss: 0.010557278162903257\n",
      "[LOG 20200502-14:43:00] epoch: 13347 train-loss: 0.010557272678448094\n",
      "[LOG 20200502-14:43:00] epoch: 13348 train-loss: 0.010557268021835221\n",
      "[LOG 20200502-14:43:00] epoch: 13349 train-loss: 0.010557263158261776\n",
      "[LOG 20200502-14:43:00] epoch: 13350 train-loss: 0.010557258398168616\n",
      "[LOG 20200502-14:43:00] epoch: 13351 train-loss: 0.010557253741555743\n",
      "[LOG 20200502-14:43:01] epoch: 13352 train-loss: 0.010557249291903444\n",
      "[LOG 20200502-14:43:01] epoch: 13353 train-loss: 0.010557244531810284\n",
      "[LOG 20200502-14:43:01] epoch: 13354 train-loss: 0.010557239461276267\n",
      "[LOG 20200502-14:43:01] epoch: 13355 train-loss: 0.01055723594294654\n",
      "[LOG 20200502-14:43:02] epoch: 13356 train-loss: 0.010557231286333667\n",
      "[LOG 20200502-14:43:02] epoch: 13357 train-loss: 0.010557227871484227\n",
      "[LOG 20200502-14:43:02] epoch: 13358 train-loss: 0.01055722300791078\n",
      "[LOG 20200502-14:43:02] epoch: 13359 train-loss: 0.010557218868699338\n",
      "[LOG 20200502-14:43:03] epoch: 13360 train-loss: 0.010557214729487896\n",
      "[LOG 20200502-14:43:03] epoch: 13361 train-loss: 0.010557209969394736\n",
      "[LOG 20200502-14:43:03] epoch: 13362 train-loss: 0.010557204898860719\n",
      "[LOG 20200502-14:43:04] epoch: 13363 train-loss: 0.010557200759649277\n",
      "[LOG 20200502-14:43:04] epoch: 13364 train-loss: 0.010557195999556117\n",
      "[LOG 20200502-14:43:04] epoch: 13365 train-loss: 0.010557191135982672\n",
      "[LOG 20200502-14:43:04] epoch: 13366 train-loss: 0.010557185548047224\n",
      "[LOG 20200502-14:43:05] epoch: 13367 train-loss: 0.010557180270552635\n",
      "[LOG 20200502-14:43:05] epoch: 13368 train-loss: 0.010557174268696044\n",
      "[LOG 20200502-14:43:05] epoch: 13369 train-loss: 0.010557168784240881\n",
      "[LOG 20200502-14:43:06] epoch: 13370 train-loss: 0.010557162058022287\n",
      "[LOG 20200502-14:43:06] epoch: 13371 train-loss: 0.01055715543528398\n",
      "[LOG 20200502-14:43:06] epoch: 13372 train-loss: 0.01055714881254567\n",
      "[LOG 20200502-14:43:06] epoch: 13373 train-loss: 0.010557141568925645\n",
      "[LOG 20200502-14:43:07] epoch: 13374 train-loss: 0.01055713432530562\n",
      "[LOG 20200502-14:43:07] epoch: 13375 train-loss: 0.010557126046882736\n",
      "[LOG 20200502-14:43:07] epoch: 13376 train-loss: 0.010557117871940136\n",
      "[LOG 20200502-14:43:07] epoch: 13377 train-loss: 0.010557109386556678\n",
      "[LOG 20200502-14:43:07] epoch: 13378 train-loss: 0.010557100073330931\n",
      "[LOG 20200502-14:43:08] epoch: 13379 train-loss: 0.010557090967065759\n",
      "[LOG 20200502-14:43:08] epoch: 13380 train-loss: 0.010557081239918867\n",
      "[LOG 20200502-14:43:08] epoch: 13381 train-loss: 0.010557070477969117\n",
      "[LOG 20200502-14:43:08] epoch: 13382 train-loss: 0.010557060336901082\n",
      "[LOG 20200502-14:43:09] epoch: 13383 train-loss: 0.010557049367990758\n",
      "[LOG 20200502-14:43:09] epoch: 13384 train-loss: 0.010557037674718432\n",
      "[LOG 20200502-14:43:09] epoch: 13385 train-loss: 0.010557026498847537\n",
      "[LOG 20200502-14:43:09] epoch: 13386 train-loss: 0.010557015012535784\n",
      "[LOG 20200502-14:43:10] epoch: 13387 train-loss: 0.010557002698381742\n",
      "[LOG 20200502-14:43:10] epoch: 13388 train-loss: 0.010556990177267127\n",
      "[LOG 20200502-14:43:10] epoch: 13389 train-loss: 0.0105569777596328\n",
      "[LOG 20200502-14:43:10] epoch: 13390 train-loss: 0.01055696482459704\n",
      "[LOG 20200502-14:43:11] epoch: 13391 train-loss: 0.010556952096521854\n",
      "[LOG 20200502-14:43:11] epoch: 13392 train-loss: 0.010556939161486097\n",
      "[LOG 20200502-14:43:11] epoch: 13393 train-loss: 0.010556926433410909\n",
      "[LOG 20200502-14:43:11] epoch: 13394 train-loss: 0.010556912877493434\n",
      "[LOG 20200502-14:43:11] epoch: 13395 train-loss: 0.010556899942457676\n",
      "[LOG 20200502-14:43:12] epoch: 13396 train-loss: 0.010556886800461344\n",
      "[LOG 20200502-14:43:12] epoch: 13397 train-loss: 0.0105568737619453\n",
      "[LOG 20200502-14:43:12] epoch: 13398 train-loss: 0.010556861033870114\n",
      "[LOG 20200502-14:43:12] epoch: 13399 train-loss: 0.0105568485127555\n",
      "[LOG 20200502-14:43:13] epoch: 13400 train-loss: 0.010556835681200027\n",
      "[LOG 20200502-14:43:13] epoch: 13401 train-loss: 0.010556823987927701\n",
      "[LOG 20200502-14:43:13] epoch: 13402 train-loss: 0.010556811880734231\n",
      "[LOG 20200502-14:43:13] epoch: 13403 train-loss: 0.010556800704863336\n",
      "[LOG 20200502-14:43:13] epoch: 13404 train-loss: 0.010556788804630438\n",
      "[LOG 20200502-14:43:14] epoch: 13405 train-loss: 0.01055677824964126\n",
      "[LOG 20200502-14:43:14] epoch: 13406 train-loss: 0.010556767384211222\n",
      "[LOG 20200502-14:43:14] epoch: 13407 train-loss: 0.010556757657064332\n",
      "[LOG 20200502-14:43:15] epoch: 13408 train-loss: 0.010556747826437155\n",
      "[LOG 20200502-14:43:15] epoch: 13409 train-loss: 0.010556738616691696\n",
      "[LOG 20200502-14:43:15] epoch: 13410 train-loss: 0.010556729510426521\n",
      "[LOG 20200502-14:43:16] epoch: 13411 train-loss: 0.010556721438964209\n",
      "[LOG 20200502-14:43:16] epoch: 13412 train-loss: 0.010556713057061037\n",
      "[LOG 20200502-14:43:16] epoch: 13413 train-loss: 0.010556705296039581\n",
      "[LOG 20200502-14:43:16] epoch: 13414 train-loss: 0.010556698259380128\n",
      "[LOG 20200502-14:43:17] epoch: 13415 train-loss: 0.010556692464484109\n",
      "[LOG 20200502-14:43:17] epoch: 13416 train-loss: 0.010556685841745801\n",
      "[LOG 20200502-14:43:17] epoch: 13417 train-loss: 0.01055668004684978\n",
      "[LOG 20200502-14:43:17] epoch: 13418 train-loss: 0.01055667507979605\n",
      "[LOG 20200502-14:43:18] epoch: 13419 train-loss: 0.01055667031970289\n",
      "[LOG 20200502-14:43:18] epoch: 13420 train-loss: 0.010556665663090017\n",
      "[LOG 20200502-14:43:18] epoch: 13421 train-loss: 0.010556661834319433\n",
      "[LOG 20200502-14:43:18] epoch: 13422 train-loss: 0.010556658005548848\n",
      "[LOG 20200502-14:43:18] epoch: 13423 train-loss: 0.010556654176778264\n",
      "[LOG 20200502-14:43:19] epoch: 13424 train-loss: 0.010556651693251397\n",
      "[LOG 20200502-14:43:19] epoch: 13425 train-loss: 0.010556648588842817\n",
      "[LOG 20200502-14:43:19] epoch: 13426 train-loss: 0.010556646415756809\n",
      "[LOG 20200502-14:43:19] epoch: 13427 train-loss: 0.010556644139190515\n",
      "[LOG 20200502-14:43:20] epoch: 13428 train-loss: 0.010556642276545366\n",
      "[LOG 20200502-14:43:20] epoch: 13429 train-loss: 0.010556640827821361\n",
      "[LOG 20200502-14:43:20] epoch: 13430 train-loss: 0.01055663875821564\n",
      "[LOG 20200502-14:43:20] epoch: 13431 train-loss: 0.010556638033853637\n",
      "[LOG 20200502-14:43:20] epoch: 13432 train-loss: 0.010556636792090204\n",
      "[LOG 20200502-14:43:21] epoch: 13433 train-loss: 0.010556635239885913\n",
      "[LOG 20200502-14:43:21] epoch: 13434 train-loss: 0.01055663451552391\n",
      "[LOG 20200502-14:43:21] epoch: 13435 train-loss: 0.010556633894642195\n",
      "[LOG 20200502-14:43:21] epoch: 13436 train-loss: 0.010556632859839333\n",
      "[LOG 20200502-14:43:22] epoch: 13437 train-loss: 0.01055663213547733\n",
      "[LOG 20200502-14:43:22] epoch: 13438 train-loss: 0.010556630997194184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:43:22] epoch: 13439 train-loss: 0.010556629962391324\n",
      "[LOG 20200502-14:43:22] epoch: 13440 train-loss: 0.010556629548470179\n",
      "[LOG 20200502-14:43:22] epoch: 13441 train-loss: 0.010556628513667319\n",
      "[LOG 20200502-14:43:23] epoch: 13442 train-loss: 0.010556627271903886\n",
      "[LOG 20200502-14:43:23] epoch: 13443 train-loss: 0.01055662582317988\n",
      "[LOG 20200502-14:43:23] epoch: 13444 train-loss: 0.010556624167495303\n",
      "[LOG 20200502-14:43:23] epoch: 13445 train-loss: 0.010556623029212156\n",
      "[LOG 20200502-14:43:24] epoch: 13446 train-loss: 0.010556620959606435\n",
      "[LOG 20200502-14:43:24] epoch: 13447 train-loss: 0.010556619303921858\n",
      "[LOG 20200502-14:43:24] epoch: 13448 train-loss: 0.010556616820394993\n",
      "[LOG 20200502-14:43:24] epoch: 13449 train-loss: 0.010556614129907556\n",
      "[LOG 20200502-14:43:24] epoch: 13450 train-loss: 0.010556611439420117\n",
      "[LOG 20200502-14:43:25] epoch: 13451 train-loss: 0.010556608335011534\n",
      "[LOG 20200502-14:43:25] epoch: 13452 train-loss: 0.010556605644524097\n",
      "[LOG 20200502-14:43:25] epoch: 13453 train-loss: 0.010556601919233799\n",
      "[LOG 20200502-14:43:25] epoch: 13454 train-loss: 0.010556597573061785\n",
      "[LOG 20200502-14:43:25] epoch: 13455 train-loss: 0.010556593226889769\n",
      "[LOG 20200502-14:43:26] epoch: 13456 train-loss: 0.010556588880717754\n",
      "[LOG 20200502-14:43:26] epoch: 13457 train-loss: 0.010556583603223165\n",
      "[LOG 20200502-14:43:26] epoch: 13458 train-loss: 0.010556578532689147\n",
      "[LOG 20200502-14:43:26] epoch: 13459 train-loss: 0.010556573048233986\n",
      "[LOG 20200502-14:43:27] epoch: 13460 train-loss: 0.010556566735936536\n",
      "[LOG 20200502-14:43:27] epoch: 13461 train-loss: 0.010556560630599657\n",
      "[LOG 20200502-14:43:27] epoch: 13462 train-loss: 0.010556553800900778\n",
      "[LOG 20200502-14:43:27] epoch: 13463 train-loss: 0.010556547592083612\n",
      "[LOG 20200502-14:43:27] epoch: 13464 train-loss: 0.010556539831062159\n",
      "[LOG 20200502-14:43:28] epoch: 13465 train-loss: 0.010556532587442134\n",
      "[LOG 20200502-14:43:28] epoch: 13466 train-loss: 0.010556525240341822\n",
      "[LOG 20200502-14:43:28] epoch: 13467 train-loss: 0.010556517479320368\n",
      "[LOG 20200502-14:43:28] epoch: 13468 train-loss: 0.01055650868349605\n",
      "[LOG 20200502-14:43:28] epoch: 13469 train-loss: 0.010556500405073166\n",
      "[LOG 20200502-14:43:29] epoch: 13470 train-loss: 0.010556491919689708\n",
      "[LOG 20200502-14:43:29] epoch: 13471 train-loss: 0.010556483330825964\n",
      "[LOG 20200502-14:43:29] epoch: 13472 train-loss: 0.010556474638481935\n",
      "[LOG 20200502-14:43:29] epoch: 13473 train-loss: 0.010556465739177333\n",
      "[LOG 20200502-14:43:30] epoch: 13474 train-loss: 0.010556457046833303\n",
      "[LOG 20200502-14:43:30] epoch: 13475 train-loss: 0.010556447733607557\n",
      "[LOG 20200502-14:43:30] epoch: 13476 train-loss: 0.010556439351704385\n",
      "[LOG 20200502-14:43:30] epoch: 13477 train-loss: 0.01055643055588007\n",
      "[LOG 20200502-14:43:30] epoch: 13478 train-loss: 0.010556421553095182\n",
      "[LOG 20200502-14:43:31] epoch: 13479 train-loss: 0.010556413067711724\n",
      "[LOG 20200502-14:43:31] epoch: 13480 train-loss: 0.010556404168407122\n",
      "[LOG 20200502-14:43:31] epoch: 13481 train-loss: 0.010556395579543378\n",
      "[LOG 20200502-14:43:31] epoch: 13482 train-loss: 0.010556387094159922\n",
      "[LOG 20200502-14:43:32] epoch: 13483 train-loss: 0.010556378505296178\n",
      "[LOG 20200502-14:43:32] epoch: 13484 train-loss: 0.010556370433833864\n",
      "[LOG 20200502-14:43:32] epoch: 13485 train-loss: 0.010556362776292695\n",
      "[LOG 20200502-14:43:32] epoch: 13486 train-loss: 0.010556355118751526\n",
      "[LOG 20200502-14:43:32] epoch: 13487 train-loss: 0.010556347150769498\n",
      "[LOG 20200502-14:43:33] epoch: 13488 train-loss: 0.010556340114110045\n",
      "[LOG 20200502-14:43:33] epoch: 13489 train-loss: 0.010556332870490022\n",
      "[LOG 20200502-14:43:33] epoch: 13490 train-loss: 0.010556326144271426\n",
      "[LOG 20200502-14:43:33] epoch: 13491 train-loss: 0.010556319625013404\n",
      "[LOG 20200502-14:43:33] epoch: 13492 train-loss: 0.010556313002275096\n",
      "[LOG 20200502-14:43:34] epoch: 13493 train-loss: 0.01055630710389879\n",
      "[LOG 20200502-14:43:34] epoch: 13494 train-loss: 0.010556301102042198\n",
      "[LOG 20200502-14:43:34] epoch: 13495 train-loss: 0.01055629582454761\n",
      "[LOG 20200502-14:43:34] epoch: 13496 train-loss: 0.01055628971921073\n",
      "[LOG 20200502-14:43:35] epoch: 13497 train-loss: 0.010556284027794996\n",
      "[LOG 20200502-14:43:35] epoch: 13498 train-loss: 0.01055627895726098\n",
      "[LOG 20200502-14:43:35] epoch: 13499 train-loss: 0.010556273783246676\n",
      "[LOG 20200502-14:43:35] epoch: 13500 train-loss: 0.010556268505752087\n",
      "[LOG 20200502-14:43:35] epoch: 13501 train-loss: 0.010556263745658927\n",
      "[LOG 20200502-14:43:36] epoch: 13502 train-loss: 0.01055625919252634\n",
      "[LOG 20200502-14:43:36] epoch: 13503 train-loss: 0.010556254535913467\n",
      "[LOG 20200502-14:43:36] epoch: 13504 train-loss: 0.010556250293221738\n",
      "[LOG 20200502-14:43:36] epoch: 13505 train-loss: 0.010556245222687721\n",
      "[LOG 20200502-14:43:37] epoch: 13506 train-loss: 0.010556240876515707\n",
      "[LOG 20200502-14:43:37] epoch: 13507 train-loss: 0.010556236633823978\n",
      "[LOG 20200502-14:43:37] epoch: 13508 train-loss: 0.010556231666770246\n",
      "[LOG 20200502-14:43:37] epoch: 13509 train-loss: 0.010556226492755942\n",
      "[LOG 20200502-14:43:38] epoch: 13510 train-loss: 0.010556222457024787\n",
      "[LOG 20200502-14:43:38] epoch: 13511 train-loss: 0.010556217283010483\n",
      "[LOG 20200502-14:43:38] epoch: 13512 train-loss: 0.010556212212476466\n",
      "[LOG 20200502-14:43:38] epoch: 13513 train-loss: 0.01055620734890302\n",
      "[LOG 20200502-14:43:38] epoch: 13514 train-loss: 0.010556202485329576\n",
      "[LOG 20200502-14:43:39] epoch: 13515 train-loss: 0.010556196690433554\n",
      "[LOG 20200502-14:43:39] epoch: 13516 train-loss: 0.010556191723379824\n",
      "[LOG 20200502-14:43:39] epoch: 13517 train-loss: 0.010556184893680943\n",
      "[LOG 20200502-14:43:39] epoch: 13518 train-loss: 0.01055617920226521\n",
      "[LOG 20200502-14:43:40] epoch: 13519 train-loss: 0.010556172476046614\n",
      "[LOG 20200502-14:43:40] epoch: 13520 train-loss: 0.010556166474190023\n",
      "[LOG 20200502-14:43:40] epoch: 13521 train-loss: 0.010556159541010857\n",
      "[LOG 20200502-14:43:40] epoch: 13522 train-loss: 0.010556151779989401\n",
      "[LOG 20200502-14:43:41] epoch: 13523 train-loss: 0.010556144639849663\n",
      "[LOG 20200502-14:43:41] epoch: 13524 train-loss: 0.010556136775347922\n",
      "[LOG 20200502-14:43:41] epoch: 13525 train-loss: 0.010556128496925036\n",
      "[LOG 20200502-14:43:41] epoch: 13526 train-loss: 0.010556119804581007\n",
      "[LOG 20200502-14:43:41] epoch: 13527 train-loss: 0.010556110801796118\n",
      "[LOG 20200502-14:43:42] epoch: 13528 train-loss: 0.010556101488570372\n",
      "[LOG 20200502-14:43:42] epoch: 13529 train-loss: 0.010556091761423482\n",
      "[LOG 20200502-14:43:42] epoch: 13530 train-loss: 0.010556082034276592\n",
      "[LOG 20200502-14:43:42] epoch: 13531 train-loss: 0.01055607127232684\n",
      "[LOG 20200502-14:43:43] epoch: 13532 train-loss: 0.01055605999297566\n",
      "[LOG 20200502-14:43:43] epoch: 13533 train-loss: 0.010556049127545621\n",
      "[LOG 20200502-14:43:43] epoch: 13534 train-loss: 0.01055603733079301\n",
      "[LOG 20200502-14:43:43] epoch: 13535 train-loss: 0.010556025430560112\n",
      "[LOG 20200502-14:43:43] epoch: 13536 train-loss: 0.010556013426846929\n",
      "[LOG 20200502-14:43:44] epoch: 13537 train-loss: 0.0105560010092126\n",
      "[LOG 20200502-14:43:44] epoch: 13538 train-loss: 0.010555989005499415\n",
      "[LOG 20200502-14:43:44] epoch: 13539 train-loss: 0.010555975863503085\n",
      "[LOG 20200502-14:43:44] epoch: 13540 train-loss: 0.0105559631354279\n",
      "[LOG 20200502-14:43:45] epoch: 13541 train-loss: 0.010555949889951281\n",
      "[LOG 20200502-14:43:45] epoch: 13542 train-loss: 0.010555936747954952\n",
      "[LOG 20200502-14:43:45] epoch: 13543 train-loss: 0.010555923398998048\n",
      "[LOG 20200502-14:43:45] epoch: 13544 train-loss: 0.010555909532639716\n",
      "[LOG 20200502-14:43:45] epoch: 13545 train-loss: 0.010555896804564528\n",
      "[LOG 20200502-14:43:46] epoch: 13546 train-loss: 0.010555882834725909\n",
      "[LOG 20200502-14:43:46] epoch: 13547 train-loss: 0.01055586938228872\n",
      "[LOG 20200502-14:43:46] epoch: 13548 train-loss: 0.010555856033331819\n",
      "[LOG 20200502-14:43:46] epoch: 13549 train-loss: 0.0105558427878552\n",
      "[LOG 20200502-14:43:46] epoch: 13550 train-loss: 0.010555830266740587\n",
      "[LOG 20200502-14:43:47] epoch: 13551 train-loss: 0.010555817745625973\n",
      "[LOG 20200502-14:43:47] epoch: 13552 train-loss: 0.010555805121031072\n",
      "[LOG 20200502-14:43:47] epoch: 13553 train-loss: 0.01055579280687703\n",
      "[LOG 20200502-14:43:47] epoch: 13554 train-loss: 0.01055578152752585\n",
      "[LOG 20200502-14:43:48] epoch: 13555 train-loss: 0.01055576942033238\n",
      "[LOG 20200502-14:43:48] epoch: 13556 train-loss: 0.01055575803750091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:43:48] epoch: 13557 train-loss: 0.01055574779295259\n",
      "[LOG 20200502-14:43:48] epoch: 13558 train-loss: 0.01055573703100284\n",
      "[LOG 20200502-14:43:48] epoch: 13559 train-loss: 0.010555726889934804\n",
      "[LOG 20200502-14:43:49] epoch: 13560 train-loss: 0.010555717473228773\n",
      "[LOG 20200502-14:43:49] epoch: 13561 train-loss: 0.010555708470443884\n",
      "[LOG 20200502-14:43:49] epoch: 13562 train-loss: 0.010555699778099855\n",
      "[LOG 20200502-14:43:49] epoch: 13563 train-loss: 0.0105556920170784\n",
      "[LOG 20200502-14:43:50] epoch: 13564 train-loss: 0.0105556838421358\n",
      "[LOG 20200502-14:43:50] epoch: 13565 train-loss: 0.01055567649503549\n",
      "[LOG 20200502-14:43:50] epoch: 13566 train-loss: 0.010555669251415465\n",
      "[LOG 20200502-14:43:50] epoch: 13567 train-loss: 0.010555663353039159\n",
      "[LOG 20200502-14:43:50] epoch: 13568 train-loss: 0.010555656730300851\n",
      "[LOG 20200502-14:43:51] epoch: 13569 train-loss: 0.01055565176324712\n",
      "[LOG 20200502-14:43:51] epoch: 13570 train-loss: 0.01055564648575253\n",
      "[LOG 20200502-14:43:51] epoch: 13571 train-loss: 0.01055564203610023\n",
      "[LOG 20200502-14:43:51] epoch: 13572 train-loss: 0.010555637586447928\n",
      "[LOG 20200502-14:43:51] epoch: 13573 train-loss: 0.010555633964637915\n",
      "[LOG 20200502-14:43:52] epoch: 13574 train-loss: 0.010555630342827903\n",
      "[LOG 20200502-14:43:52] epoch: 13575 train-loss: 0.010555627445379892\n",
      "[LOG 20200502-14:43:52] epoch: 13576 train-loss: 0.01055562413401074\n",
      "[LOG 20200502-14:43:52] epoch: 13577 train-loss: 0.010555621547003588\n",
      "[LOG 20200502-14:43:53] epoch: 13578 train-loss: 0.01055561937391758\n",
      "[LOG 20200502-14:43:53] epoch: 13579 train-loss: 0.010555617200831572\n",
      "[LOG 20200502-14:43:53] epoch: 13580 train-loss: 0.010555615752107568\n",
      "[LOG 20200502-14:43:53] epoch: 13581 train-loss: 0.010555613785982132\n",
      "[LOG 20200502-14:43:53] epoch: 13582 train-loss: 0.010555612233777842\n",
      "[LOG 20200502-14:43:54] epoch: 13583 train-loss: 0.010555611509415839\n",
      "[LOG 20200502-14:43:54] epoch: 13584 train-loss: 0.010555609543290403\n",
      "[LOG 20200502-14:43:54] epoch: 13585 train-loss: 0.010555608611967828\n",
      "[LOG 20200502-14:43:54] epoch: 13586 train-loss: 0.01055560778412554\n",
      "[LOG 20200502-14:43:54] epoch: 13587 train-loss: 0.010555606852802966\n",
      "[LOG 20200502-14:43:55] epoch: 13588 train-loss: 0.010555605921480391\n",
      "[LOG 20200502-14:43:55] epoch: 13589 train-loss: 0.010555604576236673\n",
      "[LOG 20200502-14:43:55] epoch: 13590 train-loss: 0.010555603541433811\n",
      "[LOG 20200502-14:43:55] epoch: 13591 train-loss: 0.01055560250663095\n",
      "[LOG 20200502-14:43:56] epoch: 13592 train-loss: 0.010555601057906946\n",
      "[LOG 20200502-14:43:56] epoch: 13593 train-loss: 0.01055559960918294\n",
      "[LOG 20200502-14:43:56] epoch: 13594 train-loss: 0.010555598677860366\n",
      "[LOG 20200502-14:43:56] epoch: 13595 train-loss: 0.01055559671173493\n",
      "[LOG 20200502-14:43:56] epoch: 13596 train-loss: 0.010555594952570068\n",
      "[LOG 20200502-14:43:57] epoch: 13597 train-loss: 0.010555592572523488\n",
      "[LOG 20200502-14:43:57] epoch: 13598 train-loss: 0.01055559039943748\n",
      "[LOG 20200502-14:43:57] epoch: 13599 train-loss: 0.010555587708950043\n",
      "[LOG 20200502-14:43:57] epoch: 13600 train-loss: 0.010555585018462606\n",
      "[LOG 20200502-14:43:58] epoch: 13601 train-loss: 0.010555581810573736\n",
      "[LOG 20200502-14:43:58] epoch: 13602 train-loss: 0.010555578706165155\n",
      "[LOG 20200502-14:43:58] epoch: 13603 train-loss: 0.010555574566953711\n",
      "[LOG 20200502-14:43:58] epoch: 13604 train-loss: 0.01055557063470284\n",
      "[LOG 20200502-14:43:58] epoch: 13605 train-loss: 0.01055556618505054\n",
      "[LOG 20200502-14:43:59] epoch: 13606 train-loss: 0.010555561631917953\n",
      "[LOG 20200502-14:43:59] epoch: 13607 train-loss: 0.010555556147462793\n",
      "[LOG 20200502-14:43:59] epoch: 13608 train-loss: 0.01055555066300763\n",
      "[LOG 20200502-14:43:59] epoch: 13609 train-loss: 0.010555544661151038\n",
      "[LOG 20200502-14:44:00] epoch: 13610 train-loss: 0.010555538762774732\n",
      "[LOG 20200502-14:44:00] epoch: 13611 train-loss: 0.010555531829595566\n",
      "[LOG 20200502-14:44:00] epoch: 13612 train-loss: 0.0105555248964164\n",
      "[LOG 20200502-14:44:00] epoch: 13613 train-loss: 0.010555518066717519\n",
      "[LOG 20200502-14:44:01] epoch: 13614 train-loss: 0.01055551040917635\n",
      "[LOG 20200502-14:44:01] epoch: 13615 train-loss: 0.010555502544674609\n",
      "[LOG 20200502-14:44:01] epoch: 13616 train-loss: 0.010555494473212294\n",
      "[LOG 20200502-14:44:01] epoch: 13617 train-loss: 0.01055548588434855\n",
      "[LOG 20200502-14:44:01] epoch: 13618 train-loss: 0.010555477295484807\n",
      "[LOG 20200502-14:44:02] epoch: 13619 train-loss: 0.010555468706621064\n",
      "[LOG 20200502-14:44:02] epoch: 13620 train-loss: 0.010555459910796748\n",
      "[LOG 20200502-14:44:02] epoch: 13621 train-loss: 0.010555450287130144\n",
      "[LOG 20200502-14:44:02] epoch: 13622 train-loss: 0.01055544118086497\n",
      "[LOG 20200502-14:44:03] epoch: 13623 train-loss: 0.010555432074599795\n",
      "[LOG 20200502-14:44:03] epoch: 13624 train-loss: 0.010555422450933192\n",
      "[LOG 20200502-14:44:03] epoch: 13625 train-loss: 0.01055541355162859\n",
      "[LOG 20200502-14:44:03] epoch: 13626 train-loss: 0.01055540299663941\n",
      "[LOG 20200502-14:44:03] epoch: 13627 train-loss: 0.010555393579933379\n",
      "[LOG 20200502-14:44:04] epoch: 13628 train-loss: 0.010555384266707633\n",
      "[LOG 20200502-14:44:04] epoch: 13629 train-loss: 0.0105553748500016\n",
      "[LOG 20200502-14:44:04] epoch: 13630 train-loss: 0.010555365743736425\n",
      "[LOG 20200502-14:44:04] epoch: 13631 train-loss: 0.010555356327030394\n",
      "[LOG 20200502-14:44:05] epoch: 13632 train-loss: 0.010555347427725792\n",
      "[LOG 20200502-14:44:05] epoch: 13633 train-loss: 0.010555338735381762\n",
      "[LOG 20200502-14:44:05] epoch: 13634 train-loss: 0.010555329732596874\n",
      "[LOG 20200502-14:44:05] epoch: 13635 train-loss: 0.010555320936772559\n",
      "[LOG 20200502-14:44:06] epoch: 13636 train-loss: 0.01055531327923139\n",
      "[LOG 20200502-14:44:06] epoch: 13637 train-loss: 0.010555305000808504\n",
      "[LOG 20200502-14:44:06] epoch: 13638 train-loss: 0.01055529692934619\n",
      "[LOG 20200502-14:44:06] epoch: 13639 train-loss: 0.010555289375285307\n",
      "[LOG 20200502-14:44:07] epoch: 13640 train-loss: 0.010555281303822994\n",
      "[LOG 20200502-14:44:07] epoch: 13641 train-loss: 0.0105552745776044\n",
      "[LOG 20200502-14:44:07] epoch: 13642 train-loss: 0.010555267540944947\n",
      "[LOG 20200502-14:44:07] epoch: 13643 train-loss: 0.01055526060776578\n",
      "[LOG 20200502-14:44:07] epoch: 13644 train-loss: 0.010555254088507758\n",
      "[LOG 20200502-14:44:08] epoch: 13645 train-loss: 0.010555247569249736\n",
      "[LOG 20200502-14:44:08] epoch: 13646 train-loss: 0.010555241360432573\n",
      "[LOG 20200502-14:44:08] epoch: 13647 train-loss: 0.01055523535857598\n",
      "[LOG 20200502-14:44:08] epoch: 13648 train-loss: 0.010555229046278529\n",
      "[LOG 20200502-14:44:09] epoch: 13649 train-loss: 0.010555223975744512\n",
      "[LOG 20200502-14:44:09] epoch: 13650 train-loss: 0.010555218180848492\n",
      "[LOG 20200502-14:44:09] epoch: 13651 train-loss: 0.01055521248943276\n",
      "[LOG 20200502-14:44:09] epoch: 13652 train-loss: 0.010555207211938169\n",
      "[LOG 20200502-14:44:09] epoch: 13653 train-loss: 0.010555202037923865\n",
      "[LOG 20200502-14:44:10] epoch: 13654 train-loss: 0.010555196967389848\n",
      "[LOG 20200502-14:44:10] epoch: 13655 train-loss: 0.010555192000336118\n",
      "[LOG 20200502-14:44:10] epoch: 13656 train-loss: 0.010555187136762671\n",
      "[LOG 20200502-14:44:10] epoch: 13657 train-loss: 0.01055518196274837\n",
      "[LOG 20200502-14:44:11] epoch: 13658 train-loss: 0.010555176685253779\n",
      "[LOG 20200502-14:44:11] epoch: 13659 train-loss: 0.010555171304278903\n",
      "[LOG 20200502-14:44:11] epoch: 13660 train-loss: 0.01055516664766603\n",
      "[LOG 20200502-14:44:11] epoch: 13661 train-loss: 0.010555161163210869\n",
      "[LOG 20200502-14:44:12] epoch: 13662 train-loss: 0.01055515588571628\n",
      "[LOG 20200502-14:44:12] epoch: 13663 train-loss: 0.010555150504741404\n",
      "[LOG 20200502-14:44:12] epoch: 13664 train-loss: 0.01055514481332567\n",
      "[LOG 20200502-14:44:12] epoch: 13665 train-loss: 0.010555139742791653\n",
      "[LOG 20200502-14:44:13] epoch: 13666 train-loss: 0.010555133120053344\n",
      "[LOG 20200502-14:44:13] epoch: 13667 train-loss: 0.010555127118196752\n",
      "[LOG 20200502-14:44:13] epoch: 13668 train-loss: 0.010555121323300732\n",
      "[LOG 20200502-14:44:13] epoch: 13669 train-loss: 0.010555115217963854\n",
      "[LOG 20200502-14:44:13] epoch: 13670 train-loss: 0.010555108077824116\n",
      "[LOG 20200502-14:44:14] epoch: 13671 train-loss: 0.010555101248125235\n",
      "[LOG 20200502-14:44:14] epoch: 13672 train-loss: 0.01055509400450521\n",
      "[LOG 20200502-14:44:14] epoch: 13673 train-loss: 0.010555086760885186\n",
      "[LOG 20200502-14:44:14] epoch: 13674 train-loss: 0.010555078689422872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:44:15] epoch: 13675 train-loss: 0.010555070204039415\n",
      "[LOG 20200502-14:44:15] epoch: 13676 train-loss: 0.010555062029096816\n",
      "[LOG 20200502-14:44:15] epoch: 13677 train-loss: 0.0105550532332725\n",
      "[LOG 20200502-14:44:15] epoch: 13678 train-loss: 0.01055504402352704\n",
      "[LOG 20200502-14:44:16] epoch: 13679 train-loss: 0.010555034710301293\n",
      "[LOG 20200502-14:44:16] epoch: 13680 train-loss: 0.010555025190114975\n",
      "[LOG 20200502-14:44:16] epoch: 13681 train-loss: 0.010555014324684938\n",
      "[LOG 20200502-14:44:16] epoch: 13682 train-loss: 0.010555004390577475\n",
      "[LOG 20200502-14:44:16] epoch: 13683 train-loss: 0.010554992800785435\n",
      "[LOG 20200502-14:44:17] epoch: 13684 train-loss: 0.010554982245796256\n",
      "[LOG 20200502-14:44:17] epoch: 13685 train-loss: 0.010554970862964788\n",
      "[LOG 20200502-14:44:17] epoch: 13686 train-loss: 0.010554959066212177\n",
      "[LOG 20200502-14:44:17] epoch: 13687 train-loss: 0.010554947372939851\n",
      "[LOG 20200502-14:44:18] epoch: 13688 train-loss: 0.010554934127463235\n",
      "[LOG 20200502-14:44:18] epoch: 13689 train-loss: 0.010554921813309193\n",
      "[LOG 20200502-14:44:18] epoch: 13690 train-loss: 0.010554909292194579\n",
      "[LOG 20200502-14:44:18] epoch: 13691 train-loss: 0.01055489635715882\n",
      "[LOG 20200502-14:44:18] epoch: 13692 train-loss: 0.01055488290472163\n",
      "[LOG 20200502-14:44:19] epoch: 13693 train-loss: 0.010554869866205586\n",
      "[LOG 20200502-14:44:19] epoch: 13694 train-loss: 0.010554856724209256\n",
      "[LOG 20200502-14:44:19] epoch: 13695 train-loss: 0.010554842857850922\n",
      "[LOG 20200502-14:44:19] epoch: 13696 train-loss: 0.010554828681051731\n",
      "[LOG 20200502-14:44:20] epoch: 13697 train-loss: 0.010554815746015973\n",
      "[LOG 20200502-14:44:20] epoch: 13698 train-loss: 0.010554802293578783\n",
      "[LOG 20200502-14:44:20] epoch: 13699 train-loss: 0.01055478873766131\n",
      "[LOG 20200502-14:44:20] epoch: 13700 train-loss: 0.010554775078263547\n",
      "[LOG 20200502-14:44:21] epoch: 13701 train-loss: 0.010554761108424928\n",
      "[LOG 20200502-14:44:21] epoch: 13702 train-loss: 0.010554748483830027\n",
      "[LOG 20200502-14:44:21] epoch: 13703 train-loss: 0.010554735341833698\n",
      "[LOG 20200502-14:44:22] epoch: 13704 train-loss: 0.010554722303317653\n",
      "[LOG 20200502-14:44:22] epoch: 13705 train-loss: 0.010554709678722752\n",
      "[LOG 20200502-14:44:22] epoch: 13706 train-loss: 0.010554697468048997\n",
      "[LOG 20200502-14:44:22] epoch: 13707 train-loss: 0.010554685567816099\n",
      "[LOG 20200502-14:44:23] epoch: 13708 train-loss: 0.010554673771063486\n",
      "[LOG 20200502-14:44:23] epoch: 13709 train-loss: 0.010554662491712306\n",
      "[LOG 20200502-14:44:23] epoch: 13710 train-loss: 0.010554652040203413\n",
      "[LOG 20200502-14:44:23] epoch: 13711 train-loss: 0.010554641381733947\n",
      "[LOG 20200502-14:44:24] epoch: 13712 train-loss: 0.010554631137185626\n",
      "[LOG 20200502-14:44:24] epoch: 13713 train-loss: 0.010554621513519023\n",
      "[LOG 20200502-14:44:24] epoch: 13714 train-loss: 0.010554611889852418\n",
      "[LOG 20200502-14:44:24] epoch: 13715 train-loss: 0.010554602680106958\n",
      "[LOG 20200502-14:44:24] epoch: 13716 train-loss: 0.010554595333006647\n",
      "[LOG 20200502-14:44:25] epoch: 13717 train-loss: 0.010554587261544334\n",
      "[LOG 20200502-14:44:25] epoch: 13718 train-loss: 0.010554580017924309\n",
      "[LOG 20200502-14:44:25] epoch: 13719 train-loss: 0.010554572567343712\n",
      "[LOG 20200502-14:44:25] epoch: 13720 train-loss: 0.010554565634164546\n",
      "[LOG 20200502-14:44:26] epoch: 13721 train-loss: 0.010554560046229098\n",
      "[LOG 20200502-14:44:26] epoch: 13722 train-loss: 0.010554553837411933\n",
      "[LOG 20200502-14:44:26] epoch: 13723 train-loss: 0.010554548352956772\n",
      "[LOG 20200502-14:44:26] epoch: 13724 train-loss: 0.01055454421374533\n",
      "[LOG 20200502-14:44:26] epoch: 13725 train-loss: 0.010554539350171884\n",
      "[LOG 20200502-14:44:27] epoch: 13726 train-loss: 0.010554534900519583\n",
      "[LOG 20200502-14:44:27] epoch: 13727 train-loss: 0.010554531175229285\n",
      "[LOG 20200502-14:44:27] epoch: 13728 train-loss: 0.010554528174300989\n",
      "[LOG 20200502-14:44:27] epoch: 13729 train-loss: 0.01055452496641212\n",
      "[LOG 20200502-14:44:28] epoch: 13730 train-loss: 0.01055452206896411\n",
      "[LOG 20200502-14:44:28] epoch: 13731 train-loss: 0.010554519481956959\n",
      "[LOG 20200502-14:44:28] epoch: 13732 train-loss: 0.010554516791469522\n",
      "[LOG 20200502-14:44:28] epoch: 13733 train-loss: 0.010554515135784944\n",
      "[LOG 20200502-14:44:29] epoch: 13734 train-loss: 0.010554513273139795\n",
      "[LOG 20200502-14:44:29] epoch: 13735 train-loss: 0.010554511617455218\n",
      "[LOG 20200502-14:44:29] epoch: 13736 train-loss: 0.010554509340888925\n",
      "[LOG 20200502-14:44:29] epoch: 13737 train-loss: 0.010554508513046635\n",
      "[LOG 20200502-14:44:30] epoch: 13738 train-loss: 0.010554507374763489\n",
      "[LOG 20200502-14:44:30] epoch: 13739 train-loss: 0.01055450551211834\n",
      "[LOG 20200502-14:44:30] epoch: 13740 train-loss: 0.010554504166874621\n",
      "[LOG 20200502-14:44:30] epoch: 13741 train-loss: 0.010554502407709757\n",
      "[LOG 20200502-14:44:31] epoch: 13742 train-loss: 0.010554501476387182\n",
      "[LOG 20200502-14:44:31] epoch: 13743 train-loss: 0.010554500338104036\n",
      "[LOG 20200502-14:44:31] epoch: 13744 train-loss: 0.010554498371978601\n",
      "[LOG 20200502-14:44:31] epoch: 13745 train-loss: 0.01055449681977431\n",
      "[LOG 20200502-14:44:31] epoch: 13746 train-loss: 0.01055449547453059\n",
      "[LOG 20200502-14:44:32] epoch: 13747 train-loss: 0.01055449309448401\n",
      "[LOG 20200502-14:44:32] epoch: 13748 train-loss: 0.01055449154227972\n",
      "[LOG 20200502-14:44:32] epoch: 13749 train-loss: 0.01055448864483171\n",
      "[LOG 20200502-14:44:32] epoch: 13750 train-loss: 0.010554486161304845\n",
      "[LOG 20200502-14:44:32] epoch: 13751 train-loss: 0.010554483263856836\n",
      "[LOG 20200502-14:44:33] epoch: 13752 train-loss: 0.010554479952487681\n",
      "[LOG 20200502-14:44:33] epoch: 13753 train-loss: 0.010554476848079098\n",
      "[LOG 20200502-14:44:33] epoch: 13754 train-loss: 0.010554473226269087\n",
      "[LOG 20200502-14:44:33] epoch: 13755 train-loss: 0.010554468983577358\n",
      "[LOG 20200502-14:44:34] epoch: 13756 train-loss: 0.010554465361767344\n",
      "[LOG 20200502-14:44:34] epoch: 13757 train-loss: 0.010554459877312183\n",
      "[LOG 20200502-14:44:34] epoch: 13758 train-loss: 0.01055445470329788\n",
      "[LOG 20200502-14:44:34] epoch: 13759 train-loss: 0.010554449632763863\n",
      "[LOG 20200502-14:44:35] epoch: 13760 train-loss: 0.010554442803064982\n",
      "[LOG 20200502-14:44:35] epoch: 13761 train-loss: 0.010554437215129534\n",
      "[LOG 20200502-14:44:35] epoch: 13762 train-loss: 0.01055443048891094\n",
      "[LOG 20200502-14:44:35] epoch: 13763 train-loss: 0.01055442365921206\n",
      "[LOG 20200502-14:44:36] epoch: 13764 train-loss: 0.010554416415592035\n",
      "[LOG 20200502-14:44:36] epoch: 13765 train-loss: 0.01055440917197201\n",
      "[LOG 20200502-14:44:36] epoch: 13766 train-loss: 0.01055440099702941\n",
      "[LOG 20200502-14:44:36] epoch: 13767 train-loss: 0.010554392615126239\n",
      "[LOG 20200502-14:44:36] epoch: 13768 train-loss: 0.010554383922782209\n",
      "[LOG 20200502-14:44:37] epoch: 13769 train-loss: 0.010554375023477607\n",
      "[LOG 20200502-14:44:37] epoch: 13770 train-loss: 0.010554366434613863\n",
      "[LOG 20200502-14:44:37] epoch: 13771 train-loss: 0.010554356914427545\n",
      "[LOG 20200502-14:44:37] epoch: 13772 train-loss: 0.010554348015122943\n",
      "[LOG 20200502-14:44:38] epoch: 13773 train-loss: 0.01055433756361405\n",
      "[LOG 20200502-14:44:38] epoch: 13774 train-loss: 0.01055432835386859\n",
      "[LOG 20200502-14:44:38] epoch: 13775 train-loss: 0.010554318316280842\n",
      "[LOG 20200502-14:44:38] epoch: 13776 train-loss: 0.01055430838217338\n",
      "[LOG 20200502-14:44:38] epoch: 13777 train-loss: 0.010554298241105344\n",
      "[LOG 20200502-14:44:39] epoch: 13778 train-loss: 0.010554288410478167\n",
      "[LOG 20200502-14:44:39] epoch: 13779 train-loss: 0.01055427837289042\n",
      "[LOG 20200502-14:44:39] epoch: 13780 train-loss: 0.010554268542263243\n",
      "[LOG 20200502-14:44:39] epoch: 13781 train-loss: 0.010554259332517782\n",
      "[LOG 20200502-14:44:39] epoch: 13782 train-loss: 0.010554248777528604\n",
      "[LOG 20200502-14:44:40] epoch: 13783 train-loss: 0.010554239153862\n",
      "[LOG 20200502-14:44:40] epoch: 13784 train-loss: 0.010554230358037684\n",
      "[LOG 20200502-14:44:40] epoch: 13785 train-loss: 0.010554220010009076\n",
      "[LOG 20200502-14:44:40] epoch: 13786 train-loss: 0.010554211110704474\n",
      "[LOG 20200502-14:44:41] epoch: 13787 train-loss: 0.01055420252184073\n",
      "[LOG 20200502-14:44:41] epoch: 13788 train-loss: 0.010554193415575557\n",
      "[LOG 20200502-14:44:41] epoch: 13789 train-loss: 0.0105541849301921\n",
      "[LOG 20200502-14:44:41] epoch: 13790 train-loss: 0.010554176548288928\n",
      "[LOG 20200502-14:44:41] epoch: 13791 train-loss: 0.01055416806290547\n",
      "[LOG 20200502-14:44:42] epoch: 13792 train-loss: 0.010554160612324873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:44:42] epoch: 13793 train-loss: 0.010554153058263991\n",
      "[LOG 20200502-14:44:42] epoch: 13794 train-loss: 0.010554144469400248\n",
      "[LOG 20200502-14:44:42] epoch: 13795 train-loss: 0.010554137846661938\n",
      "[LOG 20200502-14:44:43] epoch: 13796 train-loss: 0.0105541307065222\n",
      "[LOG 20200502-14:44:43] epoch: 13797 train-loss: 0.010554123462902175\n",
      "[LOG 20200502-14:44:43] epoch: 13798 train-loss: 0.010554117357565297\n",
      "[LOG 20200502-14:44:43] epoch: 13799 train-loss: 0.010554110527866416\n",
      "[LOG 20200502-14:44:44] epoch: 13800 train-loss: 0.010554103077285819\n",
      "[LOG 20200502-14:44:44] epoch: 13801 train-loss: 0.010554097489350371\n",
      "[LOG 20200502-14:44:44] epoch: 13802 train-loss: 0.010554091590974066\n",
      "[LOG 20200502-14:44:44] epoch: 13803 train-loss: 0.010554085485637188\n",
      "[LOG 20200502-14:44:44] epoch: 13804 train-loss: 0.010554080104662312\n",
      "[LOG 20200502-14:44:45] epoch: 13805 train-loss: 0.010554073895845149\n",
      "[LOG 20200502-14:44:45] epoch: 13806 train-loss: 0.010554068411389986\n",
      "[LOG 20200502-14:44:45] epoch: 13807 train-loss: 0.010554062513013681\n",
      "[LOG 20200502-14:44:45] epoch: 13808 train-loss: 0.010554056821597947\n",
      "[LOG 20200502-14:44:46] epoch: 13809 train-loss: 0.010554051233662499\n",
      "[LOG 20200502-14:44:46] epoch: 13810 train-loss: 0.010554045749207338\n",
      "[LOG 20200502-14:44:46] epoch: 13811 train-loss: 0.010554040471712748\n",
      "[LOG 20200502-14:44:46] epoch: 13812 train-loss: 0.010554034469856156\n",
      "[LOG 20200502-14:44:46] epoch: 13813 train-loss: 0.010554028985400995\n",
      "[LOG 20200502-14:44:47] epoch: 13814 train-loss: 0.01055402360442612\n",
      "[LOG 20200502-14:44:47] epoch: 13815 train-loss: 0.0105540178095301\n",
      "[LOG 20200502-14:44:47] epoch: 13816 train-loss: 0.010554011290272078\n",
      "[LOG 20200502-14:44:47] epoch: 13817 train-loss: 0.01055400570233663\n",
      "[LOG 20200502-14:44:47] epoch: 13818 train-loss: 0.010553999183078608\n",
      "[LOG 20200502-14:44:48] epoch: 13819 train-loss: 0.010553992456860013\n",
      "[LOG 20200502-14:44:48] epoch: 13820 train-loss: 0.01055398645500342\n",
      "[LOG 20200502-14:44:48] epoch: 13821 train-loss: 0.010553979521824254\n",
      "[LOG 20200502-14:44:48] epoch: 13822 train-loss: 0.01055397279560566\n",
      "[LOG 20200502-14:44:49] epoch: 13823 train-loss: 0.010553965551985635\n",
      "[LOG 20200502-14:44:49] epoch: 13824 train-loss: 0.01055395830836561\n",
      "[LOG 20200502-14:44:49] epoch: 13825 train-loss: 0.01055395065082444\n",
      "[LOG 20200502-14:44:49] epoch: 13826 train-loss: 0.010553942682842413\n",
      "[LOG 20200502-14:44:49] epoch: 13827 train-loss: 0.010553934507899813\n",
      "[LOG 20200502-14:44:50] epoch: 13828 train-loss: 0.0105539264364375\n",
      "[LOG 20200502-14:44:50] epoch: 13829 train-loss: 0.010553917433652613\n",
      "[LOG 20200502-14:44:50] epoch: 13830 train-loss: 0.010553908637828298\n",
      "[LOG 20200502-14:44:50] epoch: 13831 train-loss: 0.010553899221122265\n",
      "[LOG 20200502-14:44:51] epoch: 13832 train-loss: 0.010553889700935947\n",
      "[LOG 20200502-14:44:51] epoch: 13833 train-loss: 0.010553879663348198\n",
      "[LOG 20200502-14:44:51] epoch: 13834 train-loss: 0.010553869418799877\n",
      "[LOG 20200502-14:44:51] epoch: 13835 train-loss: 0.010553858967290984\n",
      "[LOG 20200502-14:44:51] epoch: 13836 train-loss: 0.01055384830882152\n",
      "[LOG 20200502-14:44:52] epoch: 13837 train-loss: 0.010553837650352053\n",
      "[LOG 20200502-14:44:52] epoch: 13838 train-loss: 0.010553826267520586\n",
      "[LOG 20200502-14:44:52] epoch: 13839 train-loss: 0.010553814470767975\n",
      "[LOG 20200502-14:44:52] epoch: 13840 train-loss: 0.010553802570535077\n",
      "[LOG 20200502-14:44:53] epoch: 13841 train-loss: 0.010553790670302179\n",
      "[LOG 20200502-14:44:53] epoch: 13842 train-loss: 0.010553778252667852\n",
      "[LOG 20200502-14:44:53] epoch: 13843 train-loss: 0.010553765421112379\n",
      "[LOG 20200502-14:44:53] epoch: 13844 train-loss: 0.010553752589556906\n",
      "[LOG 20200502-14:44:53] epoch: 13845 train-loss: 0.010553739551040862\n",
      "[LOG 20200502-14:44:54] epoch: 13846 train-loss: 0.010553726822965674\n",
      "[LOG 20200502-14:44:54] epoch: 13847 train-loss: 0.0105537132670482\n",
      "[LOG 20200502-14:44:54] epoch: 13848 train-loss: 0.010553699607650438\n",
      "[LOG 20200502-14:44:54] epoch: 13849 train-loss: 0.010553686879575253\n",
      "[LOG 20200502-14:44:54] epoch: 13850 train-loss: 0.010553673220177492\n",
      "[LOG 20200502-14:44:55] epoch: 13851 train-loss: 0.010553659457299445\n",
      "[LOG 20200502-14:44:55] epoch: 13852 train-loss: 0.010553646108342541\n",
      "[LOG 20200502-14:44:55] epoch: 13853 train-loss: 0.010553632448944781\n",
      "[LOG 20200502-14:44:55] epoch: 13854 train-loss: 0.010553619203468164\n",
      "[LOG 20200502-14:44:56] epoch: 13855 train-loss: 0.010553606682353549\n",
      "[LOG 20200502-14:44:56] epoch: 13856 train-loss: 0.01055359271251493\n",
      "[LOG 20200502-14:44:56] epoch: 13857 train-loss: 0.010553580294880602\n",
      "[LOG 20200502-14:44:56] epoch: 13858 train-loss: 0.01055356746332513\n",
      "[LOG 20200502-14:44:56] epoch: 13859 train-loss: 0.010553555045690801\n",
      "[LOG 20200502-14:44:57] epoch: 13860 train-loss: 0.010553542731536759\n",
      "[LOG 20200502-14:44:57] epoch: 13861 train-loss: 0.010553530520863004\n",
      "[LOG 20200502-14:44:57] epoch: 13862 train-loss: 0.010553519344992109\n",
      "[LOG 20200502-14:44:57] epoch: 13863 train-loss: 0.010553507651719782\n",
      "[LOG 20200502-14:44:58] epoch: 13864 train-loss: 0.010553497303691175\n",
      "[LOG 20200502-14:44:58] epoch: 13865 train-loss: 0.010553486334780851\n",
      "[LOG 20200502-14:44:58] epoch: 13866 train-loss: 0.010553476193712817\n",
      "[LOG 20200502-14:44:58] epoch: 13867 train-loss: 0.010553466570046213\n",
      "[LOG 20200502-14:44:58] epoch: 13868 train-loss: 0.01055345767074161\n",
      "[LOG 20200502-14:44:59] epoch: 13869 train-loss: 0.010553448357515864\n",
      "[LOG 20200502-14:44:59] epoch: 13870 train-loss: 0.010553439458211264\n",
      "[LOG 20200502-14:44:59] epoch: 13871 train-loss: 0.010553431800670095\n",
      "[LOG 20200502-14:44:59] epoch: 13872 train-loss: 0.010553424453569783\n",
      "[LOG 20200502-14:45:00] epoch: 13873 train-loss: 0.010553417002989186\n",
      "[LOG 20200502-14:45:00] epoch: 13874 train-loss: 0.010553410173290305\n",
      "[LOG 20200502-14:45:00] epoch: 13875 train-loss: 0.010553403343591426\n",
      "[LOG 20200502-14:45:00] epoch: 13876 train-loss: 0.010553397652175691\n",
      "[LOG 20200502-14:45:01] epoch: 13877 train-loss: 0.010553391443358527\n",
      "[LOG 20200502-14:45:01] epoch: 13878 train-loss: 0.010553386579785082\n",
      "[LOG 20200502-14:45:01] epoch: 13879 train-loss: 0.01055338161273135\n",
      "[LOG 20200502-14:45:01] epoch: 13880 train-loss: 0.01055337768048048\n",
      "[LOG 20200502-14:45:02] epoch: 13881 train-loss: 0.010553373334308466\n",
      "[LOG 20200502-14:45:02] epoch: 13882 train-loss: 0.010553369609018167\n",
      "[LOG 20200502-14:45:02] epoch: 13883 train-loss: 0.010553365780247582\n",
      "[LOG 20200502-14:45:02] epoch: 13884 train-loss: 0.010553363089760145\n",
      "[LOG 20200502-14:45:03] epoch: 13885 train-loss: 0.01055335977839099\n",
      "[LOG 20200502-14:45:03] epoch: 13886 train-loss: 0.010553357087903552\n",
      "[LOG 20200502-14:45:03] epoch: 13887 train-loss: 0.01055335532873869\n",
      "[LOG 20200502-14:45:03] epoch: 13888 train-loss: 0.01055335294869211\n",
      "[LOG 20200502-14:45:03] epoch: 13889 train-loss: 0.010553350258204672\n",
      "[LOG 20200502-14:45:04] epoch: 13890 train-loss: 0.010553348809480667\n",
      "[LOG 20200502-14:45:04] epoch: 13891 train-loss: 0.010553346843355231\n",
      "[LOG 20200502-14:45:04] epoch: 13892 train-loss: 0.010553344980710082\n",
      "[LOG 20200502-14:45:04] epoch: 13893 train-loss: 0.010553343325025506\n",
      "[LOG 20200502-14:45:05] epoch: 13894 train-loss: 0.010553341669340929\n",
      "[LOG 20200502-14:45:05] epoch: 13895 train-loss: 0.010553340013656352\n",
      "[LOG 20200502-14:45:05] epoch: 13896 train-loss: 0.010553338254491488\n",
      "[LOG 20200502-14:45:05] epoch: 13897 train-loss: 0.010553336288366053\n",
      "[LOG 20200502-14:45:05] epoch: 13898 train-loss: 0.010553334425720904\n",
      "[LOG 20200502-14:45:06] epoch: 13899 train-loss: 0.01055333266655604\n",
      "[LOG 20200502-14:45:06] epoch: 13900 train-loss: 0.010553330389989747\n",
      "[LOG 20200502-14:45:06] epoch: 13901 train-loss: 0.010553328009943167\n",
      "[LOG 20200502-14:45:06] epoch: 13902 train-loss: 0.010553325733376874\n",
      "[LOG 20200502-14:45:07] epoch: 13903 train-loss: 0.010553323042889437\n",
      "[LOG 20200502-14:45:07] epoch: 13904 train-loss: 0.010553320248921713\n",
      "[LOG 20200502-14:45:07] epoch: 13905 train-loss: 0.010553316730591986\n",
      "[LOG 20200502-14:45:07] epoch: 13906 train-loss: 0.010553313315742545\n",
      "[LOG 20200502-14:45:07] epoch: 13907 train-loss: 0.010553309900893105\n",
      "[LOG 20200502-14:45:08] epoch: 13908 train-loss: 0.010553305968642235\n",
      "[LOG 20200502-14:45:08] epoch: 13909 train-loss: 0.010553301932911078\n",
      "[LOG 20200502-14:45:08] epoch: 13910 train-loss: 0.010553297069337633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:45:08] epoch: 13911 train-loss: 0.010553291998803616\n",
      "[LOG 20200502-14:45:09] epoch: 13912 train-loss: 0.010553286307387881\n",
      "[LOG 20200502-14:45:09] epoch: 13913 train-loss: 0.010553280615972148\n",
      "[LOG 20200502-14:45:09] epoch: 13914 train-loss: 0.010553274096714126\n",
      "[LOG 20200502-14:45:09] epoch: 13915 train-loss: 0.01055326819833782\n",
      "[LOG 20200502-14:45:09] epoch: 13916 train-loss: 0.010553260954717794\n",
      "[LOG 20200502-14:45:10] epoch: 13917 train-loss: 0.010553254021538628\n",
      "[LOG 20200502-14:45:10] epoch: 13918 train-loss: 0.0105532460535566\n",
      "[LOG 20200502-14:45:10] epoch: 13919 train-loss: 0.010553238292535147\n",
      "[LOG 20200502-14:45:10] epoch: 13920 train-loss: 0.010553229910631975\n",
      "[LOG 20200502-14:45:10] epoch: 13921 train-loss: 0.010553221735689376\n",
      "[LOG 20200502-14:45:11] epoch: 13922 train-loss: 0.01055321242246363\n",
      "[LOG 20200502-14:45:11] epoch: 13923 train-loss: 0.010553203730119599\n",
      "[LOG 20200502-14:45:11] epoch: 13924 train-loss: 0.010553194830814997\n",
      "[LOG 20200502-14:45:11] epoch: 13925 train-loss: 0.010553185103668107\n",
      "[LOG 20200502-14:45:12] epoch: 13926 train-loss: 0.01055317579044236\n",
      "[LOG 20200502-14:45:12] epoch: 13927 train-loss: 0.010553166166775756\n",
      "[LOG 20200502-14:45:12] epoch: 13928 train-loss: 0.010553156129188009\n",
      "[LOG 20200502-14:45:12] epoch: 13929 train-loss: 0.010553146402041117\n",
      "[LOG 20200502-14:45:12] epoch: 13930 train-loss: 0.010553136467933655\n",
      "[LOG 20200502-14:45:13] epoch: 13931 train-loss: 0.010553126119905047\n",
      "[LOG 20200502-14:45:13] epoch: 13932 train-loss: 0.010553116392758157\n",
      "[LOG 20200502-14:45:13] epoch: 13933 train-loss: 0.010553106148209836\n",
      "[LOG 20200502-14:45:13] epoch: 13934 train-loss: 0.010553096628023518\n",
      "[LOG 20200502-14:45:14] epoch: 13935 train-loss: 0.010553086383475197\n",
      "[LOG 20200502-14:45:14] epoch: 13936 train-loss: 0.010553076759808593\n",
      "[LOG 20200502-14:45:14] epoch: 13937 train-loss: 0.010553067446582846\n",
      "[LOG 20200502-14:45:14] epoch: 13938 train-loss: 0.010553057719435956\n",
      "[LOG 20200502-14:45:14] epoch: 13939 train-loss: 0.010553048509690497\n",
      "[LOG 20200502-14:45:15] epoch: 13940 train-loss: 0.01055303867906332\n",
      "[LOG 20200502-14:45:15] epoch: 13941 train-loss: 0.010553029779758718\n",
      "[LOG 20200502-14:45:15] epoch: 13942 train-loss: 0.010553021294375261\n",
      "[LOG 20200502-14:45:15] epoch: 13943 train-loss: 0.010553012602031231\n",
      "[LOG 20200502-14:45:16] epoch: 13944 train-loss: 0.010553004323608346\n",
      "[LOG 20200502-14:45:16] epoch: 13945 train-loss: 0.010552995320823457\n",
      "[LOG 20200502-14:45:16] epoch: 13946 train-loss: 0.010552988284164004\n",
      "[LOG 20200502-14:45:16] epoch: 13947 train-loss: 0.010552979591819975\n",
      "[LOG 20200502-14:45:17] epoch: 13948 train-loss: 0.010552972969081666\n",
      "[LOG 20200502-14:45:17] epoch: 13949 train-loss: 0.010552964897619354\n",
      "[LOG 20200502-14:45:17] epoch: 13950 train-loss: 0.010552957653999329\n",
      "[LOG 20200502-14:45:17] epoch: 13951 train-loss: 0.010552950306899019\n",
      "[LOG 20200502-14:45:17] epoch: 13952 train-loss: 0.01055294368416071\n",
      "[LOG 20200502-14:45:18] epoch: 13953 train-loss: 0.010552936440540684\n",
      "[LOG 20200502-14:45:18] epoch: 13954 train-loss: 0.01055292971432209\n",
      "[LOG 20200502-14:45:18] epoch: 13955 train-loss: 0.010552923298544355\n",
      "[LOG 20200502-14:45:18] epoch: 13956 train-loss: 0.010552916882766617\n",
      "[LOG 20200502-14:45:19] epoch: 13957 train-loss: 0.010552910363508595\n",
      "[LOG 20200502-14:45:19] epoch: 13958 train-loss: 0.010552904775573147\n",
      "[LOG 20200502-14:45:19] epoch: 13959 train-loss: 0.010552898463275697\n",
      "[LOG 20200502-14:45:19] epoch: 13960 train-loss: 0.010552892357938819\n",
      "[LOG 20200502-14:45:19] epoch: 13961 train-loss: 0.010552885838680796\n",
      "[LOG 20200502-14:45:20] epoch: 13962 train-loss: 0.010552880147265064\n",
      "[LOG 20200502-14:45:20] epoch: 13963 train-loss: 0.010552874559329616\n",
      "[LOG 20200502-14:45:20] epoch: 13964 train-loss: 0.010552868350512452\n",
      "[LOG 20200502-14:45:20] epoch: 13965 train-loss: 0.010552862555616431\n",
      "[LOG 20200502-14:45:20] epoch: 13966 train-loss: 0.010552856657240126\n",
      "[LOG 20200502-14:45:21] epoch: 13967 train-loss: 0.01055285075886382\n",
      "[LOG 20200502-14:45:21] epoch: 13968 train-loss: 0.01055284444656637\n",
      "[LOG 20200502-14:45:21] epoch: 13969 train-loss: 0.010552838444709778\n",
      "[LOG 20200502-14:45:21] epoch: 13970 train-loss: 0.010552832028932042\n",
      "[LOG 20200502-14:45:22] epoch: 13971 train-loss: 0.010552826027075449\n",
      "[LOG 20200502-14:45:22] epoch: 13972 train-loss: 0.01055281919737657\n",
      "[LOG 20200502-14:45:22] epoch: 13973 train-loss: 0.010552812367677689\n",
      "[LOG 20200502-14:45:22] epoch: 13974 train-loss: 0.010552805848419666\n",
      "[LOG 20200502-14:45:22] epoch: 13975 train-loss: 0.0105527989152405\n",
      "[LOG 20200502-14:45:23] epoch: 13976 train-loss: 0.01055279177510076\n",
      "[LOG 20200502-14:45:23] epoch: 13977 train-loss: 0.010552784221039878\n",
      "[LOG 20200502-14:45:23] epoch: 13978 train-loss: 0.010552776460018422\n",
      "[LOG 20200502-14:45:23] epoch: 13979 train-loss: 0.010552769319878684\n",
      "[LOG 20200502-14:45:24] epoch: 13980 train-loss: 0.010552761351896657\n",
      "[LOG 20200502-14:45:24] epoch: 13981 train-loss: 0.010552753176954057\n",
      "[LOG 20200502-14:45:24] epoch: 13982 train-loss: 0.010552744795050886\n",
      "[LOG 20200502-14:45:24] epoch: 13983 train-loss: 0.010552736102706857\n",
      "[LOG 20200502-14:45:24] epoch: 13984 train-loss: 0.010552727410362827\n",
      "[LOG 20200502-14:45:25] epoch: 13985 train-loss: 0.010552717890176509\n",
      "[LOG 20200502-14:45:25] epoch: 13986 train-loss: 0.01055270888739162\n",
      "[LOG 20200502-14:45:25] epoch: 13987 train-loss: 0.010552699574165873\n",
      "[LOG 20200502-14:45:25] epoch: 13988 train-loss: 0.010552689743538698\n",
      "[LOG 20200502-14:45:26] epoch: 13989 train-loss: 0.01055267970595095\n",
      "[LOG 20200502-14:45:26] epoch: 13990 train-loss: 0.010552669254442057\n",
      "[LOG 20200502-14:45:26] epoch: 13991 train-loss: 0.01055265859597259\n",
      "[LOG 20200502-14:45:26] epoch: 13992 train-loss: 0.010552648040983412\n",
      "[LOG 20200502-14:45:27] epoch: 13993 train-loss: 0.01055263676163223\n",
      "[LOG 20200502-14:45:27] epoch: 13994 train-loss: 0.01055262548228105\n",
      "[LOG 20200502-14:45:27] epoch: 13995 train-loss: 0.010552613892489009\n",
      "[LOG 20200502-14:45:27] epoch: 13996 train-loss: 0.01055260230269697\n",
      "[LOG 20200502-14:45:27] epoch: 13997 train-loss: 0.010552590816385217\n",
      "[LOG 20200502-14:45:28] epoch: 13998 train-loss: 0.010552578398750888\n",
      "[LOG 20200502-14:45:28] epoch: 13999 train-loss: 0.010552566705478562\n",
      "[LOG 20200502-14:45:28] epoch: 14000 train-loss: 0.010552554184363948\n",
      "[LOG 20200502-14:45:28] epoch: 14001 train-loss: 0.010552541870209906\n",
      "[LOG 20200502-14:45:29] epoch: 14002 train-loss: 0.010552529245615005\n",
      "[LOG 20200502-14:45:29] epoch: 14003 train-loss: 0.010552516724500392\n",
      "[LOG 20200502-14:45:29] epoch: 14004 train-loss: 0.010552503892944919\n",
      "[LOG 20200502-14:45:29] epoch: 14005 train-loss: 0.010552491164869733\n",
      "[LOG 20200502-14:45:30] epoch: 14006 train-loss: 0.010552478747235404\n",
      "[LOG 20200502-14:45:30] epoch: 14007 train-loss: 0.010552466329601076\n",
      "[LOG 20200502-14:45:30] epoch: 14008 train-loss: 0.010552453394565318\n",
      "[LOG 20200502-14:45:30] epoch: 14009 train-loss: 0.010552441183891561\n",
      "[LOG 20200502-14:45:30] epoch: 14010 train-loss: 0.010552428869737519\n",
      "[LOG 20200502-14:45:31] epoch: 14011 train-loss: 0.01055241645210319\n",
      "[LOG 20200502-14:45:31] epoch: 14012 train-loss: 0.010552404448390007\n",
      "[LOG 20200502-14:45:31] epoch: 14013 train-loss: 0.010552392548157109\n",
      "[LOG 20200502-14:45:31] epoch: 14014 train-loss: 0.010552380751404498\n",
      "[LOG 20200502-14:45:31] epoch: 14015 train-loss: 0.010552369575533602\n",
      "[LOG 20200502-14:45:32] epoch: 14016 train-loss: 0.010552358710103564\n",
      "[LOG 20200502-14:45:32] epoch: 14017 train-loss: 0.010552347637712955\n",
      "[LOG 20200502-14:45:32] epoch: 14018 train-loss: 0.010552337186204063\n",
      "[LOG 20200502-14:45:32] epoch: 14019 train-loss: 0.010552326631214883\n",
      "[LOG 20200502-14:45:33] epoch: 14020 train-loss: 0.010552316593627134\n",
      "[LOG 20200502-14:45:33] epoch: 14021 train-loss: 0.010552307383881675\n",
      "[LOG 20200502-14:45:33] epoch: 14022 train-loss: 0.010552298070655929\n",
      "[LOG 20200502-14:45:33] epoch: 14023 train-loss: 0.010552288860910468\n",
      "[LOG 20200502-14:45:34] epoch: 14024 train-loss: 0.010552280789448155\n",
      "[LOG 20200502-14:45:34] epoch: 14025 train-loss: 0.010552272200584412\n",
      "[LOG 20200502-14:45:34] epoch: 14026 train-loss: 0.010552264543043243\n",
      "[LOG 20200502-14:45:34] epoch: 14027 train-loss: 0.010552257402903505\n",
      "[LOG 20200502-14:45:34] epoch: 14028 train-loss: 0.010552250780165195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:45:35] epoch: 14029 train-loss: 0.010552243640025457\n",
      "[LOG 20200502-14:45:35] epoch: 14030 train-loss: 0.010552237431208292\n",
      "[LOG 20200502-14:45:35] epoch: 14031 train-loss: 0.010552232153713703\n",
      "[LOG 20200502-14:45:35] epoch: 14032 train-loss: 0.010552226565778255\n",
      "[LOG 20200502-14:45:35] epoch: 14033 train-loss: 0.010552221391763952\n",
      "[LOG 20200502-14:45:36] epoch: 14034 train-loss: 0.010552216631670793\n",
      "[LOG 20200502-14:45:36] epoch: 14035 train-loss: 0.010552212285498777\n",
      "[LOG 20200502-14:45:36] epoch: 14036 train-loss: 0.010552208353247907\n",
      "[LOG 20200502-14:45:36] epoch: 14037 train-loss: 0.010552204524477323\n",
      "[LOG 20200502-14:45:37] epoch: 14038 train-loss: 0.01055220090266731\n",
      "[LOG 20200502-14:45:37] epoch: 14039 train-loss: 0.010552198212179873\n",
      "[LOG 20200502-14:45:37] epoch: 14040 train-loss: 0.010552195211251577\n",
      "[LOG 20200502-14:45:37] epoch: 14041 train-loss: 0.010552192417283853\n",
      "[LOG 20200502-14:45:37] epoch: 14042 train-loss: 0.010552190037237274\n",
      "[LOG 20200502-14:45:38] epoch: 14043 train-loss: 0.010552187553710408\n",
      "[LOG 20200502-14:45:38] epoch: 14044 train-loss: 0.010552185898025831\n",
      "[LOG 20200502-14:45:38] epoch: 14045 train-loss: 0.010552183414498964\n",
      "[LOG 20200502-14:45:38] epoch: 14046 train-loss: 0.01055218144837353\n",
      "[LOG 20200502-14:45:39] epoch: 14047 train-loss: 0.010552179792688953\n",
      "[LOG 20200502-14:45:39] epoch: 14048 train-loss: 0.010552178447445234\n",
      "[LOG 20200502-14:45:39] epoch: 14049 train-loss: 0.010552175756957795\n",
      "[LOG 20200502-14:45:39] epoch: 14050 train-loss: 0.010552173894312646\n",
      "[LOG 20200502-14:45:39] epoch: 14051 train-loss: 0.010552172238628069\n",
      "[LOG 20200502-14:45:40] epoch: 14052 train-loss: 0.010552170686423779\n",
      "[LOG 20200502-14:45:40] epoch: 14053 train-loss: 0.010552168720298342\n",
      "[LOG 20200502-14:45:40] epoch: 14054 train-loss: 0.010552166650692621\n",
      "[LOG 20200502-14:45:40] epoch: 14055 train-loss: 0.010552164270646043\n",
      "[LOG 20200502-14:45:41] epoch: 14056 train-loss: 0.01055216147667832\n",
      "[LOG 20200502-14:45:41] epoch: 14057 train-loss: 0.010552159407072596\n",
      "[LOG 20200502-14:45:41] epoch: 14058 train-loss: 0.01055215671658516\n",
      "[LOG 20200502-14:45:41] epoch: 14059 train-loss: 0.010552153198255433\n",
      "[LOG 20200502-14:45:41] epoch: 14060 train-loss: 0.01055215009384685\n",
      "[LOG 20200502-14:45:42] epoch: 14061 train-loss: 0.01055214667899741\n",
      "[LOG 20200502-14:45:42] epoch: 14062 train-loss: 0.010552142539785968\n",
      "[LOG 20200502-14:45:42] epoch: 14063 train-loss: 0.010552138917975955\n",
      "[LOG 20200502-14:45:42] epoch: 14064 train-loss: 0.010552134778764512\n",
      "[LOG 20200502-14:45:43] epoch: 14065 train-loss: 0.010552129604750209\n",
      "[LOG 20200502-14:45:43] epoch: 14066 train-loss: 0.010552124637696478\n",
      "[LOG 20200502-14:45:43] epoch: 14067 train-loss: 0.010552119256721603\n",
      "[LOG 20200502-14:45:43] epoch: 14068 train-loss: 0.010552113254865011\n",
      "[LOG 20200502-14:45:43] epoch: 14069 train-loss: 0.010552107563449277\n",
      "[LOG 20200502-14:45:44] epoch: 14070 train-loss: 0.01055210094071097\n",
      "[LOG 20200502-14:45:44] epoch: 14071 train-loss: 0.010552094214492373\n",
      "[LOG 20200502-14:45:44] epoch: 14072 train-loss: 0.010552087488273779\n",
      "[LOG 20200502-14:45:44] epoch: 14073 train-loss: 0.010552080037693182\n",
      "[LOG 20200502-14:45:45] epoch: 14074 train-loss: 0.010552072380152013\n",
      "[LOG 20200502-14:45:45] epoch: 14075 train-loss: 0.010552064722610844\n",
      "[LOG 20200502-14:45:45] epoch: 14076 train-loss: 0.010552056651148532\n",
      "[LOG 20200502-14:45:45] epoch: 14077 train-loss: 0.010552048165765073\n",
      "[LOG 20200502-14:45:45] epoch: 14078 train-loss: 0.01055203957690133\n",
      "[LOG 20200502-14:45:46] epoch: 14079 train-loss: 0.010552030781077014\n",
      "[LOG 20200502-14:45:46] epoch: 14080 train-loss: 0.010552022295693556\n",
      "[LOG 20200502-14:45:46] epoch: 14081 train-loss: 0.010552012878987525\n",
      "[LOG 20200502-14:45:46] epoch: 14082 train-loss: 0.010552004600564638\n",
      "[LOG 20200502-14:45:47] epoch: 14083 train-loss: 0.010551994976898035\n",
      "[LOG 20200502-14:45:47] epoch: 14084 train-loss: 0.010551985560192002\n",
      "[LOG 20200502-14:45:47] epoch: 14085 train-loss: 0.010551976867847972\n",
      "[LOG 20200502-14:45:47] epoch: 14086 train-loss: 0.010551967244181368\n",
      "[LOG 20200502-14:45:47] epoch: 14087 train-loss: 0.010551957930955622\n",
      "[LOG 20200502-14:45:48] epoch: 14088 train-loss: 0.01055194903165102\n",
      "[LOG 20200502-14:45:48] epoch: 14089 train-loss: 0.010551940132346418\n",
      "[LOG 20200502-14:45:48] epoch: 14090 train-loss: 0.010551931026081244\n",
      "[LOG 20200502-14:45:48] epoch: 14091 train-loss: 0.010551922126776643\n",
      "[LOG 20200502-14:45:49] epoch: 14092 train-loss: 0.01055191322747204\n",
      "[LOG 20200502-14:45:49] epoch: 14093 train-loss: 0.01055190484556887\n",
      "[LOG 20200502-14:45:49] epoch: 14094 train-loss: 0.010551895739303695\n",
      "[LOG 20200502-14:45:49] epoch: 14095 train-loss: 0.010551887771321667\n",
      "[LOG 20200502-14:45:49] epoch: 14096 train-loss: 0.010551879078977637\n",
      "[LOG 20200502-14:45:50] epoch: 14097 train-loss: 0.01055187090403504\n",
      "[LOG 20200502-14:45:50] epoch: 14098 train-loss: 0.010551863349974155\n",
      "[LOG 20200502-14:45:50] epoch: 14099 train-loss: 0.0105518555889527\n",
      "[LOG 20200502-14:45:50] epoch: 14100 train-loss: 0.01055184793141153\n",
      "[LOG 20200502-14:45:51] epoch: 14101 train-loss: 0.01055184058431122\n",
      "[LOG 20200502-14:45:51] epoch: 14102 train-loss: 0.010551833340691196\n",
      "[LOG 20200502-14:45:51] epoch: 14103 train-loss: 0.010551826510992315\n",
      "[LOG 20200502-14:45:51] epoch: 14104 train-loss: 0.010551819163892005\n",
      "[LOG 20200502-14:45:51] epoch: 14105 train-loss: 0.01055181295507484\n",
      "[LOG 20200502-14:45:52] epoch: 14106 train-loss: 0.010551805814935101\n",
      "[LOG 20200502-14:45:52] epoch: 14107 train-loss: 0.010551799088716507\n",
      "[LOG 20200502-14:45:52] epoch: 14108 train-loss: 0.010551793086859915\n",
      "[LOG 20200502-14:45:52] epoch: 14109 train-loss: 0.010551787085003324\n",
      "[LOG 20200502-14:45:53] epoch: 14110 train-loss: 0.010551780772705873\n",
      "[LOG 20200502-14:45:53] epoch: 14111 train-loss: 0.010551774874329567\n",
      "[LOG 20200502-14:45:53] epoch: 14112 train-loss: 0.01055176897595326\n",
      "[LOG 20200502-14:45:53] epoch: 14113 train-loss: 0.01055176266365581\n",
      "[LOG 20200502-14:45:53] epoch: 14114 train-loss: 0.010551756661799219\n",
      "[LOG 20200502-14:45:54] epoch: 14115 train-loss: 0.01055175138430463\n",
      "[LOG 20200502-14:45:54] epoch: 14116 train-loss: 0.010551745692888895\n",
      "[LOG 20200502-14:45:54] epoch: 14117 train-loss: 0.010551739897992875\n",
      "[LOG 20200502-14:45:54] epoch: 14118 train-loss: 0.010551733792655997\n",
      "[LOG 20200502-14:45:55] epoch: 14119 train-loss: 0.010551728101240264\n",
      "[LOG 20200502-14:45:55] epoch: 14120 train-loss: 0.01055172240982453\n",
      "[LOG 20200502-14:45:55] epoch: 14121 train-loss: 0.010551716821889082\n",
      "[LOG 20200502-14:45:55] epoch: 14122 train-loss: 0.010551710613071918\n",
      "[LOG 20200502-14:45:55] epoch: 14123 train-loss: 0.01055170502513647\n",
      "[LOG 20200502-14:45:56] epoch: 14124 train-loss: 0.01055169902327988\n",
      "[LOG 20200502-14:45:56] epoch: 14125 train-loss: 0.010551693124903573\n",
      "[LOG 20200502-14:45:56] epoch: 14126 train-loss: 0.010551686709125837\n",
      "[LOG 20200502-14:45:56] epoch: 14127 train-loss: 0.010551681017710103\n",
      "[LOG 20200502-14:45:56] epoch: 14128 train-loss: 0.010551674084530937\n",
      "[LOG 20200502-14:45:57] epoch: 14129 train-loss: 0.010551667875713773\n",
      "[LOG 20200502-14:45:57] epoch: 14130 train-loss: 0.010551661046014892\n",
      "[LOG 20200502-14:45:57] epoch: 14131 train-loss: 0.010551654009355439\n",
      "[LOG 20200502-14:45:57] epoch: 14132 train-loss: 0.010551647490097417\n",
      "[LOG 20200502-14:45:58] epoch: 14133 train-loss: 0.01055164003951682\n",
      "[LOG 20200502-14:45:58] epoch: 14134 train-loss: 0.010551633313298225\n",
      "[LOG 20200502-14:45:58] epoch: 14135 train-loss: 0.010551625655757057\n",
      "[LOG 20200502-14:45:58] epoch: 14136 train-loss: 0.01055161820517646\n",
      "[LOG 20200502-14:45:58] epoch: 14137 train-loss: 0.010551610444155004\n",
      "[LOG 20200502-14:45:59] epoch: 14138 train-loss: 0.010551602786613835\n",
      "[LOG 20200502-14:45:59] epoch: 14139 train-loss: 0.010551593783828948\n",
      "[LOG 20200502-14:45:59] epoch: 14140 train-loss: 0.010551586022807492\n",
      "[LOG 20200502-14:45:59] epoch: 14141 train-loss: 0.01055157712350289\n",
      "[LOG 20200502-14:45:59] epoch: 14142 train-loss: 0.010551568845080005\n",
      "[LOG 20200502-14:46:00] epoch: 14143 train-loss: 0.0105515592214134\n",
      "[LOG 20200502-14:46:00] epoch: 14144 train-loss: 0.010551550632549657\n",
      "[LOG 20200502-14:46:00] epoch: 14145 train-loss: 0.010551541422804197\n",
      "[LOG 20200502-14:46:00] epoch: 14146 train-loss: 0.010551532006098164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:46:01] epoch: 14147 train-loss: 0.01055152238243156\n",
      "[LOG 20200502-14:46:01] epoch: 14148 train-loss: 0.010551512551804384\n",
      "[LOG 20200502-14:46:01] epoch: 14149 train-loss: 0.010551502307256063\n",
      "[LOG 20200502-14:46:01] epoch: 14150 train-loss: 0.010551492476628887\n",
      "[LOG 20200502-14:46:01] epoch: 14151 train-loss: 0.010551482439041138\n",
      "[LOG 20200502-14:46:02] epoch: 14152 train-loss: 0.010551471987532245\n",
      "[LOG 20200502-14:46:02] epoch: 14153 train-loss: 0.010551461432543065\n",
      "[LOG 20200502-14:46:02] epoch: 14154 train-loss: 0.010551450463632742\n",
      "[LOG 20200502-14:46:02] epoch: 14155 train-loss: 0.010551440115604136\n",
      "[LOG 20200502-14:46:03] epoch: 14156 train-loss: 0.01055142893973324\n",
      "[LOG 20200502-14:46:03] epoch: 14157 train-loss: 0.010551419005625777\n",
      "[LOG 20200502-14:46:03] epoch: 14158 train-loss: 0.010551408036715455\n",
      "[LOG 20200502-14:46:03] epoch: 14159 train-loss: 0.010551397274765704\n",
      "[LOG 20200502-14:46:03] epoch: 14160 train-loss: 0.01055138661629624\n",
      "[LOG 20200502-14:46:04] epoch: 14161 train-loss: 0.010551375440425344\n",
      "[LOG 20200502-14:46:04] epoch: 14162 train-loss: 0.010551365195877023\n",
      "[LOG 20200502-14:46:04] epoch: 14163 train-loss: 0.010551354330446985\n",
      "[LOG 20200502-14:46:04] epoch: 14164 train-loss: 0.010551343878938092\n",
      "[LOG 20200502-14:46:05] epoch: 14165 train-loss: 0.010551333013508055\n",
      "[LOG 20200502-14:46:05] epoch: 14166 train-loss: 0.01055132287244002\n",
      "[LOG 20200502-14:46:05] epoch: 14167 train-loss: 0.010551312420931127\n",
      "[LOG 20200502-14:46:05] epoch: 14168 train-loss: 0.010551301969422234\n",
      "[LOG 20200502-14:46:06] epoch: 14169 train-loss: 0.010551292759676775\n",
      "[LOG 20200502-14:46:06] epoch: 14170 train-loss: 0.01055128261860874\n",
      "[LOG 20200502-14:46:06] epoch: 14171 train-loss: 0.01055127340886328\n",
      "[LOG 20200502-14:46:06] epoch: 14172 train-loss: 0.010551264095637534\n",
      "[LOG 20200502-14:46:07] epoch: 14173 train-loss: 0.010551255299813218\n",
      "[LOG 20200502-14:46:07] epoch: 14174 train-loss: 0.010551246607469188\n",
      "[LOG 20200502-14:46:07] epoch: 14175 train-loss: 0.010551238225566016\n",
      "[LOG 20200502-14:46:07] epoch: 14176 train-loss: 0.010551230154103704\n",
      "[LOG 20200502-14:46:07] epoch: 14177 train-loss: 0.010551222186121676\n",
      "[LOG 20200502-14:46:08] epoch: 14178 train-loss: 0.01055121473554108\n",
      "[LOG 20200502-14:46:08] epoch: 14179 train-loss: 0.01055120759540134\n",
      "[LOG 20200502-14:46:08] epoch: 14180 train-loss: 0.010551200455261601\n",
      "[LOG 20200502-14:46:08] epoch: 14181 train-loss: 0.010551194039483866\n",
      "[LOG 20200502-14:46:09] epoch: 14182 train-loss: 0.010551188037627272\n",
      "[LOG 20200502-14:46:09] epoch: 14183 train-loss: 0.01055118234621154\n",
      "[LOG 20200502-14:46:09] epoch: 14184 train-loss: 0.010551176654795805\n",
      "[LOG 20200502-14:46:09] epoch: 14185 train-loss: 0.010551171066860357\n",
      "[LOG 20200502-14:46:09] epoch: 14186 train-loss: 0.010551166513727771\n",
      "[LOG 20200502-14:46:10] epoch: 14187 train-loss: 0.010551161339713467\n",
      "[LOG 20200502-14:46:10] epoch: 14188 train-loss: 0.010551157407462597\n",
      "[LOG 20200502-14:46:10] epoch: 14189 train-loss: 0.010551153578692012\n",
      "[LOG 20200502-14:46:10] epoch: 14190 train-loss: 0.010551149646441141\n",
      "[LOG 20200502-14:46:11] epoch: 14191 train-loss: 0.010551146335071988\n",
      "[LOG 20200502-14:46:11] epoch: 14192 train-loss: 0.01055114260978169\n",
      "[LOG 20200502-14:46:11] epoch: 14193 train-loss: 0.010551140333215395\n",
      "[LOG 20200502-14:46:11] epoch: 14194 train-loss: 0.010551137125326527\n",
      "[LOG 20200502-14:46:11] epoch: 14195 train-loss: 0.010551134848760234\n",
      "[LOG 20200502-14:46:12] epoch: 14196 train-loss: 0.01055113205479251\n",
      "[LOG 20200502-14:46:12] epoch: 14197 train-loss: 0.010551130192147361\n",
      "[LOG 20200502-14:46:12] epoch: 14198 train-loss: 0.010551128432982497\n",
      "[LOG 20200502-14:46:12] epoch: 14199 train-loss: 0.010551126466857063\n",
      "[LOG 20200502-14:46:12] epoch: 14200 train-loss: 0.010551124811172485\n",
      "[LOG 20200502-14:46:13] epoch: 14201 train-loss: 0.010551122741566764\n",
      "[LOG 20200502-14:46:13] epoch: 14202 train-loss: 0.010551121603283618\n",
      "[LOG 20200502-14:46:13] epoch: 14203 train-loss: 0.010551119637158182\n",
      "[LOG 20200502-14:46:13] epoch: 14204 train-loss: 0.010551118291914463\n",
      "[LOG 20200502-14:46:14] epoch: 14205 train-loss: 0.010551116222308742\n",
      "[LOG 20200502-14:46:14] epoch: 14206 train-loss: 0.01055111518750588\n",
      "[LOG 20200502-14:46:14] epoch: 14207 train-loss: 0.010551113531821303\n",
      "[LOG 20200502-14:46:14] epoch: 14208 train-loss: 0.010551111876136728\n",
      "[LOG 20200502-14:46:14] epoch: 14209 train-loss: 0.010551110116971863\n",
      "[LOG 20200502-14:46:15] epoch: 14210 train-loss: 0.010551108254326714\n",
      "[LOG 20200502-14:46:15] epoch: 14211 train-loss: 0.010551105977760421\n",
      "[LOG 20200502-14:46:15] epoch: 14212 train-loss: 0.0105511039081547\n",
      "[LOG 20200502-14:46:15] epoch: 14213 train-loss: 0.010551101735068692\n",
      "[LOG 20200502-14:46:16] epoch: 14214 train-loss: 0.010551099044581255\n",
      "[LOG 20200502-14:46:16] epoch: 14215 train-loss: 0.010551096457574103\n",
      "[LOG 20200502-14:46:16] epoch: 14216 train-loss: 0.010551093249685235\n",
      "[LOG 20200502-14:46:16] epoch: 14217 train-loss: 0.010551090352237225\n",
      "[LOG 20200502-14:46:16] epoch: 14218 train-loss: 0.010551086626946926\n",
      "[LOG 20200502-14:46:17] epoch: 14219 train-loss: 0.010551083005136914\n",
      "[LOG 20200502-14:46:17] epoch: 14220 train-loss: 0.010551078969405757\n",
      "[LOG 20200502-14:46:17] epoch: 14221 train-loss: 0.010551074726714028\n",
      "[LOG 20200502-14:46:17] epoch: 14222 train-loss: 0.010551070070101155\n",
      "[LOG 20200502-14:46:18] epoch: 14223 train-loss: 0.010551066034369998\n",
      "[LOG 20200502-14:46:18] epoch: 14224 train-loss: 0.010551060756875409\n",
      "[LOG 20200502-14:46:18] epoch: 14225 train-loss: 0.010551055065459676\n",
      "[LOG 20200502-14:46:18] epoch: 14226 train-loss: 0.010551050305366516\n",
      "[LOG 20200502-14:46:18] epoch: 14227 train-loss: 0.010551044200029638\n",
      "[LOG 20200502-14:46:19] epoch: 14228 train-loss: 0.010551038405133618\n",
      "[LOG 20200502-14:46:19] epoch: 14229 train-loss: 0.010551032610237598\n",
      "[LOG 20200502-14:46:19] epoch: 14230 train-loss: 0.01055102598749929\n",
      "[LOG 20200502-14:46:19] epoch: 14231 train-loss: 0.010551019468241267\n",
      "[LOG 20200502-14:46:20] epoch: 14232 train-loss: 0.01055101284550296\n",
      "[LOG 20200502-14:46:20] epoch: 14233 train-loss: 0.010551006429725222\n",
      "[LOG 20200502-14:46:20] epoch: 14234 train-loss: 0.010550998979144625\n",
      "[LOG 20200502-14:46:20] epoch: 14235 train-loss: 0.010550991735524602\n",
      "[LOG 20200502-14:46:20] epoch: 14236 train-loss: 0.010550984905825721\n",
      "[LOG 20200502-14:46:21] epoch: 14237 train-loss: 0.010550977351764837\n",
      "[LOG 20200502-14:46:21] epoch: 14238 train-loss: 0.010550970625546243\n",
      "[LOG 20200502-14:46:21] epoch: 14239 train-loss: 0.010550963071485361\n",
      "[LOG 20200502-14:46:21] epoch: 14240 train-loss: 0.010550956138306193\n",
      "[LOG 20200502-14:46:21] epoch: 14241 train-loss: 0.01055094837728474\n",
      "[LOG 20200502-14:46:22] epoch: 14242 train-loss: 0.01055094175454643\n",
      "[LOG 20200502-14:46:22] epoch: 14243 train-loss: 0.010550934303965833\n",
      "[LOG 20200502-14:46:22] epoch: 14244 train-loss: 0.010550926956865523\n",
      "[LOG 20200502-14:46:22] epoch: 14245 train-loss: 0.0105509204376075\n",
      "[LOG 20200502-14:46:23] epoch: 14246 train-loss: 0.010550913504428334\n",
      "[LOG 20200502-14:46:23] epoch: 14247 train-loss: 0.010550906364288595\n",
      "[LOG 20200502-14:46:23] epoch: 14248 train-loss: 0.010550899534589715\n",
      "[LOG 20200502-14:46:23] epoch: 14249 train-loss: 0.010550892704890834\n",
      "[LOG 20200502-14:46:23] epoch: 14250 train-loss: 0.010550886392593384\n",
      "[LOG 20200502-14:46:24] epoch: 14251 train-loss: 0.010550880080295933\n",
      "[LOG 20200502-14:46:24] epoch: 14252 train-loss: 0.01055087335407734\n",
      "[LOG 20200502-14:46:24] epoch: 14253 train-loss: 0.010550866834819317\n",
      "[LOG 20200502-14:46:24] epoch: 14254 train-loss: 0.010550861350364156\n",
      "[LOG 20200502-14:46:24] epoch: 14255 train-loss: 0.010550855348507563\n",
      "[LOG 20200502-14:46:25] epoch: 14256 train-loss: 0.010550849967532687\n",
      "[LOG 20200502-14:46:25] epoch: 14257 train-loss: 0.010550843655235238\n",
      "[LOG 20200502-14:46:25] epoch: 14258 train-loss: 0.010550838688181506\n",
      "[LOG 20200502-14:46:25] epoch: 14259 train-loss: 0.010550832996765772\n",
      "[LOG 20200502-14:46:26] epoch: 14260 train-loss: 0.010550828133192327\n",
      "[LOG 20200502-14:46:26] epoch: 14261 train-loss: 0.010550823269618882\n",
      "[LOG 20200502-14:46:26] epoch: 14262 train-loss: 0.010550817785163721\n",
      "[LOG 20200502-14:46:26] epoch: 14263 train-loss: 0.010550812611149417\n",
      "[LOG 20200502-14:46:26] epoch: 14264 train-loss: 0.010550807851056257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:46:27] epoch: 14265 train-loss: 0.010550803090963099\n",
      "[LOG 20200502-14:46:27] epoch: 14266 train-loss: 0.010550798123909367\n",
      "[LOG 20200502-14:46:27] epoch: 14267 train-loss: 0.010550793156855635\n",
      "[LOG 20200502-14:46:27] epoch: 14268 train-loss: 0.010550788500242762\n",
      "[LOG 20200502-14:46:28] epoch: 14269 train-loss: 0.010550783636669317\n",
      "[LOG 20200502-14:46:28] epoch: 14270 train-loss: 0.010550779704418447\n",
      "[LOG 20200502-14:46:28] epoch: 14271 train-loss: 0.010550774944325289\n",
      "[LOG 20200502-14:46:28] epoch: 14272 train-loss: 0.01055076987379127\n",
      "[LOG 20200502-14:46:28] epoch: 14273 train-loss: 0.010550765838060115\n",
      "[LOG 20200502-14:46:29] epoch: 14274 train-loss: 0.010550761077966955\n",
      "[LOG 20200502-14:46:29] epoch: 14275 train-loss: 0.010550756317873796\n",
      "[LOG 20200502-14:46:29] epoch: 14276 train-loss: 0.01055075093689892\n",
      "[LOG 20200502-14:46:29] epoch: 14277 train-loss: 0.010550745969845189\n",
      "[LOG 20200502-14:46:30] epoch: 14278 train-loss: 0.010550741106271744\n",
      "[LOG 20200502-14:46:30] epoch: 14279 train-loss: 0.010550736553139158\n",
      "[LOG 20200502-14:46:30] epoch: 14280 train-loss: 0.010550731275644567\n",
      "[LOG 20200502-14:46:30] epoch: 14281 train-loss: 0.01055072620511055\n",
      "[LOG 20200502-14:46:30] epoch: 14282 train-loss: 0.010550720513694815\n",
      "[LOG 20200502-14:46:31] epoch: 14283 train-loss: 0.010550714822279083\n",
      "[LOG 20200502-14:46:31] epoch: 14284 train-loss: 0.010550709958705638\n",
      "[LOG 20200502-14:46:31] epoch: 14285 train-loss: 0.010550703956849046\n",
      "[LOG 20200502-14:46:31] epoch: 14286 train-loss: 0.010550698265433311\n",
      "[LOG 20200502-14:46:32] epoch: 14287 train-loss: 0.010550692574017577\n",
      "[LOG 20200502-14:46:32] epoch: 14288 train-loss: 0.010550686261720128\n",
      "[LOG 20200502-14:46:32] epoch: 14289 train-loss: 0.010550680052902963\n",
      "[LOG 20200502-14:46:32] epoch: 14290 train-loss: 0.010550673326684369\n",
      "[LOG 20200502-14:46:32] epoch: 14291 train-loss: 0.010550667014386918\n",
      "[LOG 20200502-14:46:33] epoch: 14292 train-loss: 0.010550659460326036\n",
      "[LOG 20200502-14:46:33] epoch: 14293 train-loss: 0.010550652734107442\n",
      "[LOG 20200502-14:46:33] epoch: 14294 train-loss: 0.010550645697447989\n",
      "[LOG 20200502-14:46:33] epoch: 14295 train-loss: 0.01055063855730825\n",
      "[LOG 20200502-14:46:34] epoch: 14296 train-loss: 0.01055063089976708\n",
      "[LOG 20200502-14:46:34] epoch: 14297 train-loss: 0.01055062355266677\n",
      "[LOG 20200502-14:46:34] epoch: 14298 train-loss: 0.010550615481204458\n",
      "[LOG 20200502-14:46:34] epoch: 14299 train-loss: 0.010550608237584433\n",
      "[LOG 20200502-14:46:35] epoch: 14300 train-loss: 0.010550599752200974\n",
      "[LOG 20200502-14:46:35] epoch: 14301 train-loss: 0.010550592301620377\n",
      "[LOG 20200502-14:46:35] epoch: 14302 train-loss: 0.010550583712756634\n",
      "[LOG 20200502-14:46:35] epoch: 14303 train-loss: 0.010550575434333749\n",
      "[LOG 20200502-14:46:35] epoch: 14304 train-loss: 0.010550566845470004\n",
      "[LOG 20200502-14:46:36] epoch: 14305 train-loss: 0.010550557842685116\n",
      "[LOG 20200502-14:46:36] epoch: 14306 train-loss: 0.010550549874703089\n",
      "[LOG 20200502-14:46:36] epoch: 14307 train-loss: 0.010550540871918201\n",
      "[LOG 20200502-14:46:36] epoch: 14308 train-loss: 0.010550532386534743\n",
      "[LOG 20200502-14:46:37] epoch: 14309 train-loss: 0.010550523383749856\n",
      "[LOG 20200502-14:46:37] epoch: 14310 train-loss: 0.010550514484445253\n",
      "[LOG 20200502-14:46:37] epoch: 14311 train-loss: 0.010550505481660366\n",
      "[LOG 20200502-14:46:37] epoch: 14312 train-loss: 0.010550497099757195\n",
      "[LOG 20200502-14:46:38] epoch: 14313 train-loss: 0.010550488200452592\n",
      "[LOG 20200502-14:46:38] epoch: 14314 train-loss: 0.010550479404628277\n",
      "[LOG 20200502-14:46:38] epoch: 14315 train-loss: 0.010550470815764533\n",
      "[LOG 20200502-14:46:38] epoch: 14316 train-loss: 0.010550462433861362\n",
      "[LOG 20200502-14:46:38] epoch: 14317 train-loss: 0.01055045353455676\n",
      "[LOG 20200502-14:46:39] epoch: 14318 train-loss: 0.01055044535961416\n",
      "[LOG 20200502-14:46:39] epoch: 14319 train-loss: 0.010550437598592706\n",
      "[LOG 20200502-14:46:39] epoch: 14320 train-loss: 0.010550429113209248\n",
      "[LOG 20200502-14:46:39] epoch: 14321 train-loss: 0.010550421869589223\n",
      "[LOG 20200502-14:46:39] epoch: 14322 train-loss: 0.01055041379812691\n",
      "[LOG 20200502-14:46:40] epoch: 14323 train-loss: 0.010550406451026598\n",
      "[LOG 20200502-14:46:40] epoch: 14324 train-loss: 0.010550399103926288\n",
      "[LOG 20200502-14:46:40] epoch: 14325 train-loss: 0.01055039196378655\n",
      "[LOG 20200502-14:46:40] epoch: 14326 train-loss: 0.010550385961929956\n",
      "[LOG 20200502-14:46:41] epoch: 14327 train-loss: 0.010550379132231077\n",
      "[LOG 20200502-14:46:41] epoch: 14328 train-loss: 0.010550373130374484\n",
      "[LOG 20200502-14:46:41] epoch: 14329 train-loss: 0.010550367438958751\n",
      "[LOG 20200502-14:46:41] epoch: 14330 train-loss: 0.010550361644062731\n",
      "[LOG 20200502-14:46:41] epoch: 14331 train-loss: 0.01055035636656814\n",
      "[LOG 20200502-14:46:42] epoch: 14332 train-loss: 0.010550351296034124\n",
      "[LOG 20200502-14:46:42] epoch: 14333 train-loss: 0.010550346742901538\n",
      "[LOG 20200502-14:46:42] epoch: 14334 train-loss: 0.010550342293249236\n",
      "[LOG 20200502-14:46:42] epoch: 14335 train-loss: 0.010550337843596935\n",
      "[LOG 20200502-14:46:43] epoch: 14336 train-loss: 0.01055033453222778\n",
      "[LOG 20200502-14:46:43] epoch: 14337 train-loss: 0.010550330910417769\n",
      "[LOG 20200502-14:46:43] epoch: 14338 train-loss: 0.010550327599048615\n",
      "[LOG 20200502-14:46:43] epoch: 14339 train-loss: 0.010550324184199175\n",
      "[LOG 20200502-14:46:43] epoch: 14340 train-loss: 0.010550322114593454\n",
      "[LOG 20200502-14:46:44] epoch: 14341 train-loss: 0.010550319424106015\n",
      "[LOG 20200502-14:46:44] epoch: 14342 train-loss: 0.010550317561460866\n",
      "[LOG 20200502-14:46:44] epoch: 14343 train-loss: 0.010550315802296003\n",
      "[LOG 20200502-14:46:44] epoch: 14344 train-loss: 0.01055031404313114\n",
      "[LOG 20200502-14:46:45] epoch: 14345 train-loss: 0.01055031218048599\n",
      "[LOG 20200502-14:46:45] epoch: 14346 train-loss: 0.01055031114568313\n",
      "[LOG 20200502-14:46:45] epoch: 14347 train-loss: 0.01055031031784084\n",
      "[LOG 20200502-14:46:45] epoch: 14348 train-loss: 0.010550309386518266\n",
      "[LOG 20200502-14:46:45] epoch: 14349 train-loss: 0.010550308041274548\n",
      "[LOG 20200502-14:46:46] epoch: 14350 train-loss: 0.01055030793779426\n",
      "[LOG 20200502-14:46:46] epoch: 14351 train-loss: 0.010550307006471686\n",
      "[LOG 20200502-14:46:46] epoch: 14352 train-loss: 0.010550306696030829\n",
      "[LOG 20200502-14:46:46] epoch: 14353 train-loss: 0.010550305971668826\n",
      "[LOG 20200502-14:46:47] epoch: 14354 train-loss: 0.01055030535078711\n",
      "[LOG 20200502-14:46:47] epoch: 14355 train-loss: 0.01055030535078711\n",
      "[LOG 20200502-14:46:47] epoch: 14356 train-loss: 0.010550304936865965\n",
      "[LOG 20200502-14:46:47] epoch: 14357 train-loss: 0.010550304315984249\n",
      "[LOG 20200502-14:46:48] epoch: 14358 train-loss: 0.010550303798582818\n",
      "[LOG 20200502-14:46:48] epoch: 14359 train-loss: 0.010550303488141961\n",
      "[LOG 20200502-14:46:48] epoch: 14360 train-loss: 0.010550302763779959\n",
      "[LOG 20200502-14:46:48] epoch: 14361 train-loss: 0.010550302246378528\n",
      "[LOG 20200502-14:46:48] epoch: 14362 train-loss: 0.01055030090113481\n",
      "[LOG 20200502-14:46:49] epoch: 14363 train-loss: 0.010550300383733379\n",
      "[LOG 20200502-14:46:49] epoch: 14364 train-loss: 0.010550299245450232\n",
      "[LOG 20200502-14:46:49] epoch: 14365 train-loss: 0.010550298003686799\n",
      "[LOG 20200502-14:46:49] epoch: 14366 train-loss: 0.010550296244521936\n",
      "[LOG 20200502-14:46:50] epoch: 14367 train-loss: 0.010550294588837359\n",
      "[LOG 20200502-14:46:50] epoch: 14368 train-loss: 0.010550292519231638\n",
      "[LOG 20200502-14:46:50] epoch: 14369 train-loss: 0.010550290242665343\n",
      "[LOG 20200502-14:46:50] epoch: 14370 train-loss: 0.010550287552177906\n",
      "[LOG 20200502-14:46:50] epoch: 14371 train-loss: 0.010550284965170754\n",
      "[LOG 20200502-14:46:51] epoch: 14372 train-loss: 0.010550282067722745\n",
      "[LOG 20200502-14:46:51] epoch: 14373 train-loss: 0.010550278859833876\n",
      "[LOG 20200502-14:46:51] epoch: 14374 train-loss: 0.010550275548464723\n",
      "[LOG 20200502-14:46:51] epoch: 14375 train-loss: 0.010550272237095568\n",
      "[LOG 20200502-14:46:52] epoch: 14376 train-loss: 0.01055026851180527\n",
      "[LOG 20200502-14:46:52] epoch: 14377 train-loss: 0.010550264889995256\n",
      "[LOG 20200502-14:46:52] epoch: 14378 train-loss: 0.010550260129902098\n",
      "[LOG 20200502-14:46:52] epoch: 14379 train-loss: 0.010550255783730082\n",
      "[LOG 20200502-14:46:52] epoch: 14380 train-loss: 0.010550251851479212\n",
      "[LOG 20200502-14:46:53] epoch: 14381 train-loss: 0.010550246884425482\n",
      "[LOG 20200502-14:46:53] epoch: 14382 train-loss: 0.010550242020852037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:46:53] epoch: 14383 train-loss: 0.010550237260758877\n",
      "[LOG 20200502-14:46:54] epoch: 14384 train-loss: 0.010550231776303716\n",
      "[LOG 20200502-14:46:54] epoch: 14385 train-loss: 0.010550227326651415\n",
      "[LOG 20200502-14:46:54] epoch: 14386 train-loss: 0.010550221842196252\n",
      "[LOG 20200502-14:46:54] epoch: 14387 train-loss: 0.010550216461221376\n",
      "[LOG 20200502-14:46:54] epoch: 14388 train-loss: 0.01055021139068736\n",
      "[LOG 20200502-14:46:55] epoch: 14389 train-loss: 0.010550205802751912\n",
      "[LOG 20200502-14:46:55] epoch: 14390 train-loss: 0.010550200628737608\n",
      "[LOG 20200502-14:46:55] epoch: 14391 train-loss: 0.010550195144282447\n",
      "[LOG 20200502-14:46:55] epoch: 14392 train-loss: 0.010550189970268143\n",
      "[LOG 20200502-14:46:56] epoch: 14393 train-loss: 0.010550184485812983\n",
      "[LOG 20200502-14:46:56] epoch: 14394 train-loss: 0.010550179622239537\n",
      "[LOG 20200502-14:46:56] epoch: 14395 train-loss: 0.010550174241264662\n",
      "[LOG 20200502-14:46:56] epoch: 14396 train-loss: 0.01055016927421093\n",
      "[LOG 20200502-14:46:57] epoch: 14397 train-loss: 0.010550164100196626\n",
      "[LOG 20200502-14:46:57] epoch: 14398 train-loss: 0.010550159340103468\n",
      "[LOG 20200502-14:46:57] epoch: 14399 train-loss: 0.010550154580010308\n",
      "[LOG 20200502-14:46:57] epoch: 14400 train-loss: 0.010550149923397435\n",
      "[LOG 20200502-14:46:57] epoch: 14401 train-loss: 0.01055014557722542\n",
      "[LOG 20200502-14:46:58] epoch: 14402 train-loss: 0.010550140610171689\n",
      "[LOG 20200502-14:46:58] epoch: 14403 train-loss: 0.010550136677920818\n",
      "[LOG 20200502-14:46:58] epoch: 14404 train-loss: 0.010550131503906515\n",
      "[LOG 20200502-14:46:58] epoch: 14405 train-loss: 0.010550127882096503\n",
      "[LOG 20200502-14:46:59] epoch: 14406 train-loss: 0.010550124363766776\n",
      "[LOG 20200502-14:46:59] epoch: 14407 train-loss: 0.010550120431515906\n",
      "[LOG 20200502-14:46:59] epoch: 14408 train-loss: 0.01055011660274532\n",
      "[LOG 20200502-14:46:59] epoch: 14409 train-loss: 0.010550112980935309\n",
      "[LOG 20200502-14:47:00] epoch: 14410 train-loss: 0.010550109359125296\n",
      "[LOG 20200502-14:47:00] epoch: 14411 train-loss: 0.010550105737315284\n",
      "[LOG 20200502-14:47:00] epoch: 14412 train-loss: 0.010550102322465844\n",
      "[LOG 20200502-14:47:00] epoch: 14413 train-loss: 0.010550099114576975\n",
      "[LOG 20200502-14:47:01] epoch: 14414 train-loss: 0.010550095699727535\n",
      "[LOG 20200502-14:47:01] epoch: 14415 train-loss: 0.010550092181397809\n",
      "[LOG 20200502-14:47:01] epoch: 14416 train-loss: 0.010550089180469513\n",
      "[LOG 20200502-14:47:01] epoch: 14417 train-loss: 0.010550085662139786\n",
      "[LOG 20200502-14:47:02] epoch: 14418 train-loss: 0.010550082557731204\n",
      "[LOG 20200502-14:47:02] epoch: 14419 train-loss: 0.010550079453322623\n",
      "[LOG 20200502-14:47:02] epoch: 14420 train-loss: 0.010550076245433755\n",
      "[LOG 20200502-14:47:02] epoch: 14421 train-loss: 0.0105500729340646\n",
      "[LOG 20200502-14:47:03] epoch: 14422 train-loss: 0.010550069208774302\n",
      "[LOG 20200502-14:47:03] epoch: 14423 train-loss: 0.010550066000885434\n",
      "[LOG 20200502-14:47:03] epoch: 14424 train-loss: 0.010550063103437424\n",
      "[LOG 20200502-14:47:03] epoch: 14425 train-loss: 0.010550059274666838\n",
      "[LOG 20200502-14:47:03] epoch: 14426 train-loss: 0.010550055652856827\n",
      "[LOG 20200502-14:47:04] epoch: 14427 train-loss: 0.010550052238007387\n",
      "[LOG 20200502-14:47:04] epoch: 14428 train-loss: 0.010550048409236802\n",
      "[LOG 20200502-14:47:04] epoch: 14429 train-loss: 0.010550044890907075\n",
      "[LOG 20200502-14:47:04] epoch: 14430 train-loss: 0.010550040544735061\n",
      "[LOG 20200502-14:47:05] epoch: 14431 train-loss: 0.010550036922925048\n",
      "[LOG 20200502-14:47:05] epoch: 14432 train-loss: 0.010550032473272748\n",
      "[LOG 20200502-14:47:05] epoch: 14433 train-loss: 0.010550027506219016\n",
      "[LOG 20200502-14:47:05] epoch: 14434 train-loss: 0.01055002398788929\n",
      "[LOG 20200502-14:47:06] epoch: 14435 train-loss: 0.010550019020835558\n",
      "[LOG 20200502-14:47:06] epoch: 14436 train-loss: 0.010550014364222685\n",
      "[LOG 20200502-14:47:06] epoch: 14437 train-loss: 0.010550009397168955\n",
      "[LOG 20200502-14:47:06] epoch: 14438 train-loss: 0.010550004223154651\n",
      "[LOG 20200502-14:47:07] epoch: 14439 train-loss: 0.010549998842179775\n",
      "[LOG 20200502-14:47:07] epoch: 14440 train-loss: 0.010549994289047189\n",
      "[LOG 20200502-14:47:07] epoch: 14441 train-loss: 0.010549987769789167\n",
      "[LOG 20200502-14:47:07] epoch: 14442 train-loss: 0.010549982388814291\n",
      "[LOG 20200502-14:47:08] epoch: 14443 train-loss: 0.010549976490437984\n",
      "[LOG 20200502-14:47:08] epoch: 14444 train-loss: 0.010549970592061678\n",
      "[LOG 20200502-14:47:08] epoch: 14445 train-loss: 0.010549964279764228\n",
      "[LOG 20200502-14:47:08] epoch: 14446 train-loss: 0.010549957967466779\n",
      "[LOG 20200502-14:47:09] epoch: 14447 train-loss: 0.010549951241248183\n",
      "[LOG 20200502-14:47:09] epoch: 14448 train-loss: 0.010549945135911306\n",
      "[LOG 20200502-14:47:09] epoch: 14449 train-loss: 0.010549938306212425\n",
      "[LOG 20200502-14:47:09] epoch: 14450 train-loss: 0.010549932200875547\n",
      "[LOG 20200502-14:47:09] epoch: 14451 train-loss: 0.010549924853775237\n",
      "[LOG 20200502-14:47:10] epoch: 14452 train-loss: 0.0105499184379975\n",
      "[LOG 20200502-14:47:10] epoch: 14453 train-loss: 0.010549911194377475\n",
      "[LOG 20200502-14:47:10] epoch: 14454 train-loss: 0.010549904054237736\n",
      "[LOG 20200502-14:47:10] epoch: 14455 train-loss: 0.010549897328019142\n",
      "[LOG 20200502-14:47:11] epoch: 14456 train-loss: 0.010549890187879404\n",
      "[LOG 20200502-14:47:11] epoch: 14457 train-loss: 0.010549883358180523\n",
      "[LOG 20200502-14:47:11] epoch: 14458 train-loss: 0.010549876425001357\n",
      "[LOG 20200502-14:47:11] epoch: 14459 train-loss: 0.010549869077901045\n",
      "[LOG 20200502-14:47:12] epoch: 14460 train-loss: 0.010549861730800735\n",
      "[LOG 20200502-14:47:12] epoch: 14461 train-loss: 0.010549855211542713\n",
      "[LOG 20200502-14:47:12] epoch: 14462 train-loss: 0.010549849106205834\n",
      "[LOG 20200502-14:47:12] epoch: 14463 train-loss: 0.010549842793908384\n",
      "[LOG 20200502-14:47:13] epoch: 14464 train-loss: 0.010549835964209504\n",
      "[LOG 20200502-14:47:13] epoch: 14465 train-loss: 0.010549829651912054\n",
      "[LOG 20200502-14:47:13] epoch: 14466 train-loss: 0.010549823857016034\n",
      "[LOG 20200502-14:47:13] epoch: 14467 train-loss: 0.010549817855159441\n",
      "[LOG 20200502-14:47:14] epoch: 14468 train-loss: 0.010549812060263421\n",
      "[LOG 20200502-14:47:14] epoch: 14469 train-loss: 0.010549806679288546\n",
      "[LOG 20200502-14:47:14] epoch: 14470 train-loss: 0.010549801091353098\n",
      "[LOG 20200502-14:47:14] epoch: 14471 train-loss: 0.010549796434740225\n",
      "[LOG 20200502-14:47:14] epoch: 14472 train-loss: 0.010549792192048497\n",
      "[LOG 20200502-14:47:15] epoch: 14473 train-loss: 0.010549787431955338\n",
      "[LOG 20200502-14:47:15] epoch: 14474 train-loss: 0.010549783085783323\n",
      "[LOG 20200502-14:47:15] epoch: 14475 train-loss: 0.010549779567453597\n",
      "[LOG 20200502-14:47:15] epoch: 14476 train-loss: 0.010549775945643583\n",
      "[LOG 20200502-14:47:16] epoch: 14477 train-loss: 0.010549772530794144\n",
      "[LOG 20200502-14:47:16] epoch: 14478 train-loss: 0.01054976973682642\n",
      "[LOG 20200502-14:47:16] epoch: 14479 train-loss: 0.010549767460260127\n",
      "[LOG 20200502-14:47:17] epoch: 14480 train-loss: 0.010549764769772688\n",
      "[LOG 20200502-14:47:17] epoch: 14481 train-loss: 0.010549763217568398\n",
      "[LOG 20200502-14:47:17] epoch: 14482 train-loss: 0.010549761147962676\n",
      "[LOG 20200502-14:47:17] epoch: 14483 train-loss: 0.010549760113159815\n",
      "[LOG 20200502-14:47:18] epoch: 14484 train-loss: 0.010549759078356955\n",
      "[LOG 20200502-14:47:18] epoch: 14485 train-loss: 0.010549758043554094\n",
      "[LOG 20200502-14:47:18] epoch: 14486 train-loss: 0.01054975762963295\n",
      "[LOG 20200502-14:47:18] epoch: 14487 train-loss: 0.010549757215711806\n",
      "[LOG 20200502-14:47:19] epoch: 14488 train-loss: 0.010549757319192091\n",
      "[LOG 20200502-14:47:19] epoch: 14489 train-loss: 0.010549756905270947\n",
      "[LOG 20200502-14:47:19] epoch: 14490 train-loss: 0.010549757940073809\n",
      "[LOG 20200502-14:47:19] epoch: 14491 train-loss: 0.01054975814703438\n",
      "[LOG 20200502-14:47:20] epoch: 14492 train-loss: 0.010549758871396383\n",
      "[LOG 20200502-14:47:20] epoch: 14493 train-loss: 0.010549759699238671\n",
      "[LOG 20200502-14:47:20] epoch: 14494 train-loss: 0.010549760216640102\n",
      "[LOG 20200502-14:47:20] epoch: 14495 train-loss: 0.01054976104448239\n",
      "[LOG 20200502-14:47:21] epoch: 14496 train-loss: 0.010549762803647254\n",
      "[LOG 20200502-14:47:21] epoch: 14497 train-loss: 0.010549763631489541\n",
      "[LOG 20200502-14:47:21] epoch: 14498 train-loss: 0.01054976497673326\n",
      "[LOG 20200502-14:47:21] epoch: 14499 train-loss: 0.010549766011536121\n",
      "[LOG 20200502-14:47:22] epoch: 14500 train-loss: 0.010549767149819268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:47:22] epoch: 14501 train-loss: 0.010549768495062986\n",
      "[LOG 20200502-14:47:22] epoch: 14502 train-loss: 0.010549769633346133\n",
      "[LOG 20200502-14:47:22] epoch: 14503 train-loss: 0.010549770771629281\n",
      "[LOG 20200502-14:47:23] epoch: 14504 train-loss: 0.010549771806432141\n",
      "[LOG 20200502-14:47:23] epoch: 14505 train-loss: 0.010549772737754716\n",
      "[LOG 20200502-14:47:23] epoch: 14506 train-loss: 0.010549773462116718\n",
      "[LOG 20200502-14:47:23] epoch: 14507 train-loss: 0.010549774703880152\n",
      "[LOG 20200502-14:47:24] epoch: 14508 train-loss: 0.010549774289959006\n",
      "[LOG 20200502-14:47:24] epoch: 14509 train-loss: 0.01054977522128158\n",
      "[LOG 20200502-14:47:24] epoch: 14510 train-loss: 0.01054977522128158\n",
      "[LOG 20200502-14:47:24] epoch: 14511 train-loss: 0.010549775014321009\n",
      "[LOG 20200502-14:47:24] epoch: 14512 train-loss: 0.010549774703880152\n",
      "[LOG 20200502-14:47:25] epoch: 14513 train-loss: 0.010549773462116718\n",
      "[LOG 20200502-14:47:25] epoch: 14514 train-loss: 0.010549773565597005\n",
      "[LOG 20200502-14:47:25] epoch: 14515 train-loss: 0.010549772427313857\n",
      "[LOG 20200502-14:47:25] epoch: 14516 train-loss: 0.010549771392510997\n",
      "[LOG 20200502-14:47:26] epoch: 14517 train-loss: 0.010549769840306707\n",
      "[LOG 20200502-14:47:26] epoch: 14518 train-loss: 0.010549768495062986\n",
      "[LOG 20200502-14:47:26] epoch: 14519 train-loss: 0.01054976683937841\n",
      "[LOG 20200502-14:47:26] epoch: 14520 train-loss: 0.010549764459331831\n",
      "[LOG 20200502-14:47:27] epoch: 14521 train-loss: 0.010549761975804964\n",
      "[LOG 20200502-14:47:27] epoch: 14522 train-loss: 0.010549760216640102\n",
      "[LOG 20200502-14:47:27] epoch: 14523 train-loss: 0.010549757112231519\n",
      "[LOG 20200502-14:47:27] epoch: 14524 train-loss: 0.01054975473218494\n",
      "[LOG 20200502-14:47:28] epoch: 14525 train-loss: 0.01054975100689464\n",
      "[LOG 20200502-14:47:28] epoch: 14526 train-loss: 0.010549748626848062\n",
      "[LOG 20200502-14:47:28] epoch: 14527 train-loss: 0.010549745108518336\n",
      "[LOG 20200502-14:47:28] epoch: 14528 train-loss: 0.010549741900629468\n",
      "[LOG 20200502-14:47:29] epoch: 14529 train-loss: 0.010549738485780027\n",
      "[LOG 20200502-14:47:29] epoch: 14530 train-loss: 0.010549735588332018\n",
      "[LOG 20200502-14:47:29] epoch: 14531 train-loss: 0.010549731759561433\n",
      "[LOG 20200502-14:47:29] epoch: 14532 train-loss: 0.010549728034271134\n",
      "[LOG 20200502-14:47:29] epoch: 14533 train-loss: 0.010549724826382266\n",
      "[LOG 20200502-14:47:30] epoch: 14534 train-loss: 0.010549721928934256\n",
      "[LOG 20200502-14:47:30] epoch: 14535 train-loss: 0.010549718514084816\n",
      "[LOG 20200502-14:47:30] epoch: 14536 train-loss: 0.010549714581833946\n",
      "[LOG 20200502-14:47:30] epoch: 14537 train-loss: 0.010549711270464791\n",
      "[LOG 20200502-14:47:31] epoch: 14538 train-loss: 0.010549708062575923\n",
      "[LOG 20200502-14:47:31] epoch: 14539 train-loss: 0.010549704544246197\n",
      "[LOG 20200502-14:47:31] epoch: 14540 train-loss: 0.010549702060719332\n",
      "[LOG 20200502-14:47:31] epoch: 14541 train-loss: 0.010549698956310749\n",
      "[LOG 20200502-14:47:32] epoch: 14542 train-loss: 0.010549696369303597\n",
      "[LOG 20200502-14:47:32] epoch: 14543 train-loss: 0.010549693471855588\n",
      "[LOG 20200502-14:47:32] epoch: 14544 train-loss: 0.010549690677887864\n",
      "[LOG 20200502-14:47:32] epoch: 14545 train-loss: 0.010549687676959567\n",
      "[LOG 20200502-14:47:33] epoch: 14546 train-loss: 0.010549685607353846\n",
      "[LOG 20200502-14:47:33] epoch: 14547 train-loss: 0.010549683537748124\n",
      "[LOG 20200502-14:47:33] epoch: 14548 train-loss: 0.01054968105422126\n",
      "[LOG 20200502-14:47:33] epoch: 14549 train-loss: 0.010549679088095823\n",
      "[LOG 20200502-14:47:33] epoch: 14550 train-loss: 0.010549677328930961\n",
      "[LOG 20200502-14:47:34] epoch: 14551 train-loss: 0.01054967443148295\n",
      "[LOG 20200502-14:47:34] epoch: 14552 train-loss: 0.010549673189719519\n",
      "[LOG 20200502-14:47:34] epoch: 14553 train-loss: 0.010549671534034941\n",
      "[LOG 20200502-14:47:34] epoch: 14554 train-loss: 0.010549669050508075\n",
      "[LOG 20200502-14:47:35] epoch: 14555 train-loss: 0.010549667808744643\n",
      "[LOG 20200502-14:47:35] epoch: 14556 train-loss: 0.010549666049579779\n",
      "[LOG 20200502-14:47:35] epoch: 14557 train-loss: 0.010549664600855775\n",
      "[LOG 20200502-14:47:35] epoch: 14558 train-loss: 0.010549662738210626\n",
      "[LOG 20200502-14:47:36] epoch: 14559 train-loss: 0.010549661392966906\n",
      "[LOG 20200502-14:47:36] epoch: 14560 train-loss: 0.0105496592198809\n",
      "[LOG 20200502-14:47:36] epoch: 14561 train-loss: 0.010549658185078038\n",
      "[LOG 20200502-14:47:37] epoch: 14562 train-loss: 0.010549656218952604\n",
      "[LOG 20200502-14:47:37] epoch: 14563 train-loss: 0.010549654356307454\n",
      "[LOG 20200502-14:47:37] epoch: 14564 train-loss: 0.010549652493662305\n",
      "[LOG 20200502-14:47:38] epoch: 14565 train-loss: 0.010549651148418585\n",
      "[LOG 20200502-14:47:38] epoch: 14566 train-loss: 0.010549649285773436\n",
      "[LOG 20200502-14:47:38] epoch: 14567 train-loss: 0.010549647526608573\n",
      "[LOG 20200502-14:47:38] epoch: 14568 train-loss: 0.010549645353522565\n",
      "[LOG 20200502-14:47:39] epoch: 14569 train-loss: 0.010549643283916844\n",
      "[LOG 20200502-14:47:39] epoch: 14570 train-loss: 0.010549641007350551\n",
      "[LOG 20200502-14:47:39] epoch: 14571 train-loss: 0.010549638627303971\n",
      "[LOG 20200502-14:47:39] epoch: 14572 train-loss: 0.010549636143777106\n",
      "[LOG 20200502-14:47:40] epoch: 14573 train-loss: 0.010549633246329095\n",
      "[LOG 20200502-14:47:40] epoch: 14574 train-loss: 0.010549630555841658\n",
      "[LOG 20200502-14:47:40] epoch: 14575 train-loss: 0.010549627761873934\n",
      "[LOG 20200502-14:47:40] epoch: 14576 train-loss: 0.010549624657465352\n",
      "[LOG 20200502-14:47:41] epoch: 14577 train-loss: 0.010549621966977915\n",
      "[LOG 20200502-14:47:41] epoch: 14578 train-loss: 0.010549617931246758\n",
      "[LOG 20200502-14:47:41] epoch: 14579 train-loss: 0.010549614826838175\n",
      "[LOG 20200502-14:47:42] epoch: 14580 train-loss: 0.010549610584146447\n",
      "[LOG 20200502-14:47:42] epoch: 14581 train-loss: 0.010549607065816721\n",
      "[LOG 20200502-14:47:42] epoch: 14582 train-loss: 0.010549603030085564\n",
      "[LOG 20200502-14:47:42] epoch: 14583 train-loss: 0.01054959837347269\n",
      "[LOG 20200502-14:47:43] epoch: 14584 train-loss: 0.010549594751662679\n",
      "[LOG 20200502-14:47:43] epoch: 14585 train-loss: 0.010549590095049806\n",
      "[LOG 20200502-14:47:43] epoch: 14586 train-loss: 0.010549585127996074\n",
      "[LOG 20200502-14:47:43] epoch: 14587 train-loss: 0.010549580678343773\n",
      "[LOG 20200502-14:47:44] epoch: 14588 train-loss: 0.010549575607809756\n",
      "[LOG 20200502-14:47:44] epoch: 14589 train-loss: 0.010549570951196883\n",
      "[LOG 20200502-14:47:44] epoch: 14590 train-loss: 0.010549565777182579\n",
      "[LOG 20200502-14:47:44] epoch: 14591 train-loss: 0.010549560292727418\n",
      "[LOG 20200502-14:47:45] epoch: 14592 train-loss: 0.010549555325673686\n",
      "[LOG 20200502-14:47:45] epoch: 14593 train-loss: 0.010549549841218524\n",
      "[LOG 20200502-14:47:45] epoch: 14594 train-loss: 0.010549544667204222\n",
      "[LOG 20200502-14:47:45] epoch: 14595 train-loss: 0.010549539596670203\n",
      "[LOG 20200502-14:47:46] epoch: 14596 train-loss: 0.010549534319175614\n",
      "[LOG 20200502-14:47:46] epoch: 14597 train-loss: 0.010549529041681025\n",
      "[LOG 20200502-14:47:46] epoch: 14598 train-loss: 0.01054952366070615\n",
      "[LOG 20200502-14:47:46] epoch: 14599 train-loss: 0.010549519004093276\n",
      "[LOG 20200502-14:47:46] epoch: 14600 train-loss: 0.010549513519638114\n",
      "[LOG 20200502-14:47:47] epoch: 14601 train-loss: 0.010549508966505527\n",
      "[LOG 20200502-14:47:47] epoch: 14602 train-loss: 0.010549504102932082\n",
      "[LOG 20200502-14:47:47] epoch: 14603 train-loss: 0.010549498928917779\n",
      "[LOG 20200502-14:47:47] epoch: 14604 train-loss: 0.010549494789706336\n",
      "[LOG 20200502-14:47:48] epoch: 14605 train-loss: 0.010549490236573748\n",
      "[LOG 20200502-14:47:48] epoch: 14606 train-loss: 0.010549486614763737\n",
      "[LOG 20200502-14:47:48] epoch: 14607 train-loss: 0.010549482785993151\n",
      "[LOG 20200502-14:47:48] epoch: 14608 train-loss: 0.010549479060702853\n",
      "[LOG 20200502-14:47:49] epoch: 14609 train-loss: 0.010549475645853413\n",
      "[LOG 20200502-14:47:49] epoch: 14610 train-loss: 0.010549473058846261\n",
      "[LOG 20200502-14:47:49] epoch: 14611 train-loss: 0.010549469850957394\n",
      "[LOG 20200502-14:47:49] epoch: 14612 train-loss: 0.010549467677871386\n",
      "[LOG 20200502-14:47:50] epoch: 14613 train-loss: 0.010549464987383949\n",
      "[LOG 20200502-14:47:50] epoch: 14614 train-loss: 0.010549463538659943\n",
      "[LOG 20200502-14:47:50] epoch: 14615 train-loss: 0.010549461779495081\n",
      "[LOG 20200502-14:47:50] epoch: 14616 train-loss: 0.010549460848172506\n",
      "[LOG 20200502-14:47:51] epoch: 14617 train-loss: 0.01054945970988936\n",
      "[LOG 20200502-14:47:51] epoch: 14618 train-loss: 0.010549459295968214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:47:51] epoch: 14619 train-loss: 0.010549459089007642\n",
      "[LOG 20200502-14:47:51] epoch: 14620 train-loss: 0.010549458985527357\n",
      "[LOG 20200502-14:47:52] epoch: 14621 train-loss: 0.010549459502928786\n",
      "[LOG 20200502-14:47:52] epoch: 14622 train-loss: 0.010549460227290789\n",
      "[LOG 20200502-14:47:52] epoch: 14623 train-loss: 0.010549460951652791\n",
      "[LOG 20200502-14:47:52] epoch: 14624 train-loss: 0.010549462607337369\n",
      "[LOG 20200502-14:47:53] epoch: 14625 train-loss: 0.010549463538659943\n",
      "[LOG 20200502-14:47:53] epoch: 14626 train-loss: 0.010549465815226236\n",
      "[LOG 20200502-14:47:53] epoch: 14627 train-loss: 0.010549467367430529\n",
      "[LOG 20200502-14:47:53] epoch: 14628 train-loss: 0.010549469643996822\n",
      "[LOG 20200502-14:47:54] epoch: 14629 train-loss: 0.010549472231003974\n",
      "[LOG 20200502-14:47:54] epoch: 14630 train-loss: 0.010549475128451983\n",
      "[LOG 20200502-14:47:54] epoch: 14631 train-loss: 0.01054947761197885\n",
      "[LOG 20200502-14:47:54] epoch: 14632 train-loss: 0.010549479992025428\n",
      "[LOG 20200502-14:47:55] epoch: 14633 train-loss: 0.010549482992953725\n",
      "[LOG 20200502-14:47:55] epoch: 14634 train-loss: 0.010549486718244024\n",
      "[LOG 20200502-14:47:55] epoch: 14635 train-loss: 0.010549489512211747\n",
      "[LOG 20200502-14:47:55] epoch: 14636 train-loss: 0.010549492513140043\n",
      "[LOG 20200502-14:47:56] epoch: 14637 train-loss: 0.010549495824509196\n",
      "[LOG 20200502-14:47:56] epoch: 14638 train-loss: 0.010549498514996635\n",
      "[LOG 20200502-14:47:56] epoch: 14639 train-loss: 0.01054950182636579\n",
      "[LOG 20200502-14:47:56] epoch: 14640 train-loss: 0.010549504723813798\n",
      "[LOG 20200502-14:47:57] epoch: 14641 train-loss: 0.010549507414301237\n",
      "[LOG 20200502-14:47:57] epoch: 14642 train-loss: 0.01054951072567039\n",
      "[LOG 20200502-14:47:57] epoch: 14643 train-loss: 0.010549513312677542\n",
      "[LOG 20200502-14:47:57] epoch: 14644 train-loss: 0.010549516106645266\n",
      "[LOG 20200502-14:47:57] epoch: 14645 train-loss: 0.01054951838321156\n",
      "[LOG 20200502-14:47:58] epoch: 14646 train-loss: 0.01054952024585671\n",
      "[LOG 20200502-14:47:58] epoch: 14647 train-loss: 0.010549522211982144\n",
      "[LOG 20200502-14:47:58] epoch: 14648 train-loss: 0.01054952417810758\n",
      "[LOG 20200502-14:47:58] epoch: 14649 train-loss: 0.010549525937272443\n",
      "[LOG 20200502-14:47:59] epoch: 14650 train-loss: 0.010549527489476733\n",
      "[LOG 20200502-14:47:59] epoch: 14651 train-loss: 0.010549528731240166\n",
      "[LOG 20200502-14:47:59] epoch: 14652 train-loss: 0.010549529248641597\n",
      "[LOG 20200502-14:47:59] epoch: 14653 train-loss: 0.01054953049040503\n",
      "[LOG 20200502-14:48:00] epoch: 14654 train-loss: 0.010549531214767031\n",
      "[LOG 20200502-14:48:00] epoch: 14655 train-loss: 0.010549531111286746\n",
      "[LOG 20200502-14:48:00] epoch: 14656 train-loss: 0.010549531421727605\n",
      "[LOG 20200502-14:48:00] epoch: 14657 train-loss: 0.010549531111286746\n",
      "[LOG 20200502-14:48:01] epoch: 14658 train-loss: 0.010549530800845888\n",
      "[LOG 20200502-14:48:01] epoch: 14659 train-loss: 0.010549530593885316\n",
      "[LOG 20200502-14:48:01] epoch: 14660 train-loss: 0.010549529248641597\n",
      "[LOG 20200502-14:48:01] epoch: 14661 train-loss: 0.010549528524279594\n",
      "[LOG 20200502-14:48:02] epoch: 14662 train-loss: 0.010549527489476733\n",
      "[LOG 20200502-14:48:02] epoch: 14663 train-loss: 0.01054952573031187\n",
      "[LOG 20200502-14:48:02] epoch: 14664 train-loss: 0.010549525005949868\n",
      "[LOG 20200502-14:48:02] epoch: 14665 train-loss: 0.01054952366070615\n",
      "[LOG 20200502-14:48:02] epoch: 14666 train-loss: 0.010549521694580713\n",
      "[LOG 20200502-14:48:03] epoch: 14667 train-loss: 0.01054952024585671\n",
      "[LOG 20200502-14:48:03] epoch: 14668 train-loss: 0.010549519314534135\n",
      "[LOG 20200502-14:48:03] epoch: 14669 train-loss: 0.010549517451888986\n",
      "[LOG 20200502-14:48:03] epoch: 14670 train-loss: 0.010549516520566411\n",
      "[LOG 20200502-14:48:04] epoch: 14671 train-loss: 0.010549514554440975\n",
      "[LOG 20200502-14:48:04] epoch: 14672 train-loss: 0.010549513002236685\n",
      "[LOG 20200502-14:48:04] epoch: 14673 train-loss: 0.010549511863953538\n",
      "[LOG 20200502-14:48:05] epoch: 14674 train-loss: 0.010549509794347815\n",
      "[LOG 20200502-14:48:05] epoch: 14675 train-loss: 0.010549509069985814\n",
      "[LOG 20200502-14:48:05] epoch: 14676 train-loss: 0.010549507724742094\n",
      "[LOG 20200502-14:48:05] epoch: 14677 train-loss: 0.01054950627601809\n",
      "[LOG 20200502-14:48:05] epoch: 14678 train-loss: 0.010549505241215229\n",
      "[LOG 20200502-14:48:06] epoch: 14679 train-loss: 0.010549504723813798\n",
      "[LOG 20200502-14:48:06] epoch: 14680 train-loss: 0.010549503689010939\n",
      "[LOG 20200502-14:48:06] epoch: 14681 train-loss: 0.010549502757688364\n",
      "[LOG 20200502-14:48:06] epoch: 14682 train-loss: 0.010549501722885503\n",
      "[LOG 20200502-14:48:07] epoch: 14683 train-loss: 0.01054950182636579\n",
      "[LOG 20200502-14:48:07] epoch: 14684 train-loss: 0.010549501412444644\n",
      "[LOG 20200502-14:48:07] epoch: 14685 train-loss: 0.010549500688082643\n",
      "[LOG 20200502-14:48:07] epoch: 14686 train-loss: 0.010549500377641784\n",
      "[LOG 20200502-14:48:08] epoch: 14687 train-loss: 0.010549500067200925\n",
      "[LOG 20200502-14:48:08] epoch: 14688 train-loss: 0.010549500688082643\n",
      "[LOG 20200502-14:48:08] epoch: 14689 train-loss: 0.010549500274161497\n",
      "[LOG 20200502-14:48:08] epoch: 14690 train-loss: 0.010549501102003787\n",
      "[LOG 20200502-14:48:09] epoch: 14691 train-loss: 0.010549500481122069\n",
      "[LOG 20200502-14:48:09] epoch: 14692 train-loss: 0.010549500481122069\n",
      "[LOG 20200502-14:48:09] epoch: 14693 train-loss: 0.010549500377641784\n",
      "[LOG 20200502-14:48:09] epoch: 14694 train-loss: 0.0105495009985235\n",
      "[LOG 20200502-14:48:10] epoch: 14695 train-loss: 0.010549500688082643\n",
      "[LOG 20200502-14:48:10] epoch: 14696 train-loss: 0.0105495009985235\n",
      "[LOG 20200502-14:48:10] epoch: 14697 train-loss: 0.010549501205484072\n",
      "[LOG 20200502-14:48:10] epoch: 14698 train-loss: 0.01054950151592493\n",
      "[LOG 20200502-14:48:10] epoch: 14699 train-loss: 0.010549500791562928\n",
      "[LOG 20200502-14:48:11] epoch: 14700 train-loss: 0.01054950182636579\n",
      "[LOG 20200502-14:48:11] epoch: 14701 train-loss: 0.010549501102003787\n",
      "[LOG 20200502-14:48:11] epoch: 14702 train-loss: 0.010549500895043215\n",
      "[LOG 20200502-14:48:11] epoch: 14703 train-loss: 0.010549500791562928\n",
      "[LOG 20200502-14:48:12] epoch: 14704 train-loss: 0.010549500377641784\n",
      "[LOG 20200502-14:48:12] epoch: 14705 train-loss: 0.010549499860240353\n",
      "[LOG 20200502-14:48:12] epoch: 14706 train-loss: 0.010549499239358637\n",
      "[LOG 20200502-14:48:12] epoch: 14707 train-loss: 0.010549497894114919\n",
      "[LOG 20200502-14:48:13] epoch: 14708 train-loss: 0.010549497997595204\n",
      "[LOG 20200502-14:48:13] epoch: 14709 train-loss: 0.010549496445390914\n",
      "[LOG 20200502-14:48:13] epoch: 14710 train-loss: 0.010549494996666908\n",
      "[LOG 20200502-14:48:13] epoch: 14711 train-loss: 0.010549493340982331\n",
      "[LOG 20200502-14:48:14] epoch: 14712 train-loss: 0.010549492306179471\n",
      "[LOG 20200502-14:48:14] epoch: 14713 train-loss: 0.010549490650494894\n",
      "[LOG 20200502-14:48:14] epoch: 14714 train-loss: 0.010549488477408886\n",
      "[LOG 20200502-14:48:14] epoch: 14715 train-loss: 0.010549486614763737\n",
      "[LOG 20200502-14:48:15] epoch: 14716 train-loss: 0.010549484234717157\n",
      "[LOG 20200502-14:48:15] epoch: 14717 train-loss: 0.010549481958150864\n",
      "[LOG 20200502-14:48:15] epoch: 14718 train-loss: 0.010549479578104284\n",
      "[LOG 20200502-14:48:15] epoch: 14719 train-loss: 0.010549476163254844\n",
      "[LOG 20200502-14:48:16] epoch: 14720 train-loss: 0.01054947336928712\n",
      "[LOG 20200502-14:48:16] epoch: 14721 train-loss: 0.010549470678799681\n",
      "[LOG 20200502-14:48:16] epoch: 14722 train-loss: 0.010549467884831958\n",
      "[LOG 20200502-14:48:16] epoch: 14723 train-loss: 0.010549463952581087\n",
      "[LOG 20200502-14:48:17] epoch: 14724 train-loss: 0.010549460951652791\n",
      "[LOG 20200502-14:48:17] epoch: 14725 train-loss: 0.010549457433323065\n",
      "[LOG 20200502-14:48:17] epoch: 14726 train-loss: 0.010549453914993338\n",
      "[LOG 20200502-14:48:17] epoch: 14727 train-loss: 0.010549449879262183\n",
      "[LOG 20200502-14:48:18] epoch: 14728 train-loss: 0.010549446671373315\n",
      "[LOG 20200502-14:48:18] epoch: 14729 train-loss: 0.010549443256523874\n",
      "[LOG 20200502-14:48:18] epoch: 14730 train-loss: 0.01054943942775329\n",
      "[LOG 20200502-14:48:18] epoch: 14731 train-loss: 0.010549436116384136\n",
      "[LOG 20200502-14:48:19] epoch: 14732 train-loss: 0.010549432805014981\n",
      "[LOG 20200502-14:48:19] epoch: 14733 train-loss: 0.010549429804086685\n",
      "[LOG 20200502-14:48:19] epoch: 14734 train-loss: 0.010549425871835815\n",
      "[LOG 20200502-14:48:19] epoch: 14735 train-loss: 0.010549422663946947\n",
      "[LOG 20200502-14:48:20] epoch: 14736 train-loss: 0.010549419869979223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:48:20] epoch: 14737 train-loss: 0.010549417386452356\n",
      "[LOG 20200502-14:48:20] epoch: 14738 train-loss: 0.010549415109886063\n",
      "[LOG 20200502-14:48:20] epoch: 14739 train-loss: 0.010549413040280342\n",
      "[LOG 20200502-14:48:21] epoch: 14740 train-loss: 0.010549410349792905\n",
      "[LOG 20200502-14:48:21] epoch: 14741 train-loss: 0.01054940941847033\n",
      "[LOG 20200502-14:48:21] epoch: 14742 train-loss: 0.010549407245384322\n",
      "[LOG 20200502-14:48:21] epoch: 14743 train-loss: 0.01054940652102232\n",
      "[LOG 20200502-14:48:22] epoch: 14744 train-loss: 0.010549405589699745\n",
      "[LOG 20200502-14:48:22] epoch: 14745 train-loss: 0.01054940465837717\n",
      "[LOG 20200502-14:48:22] epoch: 14746 train-loss: 0.010549404347936312\n",
      "[LOG 20200502-14:48:22] epoch: 14747 train-loss: 0.010549404347936312\n",
      "[LOG 20200502-14:48:23] epoch: 14748 train-loss: 0.010549404865337743\n",
      "[LOG 20200502-14:48:23] epoch: 14749 train-loss: 0.010549405175778601\n",
      "[LOG 20200502-14:48:23] epoch: 14750 train-loss: 0.010549406417542033\n",
      "[LOG 20200502-14:48:23] epoch: 14751 train-loss: 0.010549407866266038\n",
      "[LOG 20200502-14:48:24] epoch: 14752 train-loss: 0.010549409625430902\n",
      "[LOG 20200502-14:48:24] epoch: 14753 train-loss: 0.010549411488076052\n",
      "[LOG 20200502-14:48:24] epoch: 14754 train-loss: 0.010549414592484633\n",
      "[LOG 20200502-14:48:24] epoch: 14755 train-loss: 0.01054941676557064\n",
      "[LOG 20200502-14:48:24] epoch: 14756 train-loss: 0.01054942018042008\n",
      "[LOG 20200502-14:48:25] epoch: 14757 train-loss: 0.010549422870907519\n",
      "[LOG 20200502-14:48:25] epoch: 14758 train-loss: 0.010549426699678103\n",
      "[LOG 20200502-14:48:25] epoch: 14759 train-loss: 0.01054943073540926\n",
      "[LOG 20200502-14:48:25] epoch: 14760 train-loss: 0.010549434771140417\n",
      "[LOG 20200502-14:48:26] epoch: 14761 train-loss: 0.01054943942775329\n",
      "[LOG 20200502-14:48:26] epoch: 14762 train-loss: 0.010549443980885876\n",
      "[LOG 20200502-14:48:26] epoch: 14763 train-loss: 0.010549448947939608\n",
      "[LOG 20200502-14:48:26] epoch: 14764 train-loss: 0.010549453501072194\n",
      "[LOG 20200502-14:48:27] epoch: 14765 train-loss: 0.010549458157685068\n",
      "[LOG 20200502-14:48:27] epoch: 14766 train-loss: 0.010549463538659943\n",
      "[LOG 20200502-14:48:27] epoch: 14767 train-loss: 0.010549468712674247\n",
      "[LOG 20200502-14:48:27] epoch: 14768 train-loss: 0.010549473576247692\n",
      "[LOG 20200502-14:48:28] epoch: 14769 train-loss: 0.010549479267663427\n",
      "[LOG 20200502-14:48:28] epoch: 14770 train-loss: 0.010549484027756585\n",
      "[LOG 20200502-14:48:28] epoch: 14771 train-loss: 0.01054948940873146\n",
      "[LOG 20200502-14:48:28] epoch: 14772 train-loss: 0.010549494582745764\n",
      "[LOG 20200502-14:48:29] epoch: 14773 train-loss: 0.010549499342838923\n",
      "[LOG 20200502-14:48:29] epoch: 14774 train-loss: 0.01054950493077437\n",
      "[LOG 20200502-14:48:29] epoch: 14775 train-loss: 0.010549509380426671\n",
      "[LOG 20200502-14:48:29] epoch: 14776 train-loss: 0.010549514244000116\n",
      "[LOG 20200502-14:48:30] epoch: 14777 train-loss: 0.010549518486691846\n",
      "[LOG 20200502-14:48:30] epoch: 14778 train-loss: 0.010549523246785006\n",
      "[LOG 20200502-14:48:30] epoch: 14779 train-loss: 0.010549527282516161\n",
      "[LOG 20200502-14:48:30] epoch: 14780 train-loss: 0.010549531214767031\n",
      "[LOG 20200502-14:48:31] epoch: 14781 train-loss: 0.010549535043537617\n",
      "[LOG 20200502-14:48:31] epoch: 14782 train-loss: 0.010549538665347628\n",
      "[LOG 20200502-14:48:31] epoch: 14783 train-loss: 0.01054954208019707\n",
      "[LOG 20200502-14:48:31] epoch: 14784 train-loss: 0.010549544667204222\n",
      "[LOG 20200502-14:48:31] epoch: 14785 train-loss: 0.010549547461171946\n",
      "[LOG 20200502-14:48:32] epoch: 14786 train-loss: 0.010549549841218524\n",
      "[LOG 20200502-14:48:32] epoch: 14787 train-loss: 0.010549551910824247\n",
      "[LOG 20200502-14:48:32] epoch: 14788 train-loss: 0.010549554290870825\n",
      "[LOG 20200502-14:48:33] epoch: 14789 train-loss: 0.010549556153515974\n",
      "[LOG 20200502-14:48:33] epoch: 14790 train-loss: 0.01054955811964141\n",
      "[LOG 20200502-14:48:33] epoch: 14791 train-loss: 0.010549559361404844\n",
      "[LOG 20200502-14:48:33] epoch: 14792 train-loss: 0.010549560292727418\n",
      "[LOG 20200502-14:48:33] epoch: 14793 train-loss: 0.010549561224049993\n",
      "[LOG 20200502-14:48:34] epoch: 14794 train-loss: 0.010549562465813424\n",
      "[LOG 20200502-14:48:34] epoch: 14795 train-loss: 0.010549563190175427\n",
      "[LOG 20200502-14:48:34] epoch: 14796 train-loss: 0.01054956391453743\n",
      "[LOG 20200502-14:48:34] epoch: 14797 train-loss: 0.010549564535419146\n",
      "[LOG 20200502-14:48:35] epoch: 14798 train-loss: 0.010549565052820576\n",
      "[LOG 20200502-14:48:35] epoch: 14799 train-loss: 0.01054956546674172\n",
      "[LOG 20200502-14:48:35] epoch: 14800 train-loss: 0.010549565984143151\n",
      "[LOG 20200502-14:48:35] epoch: 14801 train-loss: 0.010549566501544582\n",
      "[LOG 20200502-14:48:36] epoch: 14802 train-loss: 0.010549566605024867\n",
      "[LOG 20200502-14:48:36] epoch: 14803 train-loss: 0.010549567225906584\n",
      "[LOG 20200502-14:48:36] epoch: 14804 train-loss: 0.010549567743308015\n",
      "[LOG 20200502-14:48:36] epoch: 14805 train-loss: 0.010549567950268587\n",
      "[LOG 20200502-14:48:37] epoch: 14806 train-loss: 0.010549568571150303\n",
      "[LOG 20200502-14:48:37] epoch: 14807 train-loss: 0.010549569812913736\n",
      "[LOG 20200502-14:48:37] epoch: 14808 train-loss: 0.010549570433795452\n",
      "[LOG 20200502-14:48:37] epoch: 14809 train-loss: 0.01054957126163774\n",
      "[LOG 20200502-14:48:38] epoch: 14810 train-loss: 0.010549571882519457\n",
      "[LOG 20200502-14:48:38] epoch: 14811 train-loss: 0.010549573124282889\n",
      "[LOG 20200502-14:48:38] epoch: 14812 train-loss: 0.010549574055605464\n",
      "[LOG 20200502-14:48:38] epoch: 14813 train-loss: 0.010549575814770328\n",
      "[LOG 20200502-14:48:39] epoch: 14814 train-loss: 0.010549576953053474\n",
      "[LOG 20200502-14:48:39] epoch: 14815 train-loss: 0.010549578712218337\n",
      "[LOG 20200502-14:48:39] epoch: 14816 train-loss: 0.010549579747021198\n",
      "[LOG 20200502-14:48:39] epoch: 14817 train-loss: 0.010549582540988922\n",
      "[LOG 20200502-14:48:40] epoch: 14818 train-loss: 0.010549583679272069\n",
      "[LOG 20200502-14:48:40] epoch: 14819 train-loss: 0.01054958574887779\n",
      "[LOG 20200502-14:48:40] epoch: 14820 train-loss: 0.010549587508042654\n",
      "[LOG 20200502-14:48:40] epoch: 14821 train-loss: 0.010549589474168088\n",
      "[LOG 20200502-14:48:41] epoch: 14822 train-loss: 0.01054959154377381\n",
      "[LOG 20200502-14:48:41] epoch: 14823 train-loss: 0.010549593716859818\n",
      "[LOG 20200502-14:48:41] epoch: 14824 train-loss: 0.01054959599342611\n",
      "[LOG 20200502-14:48:41] epoch: 14825 train-loss: 0.01054959785607126\n",
      "[LOG 20200502-14:48:42] epoch: 14826 train-loss: 0.010549600132637553\n",
      "[LOG 20200502-14:48:42] epoch: 14827 train-loss: 0.010549602202243276\n",
      "[LOG 20200502-14:48:42] epoch: 14828 train-loss: 0.01054960416836871\n",
      "[LOG 20200502-14:48:42] epoch: 14829 train-loss: 0.010549606134494146\n",
      "[LOG 20200502-14:48:43] epoch: 14830 train-loss: 0.010549608307580153\n",
      "[LOG 20200502-14:48:43] epoch: 14831 train-loss: 0.010549609756304158\n",
      "[LOG 20200502-14:48:43] epoch: 14832 train-loss: 0.010549611618949307\n",
      "[LOG 20200502-14:48:44] epoch: 14833 train-loss: 0.010549613792035315\n",
      "[LOG 20200502-14:48:44] epoch: 14834 train-loss: 0.010549614826838175\n",
      "[LOG 20200502-14:48:44] epoch: 14835 train-loss: 0.010549616586003039\n",
      "[LOG 20200502-14:48:44] epoch: 14836 train-loss: 0.01054961782776647\n",
      "[LOG 20200502-14:48:44] epoch: 14837 train-loss: 0.010549618345167901\n",
      "[LOG 20200502-14:48:45] epoch: 14838 train-loss: 0.010549619793891907\n",
      "[LOG 20200502-14:48:45] epoch: 14839 train-loss: 0.010549621139135625\n",
      "[LOG 20200502-14:48:45] epoch: 14840 train-loss: 0.010549621656537056\n",
      "[LOG 20200502-14:48:45] epoch: 14841 train-loss: 0.010549622277418772\n",
      "[LOG 20200502-14:48:46] epoch: 14842 train-loss: 0.010549622173938487\n",
      "[LOG 20200502-14:48:46] epoch: 14843 train-loss: 0.010549622484379344\n",
      "[LOG 20200502-14:48:46] epoch: 14844 train-loss: 0.0105496220704582\n",
      "[LOG 20200502-14:48:47] epoch: 14845 train-loss: 0.010549622380899059\n",
      "[LOG 20200502-14:48:47] epoch: 14846 train-loss: 0.010549621553056769\n",
      "[LOG 20200502-14:48:47] epoch: 14847 train-loss: 0.010549621553056769\n",
      "[LOG 20200502-14:48:47] epoch: 14848 train-loss: 0.010549620414773623\n",
      "[LOG 20200502-14:48:48] epoch: 14849 train-loss: 0.010549619483451048\n",
      "[LOG 20200502-14:48:48] epoch: 14850 train-loss: 0.010549618759089045\n",
      "[LOG 20200502-14:48:48] epoch: 14851 train-loss: 0.01054961782776647\n",
      "[LOG 20200502-14:48:48] epoch: 14852 train-loss: 0.010549616482522752\n",
      "[LOG 20200502-14:48:49] epoch: 14853 train-loss: 0.010549614826838175\n",
      "[LOG 20200502-14:48:49] epoch: 14854 train-loss: 0.010549613792035315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:48:49] epoch: 14855 train-loss: 0.010549611618949307\n",
      "[LOG 20200502-14:48:49] epoch: 14856 train-loss: 0.01054961048066616\n",
      "[LOG 20200502-14:48:50] epoch: 14857 train-loss: 0.010549608618021011\n",
      "[LOG 20200502-14:48:50] epoch: 14858 train-loss: 0.010549606962336434\n",
      "[LOG 20200502-14:48:50] epoch: 14859 train-loss: 0.010549605099691285\n",
      "[LOG 20200502-14:48:50] epoch: 14860 train-loss: 0.010549603961408138\n",
      "[LOG 20200502-14:48:51] epoch: 14861 train-loss: 0.010549601995282702\n",
      "[LOG 20200502-14:48:51] epoch: 14862 train-loss: 0.010549600339598127\n",
      "[LOG 20200502-14:48:51] epoch: 14863 train-loss: 0.010549600132637553\n",
      "[LOG 20200502-14:48:51] epoch: 14864 train-loss: 0.010549597752590975\n",
      "[LOG 20200502-14:48:51] epoch: 14865 train-loss: 0.010549596510827541\n",
      "[LOG 20200502-14:48:52] epoch: 14866 train-loss: 0.010549595682985254\n",
      "[LOG 20200502-14:48:52] epoch: 14867 train-loss: 0.010549595372544395\n",
      "[LOG 20200502-14:48:52] epoch: 14868 train-loss: 0.010549594648182392\n",
      "[LOG 20200502-14:48:53] epoch: 14869 train-loss: 0.010549594648182392\n",
      "[LOG 20200502-14:48:53] epoch: 14870 train-loss: 0.010549594337741533\n",
      "[LOG 20200502-14:48:53] epoch: 14871 train-loss: 0.010549595269064108\n",
      "[LOG 20200502-14:48:53] epoch: 14872 train-loss: 0.01054959599342611\n",
      "[LOG 20200502-14:48:53] epoch: 14873 train-loss: 0.010549596510827541\n",
      "[LOG 20200502-14:48:54] epoch: 14874 train-loss: 0.010549598166512119\n",
      "[LOG 20200502-14:48:54] epoch: 14875 train-loss: 0.010549599615236124\n",
      "[LOG 20200502-14:48:54] epoch: 14876 train-loss: 0.010549602305723561\n",
      "[LOG 20200502-14:48:54] epoch: 14877 train-loss: 0.010549604271848997\n",
      "[LOG 20200502-14:48:55] epoch: 14878 train-loss: 0.010549607169297006\n",
      "[LOG 20200502-14:48:55] epoch: 14879 train-loss: 0.01054960996326473\n",
      "[LOG 20200502-14:48:55] epoch: 14880 train-loss: 0.010549613688555028\n",
      "[LOG 20200502-14:48:55] epoch: 14881 train-loss: 0.010549617620805899\n",
      "[LOG 20200502-14:48:56] epoch: 14882 train-loss: 0.010549621553056769\n",
      "[LOG 20200502-14:48:56] epoch: 14883 train-loss: 0.010549626313149929\n",
      "[LOG 20200502-14:48:56] epoch: 14884 train-loss: 0.010549631383683946\n",
      "[LOG 20200502-14:48:56] epoch: 14885 train-loss: 0.010549636143777106\n",
      "[LOG 20200502-14:48:57] epoch: 14886 train-loss: 0.010549642559554841\n",
      "[LOG 20200502-14:48:57] epoch: 14887 train-loss: 0.010549647630088858\n",
      "[LOG 20200502-14:48:57] epoch: 14888 train-loss: 0.01054965414934688\n",
      "[LOG 20200502-14:48:57] epoch: 14889 train-loss: 0.010549660358164046\n",
      "[LOG 20200502-14:48:58] epoch: 14890 train-loss: 0.01054966656698121\n",
      "[LOG 20200502-14:48:58] epoch: 14891 train-loss: 0.010549673707120948\n",
      "[LOG 20200502-14:48:58] epoch: 14892 train-loss: 0.010549679915938113\n",
      "[LOG 20200502-14:48:58] epoch: 14893 train-loss: 0.010549687469998995\n",
      "[LOG 20200502-14:48:59] epoch: 14894 train-loss: 0.010549694506658448\n",
      "[LOG 20200502-14:48:59] epoch: 14895 train-loss: 0.010549701750278473\n",
      "[LOG 20200502-14:48:59] epoch: 14896 train-loss: 0.010549708993898498\n",
      "[LOG 20200502-14:48:59] epoch: 14897 train-loss: 0.010549716651439667\n",
      "[LOG 20200502-14:49:00] epoch: 14898 train-loss: 0.010549724308980836\n",
      "[LOG 20200502-14:49:00] epoch: 14899 train-loss: 0.010549731138679717\n",
      "[LOG 20200502-14:49:00] epoch: 14900 train-loss: 0.010549738692740599\n",
      "[LOG 20200502-14:49:00] epoch: 14901 train-loss: 0.010549746143321196\n",
      "[LOG 20200502-14:49:01] epoch: 14902 train-loss: 0.010549753490421507\n",
      "[LOG 20200502-14:49:01] epoch: 14903 train-loss: 0.010549760320120387\n",
      "[LOG 20200502-14:49:01] epoch: 14904 train-loss: 0.010549767563740412\n",
      "[LOG 20200502-14:49:01] epoch: 14905 train-loss: 0.01054977449691958\n",
      "[LOG 20200502-14:49:02] epoch: 14906 train-loss: 0.010549781430098746\n",
      "[LOG 20200502-14:49:02] epoch: 14907 train-loss: 0.010549788052837053\n",
      "[LOG 20200502-14:49:02] epoch: 14908 train-loss: 0.010549793433811929\n",
      "[LOG 20200502-14:49:02] epoch: 14909 train-loss: 0.01054979943566852\n",
      "[LOG 20200502-14:49:02] epoch: 14910 train-loss: 0.010549805437525114\n",
      "[LOG 20200502-14:49:03] epoch: 14911 train-loss: 0.010549811232421134\n",
      "[LOG 20200502-14:49:03] epoch: 14912 train-loss: 0.01054981630295515\n",
      "[LOG 20200502-14:49:03] epoch: 14913 train-loss: 0.010549821476969454\n",
      "[LOG 20200502-14:49:03] epoch: 14914 train-loss: 0.010549825719661184\n",
      "[LOG 20200502-14:49:04] epoch: 14915 train-loss: 0.0105498307901952\n",
      "[LOG 20200502-14:49:04] epoch: 14916 train-loss: 0.010549834825926356\n",
      "[LOG 20200502-14:49:04] epoch: 14917 train-loss: 0.010549838551216655\n",
      "[LOG 20200502-14:49:04] epoch: 14918 train-loss: 0.010549842173026668\n",
      "[LOG 20200502-14:49:05] epoch: 14919 train-loss: 0.01054984579483668\n",
      "[LOG 20200502-14:49:05] epoch: 14920 train-loss: 0.010549849106205834\n",
      "[LOG 20200502-14:49:05] epoch: 14921 train-loss: 0.01054985262453556\n",
      "[LOG 20200502-14:49:05] epoch: 14922 train-loss: 0.010549855625463856\n",
      "[LOG 20200502-14:49:06] epoch: 14923 train-loss: 0.010549858626392152\n",
      "[LOG 20200502-14:49:06] epoch: 14924 train-loss: 0.010549861109919019\n",
      "[LOG 20200502-14:49:06] epoch: 14925 train-loss: 0.01054986369692617\n",
      "[LOG 20200502-14:49:06] epoch: 14926 train-loss: 0.010549866490893893\n",
      "[LOG 20200502-14:49:07] epoch: 14927 train-loss: 0.010549869181381332\n",
      "[LOG 20200502-14:49:07] epoch: 14928 train-loss: 0.010549871250987053\n",
      "[LOG 20200502-14:49:07] epoch: 14929 train-loss: 0.010549873631033633\n",
      "[LOG 20200502-14:49:07] epoch: 14930 train-loss: 0.010549876011080213\n",
      "[LOG 20200502-14:49:08] epoch: 14931 train-loss: 0.010549878391126791\n",
      "[LOG 20200502-14:49:08] epoch: 14932 train-loss: 0.010549881288574802\n",
      "[LOG 20200502-14:49:08] epoch: 14933 train-loss: 0.010549883979062239\n",
      "[LOG 20200502-14:49:08] epoch: 14934 train-loss: 0.010549886255628534\n",
      "[LOG 20200502-14:49:09] epoch: 14935 train-loss: 0.010549889049596257\n",
      "[LOG 20200502-14:49:09] epoch: 14936 train-loss: 0.010549892050524553\n",
      "[LOG 20200502-14:49:09] epoch: 14937 train-loss: 0.010549894844492277\n",
      "[LOG 20200502-14:49:09] epoch: 14938 train-loss: 0.010549897845420573\n",
      "[LOG 20200502-14:49:10] epoch: 14939 train-loss: 0.010549900949829154\n",
      "[LOG 20200502-14:49:10] epoch: 14940 train-loss: 0.010549903847277164\n",
      "[LOG 20200502-14:49:10] epoch: 14941 train-loss: 0.010549908089968894\n",
      "[LOG 20200502-14:49:10] epoch: 14942 train-loss: 0.010549910987416903\n",
      "[LOG 20200502-14:49:11] epoch: 14943 train-loss: 0.010549914712707201\n",
      "[LOG 20200502-14:49:11] epoch: 14944 train-loss: 0.010549918644958071\n",
      "[LOG 20200502-14:49:11] epoch: 14945 train-loss: 0.010549922163287798\n",
      "[LOG 20200502-14:49:11] epoch: 14946 train-loss: 0.010549926199018955\n",
      "[LOG 20200502-14:49:11] epoch: 14947 train-loss: 0.010549930338230398\n",
      "[LOG 20200502-14:49:12] epoch: 14948 train-loss: 0.010549933960040411\n",
      "[LOG 20200502-14:49:12] epoch: 14949 train-loss: 0.01054993768533071\n",
      "[LOG 20200502-14:49:12] epoch: 14950 train-loss: 0.01054994213498301\n",
      "[LOG 20200502-14:49:12] epoch: 14951 train-loss: 0.010549946274194453\n",
      "[LOG 20200502-14:49:13] epoch: 14952 train-loss: 0.010549950206445323\n",
      "[LOG 20200502-14:49:13] epoch: 14953 train-loss: 0.010549954035215907\n",
      "[LOG 20200502-14:49:13] epoch: 14954 train-loss: 0.010549958484868208\n",
      "[LOG 20200502-14:49:13] epoch: 14955 train-loss: 0.010549962831040224\n",
      "[LOG 20200502-14:49:14] epoch: 14956 train-loss: 0.010549966659810808\n",
      "[LOG 20200502-14:49:14] epoch: 14957 train-loss: 0.010549970695541965\n",
      "[LOG 20200502-14:49:14] epoch: 14958 train-loss: 0.010549975041713979\n",
      "[LOG 20200502-14:49:14] epoch: 14959 train-loss: 0.01054997845656342\n",
      "[LOG 20200502-14:49:15] epoch: 14960 train-loss: 0.010549981974893145\n",
      "[LOG 20200502-14:49:15] epoch: 14961 train-loss: 0.01054998611410459\n",
      "[LOG 20200502-14:49:15] epoch: 14962 train-loss: 0.010549989321993457\n",
      "[LOG 20200502-14:49:15] epoch: 14963 train-loss: 0.010549992736842897\n",
      "[LOG 20200502-14:49:16] epoch: 14964 train-loss: 0.01054999635865291\n",
      "[LOG 20200502-14:49:16] epoch: 14965 train-loss: 0.010549999152620634\n",
      "[LOG 20200502-14:49:16] epoch: 14966 train-loss: 0.010550001429186927\n",
      "[LOG 20200502-14:49:16] epoch: 14967 train-loss: 0.010550004740556082\n",
      "[LOG 20200502-14:49:17] epoch: 14968 train-loss: 0.010550007431043519\n",
      "[LOG 20200502-14:49:17] epoch: 14969 train-loss: 0.01055001001805067\n",
      "[LOG 20200502-14:49:17] epoch: 14970 train-loss: 0.010550011570254961\n",
      "[LOG 20200502-14:49:17] epoch: 14971 train-loss: 0.010550013536380397\n",
      "[LOG 20200502-14:49:18] epoch: 14972 train-loss: 0.01055001581294669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:49:18] epoch: 14973 train-loss: 0.010550016847749552\n",
      "[LOG 20200502-14:49:18] epoch: 14974 train-loss: 0.010550018710394701\n",
      "[LOG 20200502-14:49:18] epoch: 14975 train-loss: 0.01055002005563842\n",
      "[LOG 20200502-14:49:19] epoch: 14976 train-loss: 0.01055002109044128\n",
      "[LOG 20200502-14:49:19] epoch: 14977 train-loss: 0.010550021918283569\n",
      "[LOG 20200502-14:49:19] epoch: 14978 train-loss: 0.010550022435685\n",
      "[LOG 20200502-14:49:19] epoch: 14979 train-loss: 0.010550023470487859\n",
      "[LOG 20200502-14:49:20] epoch: 14980 train-loss: 0.010550023884409003\n",
      "[LOG 20200502-14:49:20] epoch: 14981 train-loss: 0.010550024194849862\n",
      "[LOG 20200502-14:49:20] epoch: 14982 train-loss: 0.010550024608771006\n",
      "[LOG 20200502-14:49:20] epoch: 14983 train-loss: 0.010550024608771006\n",
      "[LOG 20200502-14:49:21] epoch: 14984 train-loss: 0.010550025126172436\n",
      "[LOG 20200502-14:49:21] epoch: 14985 train-loss: 0.010550025022692151\n",
      "[LOG 20200502-14:49:21] epoch: 14986 train-loss: 0.01055002554009358\n",
      "[LOG 20200502-14:49:21] epoch: 14987 train-loss: 0.010550026264455583\n",
      "[LOG 20200502-14:49:22] epoch: 14988 train-loss: 0.010550026160975298\n",
      "[LOG 20200502-14:49:22] epoch: 14989 train-loss: 0.010550026678376727\n",
      "[LOG 20200502-14:49:22] epoch: 14990 train-loss: 0.010550027195778158\n",
      "[LOG 20200502-14:49:22] epoch: 14991 train-loss: 0.010550028023620447\n",
      "[LOG 20200502-14:49:23] epoch: 14992 train-loss: 0.010550029058423307\n",
      "[LOG 20200502-14:49:23] epoch: 14993 train-loss: 0.010550030610627599\n",
      "[LOG 20200502-14:49:23] epoch: 14994 train-loss: 0.010550031438469887\n",
      "[LOG 20200502-14:49:23] epoch: 14995 train-loss: 0.010550033301115036\n",
      "[LOG 20200502-14:49:23] epoch: 14996 train-loss: 0.010550035163760185\n",
      "[LOG 20200502-14:49:24] epoch: 14997 train-loss: 0.010550037336846193\n",
      "[LOG 20200502-14:49:24] epoch: 14998 train-loss: 0.010550039613412486\n",
      "[LOG 20200502-14:49:24] epoch: 14999 train-loss: 0.010550042510860495\n",
      "[LOG 20200502-14:49:24] epoch: 15000 train-loss: 0.010550045408308506\n",
      "[LOG 20200502-14:49:25] epoch: 15001 train-loss: 0.010550048926638233\n",
      "[LOG 20200502-14:49:25] epoch: 15002 train-loss: 0.010550053376290534\n",
      "[LOG 20200502-14:49:25] epoch: 15003 train-loss: 0.010550057205061117\n",
      "[LOG 20200502-14:49:25] epoch: 15004 train-loss: 0.010550062068634562\n",
      "[LOG 20200502-14:49:26] epoch: 15005 train-loss: 0.010550067035688294\n",
      "[LOG 20200502-14:49:26] epoch: 15006 train-loss: 0.010550072727104029\n",
      "[LOG 20200502-14:49:26] epoch: 15007 train-loss: 0.010550077797638046\n",
      "[LOG 20200502-14:49:26] epoch: 15008 train-loss: 0.010550084730817212\n",
      "[LOG 20200502-14:49:27] epoch: 15009 train-loss: 0.010550091146594949\n",
      "[LOG 20200502-14:49:27] epoch: 15010 train-loss: 0.010550097872813543\n",
      "[LOG 20200502-14:49:27] epoch: 15011 train-loss: 0.010550105116433568\n",
      "[LOG 20200502-14:49:27] epoch: 15012 train-loss: 0.010550112463533878\n",
      "[LOG 20200502-14:49:28] epoch: 15013 train-loss: 0.010550120534996191\n",
      "[LOG 20200502-14:49:28] epoch: 15014 train-loss: 0.010550128399497934\n",
      "[LOG 20200502-14:49:28] epoch: 15015 train-loss: 0.010550136781401105\n",
      "[LOG 20200502-14:49:28] epoch: 15016 train-loss: 0.010550145680705706\n",
      "[LOG 20200502-14:49:29] epoch: 15017 train-loss: 0.010550153855648305\n",
      "[LOG 20200502-14:49:29] epoch: 15018 train-loss: 0.01055016347931491\n",
      "[LOG 20200502-14:49:29] epoch: 15019 train-loss: 0.010550172068178654\n",
      "[LOG 20200502-14:49:29] epoch: 15020 train-loss: 0.010550182002286116\n",
      "[LOG 20200502-14:49:29] epoch: 15021 train-loss: 0.01055019110855129\n",
      "[LOG 20200502-14:49:30] epoch: 15022 train-loss: 0.010550200732217895\n",
      "[LOG 20200502-14:49:30] epoch: 15023 train-loss: 0.010550209735002782\n",
      "[LOG 20200502-14:49:30] epoch: 15024 train-loss: 0.010550220186511675\n",
      "[LOG 20200502-14:49:30] epoch: 15025 train-loss: 0.010550230017138852\n",
      "[LOG 20200502-14:49:31] epoch: 15026 train-loss: 0.01055023850252231\n",
      "[LOG 20200502-14:49:31] epoch: 15027 train-loss: 0.010550248333149485\n",
      "[LOG 20200502-14:49:31] epoch: 15028 train-loss: 0.010550257646375232\n",
      "[LOG 20200502-14:49:31] epoch: 15029 train-loss: 0.010550267477002408\n",
      "[LOG 20200502-14:49:32] epoch: 15030 train-loss: 0.010550276479787297\n",
      "[LOG 20200502-14:49:32] epoch: 15031 train-loss: 0.010550285275611613\n",
      "[LOG 20200502-14:49:32] epoch: 15032 train-loss: 0.010550294381876787\n",
      "[LOG 20200502-14:49:32] epoch: 15033 train-loss: 0.010550302660299672\n",
      "[LOG 20200502-14:49:33] epoch: 15034 train-loss: 0.010550311249163415\n",
      "[LOG 20200502-14:49:33] epoch: 15035 train-loss: 0.010550319217145443\n",
      "[LOG 20200502-14:49:33] epoch: 15036 train-loss: 0.010550327081647184\n",
      "[LOG 20200502-14:49:33] epoch: 15037 train-loss: 0.01055033453222778\n",
      "[LOG 20200502-14:49:34] epoch: 15038 train-loss: 0.010550342293249236\n",
      "[LOG 20200502-14:49:34] epoch: 15039 train-loss: 0.010550349122948117\n",
      "[LOG 20200502-14:49:34] epoch: 15040 train-loss: 0.010550356470048428\n",
      "[LOG 20200502-14:49:34] epoch: 15041 train-loss: 0.010550362678865591\n",
      "[LOG 20200502-14:49:34] epoch: 15042 train-loss: 0.010550369508564472\n",
      "[LOG 20200502-14:49:35] epoch: 15043 train-loss: 0.010550374993019633\n",
      "[LOG 20200502-14:49:35] epoch: 15044 train-loss: 0.01055038089139594\n",
      "[LOG 20200502-14:49:35] epoch: 15045 train-loss: 0.010550386479331387\n",
      "[LOG 20200502-14:49:35] epoch: 15046 train-loss: 0.010550391446385119\n",
      "[LOG 20200502-14:49:36] epoch: 15047 train-loss: 0.01055039744824171\n",
      "[LOG 20200502-14:49:36] epoch: 15048 train-loss: 0.010550402001374297\n",
      "[LOG 20200502-14:49:36] epoch: 15049 train-loss: 0.010550407278868888\n",
      "[LOG 20200502-14:49:36] epoch: 15050 train-loss: 0.010550411728521189\n",
      "[LOG 20200502-14:49:37] epoch: 15051 train-loss: 0.010550416281653775\n",
      "[LOG 20200502-14:49:37] epoch: 15052 train-loss: 0.010550420938266648\n",
      "[LOG 20200502-14:49:37] epoch: 15053 train-loss: 0.01055042507747809\n",
      "[LOG 20200502-14:49:37] epoch: 15054 train-loss: 0.010550429941051535\n",
      "[LOG 20200502-14:49:38] epoch: 15055 train-loss: 0.010550434804624982\n",
      "[LOG 20200502-14:49:38] epoch: 15056 train-loss: 0.010550439150796996\n",
      "[LOG 20200502-14:49:38] epoch: 15057 train-loss: 0.010550443186528154\n",
      "[LOG 20200502-14:49:38] epoch: 15058 train-loss: 0.010550447843141027\n",
      "[LOG 20200502-14:49:39] epoch: 15059 train-loss: 0.010550452913675044\n",
      "[LOG 20200502-14:49:39] epoch: 15060 train-loss: 0.010550457052886486\n",
      "[LOG 20200502-14:49:39] epoch: 15061 train-loss: 0.010550461502538787\n",
      "[LOG 20200502-14:49:39] epoch: 15062 train-loss: 0.010550466676553091\n",
      "[LOG 20200502-14:49:40] epoch: 15063 train-loss: 0.01055047143664625\n",
      "[LOG 20200502-14:49:40] epoch: 15064 train-loss: 0.010550477024581697\n",
      "[LOG 20200502-14:49:40] epoch: 15065 train-loss: 0.010550481991635429\n",
      "[LOG 20200502-14:49:40] epoch: 15066 train-loss: 0.010550487062169446\n",
      "[LOG 20200502-14:49:41] epoch: 15067 train-loss: 0.010550492546624608\n",
      "[LOG 20200502-14:49:41] epoch: 15068 train-loss: 0.010550498238040341\n",
      "[LOG 20200502-14:49:41] epoch: 15069 train-loss: 0.010550503722495504\n",
      "[LOG 20200502-14:49:41] epoch: 15070 train-loss: 0.010550509103470378\n",
      "[LOG 20200502-14:49:42] epoch: 15071 train-loss: 0.010550515519248115\n",
      "[LOG 20200502-14:49:42] epoch: 15072 train-loss: 0.010550521624584993\n",
      "[LOG 20200502-14:49:42] epoch: 15073 train-loss: 0.010550527316000726\n",
      "[LOG 20200502-14:49:42] epoch: 15074 train-loss: 0.010550533524817891\n",
      "[LOG 20200502-14:49:42] epoch: 15075 train-loss: 0.010550539319713911\n",
      "[LOG 20200502-14:49:43] epoch: 15076 train-loss: 0.010550545425050788\n",
      "[LOG 20200502-14:49:43] epoch: 15077 train-loss: 0.010550551530387666\n",
      "[LOG 20200502-14:49:43] epoch: 15078 train-loss: 0.0105505572218034\n",
      "[LOG 20200502-14:49:43] epoch: 15079 train-loss: 0.010550564154982567\n",
      "[LOG 20200502-14:49:44] epoch: 15080 train-loss: 0.010550569432477156\n",
      "[LOG 20200502-14:49:44] epoch: 15081 train-loss: 0.010550576055215465\n",
      "[LOG 20200502-14:49:44] epoch: 15082 train-loss: 0.010550582160552343\n",
      "[LOG 20200502-14:49:44] epoch: 15083 train-loss: 0.010550587438046932\n",
      "[LOG 20200502-14:49:45] epoch: 15084 train-loss: 0.01055059354338381\n",
      "[LOG 20200502-14:49:45] epoch: 15085 train-loss: 0.010550599545240402\n",
      "[LOG 20200502-14:49:45] epoch: 15086 train-loss: 0.010550605443616709\n",
      "[LOG 20200502-14:49:45] epoch: 15087 train-loss: 0.010550611135032441\n",
      "[LOG 20200502-14:49:46] epoch: 15088 train-loss: 0.010550616309046745\n",
      "[LOG 20200502-14:49:46] epoch: 15089 train-loss: 0.010550620965659618\n",
      "[LOG 20200502-14:49:46] epoch: 15090 train-loss: 0.010550626139673922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:49:47] epoch: 15091 train-loss: 0.01055063121020794\n",
      "[LOG 20200502-14:49:47] epoch: 15092 train-loss: 0.010550635763340525\n",
      "[LOG 20200502-14:49:47] epoch: 15093 train-loss: 0.010550640626913972\n",
      "[LOG 20200502-14:49:47] epoch: 15094 train-loss: 0.010550645283526845\n",
      "[LOG 20200502-14:49:47] epoch: 15095 train-loss: 0.010550649526218573\n",
      "[LOG 20200502-14:49:48] epoch: 15096 train-loss: 0.0105506530445483\n",
      "[LOG 20200502-14:49:48] epoch: 15097 train-loss: 0.010550656562878026\n",
      "[LOG 20200502-14:49:48] epoch: 15098 train-loss: 0.010550660702089468\n",
      "[LOG 20200502-14:49:49] epoch: 15099 train-loss: 0.01055066411693891\n",
      "[LOG 20200502-14:49:49] epoch: 15100 train-loss: 0.010550667117867205\n",
      "[LOG 20200502-14:49:49] epoch: 15101 train-loss: 0.010550670222275786\n",
      "[LOG 20200502-14:49:49] epoch: 15102 train-loss: 0.010550672809282938\n",
      "[LOG 20200502-14:49:50] epoch: 15103 train-loss: 0.010550675603250662\n",
      "[LOG 20200502-14:49:50] epoch: 15104 train-loss: 0.010550677983297242\n",
      "[LOG 20200502-14:49:50] epoch: 15105 train-loss: 0.010550680880745253\n",
      "[LOG 20200502-14:49:50] epoch: 15106 train-loss: 0.010550683053831259\n",
      "[LOG 20200502-14:49:51] epoch: 15107 train-loss: 0.010550685226917267\n",
      "[LOG 20200502-14:49:51] epoch: 15108 train-loss: 0.01055068750348356\n",
      "[LOG 20200502-14:49:51] epoch: 15109 train-loss: 0.01055068988353014\n",
      "[LOG 20200502-14:49:51] epoch: 15110 train-loss: 0.010550691642695002\n",
      "[LOG 20200502-14:49:52] epoch: 15111 train-loss: 0.010550694022741582\n",
      "[LOG 20200502-14:49:52] epoch: 15112 train-loss: 0.010550696609748734\n",
      "[LOG 20200502-14:49:52] epoch: 15113 train-loss: 0.010550698679354455\n",
      "[LOG 20200502-14:49:52] epoch: 15114 train-loss: 0.010550700645479891\n",
      "[LOG 20200502-14:49:53] epoch: 15115 train-loss: 0.010550703439447615\n",
      "[LOG 20200502-14:49:53] epoch: 15116 train-loss: 0.010550705716013908\n",
      "[LOG 20200502-14:49:53] epoch: 15117 train-loss: 0.010550708199540773\n",
      "[LOG 20200502-14:49:53] epoch: 15118 train-loss: 0.010550711924831072\n",
      "[LOG 20200502-14:49:54] epoch: 15119 train-loss: 0.01055071461531851\n",
      "[LOG 20200502-14:49:54] epoch: 15120 train-loss: 0.01055071803016795\n",
      "[LOG 20200502-14:49:54] epoch: 15121 train-loss: 0.010550722169379393\n",
      "[LOG 20200502-14:49:54] epoch: 15122 train-loss: 0.010550726412071122\n",
      "[LOG 20200502-14:49:55] epoch: 15123 train-loss: 0.010550730861723423\n",
      "[LOG 20200502-14:49:55] epoch: 15124 train-loss: 0.010550735311375724\n",
      "[LOG 20200502-14:49:55] epoch: 15125 train-loss: 0.010550740485390028\n",
      "[LOG 20200502-14:49:55] epoch: 15126 train-loss: 0.010550745555924045\n",
      "[LOG 20200502-14:49:55] epoch: 15127 train-loss: 0.010550751661260923\n",
      "[LOG 20200502-14:49:56] epoch: 15128 train-loss: 0.010550757973558374\n",
      "[LOG 20200502-14:49:56] epoch: 15129 train-loss: 0.010550764699776968\n",
      "[LOG 20200502-14:49:56] epoch: 15130 train-loss: 0.010550771425995562\n",
      "[LOG 20200502-14:49:56] epoch: 15131 train-loss: 0.010550778876576159\n",
      "[LOG 20200502-14:49:57] epoch: 15132 train-loss: 0.0105507867410779\n",
      "[LOG 20200502-14:49:57] epoch: 15133 train-loss: 0.010550794812540213\n",
      "[LOG 20200502-14:49:57] epoch: 15134 train-loss: 0.010550803504884243\n",
      "[LOG 20200502-14:49:57] epoch: 15135 train-loss: 0.010550812093747986\n",
      "[LOG 20200502-14:49:58] epoch: 15136 train-loss: 0.010550821303493448\n",
      "[LOG 20200502-14:49:58] epoch: 15137 train-loss: 0.01055083092716005\n",
      "[LOG 20200502-14:49:58] epoch: 15138 train-loss: 0.010550841068228086\n",
      "[LOG 20200502-14:49:58] epoch: 15139 train-loss: 0.01055085120929612\n",
      "[LOG 20200502-14:49:59] epoch: 15140 train-loss: 0.010550861350364156\n",
      "[LOG 20200502-14:49:59] epoch: 15141 train-loss: 0.010550872319274478\n",
      "[LOG 20200502-14:49:59] epoch: 15142 train-loss: 0.01055088359862566\n",
      "[LOG 20200502-14:49:59] epoch: 15143 train-loss: 0.010550894257095125\n",
      "[LOG 20200502-14:49:59] epoch: 15144 train-loss: 0.01055090595036745\n",
      "[LOG 20200502-14:50:00] epoch: 15145 train-loss: 0.010550917229718633\n",
      "[LOG 20200502-14:50:00] epoch: 15146 train-loss: 0.010550928922990957\n",
      "[LOG 20200502-14:50:00] epoch: 15147 train-loss: 0.010550940098861853\n",
      "[LOG 20200502-14:50:00] epoch: 15148 train-loss: 0.010550951688653894\n",
      "[LOG 20200502-14:50:01] epoch: 15149 train-loss: 0.010550963381926218\n",
      "[LOG 20200502-14:50:01] epoch: 15150 train-loss: 0.010550974971718259\n",
      "[LOG 20200502-14:50:01] epoch: 15151 train-loss: 0.010550986871951155\n",
      "[LOG 20200502-14:50:01] epoch: 15152 train-loss: 0.01055099835826291\n",
      "[LOG 20200502-14:50:02] epoch: 15153 train-loss: 0.010551009120212661\n",
      "[LOG 20200502-14:50:02] epoch: 15154 train-loss: 0.010551021123925844\n",
      "[LOG 20200502-14:50:02] epoch: 15155 train-loss: 0.010551032196316455\n",
      "[LOG 20200502-14:50:02] epoch: 15156 train-loss: 0.010551043475667635\n",
      "[LOG 20200502-14:50:03] epoch: 15157 train-loss: 0.010551053823696243\n",
      "[LOG 20200502-14:50:03] epoch: 15158 train-loss: 0.010551064378685422\n",
      "[LOG 20200502-14:50:03] epoch: 15159 train-loss: 0.010551074726714028\n",
      "[LOG 20200502-14:50:03] epoch: 15160 train-loss: 0.010551084764301777\n",
      "[LOG 20200502-14:50:03] epoch: 15161 train-loss: 0.010551094698409239\n",
      "[LOG 20200502-14:50:04] epoch: 15162 train-loss: 0.010551104115115272\n",
      "[LOG 20200502-14:50:04] epoch: 15163 train-loss: 0.010551113531821303\n",
      "[LOG 20200502-14:50:04] epoch: 15164 train-loss: 0.01055112232764562\n",
      "[LOG 20200502-14:50:05] epoch: 15165 train-loss: 0.010551131123469936\n",
      "[LOG 20200502-14:50:05] epoch: 15166 train-loss: 0.01055113971233368\n",
      "[LOG 20200502-14:50:05] epoch: 15167 train-loss: 0.010551147990756564\n",
      "[LOG 20200502-14:50:05] epoch: 15168 train-loss: 0.01055115575177802\n",
      "[LOG 20200502-14:50:06] epoch: 15169 train-loss: 0.010551163409319188\n",
      "[LOG 20200502-14:50:06] epoch: 15170 train-loss: 0.010551170652939213\n",
      "[LOG 20200502-14:50:06] epoch: 15171 train-loss: 0.010551178310480382\n",
      "[LOG 20200502-14:50:06] epoch: 15172 train-loss: 0.010551185243659549\n",
      "[LOG 20200502-14:50:06] epoch: 15173 train-loss: 0.01055119207335843\n",
      "[LOG 20200502-14:50:07] epoch: 15174 train-loss: 0.010551199006537596\n",
      "[LOG 20200502-14:50:07] epoch: 15175 train-loss: 0.010551205215354761\n",
      "[LOG 20200502-14:50:07] epoch: 15176 train-loss: 0.010551212148533927\n",
      "[LOG 20200502-14:50:07] epoch: 15177 train-loss: 0.010551218460831378\n",
      "[LOG 20200502-14:50:08] epoch: 15178 train-loss: 0.010551224773128828\n",
      "[LOG 20200502-14:50:08] epoch: 15179 train-loss: 0.01055123129238685\n",
      "[LOG 20200502-14:50:08] epoch: 15180 train-loss: 0.010551237294243442\n",
      "[LOG 20200502-14:50:08] epoch: 15181 train-loss: 0.010551244020462036\n",
      "[LOG 20200502-14:50:09] epoch: 15182 train-loss: 0.010551249815358056\n",
      "[LOG 20200502-14:50:09] epoch: 15183 train-loss: 0.010551256852017509\n",
      "[LOG 20200502-14:50:09] epoch: 15184 train-loss: 0.010551263267795244\n",
      "[LOG 20200502-14:50:09] epoch: 15185 train-loss: 0.010551269890533553\n",
      "[LOG 20200502-14:50:10] epoch: 15186 train-loss: 0.01055127651327186\n",
      "[LOG 20200502-14:50:10] epoch: 15187 train-loss: 0.010551283239490457\n",
      "[LOG 20200502-14:50:10] epoch: 15188 train-loss: 0.010551290793551339\n",
      "[LOG 20200502-14:50:10] epoch: 15189 train-loss: 0.010551297209329076\n",
      "[LOG 20200502-14:50:11] epoch: 15190 train-loss: 0.010551304556429386\n",
      "[LOG 20200502-14:50:11] epoch: 15191 train-loss: 0.010551311489608552\n",
      "[LOG 20200502-14:50:11] epoch: 15192 train-loss: 0.010551319354110293\n",
      "[LOG 20200502-14:50:11] epoch: 15193 train-loss: 0.010551326287289461\n",
      "[LOG 20200502-14:50:12] epoch: 15194 train-loss: 0.010551334151791202\n",
      "[LOG 20200502-14:50:12] epoch: 15195 train-loss: 0.010551341705852084\n",
      "[LOG 20200502-14:50:12] epoch: 15196 train-loss: 0.010551349570353826\n",
      "[LOG 20200502-14:50:12] epoch: 15197 train-loss: 0.010551357124414708\n",
      "[LOG 20200502-14:50:13] epoch: 15198 train-loss: 0.010551365402837595\n",
      "[LOG 20200502-14:50:13] epoch: 15199 train-loss: 0.010551373267339336\n",
      "[LOG 20200502-14:50:13] epoch: 15200 train-loss: 0.010551381131841077\n",
      "[LOG 20200502-14:50:13] epoch: 15201 train-loss: 0.010551389617224535\n",
      "[LOG 20200502-14:50:14] epoch: 15202 train-loss: 0.010551397481726276\n",
      "[LOG 20200502-14:50:14] epoch: 15203 train-loss: 0.01055140576014916\n",
      "[LOG 20200502-14:50:14] epoch: 15204 train-loss: 0.010551413107249472\n",
      "[LOG 20200502-14:50:14] epoch: 15205 train-loss: 0.010551421385672357\n",
      "[LOG 20200502-14:50:15] epoch: 15206 train-loss: 0.010551429664095243\n",
      "[LOG 20200502-14:50:15] epoch: 15207 train-loss: 0.010551437321636412\n",
      "[LOG 20200502-14:50:15] epoch: 15208 train-loss: 0.010551444875697294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:50:15] epoch: 15209 train-loss: 0.010551452429758178\n",
      "[LOG 20200502-14:50:16] epoch: 15210 train-loss: 0.010551460190779634\n",
      "[LOG 20200502-14:50:16] epoch: 15211 train-loss: 0.010551467848320803\n",
      "[LOG 20200502-14:50:16] epoch: 15212 train-loss: 0.010551475091940828\n",
      "[LOG 20200502-14:50:16] epoch: 15213 train-loss: 0.01055148264600171\n",
      "[LOG 20200502-14:50:17] epoch: 15214 train-loss: 0.01055148947570059\n",
      "[LOG 20200502-14:50:17] epoch: 15215 train-loss: 0.010551496201919185\n",
      "[LOG 20200502-14:50:17] epoch: 15216 train-loss: 0.010551503135098351\n",
      "[LOG 20200502-14:50:17] epoch: 15217 train-loss: 0.010551509343915515\n",
      "[LOG 20200502-14:50:18] epoch: 15218 train-loss: 0.010551515449252393\n",
      "[LOG 20200502-14:50:18] epoch: 15219 train-loss: 0.010551521451108985\n",
      "[LOG 20200502-14:50:18] epoch: 15220 train-loss: 0.010551527970367007\n",
      "[LOG 20200502-14:50:18] epoch: 15221 train-loss: 0.010551533351341883\n",
      "[LOG 20200502-14:50:19] epoch: 15222 train-loss: 0.010551538628836473\n",
      "[LOG 20200502-14:50:19] epoch: 15223 train-loss: 0.01055154400981135\n",
      "[LOG 20200502-14:50:19] epoch: 15224 train-loss: 0.010551549183825651\n",
      "[LOG 20200502-14:50:19] epoch: 15225 train-loss: 0.010551553943918811\n",
      "[LOG 20200502-14:50:20] epoch: 15226 train-loss: 0.010551558497051397\n",
      "[LOG 20200502-14:50:20] epoch: 15227 train-loss: 0.010551563257144557\n",
      "[LOG 20200502-14:50:20] epoch: 15228 train-loss: 0.010551567603316572\n",
      "[LOG 20200502-14:50:20] epoch: 15229 train-loss: 0.010551572259929445\n",
      "[LOG 20200502-14:50:20] epoch: 15230 train-loss: 0.010551576399140887\n",
      "[LOG 20200502-14:50:21] epoch: 15231 train-loss: 0.01055158033139176\n",
      "[LOG 20200502-14:50:21] epoch: 15232 train-loss: 0.01055158426364263\n",
      "[LOG 20200502-14:50:21] epoch: 15233 train-loss: 0.01055158767849207\n",
      "[LOG 20200502-14:50:21] epoch: 15234 train-loss: 0.010551591817703512\n",
      "[LOG 20200502-14:50:22] epoch: 15235 train-loss: 0.010551595646474097\n",
      "[LOG 20200502-14:50:22] epoch: 15236 train-loss: 0.010551599682205252\n",
      "[LOG 20200502-14:50:22] epoch: 15237 train-loss: 0.010551603924896982\n",
      "[LOG 20200502-14:50:22] epoch: 15238 train-loss: 0.010551607753667567\n",
      "[LOG 20200502-14:50:23] epoch: 15239 train-loss: 0.01055161241028044\n",
      "[LOG 20200502-14:50:23] epoch: 15240 train-loss: 0.01055161634253131\n",
      "[LOG 20200502-14:50:23] epoch: 15241 train-loss: 0.010551621516545614\n",
      "[LOG 20200502-14:50:23] epoch: 15242 train-loss: 0.010551625448796485\n",
      "[LOG 20200502-14:50:24] epoch: 15243 train-loss: 0.010551631036731932\n",
      "[LOG 20200502-14:50:24] epoch: 15244 train-loss: 0.010551635900305377\n",
      "[LOG 20200502-14:50:24] epoch: 15245 train-loss: 0.010551641384760538\n",
      "[LOG 20200502-14:50:24] epoch: 15246 train-loss: 0.010551647800538275\n",
      "[LOG 20200502-14:50:25] epoch: 15247 train-loss: 0.010551654319796298\n",
      "[LOG 20200502-14:50:25] epoch: 15248 train-loss: 0.010551660735574033\n",
      "[LOG 20200502-14:50:25] epoch: 15249 train-loss: 0.010551667461792627\n",
      "[LOG 20200502-14:50:25] epoch: 15250 train-loss: 0.01055167480889294\n",
      "[LOG 20200502-14:50:25] epoch: 15251 train-loss: 0.010551682259473536\n",
      "[LOG 20200502-14:50:26] epoch: 15252 train-loss: 0.010551690434416136\n",
      "[LOG 20200502-14:50:26] epoch: 15253 train-loss: 0.010551698919799592\n",
      "[LOG 20200502-14:50:26] epoch: 15254 train-loss: 0.010551707922584481\n",
      "[LOG 20200502-14:50:26] epoch: 15255 train-loss: 0.010551717235810228\n",
      "[LOG 20200502-14:50:27] epoch: 15256 train-loss: 0.010551727480358548\n",
      "[LOG 20200502-14:50:27] epoch: 15257 train-loss: 0.010551736483143436\n",
      "[LOG 20200502-14:50:27] epoch: 15258 train-loss: 0.010551747038132615\n",
      "[LOG 20200502-14:50:27] epoch: 15259 train-loss: 0.010551758007042937\n",
      "[LOG 20200502-14:50:28] epoch: 15260 train-loss: 0.010551769079433547\n",
      "[LOG 20200502-14:50:28] epoch: 15261 train-loss: 0.010551780358784728\n",
      "[LOG 20200502-14:50:28] epoch: 15262 train-loss: 0.010551792259017626\n",
      "[LOG 20200502-14:50:28] epoch: 15263 train-loss: 0.01055180426273081\n",
      "[LOG 20200502-14:50:29] epoch: 15264 train-loss: 0.010551816783845425\n",
      "[LOG 20200502-14:50:29] epoch: 15265 train-loss: 0.010551829718881182\n",
      "[LOG 20200502-14:50:29] epoch: 15266 train-loss: 0.010551842653916942\n",
      "[LOG 20200502-14:50:29] epoch: 15267 train-loss: 0.010551855381992128\n",
      "[LOG 20200502-14:50:29] epoch: 15268 train-loss: 0.010551868627468744\n",
      "[LOG 20200502-14:50:30] epoch: 15269 train-loss: 0.01055188218338622\n",
      "[LOG 20200502-14:50:30] epoch: 15270 train-loss: 0.010551895428862836\n",
      "[LOG 20200502-14:50:30] epoch: 15271 train-loss: 0.010551908984780312\n",
      "[LOG 20200502-14:50:30] epoch: 15272 train-loss: 0.01055192295461893\n",
      "[LOG 20200502-14:50:31] epoch: 15273 train-loss: 0.010551936200095547\n",
      "[LOG 20200502-14:50:31] epoch: 15274 train-loss: 0.010551949652532736\n",
      "[LOG 20200502-14:50:31] epoch: 15275 train-loss: 0.010551963208450211\n",
      "[LOG 20200502-14:50:31] epoch: 15276 train-loss: 0.010551976764367687\n",
      "[LOG 20200502-14:50:32] epoch: 15277 train-loss: 0.01055199011332459\n",
      "[LOG 20200502-14:50:32] epoch: 15278 train-loss: 0.010552003151840635\n",
      "[LOG 20200502-14:50:32] epoch: 15279 train-loss: 0.010552016293836964\n",
      "[LOG 20200502-14:50:32] epoch: 15280 train-loss: 0.010552029228872724\n",
      "[LOG 20200502-14:50:33] epoch: 15281 train-loss: 0.01055204195694791\n",
      "[LOG 20200502-14:50:33] epoch: 15282 train-loss: 0.010552054167621665\n",
      "[LOG 20200502-14:50:33] epoch: 15283 train-loss: 0.010552066481775708\n",
      "[LOG 20200502-14:50:33] epoch: 15284 train-loss: 0.01055207827852832\n",
      "[LOG 20200502-14:50:34] epoch: 15285 train-loss: 0.010552089764840074\n",
      "[LOG 20200502-14:50:34] epoch: 15286 train-loss: 0.010552101354632113\n",
      "[LOG 20200502-14:50:34] epoch: 15287 train-loss: 0.01055211273746358\n",
      "[LOG 20200502-14:50:34] epoch: 15288 train-loss: 0.01055212329245276\n",
      "[LOG 20200502-14:50:34] epoch: 15289 train-loss: 0.010552133950922225\n",
      "[LOG 20200502-14:50:35] epoch: 15290 train-loss: 0.010552144195470545\n",
      "[LOG 20200502-14:50:35] epoch: 15291 train-loss: 0.010552153922617435\n",
      "[LOG 20200502-14:50:35] epoch: 15292 train-loss: 0.01055216406368547\n",
      "[LOG 20200502-14:50:35] epoch: 15293 train-loss: 0.01055217327343093\n",
      "[LOG 20200502-14:50:36] epoch: 15294 train-loss: 0.010552182690136962\n",
      "[LOG 20200502-14:50:36] epoch: 15295 train-loss: 0.01055219117552042\n",
      "[LOG 20200502-14:50:36] epoch: 15296 train-loss: 0.010552200695706738\n",
      "[LOG 20200502-14:50:36] epoch: 15297 train-loss: 0.010552208974129625\n",
      "[LOG 20200502-14:50:37] epoch: 15298 train-loss: 0.010552217149072222\n",
      "[LOG 20200502-14:50:37] epoch: 15299 train-loss: 0.010552225737935968\n",
      "[LOG 20200502-14:50:37] epoch: 15300 train-loss: 0.01055223380939828\n",
      "[LOG 20200502-14:50:37] epoch: 15301 train-loss: 0.010552241880860593\n",
      "[LOG 20200502-14:50:38] epoch: 15302 train-loss: 0.010552249952322908\n",
      "[LOG 20200502-14:50:38] epoch: 15303 train-loss: 0.010552258230745792\n",
      "[LOG 20200502-14:50:38] epoch: 15304 train-loss: 0.010552266302208105\n",
      "[LOG 20200502-14:50:38] epoch: 15305 train-loss: 0.010552274477150705\n",
      "[LOG 20200502-14:50:39] epoch: 15306 train-loss: 0.010552282445132732\n",
      "[LOG 20200502-14:50:39] epoch: 15307 train-loss: 0.010552290723555617\n",
      "[LOG 20200502-14:50:39] epoch: 15308 train-loss: 0.010552298898498217\n",
      "[LOG 20200502-14:50:39] epoch: 15309 train-loss: 0.01055230748736196\n",
      "[LOG 20200502-14:50:40] epoch: 15310 train-loss: 0.01055231566230456\n",
      "[LOG 20200502-14:50:40] epoch: 15311 train-loss: 0.010552324561609162\n",
      "[LOG 20200502-14:50:40] epoch: 15312 train-loss: 0.010552333150472906\n",
      "[LOG 20200502-14:50:40] epoch: 15313 train-loss: 0.010552342049777508\n",
      "[LOG 20200502-14:50:41] epoch: 15314 train-loss: 0.010552350328200392\n",
      "[LOG 20200502-14:50:41] epoch: 15315 train-loss: 0.010552359848386712\n",
      "[LOG 20200502-14:50:41] epoch: 15316 train-loss: 0.010552368644211028\n",
      "[LOG 20200502-14:50:41] epoch: 15317 train-loss: 0.010552377853956487\n",
      "[LOG 20200502-14:50:41] epoch: 15318 train-loss: 0.010552387167182233\n",
      "[LOG 20200502-14:50:42] epoch: 15319 train-loss: 0.01055239648040798\n",
      "[LOG 20200502-14:50:42] epoch: 15320 train-loss: 0.010552406000594297\n",
      "[LOG 20200502-14:50:42] epoch: 15321 train-loss: 0.01055241541730033\n",
      "[LOG 20200502-14:50:42] epoch: 15322 train-loss: 0.010552425558368364\n",
      "[LOG 20200502-14:50:43] epoch: 15323 train-loss: 0.01055243518203497\n",
      "[LOG 20200502-14:50:43] epoch: 15324 train-loss: 0.010552444805701574\n",
      "[LOG 20200502-14:50:43] epoch: 15325 train-loss: 0.010552454532848464\n",
      "[LOG 20200502-14:50:43] epoch: 15326 train-loss: 0.010552464466955926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:50:44] epoch: 15327 train-loss: 0.010552474090622531\n",
      "[LOG 20200502-14:50:44] epoch: 15328 train-loss: 0.010552484335170852\n",
      "[LOG 20200502-14:50:44] epoch: 15329 train-loss: 0.010552493648396598\n",
      "[LOG 20200502-14:50:44] epoch: 15330 train-loss: 0.010552503168582916\n",
      "[LOG 20200502-14:50:45] epoch: 15331 train-loss: 0.010552513102690378\n",
      "[LOG 20200502-14:50:45] epoch: 15332 train-loss: 0.01055252231243584\n",
      "[LOG 20200502-14:50:45] epoch: 15333 train-loss: 0.010552531936102443\n",
      "[LOG 20200502-14:50:45] epoch: 15334 train-loss: 0.010552540524966188\n",
      "[LOG 20200502-14:50:46] epoch: 15335 train-loss: 0.010552550045152506\n",
      "[LOG 20200502-14:50:46] epoch: 15336 train-loss: 0.010552559254897965\n",
      "[LOG 20200502-14:50:46] epoch: 15337 train-loss: 0.010552568464643426\n",
      "[LOG 20200502-14:50:46] epoch: 15338 train-loss: 0.010552576639586024\n",
      "[LOG 20200502-14:50:47] epoch: 15339 train-loss: 0.01055258491800891\n",
      "[LOG 20200502-14:50:47] epoch: 15340 train-loss: 0.010552593403392367\n",
      "[LOG 20200502-14:50:47] epoch: 15341 train-loss: 0.010552601060933538\n",
      "[LOG 20200502-14:50:47] epoch: 15342 train-loss: 0.010552609339356422\n",
      "[LOG 20200502-14:50:47] epoch: 15343 train-loss: 0.010552616996897591\n",
      "[LOG 20200502-14:50:48] epoch: 15344 train-loss: 0.010552624240517616\n",
      "[LOG 20200502-14:50:48] epoch: 15345 train-loss: 0.010552631898058785\n",
      "[LOG 20200502-14:50:48] epoch: 15346 train-loss: 0.010552638313836522\n",
      "[LOG 20200502-14:50:48] epoch: 15347 train-loss: 0.010552645350495974\n",
      "[LOG 20200502-14:50:49] epoch: 15348 train-loss: 0.010552652387155427\n",
      "[LOG 20200502-14:50:49] epoch: 15349 train-loss: 0.010552659113374021\n",
      "[LOG 20200502-14:50:49] epoch: 15350 train-loss: 0.0105526652187109\n",
      "[LOG 20200502-14:50:49] epoch: 15351 train-loss: 0.010552671324047778\n",
      "[LOG 20200502-14:50:50] epoch: 15352 train-loss: 0.010552677222424083\n",
      "[LOG 20200502-14:50:50] epoch: 15353 train-loss: 0.010552682913839817\n",
      "[LOG 20200502-14:50:50] epoch: 15354 train-loss: 0.010552688708735837\n",
      "[LOG 20200502-14:50:50] epoch: 15355 train-loss: 0.010552694089710712\n",
      "[LOG 20200502-14:50:51] epoch: 15356 train-loss: 0.010552700608968735\n",
      "[LOG 20200502-14:50:51] epoch: 15357 train-loss: 0.01055270547254218\n",
      "[LOG 20200502-14:50:51] epoch: 15358 train-loss: 0.010552710853517056\n",
      "[LOG 20200502-14:50:51] epoch: 15359 train-loss: 0.010552716131011644\n",
      "[LOG 20200502-14:50:51] epoch: 15360 train-loss: 0.010552721822427379\n",
      "[LOG 20200502-14:50:52] epoch: 15361 train-loss: 0.010552727617323399\n",
      "[LOG 20200502-14:50:52] epoch: 15362 train-loss: 0.010552733826140562\n",
      "[LOG 20200502-14:50:52] epoch: 15363 train-loss: 0.010552739103635153\n",
      "[LOG 20200502-14:50:52] epoch: 15364 train-loss: 0.010552745105491744\n",
      "[LOG 20200502-14:50:53] epoch: 15365 train-loss: 0.010552751417789195\n",
      "[LOG 20200502-14:50:53] epoch: 15366 train-loss: 0.010552758247488074\n",
      "[LOG 20200502-14:50:53] epoch: 15367 train-loss: 0.010552764870226383\n",
      "[LOG 20200502-14:50:53] epoch: 15368 train-loss: 0.010552771906885836\n",
      "[LOG 20200502-14:50:54] epoch: 15369 train-loss: 0.010552779357466433\n",
      "[LOG 20200502-14:50:54] epoch: 15370 train-loss: 0.0105527862906456\n",
      "[LOG 20200502-14:50:54] epoch: 15371 train-loss: 0.010552794672548771\n",
      "[LOG 20200502-14:50:54] epoch: 15372 train-loss: 0.01055280284749137\n",
      "[LOG 20200502-14:50:55] epoch: 15373 train-loss: 0.0105528115398354\n",
      "[LOG 20200502-14:50:55] epoch: 15374 train-loss: 0.010552820025218857\n",
      "[LOG 20200502-14:50:55] epoch: 15375 train-loss: 0.010552829855846034\n",
      "[LOG 20200502-14:50:56] epoch: 15376 train-loss: 0.01055283947951264\n",
      "[LOG 20200502-14:50:56] epoch: 15377 train-loss: 0.010552850344942676\n",
      "[LOG 20200502-14:50:56] epoch: 15378 train-loss: 0.010552860175569853\n",
      "[LOG 20200502-14:50:56] epoch: 15379 train-loss: 0.010552871454921033\n",
      "[LOG 20200502-14:50:57] epoch: 15380 train-loss: 0.010552882734272215\n",
      "[LOG 20200502-14:50:57] epoch: 15381 train-loss: 0.01055289525538683\n",
      "[LOG 20200502-14:50:57] epoch: 15382 train-loss: 0.010552906948659155\n",
      "[LOG 20200502-14:50:57] epoch: 15383 train-loss: 0.01055291946977377\n",
      "[LOG 20200502-14:50:58] epoch: 15384 train-loss: 0.010552932301329242\n",
      "[LOG 20200502-14:50:58] epoch: 15385 train-loss: 0.010552945132884715\n",
      "[LOG 20200502-14:50:58] epoch: 15386 train-loss: 0.01055295868880219\n",
      "[LOG 20200502-14:50:59] epoch: 15387 train-loss: 0.010552972969081666\n",
      "[LOG 20200502-14:50:59] epoch: 15388 train-loss: 0.010552986628479429\n",
      "[LOG 20200502-14:50:59] epoch: 15389 train-loss: 0.010553001115719477\n",
      "[LOG 20200502-14:50:59] epoch: 15390 train-loss: 0.01055301601688067\n",
      "[LOG 20200502-14:50:59] epoch: 15391 train-loss: 0.010553030400640435\n",
      "[LOG 20200502-14:51:00] epoch: 15392 train-loss: 0.010553044577439627\n",
      "[LOG 20200502-14:51:00] epoch: 15393 train-loss: 0.010553059685561392\n",
      "[LOG 20200502-14:51:00] epoch: 15394 train-loss: 0.01055307500064373\n",
      "[LOG 20200502-14:51:00] epoch: 15395 train-loss: 0.01055309052268664\n",
      "[LOG 20200502-14:51:01] epoch: 15396 train-loss: 0.01055310552732812\n",
      "[LOG 20200502-14:51:01] epoch: 15397 train-loss: 0.01055312001456817\n",
      "[LOG 20200502-14:51:01] epoch: 15398 train-loss: 0.010553135640091367\n",
      "[LOG 20200502-14:51:01] epoch: 15399 train-loss: 0.010553150955173705\n",
      "[LOG 20200502-14:51:02] epoch: 15400 train-loss: 0.01055316554589404\n",
      "[LOG 20200502-14:51:02] epoch: 15401 train-loss: 0.010553180343574949\n",
      "[LOG 20200502-14:51:02] epoch: 15402 train-loss: 0.010553194934295284\n",
      "[LOG 20200502-14:51:02] epoch: 15403 train-loss: 0.010553209525015619\n",
      "[LOG 20200502-14:51:03] epoch: 15404 train-loss: 0.010553223494854238\n",
      "[LOG 20200502-14:51:03] epoch: 15405 train-loss: 0.010553237568173144\n",
      "[LOG 20200502-14:51:03] epoch: 15406 train-loss: 0.01055325184845262\n",
      "[LOG 20200502-14:51:03] epoch: 15407 train-loss: 0.010553265507850382\n",
      "[LOG 20200502-14:51:04] epoch: 15408 train-loss: 0.010553278753326999\n",
      "[LOG 20200502-14:51:04] epoch: 15409 train-loss: 0.010553291584882472\n",
      "[LOG 20200502-14:51:04] epoch: 15410 train-loss: 0.010553304623398516\n",
      "[LOG 20200502-14:51:04] epoch: 15411 train-loss: 0.010553317041032843\n",
      "[LOG 20200502-14:51:04] epoch: 15412 train-loss: 0.010553329665627744\n",
      "[LOG 20200502-14:51:05] epoch: 15413 train-loss: 0.010553340427577496\n",
      "[LOG 20200502-14:51:05] epoch: 15414 train-loss: 0.010553352638251252\n",
      "[LOG 20200502-14:51:05] epoch: 15415 train-loss: 0.01055336402108272\n",
      "[LOG 20200502-14:51:06] epoch: 15416 train-loss: 0.010553375093473328\n",
      "[LOG 20200502-14:51:06] epoch: 15417 train-loss: 0.010553385544982221\n",
      "[LOG 20200502-14:51:06] epoch: 15418 train-loss: 0.010553396203451686\n",
      "[LOG 20200502-14:51:06] epoch: 15419 train-loss: 0.010553406758440865\n",
      "[LOG 20200502-14:51:06] epoch: 15420 train-loss: 0.010553416796028614\n",
      "[LOG 20200502-14:51:07] epoch: 15421 train-loss: 0.01055342610925436\n",
      "[LOG 20200502-14:51:07] epoch: 15422 train-loss: 0.01055343666424354\n",
      "[LOG 20200502-14:51:07] epoch: 15423 train-loss: 0.010553446598351002\n",
      "[LOG 20200502-14:51:07] epoch: 15424 train-loss: 0.010553456015057035\n",
      "[LOG 20200502-14:51:08] epoch: 15425 train-loss: 0.010553465742203925\n",
      "[LOG 20200502-14:51:08] epoch: 15426 train-loss: 0.010553475365870528\n",
      "[LOG 20200502-14:51:08] epoch: 15427 train-loss: 0.010553484472135702\n",
      "[LOG 20200502-14:51:08] epoch: 15428 train-loss: 0.010553493888841735\n",
      "[LOG 20200502-14:51:09] epoch: 15429 train-loss: 0.010553503719468912\n",
      "[LOG 20200502-14:51:09] epoch: 15430 train-loss: 0.010553513136174943\n",
      "[LOG 20200502-14:51:09] epoch: 15431 train-loss: 0.01055352244940069\n",
      "[LOG 20200502-14:51:09] epoch: 15432 train-loss: 0.010553532383508153\n",
      "[LOG 20200502-14:51:10] epoch: 15433 train-loss: 0.0105535424210959\n",
      "[LOG 20200502-14:51:10] epoch: 15434 train-loss: 0.010553552148242792\n",
      "[LOG 20200502-14:51:10] epoch: 15435 train-loss: 0.010553562496271398\n",
      "[LOG 20200502-14:51:10] epoch: 15436 train-loss: 0.010553572533859147\n",
      "[LOG 20200502-14:51:11] epoch: 15437 train-loss: 0.010553582467966609\n",
      "[LOG 20200502-14:51:11] epoch: 15438 train-loss: 0.010553592919475503\n",
      "[LOG 20200502-14:51:11] epoch: 15439 train-loss: 0.010553603577944968\n",
      "[LOG 20200502-14:51:11] epoch: 15440 train-loss: 0.010553613925973574\n",
      "[LOG 20200502-14:51:12] epoch: 15441 train-loss: 0.01055362458444304\n",
      "[LOG 20200502-14:51:12] epoch: 15442 train-loss: 0.010553635139432218\n",
      "[LOG 20200502-14:51:12] epoch: 15443 train-loss: 0.010553645797901683\n",
      "[LOG 20200502-14:51:12] epoch: 15444 train-loss: 0.010553656766812006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:51:13] epoch: 15445 train-loss: 0.010553668149643473\n",
      "[LOG 20200502-14:51:13] epoch: 15446 train-loss: 0.010553679325514369\n",
      "[LOG 20200502-14:51:13] epoch: 15447 train-loss: 0.010553689983983835\n",
      "[LOG 20200502-14:51:13] epoch: 15448 train-loss: 0.010553701470295588\n",
      "[LOG 20200502-14:51:13] epoch: 15449 train-loss: 0.010553712335725626\n",
      "[LOG 20200502-14:51:14] epoch: 15450 train-loss: 0.010553723511596521\n",
      "[LOG 20200502-14:51:14] epoch: 15451 train-loss: 0.010553734687467417\n",
      "[LOG 20200502-14:51:14] epoch: 15452 train-loss: 0.010553745552897453\n",
      "[LOG 20200502-14:51:14] epoch: 15453 train-loss: 0.010553756625288062\n",
      "[LOG 20200502-14:51:15] epoch: 15454 train-loss: 0.010553767283757528\n",
      "[LOG 20200502-14:51:15] epoch: 15455 train-loss: 0.010553778666588996\n",
      "[LOG 20200502-14:51:15] epoch: 15456 train-loss: 0.010553789532019032\n",
      "[LOG 20200502-14:51:15] epoch: 15457 train-loss: 0.010553799983527925\n",
      "[LOG 20200502-14:51:16] epoch: 15458 train-loss: 0.010553810745477676\n",
      "[LOG 20200502-14:51:16] epoch: 15459 train-loss: 0.010553820990025997\n",
      "[LOG 20200502-14:51:16] epoch: 15460 train-loss: 0.010553831234574318\n",
      "[LOG 20200502-14:51:16] epoch: 15461 train-loss: 0.010553841375642352\n",
      "[LOG 20200502-14:51:17] epoch: 15462 train-loss: 0.010553851516710388\n",
      "[LOG 20200502-14:51:17] epoch: 15463 train-loss: 0.010553860726455847\n",
      "[LOG 20200502-14:51:17] epoch: 15464 train-loss: 0.010553870350122452\n",
      "[LOG 20200502-14:51:17] epoch: 15465 train-loss: 0.010553880077269342\n",
      "[LOG 20200502-14:51:18] epoch: 15466 train-loss: 0.010553888976573944\n",
      "[LOG 20200502-14:51:18] epoch: 15467 train-loss: 0.010553897875878546\n",
      "[LOG 20200502-14:51:18] epoch: 15468 train-loss: 0.010553906775183149\n",
      "[LOG 20200502-14:51:18] epoch: 15469 train-loss: 0.010553914536204603\n",
      "[LOG 20200502-14:51:19] epoch: 15470 train-loss: 0.010553923021588061\n",
      "[LOG 20200502-14:51:19] epoch: 15471 train-loss: 0.010553931300010946\n",
      "[LOG 20200502-14:51:19] epoch: 15472 train-loss: 0.010553939267992973\n",
      "[LOG 20200502-14:51:19] epoch: 15473 train-loss: 0.010553947235975001\n",
      "[LOG 20200502-14:51:19] epoch: 15474 train-loss: 0.010553953962193595\n",
      "[LOG 20200502-14:51:20] epoch: 15475 train-loss: 0.01055396120581362\n",
      "[LOG 20200502-14:51:20] epoch: 15476 train-loss: 0.010553968449433645\n",
      "[LOG 20200502-14:51:20] epoch: 15477 train-loss: 0.010553975589573383\n",
      "[LOG 20200502-14:51:20] epoch: 15478 train-loss: 0.010553982212311692\n",
      "[LOG 20200502-14:51:21] epoch: 15479 train-loss: 0.01055398966289229\n",
      "[LOG 20200502-14:51:21] epoch: 15480 train-loss: 0.010553996182150312\n",
      "[LOG 20200502-14:51:21] epoch: 15481 train-loss: 0.010554002804888619\n",
      "[LOG 20200502-14:51:21] epoch: 15482 train-loss: 0.0105540096345875\n",
      "[LOG 20200502-14:51:22] epoch: 15483 train-loss: 0.010554016567766666\n",
      "[LOG 20200502-14:51:22] epoch: 15484 train-loss: 0.010554023190504976\n",
      "[LOG 20200502-14:51:22] epoch: 15485 train-loss: 0.010554030020203855\n",
      "[LOG 20200502-14:51:22] epoch: 15486 train-loss: 0.010554037056863308\n",
      "[LOG 20200502-14:51:23] epoch: 15487 train-loss: 0.010554044093522761\n",
      "[LOG 20200502-14:51:23] epoch: 15488 train-loss: 0.010554051026701927\n",
      "[LOG 20200502-14:51:23] epoch: 15489 train-loss: 0.010554058787723383\n",
      "[LOG 20200502-14:51:23] epoch: 15490 train-loss: 0.010554066652225124\n",
      "[LOG 20200502-14:51:23] epoch: 15491 train-loss: 0.010554074930648008\n",
      "[LOG 20200502-14:51:24] epoch: 15492 train-loss: 0.010554083002110323\n",
      "[LOG 20200502-14:51:24] epoch: 15493 train-loss: 0.010554091901414923\n",
      "[LOG 20200502-14:51:24] epoch: 15494 train-loss: 0.010554100283318095\n",
      "[LOG 20200502-14:51:25] epoch: 15495 train-loss: 0.010554109493063556\n",
      "[LOG 20200502-14:51:25] epoch: 15496 train-loss: 0.010554119427171018\n",
      "[LOG 20200502-14:51:25] epoch: 15497 train-loss: 0.010554129050837623\n",
      "[LOG 20200502-14:51:25] epoch: 15498 train-loss: 0.010554139709307088\n",
      "[LOG 20200502-14:51:25] epoch: 15499 train-loss: 0.010554150471256839\n",
      "[LOG 20200502-14:51:26] epoch: 15500 train-loss: 0.01055416175060802\n",
      "[LOG 20200502-14:51:26] epoch: 15501 train-loss: 0.01055417334040006\n",
      "[LOG 20200502-14:51:26] epoch: 15502 train-loss: 0.010554185240632959\n",
      "[LOG 20200502-14:51:26] epoch: 15503 train-loss: 0.010554197865227858\n",
      "[LOG 20200502-14:51:27] epoch: 15504 train-loss: 0.010554210903743902\n",
      "[LOG 20200502-14:51:27] epoch: 15505 train-loss: 0.010554223528338803\n",
      "[LOG 20200502-14:51:27] epoch: 15506 train-loss: 0.010554237705137994\n",
      "[LOG 20200502-14:51:27] epoch: 15507 train-loss: 0.010554251364535756\n",
      "[LOG 20200502-14:51:28] epoch: 15508 train-loss: 0.010554265334374376\n",
      "[LOG 20200502-14:51:28] epoch: 15509 train-loss: 0.010554280028574996\n",
      "[LOG 20200502-14:51:28] epoch: 15510 train-loss: 0.01055429492973619\n",
      "[LOG 20200502-14:51:28] epoch: 15511 train-loss: 0.010554310037857957\n",
      "[LOG 20200502-14:51:29] epoch: 15512 train-loss: 0.010554324939019151\n",
      "[LOG 20200502-14:51:29] epoch: 15513 train-loss: 0.01055434077150292\n",
      "[LOG 20200502-14:51:29] epoch: 15514 train-loss: 0.01055435681094726\n",
      "[LOG 20200502-14:51:29] epoch: 15515 train-loss: 0.0105543728503916\n",
      "[LOG 20200502-14:51:30] epoch: 15516 train-loss: 0.010554388475914797\n",
      "[LOG 20200502-14:51:30] epoch: 15517 train-loss: 0.010554405032760568\n",
      "[LOG 20200502-14:51:30] epoch: 15518 train-loss: 0.010554420865244336\n",
      "[LOG 20200502-14:51:30] epoch: 15519 train-loss: 0.010554437215129534\n",
      "[LOG 20200502-14:51:30] epoch: 15520 train-loss: 0.010554453771975305\n",
      "[LOG 20200502-14:51:31] epoch: 15521 train-loss: 0.010554470432301363\n",
      "[LOG 20200502-14:51:31] epoch: 15522 train-loss: 0.010554485954344273\n",
      "[LOG 20200502-14:51:31] epoch: 15523 train-loss: 0.0105545020972689\n",
      "[LOG 20200502-14:51:31] epoch: 15524 train-loss: 0.01055451813671324\n",
      "[LOG 20200502-14:51:32] epoch: 15525 train-loss: 0.010554534590078725\n",
      "[LOG 20200502-14:51:32] epoch: 15526 train-loss: 0.010554549905161062\n",
      "[LOG 20200502-14:51:32] epoch: 15527 train-loss: 0.010554565323723687\n",
      "[LOG 20200502-14:51:32] epoch: 15528 train-loss: 0.010554580845766597\n",
      "[LOG 20200502-14:51:33] epoch: 15529 train-loss: 0.010554596264329221\n",
      "[LOG 20200502-14:51:33] epoch: 15530 train-loss: 0.01055461106201013\n",
      "[LOG 20200502-14:51:33] epoch: 15531 train-loss: 0.010554625859691037\n",
      "[LOG 20200502-14:51:33] epoch: 15532 train-loss: 0.010554639519088797\n",
      "[LOG 20200502-14:51:34] epoch: 15533 train-loss: 0.010554654006328847\n",
      "[LOG 20200502-14:51:34] epoch: 15534 train-loss: 0.010554667562246323\n",
      "[LOG 20200502-14:51:34] epoch: 15535 train-loss: 0.010554681221644083\n",
      "[LOG 20200502-14:51:34] epoch: 15536 train-loss: 0.01055469394971927\n",
      "[LOG 20200502-14:51:35] epoch: 15537 train-loss: 0.01055470740215646\n",
      "[LOG 20200502-14:51:35] epoch: 15538 train-loss: 0.010554719923271073\n",
      "[LOG 20200502-14:51:35] epoch: 15539 train-loss: 0.010554732237425115\n",
      "[LOG 20200502-14:51:35] epoch: 15540 train-loss: 0.010554743827217154\n",
      "[LOG 20200502-14:51:36] epoch: 15541 train-loss: 0.010554755106568336\n",
      "[LOG 20200502-14:51:36] epoch: 15542 train-loss: 0.010554767213761806\n",
      "[LOG 20200502-14:51:36] epoch: 15543 train-loss: 0.010554778493112989\n",
      "[LOG 20200502-14:51:36] epoch: 15544 train-loss: 0.010554789565503597\n",
      "[LOG 20200502-14:51:37] epoch: 15545 train-loss: 0.010554801155295637\n",
      "[LOG 20200502-14:51:37] epoch: 15546 train-loss: 0.010554811192883385\n",
      "[LOG 20200502-14:51:37] epoch: 15547 train-loss: 0.010554822161793709\n",
      "[LOG 20200502-14:51:37] epoch: 15548 train-loss: 0.010554832820263173\n",
      "[LOG 20200502-14:51:38] epoch: 15549 train-loss: 0.010554843375252353\n",
      "[LOG 20200502-14:51:38] epoch: 15550 train-loss: 0.010554854344162676\n",
      "[LOG 20200502-14:51:38] epoch: 15551 train-loss: 0.010554863760868708\n",
      "[LOG 20200502-14:51:38] epoch: 15552 train-loss: 0.010554874729779031\n",
      "[LOG 20200502-14:51:39] epoch: 15553 train-loss: 0.01055488528476821\n",
      "[LOG 20200502-14:51:39] epoch: 15554 train-loss: 0.010554896046717962\n",
      "[LOG 20200502-14:51:39] epoch: 15555 train-loss: 0.01055490660170714\n",
      "[LOG 20200502-14:51:39] epoch: 15556 train-loss: 0.010554916949735748\n",
      "[LOG 20200502-14:51:40] epoch: 15557 train-loss: 0.010554927504724927\n",
      "[LOG 20200502-14:51:40] epoch: 15558 train-loss: 0.010554938887556395\n",
      "[LOG 20200502-14:51:40] epoch: 15559 train-loss: 0.01055495006342729\n",
      "[LOG 20200502-14:51:40] epoch: 15560 train-loss: 0.01055496134277847\n",
      "[LOG 20200502-14:51:41] epoch: 15561 train-loss: 0.01055497241516908\n",
      "[LOG 20200502-14:51:41] epoch: 15562 train-loss: 0.010554984211921692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:51:41] epoch: 15563 train-loss: 0.01055499559475316\n",
      "[LOG 20200502-14:51:41] epoch: 15564 train-loss: 0.010555006977584627\n",
      "[LOG 20200502-14:51:42] epoch: 15565 train-loss: 0.010555018567376666\n",
      "[LOG 20200502-14:51:42] epoch: 15566 train-loss: 0.01055503057108985\n",
      "[LOG 20200502-14:51:42] epoch: 15567 train-loss: 0.010555042471322749\n",
      "[LOG 20200502-14:51:42] epoch: 15568 train-loss: 0.010555054164595075\n",
      "[LOG 20200502-14:51:43] epoch: 15569 train-loss: 0.010555066685709689\n",
      "[LOG 20200502-14:51:43] epoch: 15570 train-loss: 0.010555078689422872\n",
      "[LOG 20200502-14:51:43] epoch: 15571 train-loss: 0.01055509058965577\n",
      "[LOG 20200502-14:51:43] epoch: 15572 train-loss: 0.010555102489888668\n",
      "[LOG 20200502-14:51:43] epoch: 15573 train-loss: 0.010555114907522997\n",
      "[LOG 20200502-14:51:44] epoch: 15574 train-loss: 0.010555126704275608\n",
      "[LOG 20200502-14:51:44] epoch: 15575 train-loss: 0.010555139121909937\n",
      "[LOG 20200502-14:51:44] epoch: 15576 train-loss: 0.010555151022142835\n",
      "[LOG 20200502-14:51:44] epoch: 15577 train-loss: 0.010555163129336305\n",
      "[LOG 20200502-14:51:45] epoch: 15578 train-loss: 0.010555175133049488\n",
      "[LOG 20200502-14:51:45] epoch: 15579 train-loss: 0.010555186619361242\n",
      "[LOG 20200502-14:51:45] epoch: 15580 train-loss: 0.010555198209153282\n",
      "[LOG 20200502-14:51:45] epoch: 15581 train-loss: 0.01055520979894532\n",
      "[LOG 20200502-14:51:46] epoch: 15582 train-loss: 0.010555221078296503\n",
      "[LOG 20200502-14:51:46] epoch: 15583 train-loss: 0.010555232150687112\n",
      "[LOG 20200502-14:51:46] epoch: 15584 train-loss: 0.010555243016117148\n",
      "[LOG 20200502-14:51:46] epoch: 15585 train-loss: 0.01055525429546833\n",
      "[LOG 20200502-14:51:47] epoch: 15586 train-loss: 0.010555265160898367\n",
      "[LOG 20200502-14:51:47] epoch: 15587 train-loss: 0.01055527509500583\n",
      "[LOG 20200502-14:51:47] epoch: 15588 train-loss: 0.010555285546514723\n",
      "[LOG 20200502-14:51:47] epoch: 15589 train-loss: 0.010555295066701042\n",
      "[LOG 20200502-14:51:48] epoch: 15590 train-loss: 0.010555305414729647\n",
      "[LOG 20200502-14:51:48] epoch: 15591 train-loss: 0.010555315038396252\n",
      "[LOG 20200502-14:51:48] epoch: 15592 train-loss: 0.010555324351621998\n",
      "[LOG 20200502-14:51:48] epoch: 15593 train-loss: 0.010555333457887173\n",
      "[LOG 20200502-14:51:48] epoch: 15594 train-loss: 0.010555342564152347\n",
      "[LOG 20200502-14:51:49] epoch: 15595 train-loss: 0.01055535063561466\n",
      "[LOG 20200502-14:51:49] epoch: 15596 train-loss: 0.010555358914037546\n",
      "[LOG 20200502-14:51:49] epoch: 15597 train-loss: 0.010555367295940718\n",
      "[LOG 20200502-14:51:49] epoch: 15598 train-loss: 0.01055537567784389\n",
      "[LOG 20200502-14:51:50] epoch: 15599 train-loss: 0.0105553830249442\n",
      "[LOG 20200502-14:51:50] epoch: 15600 train-loss: 0.010555391096406512\n",
      "[LOG 20200502-14:51:50] epoch: 15601 train-loss: 0.010555398650467396\n",
      "[LOG 20200502-14:51:50] epoch: 15602 train-loss: 0.010555406308008565\n",
      "[LOG 20200502-14:51:51] epoch: 15603 train-loss: 0.010555413862069448\n",
      "[LOG 20200502-14:51:51] epoch: 15604 train-loss: 0.010555421209169758\n",
      "[LOG 20200502-14:51:51] epoch: 15605 train-loss: 0.010555428866710927\n",
      "[LOG 20200502-14:51:51] epoch: 15606 train-loss: 0.01055543590337038\n",
      "[LOG 20200502-14:51:52] epoch: 15607 train-loss: 0.010555443353950977\n",
      "[LOG 20200502-14:51:52] epoch: 15608 train-loss: 0.010555450597571002\n",
      "[LOG 20200502-14:51:52] epoch: 15609 train-loss: 0.010555458669033315\n",
      "[LOG 20200502-14:51:52] epoch: 15610 train-loss: 0.010555466223094199\n",
      "[LOG 20200502-14:51:53] epoch: 15611 train-loss: 0.010555474398036798\n",
      "[LOG 20200502-14:51:53] epoch: 15612 train-loss: 0.01055548226253854\n",
      "[LOG 20200502-14:51:53] epoch: 15613 train-loss: 0.01055549064444171\n",
      "[LOG 20200502-14:51:53] epoch: 15614 train-loss: 0.010555499543746313\n",
      "[LOG 20200502-14:51:54] epoch: 15615 train-loss: 0.010555508132610057\n",
      "[LOG 20200502-14:51:54] epoch: 15616 train-loss: 0.010555517445835803\n",
      "[LOG 20200502-14:51:54] epoch: 15617 train-loss: 0.010555527172982693\n",
      "[LOG 20200502-14:51:54] epoch: 15618 train-loss: 0.010555536589688726\n",
      "[LOG 20200502-14:51:55] epoch: 15619 train-loss: 0.010555547351638475\n",
      "[LOG 20200502-14:51:55] epoch: 15620 train-loss: 0.010555558113588227\n",
      "[LOG 20200502-14:51:55] epoch: 15621 train-loss: 0.010555569289459122\n",
      "[LOG 20200502-14:51:55] epoch: 15622 train-loss: 0.010555580465330018\n",
      "[LOG 20200502-14:51:56] epoch: 15623 train-loss: 0.010555592262082629\n",
      "[LOG 20200502-14:51:56] epoch: 15624 train-loss: 0.01055560488667753\n",
      "[LOG 20200502-14:51:56] epoch: 15625 train-loss: 0.010555617097351287\n",
      "[LOG 20200502-14:51:56] epoch: 15626 train-loss: 0.010555630860229334\n",
      "[LOG 20200502-14:51:56] epoch: 15627 train-loss: 0.010555643898745378\n",
      "[LOG 20200502-14:51:57] epoch: 15628 train-loss: 0.010555657972064283\n",
      "[LOG 20200502-14:51:57] epoch: 15629 train-loss: 0.01055567225234376\n",
      "[LOG 20200502-14:51:57] epoch: 15630 train-loss: 0.010555686946544383\n",
      "[LOG 20200502-14:51:57] epoch: 15631 train-loss: 0.010555701847705577\n",
      "[LOG 20200502-14:51:58] epoch: 15632 train-loss: 0.010555716541906198\n",
      "[LOG 20200502-14:51:58] epoch: 15633 train-loss: 0.010555732684830824\n",
      "[LOG 20200502-14:51:58] epoch: 15634 train-loss: 0.010555748724275164\n",
      "[LOG 20200502-14:51:58] epoch: 15635 train-loss: 0.010555764867199792\n",
      "[LOG 20200502-14:51:59] epoch: 15636 train-loss: 0.010555781113604704\n",
      "[LOG 20200502-14:51:59] epoch: 15637 train-loss: 0.010555797877411047\n",
      "[LOG 20200502-14:51:59] epoch: 15638 train-loss: 0.010555814434256818\n",
      "[LOG 20200502-14:51:59] epoch: 15639 train-loss: 0.010555831715464592\n",
      "[LOG 20200502-14:52:00] epoch: 15640 train-loss: 0.010555848272310363\n",
      "[LOG 20200502-14:52:00] epoch: 15641 train-loss: 0.01055586545003785\n",
      "[LOG 20200502-14:52:00] epoch: 15642 train-loss: 0.01055588231732448\n",
      "[LOG 20200502-14:52:00] epoch: 15643 train-loss: 0.010555898874170251\n",
      "[LOG 20200502-14:52:01] epoch: 15644 train-loss: 0.01055591677625974\n",
      "[LOG 20200502-14:52:01] epoch: 15645 train-loss: 0.010555934057467513\n",
      "[LOG 20200502-14:52:01] epoch: 15646 train-loss: 0.01055595071779357\n",
      "[LOG 20200502-14:52:01] epoch: 15647 train-loss: 0.01055596706767877\n",
      "[LOG 20200502-14:52:02] epoch: 15648 train-loss: 0.010555984038445685\n",
      "[LOG 20200502-14:52:02] epoch: 15649 train-loss: 0.010556000284850597\n",
      "[LOG 20200502-14:52:02] epoch: 15650 train-loss: 0.010556016945176654\n",
      "[LOG 20200502-14:52:02] epoch: 15651 train-loss: 0.010556032881140709\n",
      "[LOG 20200502-14:52:03] epoch: 15652 train-loss: 0.010556048092742762\n",
      "[LOG 20200502-14:52:03] epoch: 15653 train-loss: 0.010556064028706815\n",
      "[LOG 20200502-14:52:03] epoch: 15654 train-loss: 0.010556079136828581\n",
      "[LOG 20200502-14:52:03] epoch: 15655 train-loss: 0.01055609393450949\n",
      "[LOG 20200502-14:52:04] epoch: 15656 train-loss: 0.010556108835670684\n",
      "[LOG 20200502-14:52:04] epoch: 15657 train-loss: 0.010556123529871305\n",
      "[LOG 20200502-14:52:04] epoch: 15658 train-loss: 0.010556137499709925\n",
      "[LOG 20200502-14:52:04] epoch: 15659 train-loss: 0.010556151055627398\n",
      "[LOG 20200502-14:52:04] epoch: 15660 train-loss: 0.010556164301104017\n",
      "[LOG 20200502-14:52:05] epoch: 15661 train-loss: 0.010556177650060918\n",
      "[LOG 20200502-14:52:05] epoch: 15662 train-loss: 0.01055618996421496\n",
      "[LOG 20200502-14:52:05] epoch: 15663 train-loss: 0.010556202795770433\n",
      "[LOG 20200502-14:52:05] epoch: 15664 train-loss: 0.010556214902963903\n",
      "[LOG 20200502-14:52:06] epoch: 15665 train-loss: 0.01055622711363766\n",
      "[LOG 20200502-14:52:06] epoch: 15666 train-loss: 0.010556238703429699\n",
      "[LOG 20200502-14:52:06] epoch: 15667 train-loss: 0.010556250396702025\n",
      "[LOG 20200502-14:52:06] epoch: 15668 train-loss: 0.010556261676053206\n",
      "[LOG 20200502-14:52:07] epoch: 15669 train-loss: 0.010556273265845247\n",
      "[LOG 20200502-14:52:07] epoch: 15670 train-loss: 0.010556284338235855\n",
      "[LOG 20200502-14:52:07] epoch: 15671 train-loss: 0.010556295203665892\n",
      "[LOG 20200502-14:52:07] epoch: 15672 train-loss: 0.010556306379536787\n",
      "[LOG 20200502-14:52:08] epoch: 15673 train-loss: 0.010556317451927397\n",
      "[LOG 20200502-14:52:08] epoch: 15674 train-loss: 0.01055632842083772\n",
      "[LOG 20200502-14:52:08] epoch: 15675 train-loss: 0.010556339389748044\n",
      "[LOG 20200502-14:52:08] epoch: 15676 train-loss: 0.01055635056561894\n",
      "[LOG 20200502-14:52:09] epoch: 15677 train-loss: 0.01055636132756869\n",
      "[LOG 20200502-14:52:09] epoch: 15678 train-loss: 0.010556372710400157\n",
      "[LOG 20200502-14:52:09] epoch: 15679 train-loss: 0.010556383472349908\n",
      "[LOG 20200502-14:52:09] epoch: 15680 train-loss: 0.010556395165622234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:52:10] epoch: 15681 train-loss: 0.010556406755414274\n",
      "[LOG 20200502-14:52:10] epoch: 15682 train-loss: 0.010556417517364025\n",
      "[LOG 20200502-14:52:10] epoch: 15683 train-loss: 0.010556429624557495\n",
      "[LOG 20200502-14:52:10] epoch: 15684 train-loss: 0.01055644111086925\n",
      "[LOG 20200502-14:52:11] epoch: 15685 train-loss: 0.01055645290762186\n",
      "[LOG 20200502-14:52:11] epoch: 15686 train-loss: 0.010556465118295617\n",
      "[LOG 20200502-14:52:11] epoch: 15687 train-loss: 0.010556477018528514\n",
      "[LOG 20200502-14:52:11] epoch: 15688 train-loss: 0.010556489436162842\n",
      "[LOG 20200502-14:52:12] epoch: 15689 train-loss: 0.010556501232915454\n",
      "[LOG 20200502-14:52:12] epoch: 15690 train-loss: 0.010556513857510354\n",
      "[LOG 20200502-14:52:12] epoch: 15691 train-loss: 0.010556526275144683\n",
      "[LOG 20200502-14:52:12] epoch: 15692 train-loss: 0.010556538796259297\n",
      "[LOG 20200502-14:52:13] epoch: 15693 train-loss: 0.01055655162781477\n",
      "[LOG 20200502-14:52:13] epoch: 15694 train-loss: 0.010556564355889956\n",
      "[LOG 20200502-14:52:13] epoch: 15695 train-loss: 0.01055657635960314\n",
      "[LOG 20200502-14:52:13] epoch: 15696 train-loss: 0.010556589191158613\n",
      "[LOG 20200502-14:52:13] epoch: 15697 train-loss: 0.01055660160879294\n",
      "[LOG 20200502-14:52:14] epoch: 15698 train-loss: 0.01055661371598641\n",
      "[LOG 20200502-14:52:14] epoch: 15699 train-loss: 0.010556627064943314\n",
      "[LOG 20200502-14:52:14] epoch: 15700 train-loss: 0.010556638861695925\n",
      "[LOG 20200502-14:52:14] epoch: 15701 train-loss: 0.010556650968889395\n",
      "[LOG 20200502-14:52:15] epoch: 15702 train-loss: 0.010556663593484296\n",
      "[LOG 20200502-14:52:15] epoch: 15703 train-loss: 0.010556675493717194\n",
      "[LOG 20200502-14:52:15] epoch: 15704 train-loss: 0.010556686980028948\n",
      "[LOG 20200502-14:52:15] epoch: 15705 train-loss: 0.010556698673301272\n",
      "[LOG 20200502-14:52:16] epoch: 15706 train-loss: 0.010556709745691882\n",
      "[LOG 20200502-14:52:16] epoch: 15707 train-loss: 0.01055672164592478\n",
      "[LOG 20200502-14:52:16] epoch: 15708 train-loss: 0.010556732407874532\n",
      "[LOG 20200502-14:52:16] epoch: 15709 train-loss: 0.010556743273304569\n",
      "[LOG 20200502-14:52:17] epoch: 15710 train-loss: 0.010556753621333174\n",
      "[LOG 20200502-14:52:17] epoch: 15711 train-loss: 0.010556764590243498\n",
      "[LOG 20200502-14:52:17] epoch: 15712 train-loss: 0.010556773903469244\n",
      "[LOG 20200502-14:52:17] epoch: 15713 train-loss: 0.01055678456193871\n",
      "[LOG 20200502-14:52:18] epoch: 15714 train-loss: 0.010556793875164457\n",
      "[LOG 20200502-14:52:18] epoch: 15715 train-loss: 0.010556803188390203\n",
      "[LOG 20200502-14:52:18] epoch: 15716 train-loss: 0.010556812812056806\n",
      "[LOG 20200502-14:52:18] epoch: 15717 train-loss: 0.010556821090479692\n",
      "[LOG 20200502-14:52:19] epoch: 15718 train-loss: 0.010556829679343436\n",
      "[LOG 20200502-14:52:19] epoch: 15719 train-loss: 0.010556838371687464\n",
      "[LOG 20200502-14:52:19] epoch: 15720 train-loss: 0.010556845511827204\n",
      "[LOG 20200502-14:52:19] epoch: 15721 train-loss: 0.010556854100690948\n",
      "[LOG 20200502-14:52:20] epoch: 15722 train-loss: 0.010556862275633547\n",
      "[LOG 20200502-14:52:20] epoch: 15723 train-loss: 0.010556869415773286\n",
      "[LOG 20200502-14:52:20] epoch: 15724 train-loss: 0.01055687696983417\n",
      "[LOG 20200502-14:52:20] epoch: 15725 train-loss: 0.010556884937816195\n",
      "[LOG 20200502-14:52:21] epoch: 15726 train-loss: 0.010556891870995363\n",
      "[LOG 20200502-14:52:21] epoch: 15727 train-loss: 0.010556899114615388\n",
      "[LOG 20200502-14:52:21] epoch: 15728 train-loss: 0.01055690615127484\n",
      "[LOG 20200502-14:52:21] epoch: 15729 train-loss: 0.010556913601855436\n",
      "[LOG 20200502-14:52:22] epoch: 15730 train-loss: 0.010556920948955748\n",
      "[LOG 20200502-14:52:22] epoch: 15731 train-loss: 0.010556928399536345\n",
      "[LOG 20200502-14:52:22] epoch: 15732 train-loss: 0.010556935746636655\n",
      "[LOG 20200502-14:52:22] epoch: 15733 train-loss: 0.010556943611138396\n",
      "[LOG 20200502-14:52:22] epoch: 15734 train-loss: 0.010556951475640139\n",
      "[LOG 20200502-14:52:23] epoch: 15735 train-loss: 0.010556959547102451\n",
      "[LOG 20200502-14:52:23] epoch: 15736 train-loss: 0.010556968239446482\n",
      "[LOG 20200502-14:52:23] epoch: 15737 train-loss: 0.010556976621349653\n",
      "[LOG 20200502-14:52:24] epoch: 15738 train-loss: 0.010556985727614827\n",
      "[LOG 20200502-14:52:24] epoch: 15739 train-loss: 0.010556994937360287\n",
      "[LOG 20200502-14:52:24] epoch: 15740 train-loss: 0.010557004561026892\n",
      "[LOG 20200502-14:52:24] epoch: 15741 train-loss: 0.010557014702094926\n",
      "[LOG 20200502-14:52:24] epoch: 15742 train-loss: 0.010557025050123533\n",
      "[LOG 20200502-14:52:25] epoch: 15743 train-loss: 0.010557035501632426\n",
      "[LOG 20200502-14:52:25] epoch: 15744 train-loss: 0.010557046987944178\n",
      "[LOG 20200502-14:52:25] epoch: 15745 train-loss: 0.01055705857773622\n",
      "[LOG 20200502-14:52:25] epoch: 15746 train-loss: 0.010557070891890261\n",
      "[LOG 20200502-14:52:26] epoch: 15747 train-loss: 0.010557083619965447\n",
      "[LOG 20200502-14:52:26] epoch: 15748 train-loss: 0.010557096555001207\n",
      "[LOG 20200502-14:52:26] epoch: 15749 train-loss: 0.010557109696997536\n",
      "[LOG 20200502-14:52:26] epoch: 15750 train-loss: 0.010557123356395297\n",
      "[LOG 20200502-14:52:27] epoch: 15751 train-loss: 0.01055713753319449\n",
      "[LOG 20200502-14:52:27] epoch: 15752 train-loss: 0.010557152123914825\n",
      "[LOG 20200502-14:52:27] epoch: 15753 train-loss: 0.01055716723203659\n",
      "[LOG 20200502-14:52:27] epoch: 15754 train-loss: 0.010557182236678071\n",
      "[LOG 20200502-14:52:28] epoch: 15755 train-loss: 0.010557198379602697\n",
      "[LOG 20200502-14:52:28] epoch: 15756 train-loss: 0.010557213798165321\n",
      "[LOG 20200502-14:52:28] epoch: 15757 train-loss: 0.010557229734129377\n",
      "[LOG 20200502-14:52:28] epoch: 15758 train-loss: 0.010557246187494861\n",
      "[LOG 20200502-14:52:29] epoch: 15759 train-loss: 0.010557262847820917\n",
      "[LOG 20200502-14:52:29] epoch: 15760 train-loss: 0.01055727961162726\n",
      "[LOG 20200502-14:52:29] epoch: 15761 train-loss: 0.010557296685874462\n",
      "[LOG 20200502-14:52:29] epoch: 15762 train-loss: 0.010557313346200518\n",
      "[LOG 20200502-14:52:29] epoch: 15763 train-loss: 0.010557329799566004\n",
      "[LOG 20200502-14:52:30] epoch: 15764 train-loss: 0.01055734728773435\n",
      "[LOG 20200502-14:52:30] epoch: 15765 train-loss: 0.01055736436198155\n",
      "[LOG 20200502-14:52:30] epoch: 15766 train-loss: 0.010557381332748465\n",
      "[LOG 20200502-14:52:31] epoch: 15767 train-loss: 0.010557398096554808\n",
      "[LOG 20200502-14:52:31] epoch: 15768 train-loss: 0.010557415584723154\n",
      "[LOG 20200502-14:52:31] epoch: 15769 train-loss: 0.010557432348529497\n",
      "[LOG 20200502-14:52:31] epoch: 15770 train-loss: 0.010557448698414696\n",
      "[LOG 20200502-14:52:32] epoch: 15771 train-loss: 0.010557464530898465\n",
      "[LOG 20200502-14:52:32] epoch: 15772 train-loss: 0.010557481087744236\n",
      "[LOG 20200502-14:52:32] epoch: 15773 train-loss: 0.010557497127188576\n",
      "[LOG 20200502-14:52:32] epoch: 15774 train-loss: 0.010557512752711773\n",
      "[LOG 20200502-14:52:33] epoch: 15775 train-loss: 0.010557528792156113\n",
      "[LOG 20200502-14:52:33] epoch: 15776 train-loss: 0.010557543900277879\n",
      "[LOG 20200502-14:52:33] epoch: 15777 train-loss: 0.010557558180557357\n",
      "[LOG 20200502-14:52:33] epoch: 15778 train-loss: 0.010557572874757979\n",
      "[LOG 20200502-14:52:34] epoch: 15779 train-loss: 0.010557587155037455\n",
      "[LOG 20200502-14:52:34] epoch: 15780 train-loss: 0.01055760071095493\n",
      "[LOG 20200502-14:52:34] epoch: 15781 train-loss: 0.010557614059911834\n",
      "[LOG 20200502-14:52:34] epoch: 15782 train-loss: 0.010557626891467307\n",
      "[LOG 20200502-14:52:34] epoch: 15783 train-loss: 0.010557640550865067\n",
      "[LOG 20200502-14:52:35] epoch: 15784 train-loss: 0.010557652658058537\n",
      "[LOG 20200502-14:52:35] epoch: 15785 train-loss: 0.010557664765252007\n",
      "[LOG 20200502-14:52:35] epoch: 15786 train-loss: 0.010557676768965192\n",
      "[LOG 20200502-14:52:35] epoch: 15787 train-loss: 0.010557688565717803\n",
      "[LOG 20200502-14:52:36] epoch: 15788 train-loss: 0.010557700362470415\n",
      "[LOG 20200502-14:52:36] epoch: 15789 train-loss: 0.010557711331380738\n",
      "[LOG 20200502-14:52:36] epoch: 15790 train-loss: 0.010557721989850203\n",
      "[LOG 20200502-14:52:37] epoch: 15791 train-loss: 0.010557733269201385\n",
      "[LOG 20200502-14:52:37] epoch: 15792 train-loss: 0.010557743720710278\n",
      "[LOG 20200502-14:52:37] epoch: 15793 train-loss: 0.010557753861778311\n",
      "[LOG 20200502-14:52:37] epoch: 15794 train-loss: 0.010557764934168922\n",
      "[LOG 20200502-14:52:37] epoch: 15795 train-loss: 0.010557775075236956\n",
      "[LOG 20200502-14:52:38] epoch: 15796 train-loss: 0.010557785216304991\n",
      "[LOG 20200502-14:52:38] epoch: 15797 train-loss: 0.010557795874774456\n",
      "[LOG 20200502-14:52:38] epoch: 15798 train-loss: 0.010557806326283349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:52:38] epoch: 15799 train-loss: 0.010557817295193672\n",
      "[LOG 20200502-14:52:39] epoch: 15800 train-loss: 0.010557827229301134\n",
      "[LOG 20200502-14:52:39] epoch: 15801 train-loss: 0.010557838301691744\n",
      "[LOG 20200502-14:52:39] epoch: 15802 train-loss: 0.01055784864972035\n",
      "[LOG 20200502-14:52:39] epoch: 15803 train-loss: 0.010557859618630674\n",
      "[LOG 20200502-14:52:40] epoch: 15804 train-loss: 0.010557870587540997\n",
      "[LOG 20200502-14:52:40] epoch: 15805 train-loss: 0.010557881349490749\n",
      "[LOG 20200502-14:52:40] epoch: 15806 train-loss: 0.01055789262884193\n",
      "[LOG 20200502-14:52:40] epoch: 15807 train-loss: 0.01055790370123254\n",
      "[LOG 20200502-14:52:41] epoch: 15808 train-loss: 0.010557915394504866\n",
      "[LOG 20200502-14:52:41] epoch: 15809 train-loss: 0.010557927191257477\n",
      "[LOG 20200502-14:52:41] epoch: 15810 train-loss: 0.010557938574088944\n",
      "[LOG 20200502-14:52:41] epoch: 15811 train-loss: 0.010557949956920411\n",
      "[LOG 20200502-14:52:42] epoch: 15812 train-loss: 0.010557961753673024\n",
      "[LOG 20200502-14:52:42] epoch: 15813 train-loss: 0.010557973757386208\n",
      "[LOG 20200502-14:52:42] epoch: 15814 train-loss: 0.01055798576109939\n",
      "[LOG 20200502-14:52:42] epoch: 15815 train-loss: 0.01055799786829286\n",
      "[LOG 20200502-14:52:43] epoch: 15816 train-loss: 0.010558009872006046\n",
      "[LOG 20200502-14:52:43] epoch: 15817 train-loss: 0.010558021772238944\n",
      "[LOG 20200502-14:52:43] epoch: 15818 train-loss: 0.010558033362030983\n",
      "[LOG 20200502-14:52:43] epoch: 15819 train-loss: 0.010558045779665312\n",
      "[LOG 20200502-14:52:44] epoch: 15820 train-loss: 0.010558057472937636\n",
      "[LOG 20200502-14:52:44] epoch: 15821 train-loss: 0.010558069062729677\n",
      "[LOG 20200502-14:52:44] epoch: 15822 train-loss: 0.010558080652521716\n",
      "[LOG 20200502-14:52:44] epoch: 15823 train-loss: 0.010558092449274328\n",
      "[LOG 20200502-14:52:44] epoch: 15824 train-loss: 0.010558103935586082\n",
      "[LOG 20200502-14:52:45] epoch: 15825 train-loss: 0.010558115111456977\n",
      "[LOG 20200502-14:52:45] epoch: 15826 train-loss: 0.010558126908209588\n",
      "[LOG 20200502-14:52:45] epoch: 15827 train-loss: 0.010558136738836765\n",
      "[LOG 20200502-14:52:45] epoch: 15828 train-loss: 0.01055814791470766\n",
      "[LOG 20200502-14:52:46] epoch: 15829 train-loss: 0.010558158366216553\n",
      "[LOG 20200502-14:52:46] epoch: 15830 train-loss: 0.010558169335126877\n",
      "[LOG 20200502-14:52:46] epoch: 15831 train-loss: 0.010558178337911764\n",
      "[LOG 20200502-14:52:46] epoch: 15832 train-loss: 0.010558188065058656\n",
      "[LOG 20200502-14:52:47] epoch: 15833 train-loss: 0.010558197585244974\n",
      "[LOG 20200502-14:52:47] epoch: 15834 train-loss: 0.010558206588029861\n",
      "[LOG 20200502-14:52:47] epoch: 15835 train-loss: 0.010558214762972461\n",
      "[LOG 20200502-14:52:47] epoch: 15836 train-loss: 0.010558223765757348\n",
      "[LOG 20200502-14:52:48] epoch: 15837 train-loss: 0.010558231837219663\n",
      "[LOG 20200502-14:52:48] epoch: 15838 train-loss: 0.010558239701721404\n",
      "[LOG 20200502-14:52:48] epoch: 15839 train-loss: 0.010558247359262573\n",
      "[LOG 20200502-14:52:48] epoch: 15840 train-loss: 0.01055825449940231\n",
      "[LOG 20200502-14:52:49] epoch: 15841 train-loss: 0.010558261536061764\n",
      "[LOG 20200502-14:52:49] epoch: 15842 train-loss: 0.0105582679518395\n",
      "[LOG 20200502-14:52:49] epoch: 15843 train-loss: 0.010558274367617236\n",
      "[LOG 20200502-14:52:49] epoch: 15844 train-loss: 0.010558280472954115\n",
      "[LOG 20200502-14:52:50] epoch: 15845 train-loss: 0.01055828688873185\n",
      "[LOG 20200502-14:52:50] epoch: 15846 train-loss: 0.01055829268362787\n",
      "[LOG 20200502-14:52:50] epoch: 15847 train-loss: 0.01055829847852389\n",
      "[LOG 20200502-14:52:50] epoch: 15848 train-loss: 0.010558303962979052\n",
      "[LOG 20200502-14:52:50] epoch: 15849 train-loss: 0.010558309757875072\n",
      "[LOG 20200502-14:52:51] epoch: 15850 train-loss: 0.01055831503536966\n",
      "[LOG 20200502-14:52:51] epoch: 15851 train-loss: 0.010558320726785395\n",
      "[LOG 20200502-14:52:51] epoch: 15852 train-loss: 0.010558326521681415\n",
      "[LOG 20200502-14:52:51] epoch: 15853 train-loss: 0.010558332627018293\n",
      "[LOG 20200502-14:52:52] epoch: 15854 train-loss: 0.010558338111473454\n",
      "[LOG 20200502-14:52:52] epoch: 15855 train-loss: 0.010558344527251191\n",
      "[LOG 20200502-14:52:52] epoch: 15856 train-loss: 0.010558350632588068\n",
      "[LOG 20200502-14:52:52] epoch: 15857 train-loss: 0.010558357876208093\n",
      "[LOG 20200502-14:52:53] epoch: 15858 train-loss: 0.010558364705906974\n",
      "[LOG 20200502-14:52:53] epoch: 15859 train-loss: 0.01055837163908614\n",
      "[LOG 20200502-14:52:53] epoch: 15860 train-loss: 0.010558379503587881\n",
      "[LOG 20200502-14:52:53] epoch: 15861 train-loss: 0.010558387368089624\n",
      "[LOG 20200502-14:52:54] epoch: 15862 train-loss: 0.010558395646512508\n",
      "[LOG 20200502-14:52:54] epoch: 15863 train-loss: 0.010558404649297396\n",
      "[LOG 20200502-14:52:54] epoch: 15864 train-loss: 0.010558414272964\n",
      "[LOG 20200502-14:52:54] epoch: 15865 train-loss: 0.010558423068788316\n",
      "[LOG 20200502-14:52:55] epoch: 15866 train-loss: 0.010558433623777496\n",
      "[LOG 20200502-14:52:55] epoch: 15867 train-loss: 0.010558444075286388\n",
      "[LOG 20200502-14:52:55] epoch: 15868 train-loss: 0.010558455147676997\n",
      "[LOG 20200502-14:52:55] epoch: 15869 train-loss: 0.01055846694442961\n",
      "[LOG 20200502-14:52:55] epoch: 15870 train-loss: 0.010558478844662508\n",
      "[LOG 20200502-14:52:56] epoch: 15871 train-loss: 0.010558491365777122\n",
      "[LOG 20200502-14:52:56] epoch: 15872 train-loss: 0.01055850430081288\n",
      "[LOG 20200502-14:52:56] epoch: 15873 train-loss: 0.010558517028888067\n",
      "[LOG 20200502-14:52:57] epoch: 15874 train-loss: 0.01055853141264783\n",
      "[LOG 20200502-14:52:57] epoch: 15875 train-loss: 0.010558544968565306\n",
      "[LOG 20200502-14:52:57] epoch: 15876 train-loss: 0.010558559145364497\n",
      "[LOG 20200502-14:52:57] epoch: 15877 train-loss: 0.010558573736084832\n",
      "[LOG 20200502-14:52:57] epoch: 15878 train-loss: 0.010558589258127742\n",
      "[LOG 20200502-14:52:58] epoch: 15879 train-loss: 0.010558604573210081\n",
      "[LOG 20200502-14:52:58] epoch: 15880 train-loss: 0.010558619370890988\n",
      "[LOG 20200502-14:52:58] epoch: 15881 train-loss: 0.010558635099894471\n",
      "[LOG 20200502-14:52:58] epoch: 15882 train-loss: 0.010558650518457094\n",
      "[LOG 20200502-14:52:59] epoch: 15883 train-loss: 0.010558666557901435\n",
      "[LOG 20200502-14:52:59] epoch: 15884 train-loss: 0.010558682183424631\n",
      "[LOG 20200502-14:52:59] epoch: 15885 train-loss: 0.010558698740270402\n",
      "[LOG 20200502-14:52:59] epoch: 15886 train-loss: 0.010558714262313314\n",
      "[LOG 20200502-14:53:00] epoch: 15887 train-loss: 0.010558730612198511\n",
      "[LOG 20200502-14:53:00] epoch: 15888 train-loss: 0.010558746858603425\n",
      "[LOG 20200502-14:53:00] epoch: 15889 train-loss: 0.010558762691087194\n",
      "[LOG 20200502-14:53:00] epoch: 15890 train-loss: 0.010558778213130103\n",
      "[LOG 20200502-14:53:01] epoch: 15891 train-loss: 0.0105587938386533\n",
      "[LOG 20200502-14:53:01] epoch: 15892 train-loss: 0.01055880832589335\n",
      "[LOG 20200502-14:53:01] epoch: 15893 train-loss: 0.010558823951416545\n",
      "[LOG 20200502-14:53:01] epoch: 15894 train-loss: 0.010558838852577739\n",
      "[LOG 20200502-14:53:02] epoch: 15895 train-loss: 0.010558852822416358\n",
      "[LOG 20200502-14:53:02] epoch: 15896 train-loss: 0.010558867930538125\n",
      "[LOG 20200502-14:53:02] epoch: 15897 train-loss: 0.01055888169341617\n",
      "[LOG 20200502-14:53:02] epoch: 15898 train-loss: 0.010558895145853361\n",
      "[LOG 20200502-14:53:03] epoch: 15899 train-loss: 0.010558908805251122\n",
      "[LOG 20200502-14:53:03] epoch: 15900 train-loss: 0.01055892174028688\n",
      "[LOG 20200502-14:53:03] epoch: 15901 train-loss: 0.010558933950960636\n",
      "[LOG 20200502-14:53:03] epoch: 15902 train-loss: 0.010558946368594965\n",
      "[LOG 20200502-14:53:04] epoch: 15903 train-loss: 0.01055895806186729\n",
      "[LOG 20200502-14:53:04] epoch: 15904 train-loss: 0.01055896965165933\n",
      "[LOG 20200502-14:53:04] epoch: 15905 train-loss: 0.010558981034490798\n",
      "[LOG 20200502-14:53:04] epoch: 15906 train-loss: 0.010558991692960262\n",
      "[LOG 20200502-14:53:05] epoch: 15907 train-loss: 0.010559002144469155\n",
      "[LOG 20200502-14:53:05] epoch: 15908 train-loss: 0.010559012699458335\n",
      "[LOG 20200502-14:53:05] epoch: 15909 train-loss: 0.010559022116164366\n",
      "[LOG 20200502-14:53:05] epoch: 15910 train-loss: 0.010559031636350684\n",
      "[LOG 20200502-14:53:06] epoch: 15911 train-loss: 0.010559041053056717\n",
      "[LOG 20200502-14:53:06] epoch: 15912 train-loss: 0.010559050159321891\n",
      "[LOG 20200502-14:53:06] epoch: 15913 train-loss: 0.010559059058626493\n",
      "[LOG 20200502-14:53:06] epoch: 15914 train-loss: 0.010559068578812811\n",
      "[LOG 20200502-14:53:07] epoch: 15915 train-loss: 0.010559076753755411\n",
      "[LOG 20200502-14:53:07] epoch: 15916 train-loss: 0.010559085860020585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:53:07] epoch: 15917 train-loss: 0.010559093931482898\n",
      "[LOG 20200502-14:53:07] epoch: 15918 train-loss: 0.010559102727307213\n",
      "[LOG 20200502-14:53:07] epoch: 15919 train-loss: 0.010559110695289241\n",
      "[LOG 20200502-14:53:08] epoch: 15920 train-loss: 0.010559119698074129\n",
      "[LOG 20200502-14:53:08] epoch: 15921 train-loss: 0.010559128183457587\n",
      "[LOG 20200502-14:53:08] epoch: 15922 train-loss: 0.010559136979281902\n",
      "[LOG 20200502-14:53:08] epoch: 15923 train-loss: 0.010559145775106218\n",
      "[LOG 20200502-14:53:09] epoch: 15924 train-loss: 0.01055915467441082\n",
      "[LOG 20200502-14:53:09] epoch: 15925 train-loss: 0.010559163780675994\n",
      "[LOG 20200502-14:53:09] epoch: 15926 train-loss: 0.010559172990421454\n",
      "[LOG 20200502-14:53:09] epoch: 15927 train-loss: 0.010559182096686628\n",
      "[LOG 20200502-14:53:10] epoch: 15928 train-loss: 0.010559191616872946\n",
      "[LOG 20200502-14:53:10] epoch: 15929 train-loss: 0.010559201033578979\n",
      "[LOG 20200502-14:53:10] epoch: 15930 train-loss: 0.010559210553765297\n",
      "[LOG 20200502-14:53:11] epoch: 15931 train-loss: 0.010559220384392474\n",
      "[LOG 20200502-14:53:11] epoch: 15932 train-loss: 0.010559230111539364\n",
      "[LOG 20200502-14:53:11] epoch: 15933 train-loss: 0.010559240459567971\n",
      "[LOG 20200502-14:53:11] epoch: 15934 train-loss: 0.010559250393675433\n",
      "[LOG 20200502-14:53:11] epoch: 15935 train-loss: 0.010559260431263182\n",
      "[LOG 20200502-14:53:12] epoch: 15936 train-loss: 0.010559270675811503\n",
      "[LOG 20200502-14:53:12] epoch: 15937 train-loss: 0.010559280920359824\n",
      "[LOG 20200502-14:53:12] epoch: 15938 train-loss: 0.010559291164908145\n",
      "[LOG 20200502-14:53:12] epoch: 15939 train-loss: 0.01055930151293675\n",
      "[LOG 20200502-14:53:13] epoch: 15940 train-loss: 0.010559311654004786\n",
      "[LOG 20200502-14:53:13] epoch: 15941 train-loss: 0.010559322002033392\n",
      "[LOG 20200502-14:53:13] epoch: 15942 train-loss: 0.010559332246581713\n",
      "[LOG 20200502-14:53:13] epoch: 15943 train-loss: 0.01055934207720889\n",
      "[LOG 20200502-14:53:14] epoch: 15944 train-loss: 0.010559351907836067\n",
      "[LOG 20200502-14:53:14] epoch: 15945 train-loss: 0.0105593620489041\n",
      "[LOG 20200502-14:53:14] epoch: 15946 train-loss: 0.010559371672570705\n",
      "[LOG 20200502-14:53:14] epoch: 15947 train-loss: 0.010559381192757024\n",
      "[LOG 20200502-14:53:15] epoch: 15948 train-loss: 0.010559390402502485\n",
      "[LOG 20200502-14:53:15] epoch: 15949 train-loss: 0.010559399405287372\n",
      "[LOG 20200502-14:53:15] epoch: 15950 train-loss: 0.010559408304591974\n",
      "[LOG 20200502-14:53:15] epoch: 15951 train-loss: 0.010559416996936003\n",
      "[LOG 20200502-14:53:16] epoch: 15952 train-loss: 0.010559425171878602\n",
      "[LOG 20200502-14:53:16] epoch: 15953 train-loss: 0.0105594326224592\n",
      "[LOG 20200502-14:53:16] epoch: 15954 train-loss: 0.010559440073039796\n",
      "[LOG 20200502-14:53:16] epoch: 15955 train-loss: 0.010559447523620393\n",
      "[LOG 20200502-14:53:17] epoch: 15956 train-loss: 0.010559454353319274\n",
      "[LOG 20200502-14:53:17] epoch: 15957 train-loss: 0.010559460976057582\n",
      "[LOG 20200502-14:53:17] epoch: 15958 train-loss: 0.010559467288355032\n",
      "[LOG 20200502-14:53:17] epoch: 15959 train-loss: 0.010559472772810195\n",
      "[LOG 20200502-14:53:18] epoch: 15960 train-loss: 0.010559478567706214\n",
      "[LOG 20200502-14:53:18] epoch: 15961 train-loss: 0.010559483327799372\n",
      "[LOG 20200502-14:53:18] epoch: 15962 train-loss: 0.010559488501813676\n",
      "[LOG 20200502-14:53:18] epoch: 15963 train-loss: 0.010559493468867408\n",
      "[LOG 20200502-14:53:19] epoch: 15964 train-loss: 0.010559498021999994\n",
      "[LOG 20200502-14:53:19] epoch: 15965 train-loss: 0.010559502264691724\n",
      "[LOG 20200502-14:53:19] epoch: 15966 train-loss: 0.010559505576060878\n",
      "[LOG 20200502-14:53:19] epoch: 15967 train-loss: 0.01055950919787089\n",
      "[LOG 20200502-14:53:19] epoch: 15968 train-loss: 0.010559512716200616\n",
      "[LOG 20200502-14:53:20] epoch: 15969 train-loss: 0.01055951633801063\n",
      "[LOG 20200502-14:53:20] epoch: 15970 train-loss: 0.010559519338938925\n",
      "[LOG 20200502-14:53:20] epoch: 15971 train-loss: 0.010559522960748937\n",
      "[LOG 20200502-14:53:20] epoch: 15972 train-loss: 0.010559525961677233\n",
      "[LOG 20200502-14:53:21] epoch: 15973 train-loss: 0.010559528548684385\n",
      "[LOG 20200502-14:53:21] epoch: 15974 train-loss: 0.01055953186005354\n",
      "[LOG 20200502-14:53:21] epoch: 15975 train-loss: 0.010559535688824125\n",
      "[LOG 20200502-14:53:21] epoch: 15976 train-loss: 0.010559539000193277\n",
      "[LOG 20200502-14:53:22] epoch: 15977 train-loss: 0.010559542725483576\n",
      "[LOG 20200502-14:53:22] epoch: 15978 train-loss: 0.010559546450773874\n",
      "[LOG 20200502-14:53:22] epoch: 15979 train-loss: 0.010559550383024745\n",
      "[LOG 20200502-14:53:22] epoch: 15980 train-loss: 0.010559555143117905\n",
      "[LOG 20200502-14:53:23] epoch: 15981 train-loss: 0.010559560213651922\n",
      "[LOG 20200502-14:53:23] epoch: 15982 train-loss: 0.010559565698107084\n",
      "[LOG 20200502-14:53:23] epoch: 15983 train-loss: 0.010559571286042532\n",
      "[LOG 20200502-14:53:23] epoch: 15984 train-loss: 0.010559577805300554\n",
      "[LOG 20200502-14:53:24] epoch: 15985 train-loss: 0.010559584014117718\n",
      "[LOG 20200502-14:53:24] epoch: 15986 train-loss: 0.010559591154257456\n",
      "[LOG 20200502-14:53:24] epoch: 15987 train-loss: 0.010559598190916909\n",
      "[LOG 20200502-14:53:24] epoch: 15988 train-loss: 0.01055960605541865\n",
      "[LOG 20200502-14:53:25] epoch: 15989 train-loss: 0.01055961474776268\n",
      "[LOG 20200502-14:53:25] epoch: 15990 train-loss: 0.010559623543586995\n",
      "[LOG 20200502-14:53:25] epoch: 15991 train-loss: 0.01055963264985217\n",
      "[LOG 20200502-14:53:25] epoch: 15992 train-loss: 0.010559643101361062\n",
      "[LOG 20200502-14:53:26] epoch: 15993 train-loss: 0.010559653138948811\n",
      "[LOG 20200502-14:53:26] epoch: 15994 train-loss: 0.010559663590457704\n",
      "[LOG 20200502-14:53:26] epoch: 15995 train-loss: 0.010559674248927169\n",
      "[LOG 20200502-14:53:26] epoch: 15996 train-loss: 0.010559686045679782\n",
      "[LOG 20200502-14:53:27] epoch: 15997 train-loss: 0.010559698359833824\n",
      "[LOG 20200502-14:53:27] epoch: 15998 train-loss: 0.010559710363547007\n",
      "[LOG 20200502-14:53:27] epoch: 15999 train-loss: 0.010559722677701049\n",
      "[LOG 20200502-14:53:27] epoch: 16000 train-loss: 0.010559735716217093\n",
      "[LOG 20200502-14:53:28] epoch: 16001 train-loss: 0.010559748340811994\n",
      "[LOG 20200502-14:53:28] epoch: 16002 train-loss: 0.01055976189672947\n",
      "[LOG 20200502-14:53:28] epoch: 16003 train-loss: 0.010559775142206086\n",
      "[LOG 20200502-14:53:28] epoch: 16004 train-loss: 0.010559788905084133\n",
      "[LOG 20200502-14:53:29] epoch: 16005 train-loss: 0.010559802357521322\n",
      "[LOG 20200502-14:53:29] epoch: 16006 train-loss: 0.010559816016919084\n",
      "[LOG 20200502-14:53:29] epoch: 16007 train-loss: 0.010559830193718275\n",
      "[LOG 20200502-14:53:29] epoch: 16008 train-loss: 0.010559843749635749\n",
      "[LOG 20200502-14:53:29] epoch: 16009 train-loss: 0.010559857512513796\n",
      "[LOG 20200502-14:53:30] epoch: 16010 train-loss: 0.010559871068431271\n",
      "[LOG 20200502-14:53:30] epoch: 16011 train-loss: 0.01055988452086846\n",
      "[LOG 20200502-14:53:30] epoch: 16012 train-loss: 0.010559897766345076\n",
      "[LOG 20200502-14:53:30] epoch: 16013 train-loss: 0.010559910908341408\n",
      "[LOG 20200502-14:53:31] epoch: 16014 train-loss: 0.010559924257298311\n",
      "[LOG 20200502-14:53:31] epoch: 16015 train-loss: 0.010559936985373497\n",
      "[LOG 20200502-14:53:31] epoch: 16016 train-loss: 0.010559949506488111\n",
      "[LOG 20200502-14:53:31] epoch: 16017 train-loss: 0.01055996192412244\n",
      "[LOG 20200502-14:53:32] epoch: 16018 train-loss: 0.010559973410434194\n",
      "[LOG 20200502-14:53:32] epoch: 16019 train-loss: 0.01055998458630509\n",
      "[LOG 20200502-14:53:32] epoch: 16020 train-loss: 0.010559996486537986\n",
      "[LOG 20200502-14:53:32] epoch: 16021 train-loss: 0.010560007558928596\n",
      "[LOG 20200502-14:53:33] epoch: 16022 train-loss: 0.010560017493036058\n",
      "[LOG 20200502-14:53:33] epoch: 16023 train-loss: 0.010560027634104093\n",
      "[LOG 20200502-14:53:33] epoch: 16024 train-loss: 0.01056003694732984\n",
      "[LOG 20200502-14:53:33] epoch: 16025 train-loss: 0.010560045950114727\n",
      "[LOG 20200502-14:53:34] epoch: 16026 train-loss: 0.010560055056379901\n",
      "[LOG 20200502-14:53:34] epoch: 16027 train-loss: 0.0105600632313225\n",
      "[LOG 20200502-14:53:34] epoch: 16028 train-loss: 0.010560071509745386\n",
      "[LOG 20200502-14:53:34] epoch: 16029 train-loss: 0.010560079374247126\n",
      "[LOG 20200502-14:53:35] epoch: 16030 train-loss: 0.010560086721347438\n",
      "[LOG 20200502-14:53:35] epoch: 16031 train-loss: 0.010560093654526604\n",
      "[LOG 20200502-14:53:35] epoch: 16032 train-loss: 0.01056010058770577\n",
      "[LOG 20200502-14:53:35] epoch: 16033 train-loss: 0.010560106693042649\n",
      "[LOG 20200502-14:53:36] epoch: 16034 train-loss: 0.0105601130053401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:53:36] epoch: 16035 train-loss: 0.010560119214157263\n",
      "[LOG 20200502-14:53:36] epoch: 16036 train-loss: 0.010560125319494141\n",
      "[LOG 20200502-14:53:37] epoch: 16037 train-loss: 0.010560131321350733\n",
      "[LOG 20200502-14:53:37] epoch: 16038 train-loss: 0.010560136598845324\n",
      "[LOG 20200502-14:53:37] epoch: 16039 train-loss: 0.010560142497221628\n",
      "[LOG 20200502-14:53:37] epoch: 16040 train-loss: 0.010560147567755647\n",
      "[LOG 20200502-14:53:37] epoch: 16041 train-loss: 0.010560153362651667\n",
      "[LOG 20200502-14:53:38] epoch: 16042 train-loss: 0.010560159157547686\n",
      "[LOG 20200502-14:53:38] epoch: 16043 train-loss: 0.010560164745483134\n",
      "[LOG 20200502-14:53:38] epoch: 16044 train-loss: 0.010560170022977723\n",
      "[LOG 20200502-14:53:38] epoch: 16045 train-loss: 0.010560176024834314\n",
      "[LOG 20200502-14:53:39] epoch: 16046 train-loss: 0.010560182026690908\n",
      "[LOG 20200502-14:53:39] epoch: 16047 train-loss: 0.010560187925067212\n",
      "[LOG 20200502-14:53:39] epoch: 16048 train-loss: 0.010560193616482947\n",
      "[LOG 20200502-14:53:39] epoch: 16049 train-loss: 0.01056019982530011\n",
      "[LOG 20200502-14:53:40] epoch: 16050 train-loss: 0.010560206551518705\n",
      "[LOG 20200502-14:53:40] epoch: 16051 train-loss: 0.01056021276033587\n",
      "[LOG 20200502-14:53:40] epoch: 16052 train-loss: 0.010560219383074177\n",
      "[LOG 20200502-14:53:40] epoch: 16053 train-loss: 0.010560226212773059\n",
      "[LOG 20200502-14:53:41] epoch: 16054 train-loss: 0.010560233145952225\n",
      "[LOG 20200502-14:53:41] epoch: 16055 train-loss: 0.010560239872170819\n",
      "[LOG 20200502-14:53:41] epoch: 16056 train-loss: 0.010560247115790844\n",
      "[LOG 20200502-14:53:41] epoch: 16057 train-loss: 0.010560254566371441\n",
      "[LOG 20200502-14:53:42] epoch: 16058 train-loss: 0.010560261499550607\n",
      "[LOG 20200502-14:53:42] epoch: 16059 train-loss: 0.010560268743170632\n",
      "[LOG 20200502-14:53:42] epoch: 16060 train-loss: 0.010560276400711801\n",
      "[LOG 20200502-14:53:42] epoch: 16061 train-loss: 0.01056028354085154\n",
      "[LOG 20200502-14:53:43] epoch: 16062 train-loss: 0.010560290680991279\n",
      "[LOG 20200502-14:53:43] epoch: 16063 train-loss: 0.010560298235052161\n",
      "[LOG 20200502-14:53:43] epoch: 16064 train-loss: 0.010560305685632758\n",
      "[LOG 20200502-14:53:43] epoch: 16065 train-loss: 0.010560313136213355\n",
      "[LOG 20200502-14:53:44] epoch: 16066 train-loss: 0.010560319965912236\n",
      "[LOG 20200502-14:53:44] epoch: 16067 train-loss: 0.010560326795611117\n",
      "[LOG 20200502-14:53:44] epoch: 16068 train-loss: 0.010560333418349424\n",
      "[LOG 20200502-14:53:44] epoch: 16069 train-loss: 0.010560340248048306\n",
      "[LOG 20200502-14:53:45] epoch: 16070 train-loss: 0.0105603469742669\n",
      "[LOG 20200502-14:53:45] epoch: 16071 train-loss: 0.010560353390044637\n",
      "[LOG 20200502-14:53:45] epoch: 16072 train-loss: 0.01056035908146037\n",
      "[LOG 20200502-14:53:45] epoch: 16073 train-loss: 0.010560364772876104\n",
      "[LOG 20200502-14:53:45] epoch: 16074 train-loss: 0.010560369946890406\n",
      "[LOG 20200502-14:53:46] epoch: 16075 train-loss: 0.010560375327865282\n",
      "[LOG 20200502-14:53:46] epoch: 16076 train-loss: 0.010560380501879586\n",
      "[LOG 20200502-14:53:46] epoch: 16077 train-loss: 0.010560384744571315\n",
      "[LOG 20200502-14:53:46] epoch: 16078 train-loss: 0.010560388987263044\n",
      "[LOG 20200502-14:53:47] epoch: 16079 train-loss: 0.010560392919513915\n",
      "[LOG 20200502-14:53:47] epoch: 16080 train-loss: 0.010560395816961924\n",
      "[LOG 20200502-14:53:47] epoch: 16081 train-loss: 0.010560399438771937\n",
      "[LOG 20200502-14:53:47] epoch: 16082 train-loss: 0.010560402750141092\n",
      "[LOG 20200502-14:53:48] epoch: 16083 train-loss: 0.010560404923227098\n",
      "[LOG 20200502-14:53:48] epoch: 16084 train-loss: 0.010560407199793391\n",
      "[LOG 20200502-14:53:48] epoch: 16085 train-loss: 0.010560409476359686\n",
      "[LOG 20200502-14:53:48] epoch: 16086 train-loss: 0.010560411028563976\n",
      "[LOG 20200502-14:53:49] epoch: 16087 train-loss: 0.010560411856406264\n",
      "[LOG 20200502-14:53:49] epoch: 16088 train-loss: 0.010560412787728839\n",
      "[LOG 20200502-14:53:49] epoch: 16089 train-loss: 0.010560413201649984\n",
      "[LOG 20200502-14:53:49] epoch: 16090 train-loss: 0.010560413926011987\n",
      "[LOG 20200502-14:53:50] epoch: 16091 train-loss: 0.010560413926011987\n",
      "[LOG 20200502-14:53:50] epoch: 16092 train-loss: 0.010560414029492272\n",
      "[LOG 20200502-14:53:50] epoch: 16093 train-loss: 0.010560414132972559\n",
      "[LOG 20200502-14:53:50] epoch: 16094 train-loss: 0.0105604138225317\n",
      "[LOG 20200502-14:53:51] epoch: 16095 train-loss: 0.0105604138225317\n",
      "[LOG 20200502-14:53:51] epoch: 16096 train-loss: 0.010560413201649984\n",
      "[LOG 20200502-14:53:51] epoch: 16097 train-loss: 0.010560412994689412\n",
      "[LOG 20200502-14:53:51] epoch: 16098 train-loss: 0.010560413098169697\n",
      "[LOG 20200502-14:53:52] epoch: 16099 train-loss: 0.010560412684248554\n",
      "[LOG 20200502-14:53:52] epoch: 16100 train-loss: 0.010560413098169697\n",
      "[LOG 20200502-14:53:52] epoch: 16101 train-loss: 0.010560413408610556\n",
      "[LOG 20200502-14:53:52] epoch: 16102 train-loss: 0.0105604138225317\n",
      "[LOG 20200502-14:53:52] epoch: 16103 train-loss: 0.010560414857334562\n",
      "[LOG 20200502-14:53:53] epoch: 16104 train-loss: 0.010560416306058565\n",
      "[LOG 20200502-14:53:53] epoch: 16105 train-loss: 0.010560417340861427\n",
      "[LOG 20200502-14:53:53] epoch: 16106 train-loss: 0.010560419306986861\n",
      "[LOG 20200502-14:53:53] epoch: 16107 train-loss: 0.010560421273112297\n",
      "[LOG 20200502-14:53:54] epoch: 16108 train-loss: 0.010560424584481452\n",
      "[LOG 20200502-14:53:54] epoch: 16109 train-loss: 0.010560427274968889\n",
      "[LOG 20200502-14:53:54] epoch: 16110 train-loss: 0.010560431207219759\n",
      "[LOG 20200502-14:53:54] epoch: 16111 train-loss: 0.010560435449911488\n",
      "[LOG 20200502-14:53:55] epoch: 16112 train-loss: 0.010560439796083503\n",
      "[LOG 20200502-14:53:55] epoch: 16113 train-loss: 0.01056044465965695\n",
      "[LOG 20200502-14:53:55] epoch: 16114 train-loss: 0.010560449833671251\n",
      "[LOG 20200502-14:53:56] epoch: 16115 train-loss: 0.010560456042488417\n",
      "[LOG 20200502-14:53:56] epoch: 16116 train-loss: 0.01056046225130558\n",
      "[LOG 20200502-14:53:56] epoch: 16117 train-loss: 0.010560468977524174\n",
      "[LOG 20200502-14:53:56] epoch: 16118 train-loss: 0.010560476324624486\n",
      "[LOG 20200502-14:53:56] epoch: 16119 train-loss: 0.010560484189126227\n",
      "[LOG 20200502-14:53:57] epoch: 16120 train-loss: 0.010560492053627968\n",
      "[LOG 20200502-14:53:57] epoch: 16121 train-loss: 0.010560500745971998\n",
      "[LOG 20200502-14:53:57] epoch: 16122 train-loss: 0.010560509334835742\n",
      "[LOG 20200502-14:53:57] epoch: 16123 train-loss: 0.01056051802717977\n",
      "[LOG 20200502-14:53:58] epoch: 16124 train-loss: 0.010560527443885803\n",
      "[LOG 20200502-14:53:58] epoch: 16125 train-loss: 0.010560536653631263\n",
      "[LOG 20200502-14:53:58] epoch: 16126 train-loss: 0.010560546794699298\n",
      "[LOG 20200502-14:53:58] epoch: 16127 train-loss: 0.010560556314885616\n",
      "[LOG 20200502-14:53:59] epoch: 16128 train-loss: 0.010560566662914224\n",
      "[LOG 20200502-14:53:59] epoch: 16129 train-loss: 0.010560576597021686\n",
      "[LOG 20200502-14:53:59] epoch: 16130 train-loss: 0.010560586531129148\n",
      "[LOG 20200502-14:53:59] epoch: 16131 train-loss: 0.010560596879157755\n",
      "[LOG 20200502-14:54:00] epoch: 16132 train-loss: 0.01056060702022579\n",
      "[LOG 20200502-14:54:00] epoch: 16133 train-loss: 0.010560617471734682\n",
      "[LOG 20200502-14:54:00] epoch: 16134 train-loss: 0.010560627612802718\n",
      "[LOG 20200502-14:54:00] epoch: 16135 train-loss: 0.010560637132989036\n",
      "[LOG 20200502-14:54:00] epoch: 16136 train-loss: 0.01056064675665564\n",
      "[LOG 20200502-14:54:01] epoch: 16137 train-loss: 0.010560656690763103\n",
      "[LOG 20200502-14:54:01] epoch: 16138 train-loss: 0.010560666003988849\n",
      "[LOG 20200502-14:54:01] epoch: 16139 train-loss: 0.010560674903293451\n",
      "[LOG 20200502-14:54:01] epoch: 16140 train-loss: 0.010560683906078339\n",
      "[LOG 20200502-14:54:02] epoch: 16141 train-loss: 0.010560692391461797\n",
      "[LOG 20200502-14:54:02] epoch: 16142 train-loss: 0.010560700359443823\n",
      "[LOG 20200502-14:54:02] epoch: 16143 train-loss: 0.010560708741346994\n",
      "[LOG 20200502-14:54:02] epoch: 16144 train-loss: 0.010560716088447306\n",
      "[LOG 20200502-14:54:03] epoch: 16145 train-loss: 0.01056072312510676\n",
      "[LOG 20200502-14:54:03] epoch: 16146 train-loss: 0.010560729644364782\n",
      "[LOG 20200502-14:54:03] epoch: 16147 train-loss: 0.01056073595666223\n",
      "[LOG 20200502-14:54:03] epoch: 16148 train-loss: 0.01056074226895968\n",
      "[LOG 20200502-14:54:04] epoch: 16149 train-loss: 0.010560747649934556\n",
      "[LOG 20200502-14:54:04] epoch: 16150 train-loss: 0.010560752927429147\n",
      "[LOG 20200502-14:54:04] epoch: 16151 train-loss: 0.010560757687522305\n",
      "[LOG 20200502-14:54:04] epoch: 16152 train-loss: 0.01056076203369432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:54:05] epoch: 16153 train-loss: 0.010560765552024046\n",
      "[LOG 20200502-14:54:05] epoch: 16154 train-loss: 0.010560769484274916\n",
      "[LOG 20200502-14:54:05] epoch: 16155 train-loss: 0.010560772588683499\n",
      "[LOG 20200502-14:54:05] epoch: 16156 train-loss: 0.01056077548613151\n",
      "[LOG 20200502-14:54:06] epoch: 16157 train-loss: 0.010560778487059806\n",
      "[LOG 20200502-14:54:06] epoch: 16158 train-loss: 0.010560780349704955\n",
      "[LOG 20200502-14:54:06] epoch: 16159 train-loss: 0.01056078283323182\n",
      "[LOG 20200502-14:54:06] epoch: 16160 train-loss: 0.010560785109798113\n",
      "[LOG 20200502-14:54:07] epoch: 16161 train-loss: 0.010560786662002405\n",
      "[LOG 20200502-14:54:07] epoch: 16162 train-loss: 0.01056078862812784\n",
      "[LOG 20200502-14:54:07] epoch: 16163 train-loss: 0.010560790180332132\n",
      "[LOG 20200502-14:54:07] epoch: 16164 train-loss: 0.010560791836016707\n",
      "[LOG 20200502-14:54:07] epoch: 16165 train-loss: 0.010560793388221\n",
      "[LOG 20200502-14:54:08] epoch: 16166 train-loss: 0.010560794216063287\n",
      "[LOG 20200502-14:54:08] epoch: 16167 train-loss: 0.010560796182188723\n",
      "[LOG 20200502-14:54:08] epoch: 16168 train-loss: 0.0105607978378733\n",
      "[LOG 20200502-14:54:08] epoch: 16169 train-loss: 0.01056079887267616\n",
      "[LOG 20200502-14:54:09] epoch: 16170 train-loss: 0.010560801045762168\n",
      "[LOG 20200502-14:54:09] epoch: 16171 train-loss: 0.010560802597966459\n",
      "[LOG 20200502-14:54:09] epoch: 16172 train-loss: 0.010560804564091895\n",
      "[LOG 20200502-14:54:09] epoch: 16173 train-loss: 0.010560806633697616\n",
      "[LOG 20200502-14:54:10] epoch: 16174 train-loss: 0.010560808806783624\n",
      "[LOG 20200502-14:54:10] epoch: 16175 train-loss: 0.010560811290310489\n",
      "[LOG 20200502-14:54:10] epoch: 16176 train-loss: 0.010560813670357069\n",
      "[LOG 20200502-14:54:10] epoch: 16177 train-loss: 0.010560816464324793\n",
      "[LOG 20200502-14:54:11] epoch: 16178 train-loss: 0.010560818740891086\n",
      "[LOG 20200502-14:54:11] epoch: 16179 train-loss: 0.010560822155740526\n",
      "[LOG 20200502-14:54:11] epoch: 16180 train-loss: 0.010560824846227964\n",
      "[LOG 20200502-14:54:11] epoch: 16181 train-loss: 0.010560828054116832\n",
      "[LOG 20200502-14:54:12] epoch: 16182 train-loss: 0.010560831468966272\n",
      "[LOG 20200502-14:54:12] epoch: 16183 train-loss: 0.010560833952493139\n",
      "[LOG 20200502-14:54:12] epoch: 16184 train-loss: 0.01056083757430315\n",
      "[LOG 20200502-14:54:13] epoch: 16185 train-loss: 0.010560840782192018\n",
      "[LOG 20200502-14:54:13] epoch: 16186 train-loss: 0.010560844300521744\n",
      "[LOG 20200502-14:54:13] epoch: 16187 train-loss: 0.010560847611890899\n",
      "[LOG 20200502-14:54:13] epoch: 16188 train-loss: 0.010560850509338908\n",
      "[LOG 20200502-14:54:14] epoch: 16189 train-loss: 0.010560853820708063\n",
      "[LOG 20200502-14:54:14] epoch: 16190 train-loss: 0.010560857235557504\n",
      "[LOG 20200502-14:54:14] epoch: 16191 train-loss: 0.010560860029525228\n",
      "[LOG 20200502-14:54:14] epoch: 16192 train-loss: 0.010560862926973237\n",
      "[LOG 20200502-14:54:15] epoch: 16193 train-loss: 0.010560866445302963\n",
      "[LOG 20200502-14:54:15] epoch: 16194 train-loss: 0.010560868928829828\n",
      "[LOG 20200502-14:54:15] epoch: 16195 train-loss: 0.01056087048103412\n",
      "[LOG 20200502-14:54:15] epoch: 16196 train-loss: 0.010560873068041272\n",
      "[LOG 20200502-14:54:16] epoch: 16197 train-loss: 0.010560875344607566\n",
      "[LOG 20200502-14:54:16] epoch: 16198 train-loss: 0.010560876896811856\n",
      "[LOG 20200502-14:54:16] epoch: 16199 train-loss: 0.010560878242055574\n",
      "[LOG 20200502-14:54:16] epoch: 16200 train-loss: 0.010560879173378149\n",
      "[LOG 20200502-14:54:17] epoch: 16201 train-loss: 0.01056088020818101\n",
      "[LOG 20200502-14:54:17] epoch: 16202 train-loss: 0.010560880829062726\n",
      "[LOG 20200502-14:54:17] epoch: 16203 train-loss: 0.010560880829062726\n",
      "[LOG 20200502-14:54:17] epoch: 16204 train-loss: 0.010560880932543013\n",
      "[LOG 20200502-14:54:17] epoch: 16205 train-loss: 0.01056088020818101\n",
      "[LOG 20200502-14:54:18] epoch: 16206 train-loss: 0.010560878345535861\n",
      "[LOG 20200502-14:54:18] epoch: 16207 train-loss: 0.010560878242055574\n",
      "[LOG 20200502-14:54:18] epoch: 16208 train-loss: 0.010560876172449853\n",
      "[LOG 20200502-14:54:18] epoch: 16209 train-loss: 0.01056087389588356\n",
      "[LOG 20200502-14:54:19] epoch: 16210 train-loss: 0.010560872136718698\n",
      "[LOG 20200502-14:54:19] epoch: 16211 train-loss: 0.010560869239270687\n",
      "[LOG 20200502-14:54:19] epoch: 16212 train-loss: 0.01056086603138182\n",
      "[LOG 20200502-14:54:19] epoch: 16213 train-loss: 0.010560863133933809\n",
      "[LOG 20200502-14:54:20] epoch: 16214 train-loss: 0.01056085940864351\n",
      "[LOG 20200502-14:54:20] epoch: 16215 train-loss: 0.010560855372912355\n",
      "[LOG 20200502-14:54:20] epoch: 16216 train-loss: 0.01056085154414177\n",
      "[LOG 20200502-14:54:20] epoch: 16217 train-loss: 0.010560847508410612\n",
      "[LOG 20200502-14:54:21] epoch: 16218 train-loss: 0.010560842541356882\n",
      "[LOG 20200502-14:54:21] epoch: 16219 train-loss: 0.01056083757430315\n",
      "[LOG 20200502-14:54:21] epoch: 16220 train-loss: 0.010560833538571993\n",
      "[LOG 20200502-14:54:21] epoch: 16221 train-loss: 0.01056082939936055\n",
      "[LOG 20200502-14:54:22] epoch: 16222 train-loss: 0.01056082391490539\n",
      "[LOG 20200502-14:54:22] epoch: 16223 train-loss: 0.010560819568733374\n",
      "[LOG 20200502-14:54:22] epoch: 16224 train-loss: 0.010560815119081073\n",
      "[LOG 20200502-14:54:22] epoch: 16225 train-loss: 0.010560811083349917\n",
      "[LOG 20200502-14:54:23] epoch: 16226 train-loss: 0.010560806426737044\n",
      "[LOG 20200502-14:54:23] epoch: 16227 train-loss: 0.010560802391005887\n",
      "[LOG 20200502-14:54:23] epoch: 16228 train-loss: 0.01056079835527473\n",
      "[LOG 20200502-14:54:23] epoch: 16229 train-loss: 0.010560795250866149\n",
      "[LOG 20200502-14:54:24] epoch: 16230 train-loss: 0.010560791939496994\n",
      "[LOG 20200502-14:54:24] epoch: 16231 train-loss: 0.01056078914552927\n",
      "[LOG 20200502-14:54:24] epoch: 16232 train-loss: 0.010560786558522118\n",
      "[LOG 20200502-14:54:24] epoch: 16233 train-loss: 0.010560784178475538\n",
      "[LOG 20200502-14:54:25] epoch: 16234 train-loss: 0.010560782729751535\n",
      "[LOG 20200502-14:54:25] epoch: 16235 train-loss: 0.010560781591468386\n",
      "[LOG 20200502-14:54:25] epoch: 16236 train-loss: 0.010560780660145812\n",
      "[LOG 20200502-14:54:25] epoch: 16237 train-loss: 0.01056077993578381\n",
      "[LOG 20200502-14:54:25] epoch: 16238 train-loss: 0.01056078045318524\n",
      "[LOG 20200502-14:54:26] epoch: 16239 train-loss: 0.01056078045318524\n",
      "[LOG 20200502-14:54:26] epoch: 16240 train-loss: 0.010560782108869817\n",
      "[LOG 20200502-14:54:26] epoch: 16241 train-loss: 0.010560783143672679\n",
      "[LOG 20200502-14:54:26] epoch: 16242 train-loss: 0.010560785523719259\n",
      "[LOG 20200502-14:54:27] epoch: 16243 train-loss: 0.010560787696805265\n",
      "[LOG 20200502-14:54:27] epoch: 16244 train-loss: 0.01056079069773356\n",
      "[LOG 20200502-14:54:27] epoch: 16245 train-loss: 0.010560793491701284\n",
      "[LOG 20200502-14:54:27] epoch: 16246 train-loss: 0.010560796389149295\n",
      "[LOG 20200502-14:54:28] epoch: 16247 train-loss: 0.010560800838801596\n",
      "[LOG 20200502-14:54:28] epoch: 16248 train-loss: 0.010560804564091895\n",
      "[LOG 20200502-14:54:28] epoch: 16249 train-loss: 0.010560809531145625\n",
      "[LOG 20200502-14:54:28] epoch: 16250 train-loss: 0.010560813773837354\n",
      "[LOG 20200502-14:54:29] epoch: 16251 train-loss: 0.01056081915481223\n",
      "[LOG 20200502-14:54:29] epoch: 16252 train-loss: 0.010560824225346247\n",
      "[LOG 20200502-14:54:29] epoch: 16253 train-loss: 0.010560829813281694\n",
      "[LOG 20200502-14:54:29] epoch: 16254 train-loss: 0.01056083519425657\n",
      "[LOG 20200502-14:54:30] epoch: 16255 train-loss: 0.010560841092632877\n",
      "[LOG 20200502-14:54:30] epoch: 16256 train-loss: 0.010560845852726035\n",
      "[LOG 20200502-14:54:30] epoch: 16257 train-loss: 0.010560851440661483\n",
      "[LOG 20200502-14:54:30] epoch: 16258 train-loss: 0.010560857442518076\n",
      "[LOG 20200502-14:54:31] epoch: 16259 train-loss: 0.010560862616532378\n",
      "[LOG 20200502-14:54:31] epoch: 16260 train-loss: 0.010560868307948112\n",
      "[LOG 20200502-14:54:31] epoch: 16261 train-loss: 0.010560873275001844\n",
      "[LOG 20200502-14:54:31] epoch: 16262 train-loss: 0.010560879069897864\n",
      "[LOG 20200502-14:54:32] epoch: 16263 train-loss: 0.010560884243912168\n",
      "[LOG 20200502-14:54:32] epoch: 16264 train-loss: 0.010560888693564467\n",
      "[LOG 20200502-14:54:32] epoch: 16265 train-loss: 0.01056089335017734\n",
      "[LOG 20200502-14:54:32] epoch: 16266 train-loss: 0.010560897385908498\n",
      "[LOG 20200502-14:54:33] epoch: 16267 train-loss: 0.01056090152511994\n",
      "[LOG 20200502-14:54:33] epoch: 16268 train-loss: 0.010560904939969381\n",
      "[LOG 20200502-14:54:33] epoch: 16269 train-loss: 0.010560908147858249\n",
      "[LOG 20200502-14:54:33] epoch: 16270 train-loss: 0.010560911252266832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:54:34] epoch: 16271 train-loss: 0.010560913942754269\n",
      "[LOG 20200502-14:54:34] epoch: 16272 train-loss: 0.010560916115840277\n",
      "[LOG 20200502-14:54:34] epoch: 16273 train-loss: 0.010560917875005139\n",
      "[LOG 20200502-14:54:34] epoch: 16274 train-loss: 0.01056091942720943\n",
      "[LOG 20200502-14:54:35] epoch: 16275 train-loss: 0.010560920151571432\n",
      "[LOG 20200502-14:54:35] epoch: 16276 train-loss: 0.010560920668972863\n",
      "[LOG 20200502-14:54:35] epoch: 16277 train-loss: 0.010560921082894007\n",
      "[LOG 20200502-14:54:35] epoch: 16278 train-loss: 0.010560921186374294\n",
      "[LOG 20200502-14:54:36] epoch: 16279 train-loss: 0.010560919737650288\n",
      "[LOG 20200502-14:54:36] epoch: 16280 train-loss: 0.010560918702847429\n",
      "[LOG 20200502-14:54:36] epoch: 16281 train-loss: 0.010560917357603708\n",
      "[LOG 20200502-14:54:36] epoch: 16282 train-loss: 0.010560916115840277\n",
      "[LOG 20200502-14:54:37] epoch: 16283 train-loss: 0.010560913839273982\n",
      "[LOG 20200502-14:54:37] epoch: 16284 train-loss: 0.010560911873148547\n",
      "[LOG 20200502-14:54:37] epoch: 16285 train-loss: 0.010560908975700537\n",
      "[LOG 20200502-14:54:37] epoch: 16286 train-loss: 0.0105609062852131\n",
      "[LOG 20200502-14:54:38] epoch: 16287 train-loss: 0.010560903284284804\n",
      "[LOG 20200502-14:54:38] epoch: 16288 train-loss: 0.010560900283356508\n",
      "[LOG 20200502-14:54:38] epoch: 16289 train-loss: 0.010560897282428212\n",
      "[LOG 20200502-14:54:39] epoch: 16290 train-loss: 0.010560893867578771\n",
      "[LOG 20200502-14:54:39] epoch: 16291 train-loss: 0.01056089024576876\n",
      "[LOG 20200502-14:54:39] epoch: 16292 train-loss: 0.010560887348320749\n",
      "[LOG 20200502-14:54:39] epoch: 16293 train-loss: 0.010560883829991022\n",
      "[LOG 20200502-14:54:40] epoch: 16294 train-loss: 0.01056088020818101\n",
      "[LOG 20200502-14:54:40] epoch: 16295 train-loss: 0.010560877207252715\n",
      "[LOG 20200502-14:54:40] epoch: 16296 train-loss: 0.01056087389588356\n",
      "[LOG 20200502-14:54:40] epoch: 16297 train-loss: 0.010560870377553834\n",
      "[LOG 20200502-14:54:41] epoch: 16298 train-loss: 0.010560867687066397\n",
      "[LOG 20200502-14:54:41] epoch: 16299 train-loss: 0.010560864375697242\n",
      "[LOG 20200502-14:54:41] epoch: 16300 train-loss: 0.010560861892170377\n",
      "[LOG 20200502-14:54:41] epoch: 16301 train-loss: 0.010560859201682938\n",
      "[LOG 20200502-14:54:42] epoch: 16302 train-loss: 0.010560856511195501\n",
      "[LOG 20200502-14:54:42] epoch: 16303 train-loss: 0.01056085444158978\n",
      "[LOG 20200502-14:54:42] epoch: 16304 train-loss: 0.010560852475464344\n",
      "[LOG 20200502-14:54:42] epoch: 16305 train-loss: 0.010560850198898051\n",
      "[LOG 20200502-14:54:43] epoch: 16306 train-loss: 0.01056084812929233\n",
      "[LOG 20200502-14:54:43] epoch: 16307 train-loss: 0.01056084626664718\n",
      "[LOG 20200502-14:54:43] epoch: 16308 train-loss: 0.01056084492140346\n",
      "[LOG 20200502-14:54:44] epoch: 16309 train-loss: 0.010560842851797739\n",
      "[LOG 20200502-14:54:44] epoch: 16310 train-loss: 0.010560841092632877\n",
      "[LOG 20200502-14:54:44] epoch: 16311 train-loss: 0.010560840161310302\n",
      "[LOG 20200502-14:54:44] epoch: 16312 train-loss: 0.010560838298665153\n",
      "[LOG 20200502-14:54:45] epoch: 16313 train-loss: 0.010560837160382006\n",
      "[LOG 20200502-14:54:45] epoch: 16314 train-loss: 0.010560835711658001\n",
      "[LOG 20200502-14:54:45] epoch: 16315 train-loss: 0.010560834366414282\n",
      "[LOG 20200502-14:54:45] epoch: 16316 train-loss: 0.01056083281420999\n",
      "[LOG 20200502-14:54:46] epoch: 16317 train-loss: 0.010560830434163412\n",
      "[LOG 20200502-14:54:46] epoch: 16318 train-loss: 0.010560828985439407\n",
      "[LOG 20200502-14:54:46] epoch: 16319 train-loss: 0.01056082701931397\n",
      "[LOG 20200502-14:54:46] epoch: 16320 train-loss: 0.010560825570589967\n",
      "[LOG 20200502-14:54:47] epoch: 16321 train-loss: 0.010560823397503959\n",
      "[LOG 20200502-14:54:47] epoch: 16322 train-loss: 0.01056082070701652\n",
      "[LOG 20200502-14:54:47] epoch: 16323 train-loss: 0.010560818326969942\n",
      "[LOG 20200502-14:54:47] epoch: 16324 train-loss: 0.0105608149121205\n",
      "[LOG 20200502-14:54:48] epoch: 16325 train-loss: 0.010560811704231633\n",
      "[LOG 20200502-14:54:48] epoch: 16326 train-loss: 0.010560808806783624\n",
      "[LOG 20200502-14:54:48] epoch: 16327 train-loss: 0.01056080466757218\n",
      "[LOG 20200502-14:54:48] epoch: 16328 train-loss: 0.010560801149242453\n",
      "[LOG 20200502-14:54:49] epoch: 16329 train-loss: 0.01056079649262958\n",
      "[LOG 20200502-14:54:49] epoch: 16330 train-loss: 0.010560791629056135\n",
      "[LOG 20200502-14:54:49] epoch: 16331 train-loss: 0.010560786558522118\n",
      "[LOG 20200502-14:54:49] epoch: 16332 train-loss: 0.010560781074066957\n",
      "[LOG 20200502-14:54:50] epoch: 16333 train-loss: 0.010560774968730079\n",
      "[LOG 20200502-14:54:50] epoch: 16334 train-loss: 0.010560768656432629\n",
      "[LOG 20200502-14:54:50] epoch: 16335 train-loss: 0.010560761826733748\n",
      "[LOG 20200502-14:54:50] epoch: 16336 train-loss: 0.010560755410956012\n",
      "[LOG 20200502-14:54:51] epoch: 16337 train-loss: 0.010560747960375415\n",
      "[LOG 20200502-14:54:51] epoch: 16338 train-loss: 0.010560740406314531\n",
      "[LOG 20200502-14:54:51] epoch: 16339 train-loss: 0.010560732438332505\n",
      "[LOG 20200502-14:54:51] epoch: 16340 train-loss: 0.01056072384946876\n",
      "[LOG 20200502-14:54:52] epoch: 16341 train-loss: 0.010560715260605017\n",
      "[LOG 20200502-14:54:52] epoch: 16342 train-loss: 0.010560706568260988\n",
      "[LOG 20200502-14:54:52] epoch: 16343 train-loss: 0.010560697358515527\n",
      "[LOG 20200502-14:54:52] epoch: 16344 train-loss: 0.010560688045289781\n",
      "[LOG 20200502-14:54:52] epoch: 16345 train-loss: 0.010560678421623178\n",
      "[LOG 20200502-14:54:53] epoch: 16346 train-loss: 0.010560669108397432\n",
      "[LOG 20200502-14:54:53] epoch: 16347 train-loss: 0.010560658656888537\n",
      "[LOG 20200502-14:54:53] epoch: 16348 train-loss: 0.01056064861930079\n",
      "[LOG 20200502-14:54:54] epoch: 16349 train-loss: 0.010560638788673613\n",
      "[LOG 20200502-14:54:54] epoch: 16350 train-loss: 0.01056062885456615\n",
      "[LOG 20200502-14:54:54] epoch: 16351 train-loss: 0.010560619437860118\n",
      "[LOG 20200502-14:54:54] epoch: 16352 train-loss: 0.010560609710713228\n",
      "[LOG 20200502-14:54:55] epoch: 16353 train-loss: 0.010560599880086051\n",
      "[LOG 20200502-14:54:55] epoch: 16354 train-loss: 0.010560590359899733\n",
      "[LOG 20200502-14:54:55] epoch: 16355 train-loss: 0.010560581046673987\n",
      "[LOG 20200502-14:54:55] epoch: 16356 train-loss: 0.010560572664770815\n",
      "[LOG 20200502-14:54:56] epoch: 16357 train-loss: 0.010560564075907072\n",
      "[LOG 20200502-14:54:56] epoch: 16358 train-loss: 0.010560555797484186\n",
      "[LOG 20200502-14:54:56] epoch: 16359 train-loss: 0.010560547726021873\n",
      "[LOG 20200502-14:54:56] epoch: 16360 train-loss: 0.010560540275441276\n",
      "[LOG 20200502-14:54:56] epoch: 16361 train-loss: 0.010560533031821251\n",
      "[LOG 20200502-14:54:57] epoch: 16362 train-loss: 0.010560526305602657\n",
      "[LOG 20200502-14:54:57] epoch: 16363 train-loss: 0.010560520303746065\n",
      "[LOG 20200502-14:54:57] epoch: 16364 train-loss: 0.01056051461233033\n",
      "[LOG 20200502-14:54:57] epoch: 16365 train-loss: 0.010560508920914598\n",
      "[LOG 20200502-14:54:58] epoch: 16366 train-loss: 0.01056050436778201\n",
      "[LOG 20200502-14:54:58] epoch: 16367 train-loss: 0.010560499711169137\n",
      "[LOG 20200502-14:54:58] epoch: 16368 train-loss: 0.010560495985878838\n",
      "[LOG 20200502-14:54:58] epoch: 16369 train-loss: 0.010560492157108255\n",
      "[LOG 20200502-14:54:59] epoch: 16370 train-loss: 0.01056048936314053\n",
      "[LOG 20200502-14:54:59] epoch: 16371 train-loss: 0.010560486776133379\n",
      "[LOG 20200502-14:54:59] epoch: 16372 train-loss: 0.010560484189126227\n",
      "[LOG 20200502-14:54:59] epoch: 16373 train-loss: 0.010560482326481078\n",
      "[LOG 20200502-14:55:00] epoch: 16374 train-loss: 0.010560480774276786\n",
      "[LOG 20200502-14:55:00] epoch: 16375 train-loss: 0.010560479842954211\n",
      "[LOG 20200502-14:55:00] epoch: 16376 train-loss: 0.010560478497710492\n",
      "[LOG 20200502-14:55:01] epoch: 16377 train-loss: 0.01056047829074992\n",
      "[LOG 20200502-14:55:01] epoch: 16378 train-loss: 0.010560477566387918\n",
      "[LOG 20200502-14:55:01] epoch: 16379 train-loss: 0.010560477359427346\n",
      "[LOG 20200502-14:55:01] epoch: 16380 train-loss: 0.010560477669868205\n",
      "[LOG 20200502-14:55:02] epoch: 16381 train-loss: 0.010560477566387918\n",
      "[LOG 20200502-14:55:02] epoch: 16382 train-loss: 0.010560477980309062\n",
      "[LOG 20200502-14:55:02] epoch: 16383 train-loss: 0.010560478394230207\n",
      "[LOG 20200502-14:55:02] epoch: 16384 train-loss: 0.010560478497710492\n",
      "[LOG 20200502-14:55:03] epoch: 16385 train-loss: 0.010560478911631636\n",
      "[LOG 20200502-14:55:03] epoch: 16386 train-loss: 0.010560479222072495\n",
      "[LOG 20200502-14:55:03] epoch: 16387 train-loss: 0.01056048015339507\n",
      "[LOG 20200502-14:55:03] epoch: 16388 train-loss: 0.010560480049914785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:55:04] epoch: 16389 train-loss: 0.010560479842954211\n",
      "[LOG 20200502-14:55:04] epoch: 16390 train-loss: 0.010560479635993639\n",
      "[LOG 20200502-14:55:04] epoch: 16391 train-loss: 0.010560479429033067\n",
      "[LOG 20200502-14:55:04] epoch: 16392 train-loss: 0.010560478911631636\n",
      "[LOG 20200502-14:55:05] epoch: 16393 train-loss: 0.010560478497710492\n",
      "[LOG 20200502-14:55:05] epoch: 16394 train-loss: 0.010560477462907633\n",
      "[LOG 20200502-14:55:05] epoch: 16395 train-loss: 0.01056047591070334\n",
      "[LOG 20200502-14:55:05] epoch: 16396 train-loss: 0.010560474048058191\n",
      "[LOG 20200502-14:55:06] epoch: 16397 train-loss: 0.01056047197845247\n",
      "[LOG 20200502-14:55:06] epoch: 16398 train-loss: 0.010560469701886177\n",
      "[LOG 20200502-14:55:06] epoch: 16399 train-loss: 0.010560466804438166\n",
      "[LOG 20200502-14:55:06] epoch: 16400 train-loss: 0.010560463596549299\n",
      "[LOG 20200502-14:55:06] epoch: 16401 train-loss: 0.010560459664298428\n",
      "[LOG 20200502-14:55:07] epoch: 16402 train-loss: 0.010560455835527845\n",
      "[LOG 20200502-14:55:07] epoch: 16403 train-loss: 0.010560451178914972\n",
      "[LOG 20200502-14:55:07] epoch: 16404 train-loss: 0.010560446418821812\n",
      "[LOG 20200502-14:55:07] epoch: 16405 train-loss: 0.010560441141327223\n",
      "[LOG 20200502-14:55:08] epoch: 16406 train-loss: 0.010560435449911488\n",
      "[LOG 20200502-14:55:08] epoch: 16407 train-loss: 0.01056042934457461\n",
      "[LOG 20200502-14:55:08] epoch: 16408 train-loss: 0.010560422618356016\n",
      "[LOG 20200502-14:55:08] epoch: 16409 train-loss: 0.010560415892137421\n",
      "[LOG 20200502-14:55:09] epoch: 16410 train-loss: 0.010560408751997683\n",
      "[LOG 20200502-14:55:09] epoch: 16411 train-loss: 0.010560400887495942\n",
      "[LOG 20200502-14:55:09] epoch: 16412 train-loss: 0.01056039354039563\n",
      "[LOG 20200502-14:55:09] epoch: 16413 train-loss: 0.010560385158492459\n",
      "[LOG 20200502-14:55:10] epoch: 16414 train-loss: 0.010560376673109002\n",
      "[LOG 20200502-14:55:10] epoch: 16415 train-loss: 0.01056036829120583\n",
      "[LOG 20200502-14:55:10] epoch: 16416 train-loss: 0.010560359495381514\n",
      "[LOG 20200502-14:55:10] epoch: 16417 train-loss: 0.010560351320438914\n",
      "[LOG 20200502-14:55:11] epoch: 16418 train-loss: 0.010560342110693455\n",
      "[LOG 20200502-14:55:11] epoch: 16419 train-loss: 0.010560332693987422\n",
      "[LOG 20200502-14:55:11] epoch: 16420 train-loss: 0.010560324001643393\n",
      "[LOG 20200502-14:55:11] epoch: 16421 train-loss: 0.010560315309299363\n",
      "[LOG 20200502-14:55:12] epoch: 16422 train-loss: 0.010560305685632758\n",
      "[LOG 20200502-14:55:12] epoch: 16423 train-loss: 0.010560297303729586\n",
      "[LOG 20200502-14:55:12] epoch: 16424 train-loss: 0.010560287783543268\n",
      "[LOG 20200502-14:55:12] epoch: 16425 train-loss: 0.010560279505120384\n",
      "[LOG 20200502-14:55:13] epoch: 16426 train-loss: 0.010560270812776353\n",
      "[LOG 20200502-14:55:13] epoch: 16427 train-loss: 0.01056026222391261\n",
      "[LOG 20200502-14:55:13] epoch: 16428 train-loss: 0.010560253842009438\n",
      "[LOG 20200502-14:55:13] epoch: 16429 train-loss: 0.010560245563586554\n",
      "[LOG 20200502-14:55:14] epoch: 16430 train-loss: 0.010560237492124239\n",
      "[LOG 20200502-14:55:14] epoch: 16431 train-loss: 0.010560229938063357\n",
      "[LOG 20200502-14:55:14] epoch: 16432 train-loss: 0.010560221763120757\n",
      "[LOG 20200502-14:55:14] epoch: 16433 train-loss: 0.010560214105579589\n",
      "[LOG 20200502-14:55:15] epoch: 16434 train-loss: 0.010560207275880707\n",
      "[LOG 20200502-14:55:15] epoch: 16435 train-loss: 0.010560199721819825\n",
      "[LOG 20200502-14:55:15] epoch: 16436 train-loss: 0.010560192064278655\n",
      "[LOG 20200502-14:55:15] epoch: 16437 train-loss: 0.010560185234579775\n",
      "[LOG 20200502-14:55:16] epoch: 16438 train-loss: 0.01056017799095975\n",
      "[LOG 20200502-14:55:16] epoch: 16439 train-loss: 0.010560170954300297\n",
      "[LOG 20200502-14:55:16] epoch: 16440 train-loss: 0.010560164538522562\n",
      "[LOG 20200502-14:55:16] epoch: 16441 train-loss: 0.010560157501863109\n",
      "[LOG 20200502-14:55:16] epoch: 16442 train-loss: 0.010560150051282512\n",
      "[LOG 20200502-14:55:17] epoch: 16443 train-loss: 0.010560143945945634\n",
      "[LOG 20200502-14:55:17] epoch: 16444 train-loss: 0.010560136495365037\n",
      "[LOG 20200502-14:55:17] epoch: 16445 train-loss: 0.01056012956218587\n",
      "[LOG 20200502-14:55:17] epoch: 16446 train-loss: 0.010560122422046132\n",
      "[LOG 20200502-14:55:18] epoch: 16447 train-loss: 0.010560115281906392\n",
      "[LOG 20200502-14:55:18] epoch: 16448 train-loss: 0.01056010876264837\n",
      "[LOG 20200502-14:55:18] epoch: 16449 train-loss: 0.010560101105107201\n",
      "[LOG 20200502-14:55:18] epoch: 16450 train-loss: 0.010560093654526604\n",
      "[LOG 20200502-14:55:19] epoch: 16451 train-loss: 0.01056008537610372\n",
      "[LOG 20200502-14:55:19] epoch: 16452 train-loss: 0.010560076994200548\n",
      "[LOG 20200502-14:55:19] epoch: 16453 train-loss: 0.010560069233179092\n",
      "[LOG 20200502-14:55:19] epoch: 16454 train-loss: 0.01056006033387449\n",
      "[LOG 20200502-14:55:20] epoch: 16455 train-loss: 0.010560051434569888\n",
      "[LOG 20200502-14:55:20] epoch: 16456 train-loss: 0.010560042535265287\n",
      "[LOG 20200502-14:55:20] epoch: 16457 train-loss: 0.010560033325519826\n",
      "[LOG 20200502-14:55:21] epoch: 16458 train-loss: 0.010560023184451792\n",
      "[LOG 20200502-14:55:21] epoch: 16459 train-loss: 0.010560013146864044\n",
      "[LOG 20200502-14:55:21] epoch: 16460 train-loss: 0.010560002384914292\n",
      "[LOG 20200502-14:55:21] epoch: 16461 train-loss: 0.010559991726444827\n",
      "[LOG 20200502-14:55:22] epoch: 16462 train-loss: 0.010559980033172501\n",
      "[LOG 20200502-14:55:22] epoch: 16463 train-loss: 0.010559968857301606\n",
      "[LOG 20200502-14:55:22] epoch: 16464 train-loss: 0.010559956439667277\n",
      "[LOG 20200502-14:55:22] epoch: 16465 train-loss: 0.010559943815072378\n",
      "[LOG 20200502-14:55:23] epoch: 16466 train-loss: 0.010559931293957762\n",
      "[LOG 20200502-14:55:23] epoch: 16467 train-loss: 0.010559917841520574\n",
      "[LOG 20200502-14:55:23] epoch: 16468 train-loss: 0.010559904182122814\n",
      "[LOG 20200502-14:55:23] epoch: 16469 train-loss: 0.010559890108803907\n",
      "[LOG 20200502-14:55:24] epoch: 16470 train-loss: 0.010559876035485003\n",
      "[LOG 20200502-14:55:24] epoch: 16471 train-loss: 0.01055986165172524\n",
      "[LOG 20200502-14:55:24] epoch: 16472 train-loss: 0.010559846854044331\n",
      "[LOG 20200502-14:55:24] epoch: 16473 train-loss: 0.010559831332001422\n",
      "[LOG 20200502-14:55:25] epoch: 16474 train-loss: 0.010559816741281085\n",
      "[LOG 20200502-14:55:25] epoch: 16475 train-loss: 0.01055980059835646\n",
      "[LOG 20200502-14:55:25] epoch: 16476 train-loss: 0.010559786007636122\n",
      "[LOG 20200502-14:55:25] epoch: 16477 train-loss: 0.010559770692553785\n",
      "[LOG 20200502-14:55:25] epoch: 16478 train-loss: 0.010559754653109444\n",
      "[LOG 20200502-14:55:26] epoch: 16479 train-loss: 0.010559739441507392\n",
      "[LOG 20200502-14:55:26] epoch: 16480 train-loss: 0.010559724022944769\n",
      "[LOG 20200502-14:55:26] epoch: 16481 train-loss: 0.01055970870786243\n",
      "[LOG 20200502-14:55:26] epoch: 16482 train-loss: 0.010559692978858948\n",
      "[LOG 20200502-14:55:27] epoch: 16483 train-loss: 0.010559678491618898\n",
      "[LOG 20200502-14:55:27] epoch: 16484 train-loss: 0.01055966369393799\n",
      "[LOG 20200502-14:55:27] epoch: 16485 train-loss: 0.0105596495171388\n",
      "[LOG 20200502-14:55:27] epoch: 16486 train-loss: 0.010559635236859322\n",
      "[LOG 20200502-14:55:28] epoch: 16487 train-loss: 0.010559621267020702\n",
      "[LOG 20200502-14:55:28] epoch: 16488 train-loss: 0.010559607918063799\n",
      "[LOG 20200502-14:55:28] epoch: 16489 train-loss: 0.010559594672587182\n",
      "[LOG 20200502-14:55:28] epoch: 16490 train-loss: 0.01055958235843314\n",
      "[LOG 20200502-14:55:29] epoch: 16491 train-loss: 0.010559570044279099\n",
      "[LOG 20200502-14:55:29] epoch: 16492 train-loss: 0.010559558557967344\n",
      "[LOG 20200502-14:55:29] epoch: 16493 train-loss: 0.010559547175135877\n",
      "[LOG 20200502-14:55:29] epoch: 16494 train-loss: 0.010559535895784697\n",
      "[LOG 20200502-14:55:30] epoch: 16495 train-loss: 0.010559526168637805\n",
      "[LOG 20200502-14:55:30] epoch: 16496 train-loss: 0.01055951602756977\n",
      "[LOG 20200502-14:55:30] epoch: 16497 train-loss: 0.010559506196942594\n",
      "[LOG 20200502-14:55:30] epoch: 16498 train-loss: 0.01055949760807885\n",
      "[LOG 20200502-14:55:30] epoch: 16499 train-loss: 0.010559489019215107\n",
      "[LOG 20200502-14:55:31] epoch: 16500 train-loss: 0.010559480637311935\n",
      "[LOG 20200502-14:55:31] epoch: 16501 train-loss: 0.010559473186731339\n",
      "[LOG 20200502-14:55:31] epoch: 16502 train-loss: 0.010559465839631028\n",
      "[LOG 20200502-14:55:31] epoch: 16503 train-loss: 0.010559458699491289\n",
      "[LOG 20200502-14:55:32] epoch: 16504 train-loss: 0.010559452490674125\n",
      "[LOG 20200502-14:55:32] epoch: 16505 train-loss: 0.010559446178376675\n",
      "[LOG 20200502-14:55:32] epoch: 16506 train-loss: 0.01055943976259894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:55:32] epoch: 16507 train-loss: 0.01055943469206492\n",
      "[LOG 20200502-14:55:33] epoch: 16508 train-loss: 0.010559429000649188\n",
      "[LOG 20200502-14:55:33] epoch: 16509 train-loss: 0.01055942393011517\n",
      "[LOG 20200502-14:55:33] epoch: 16510 train-loss: 0.010559418238699436\n",
      "[LOG 20200502-14:55:33] epoch: 16511 train-loss: 0.010559413271645704\n",
      "[LOG 20200502-14:55:34] epoch: 16512 train-loss: 0.010559407994151115\n",
      "[LOG 20200502-14:55:34] epoch: 16513 train-loss: 0.010559403544498814\n",
      "[LOG 20200502-14:55:34] epoch: 16514 train-loss: 0.010559399094846513\n",
      "[LOG 20200502-14:55:34] epoch: 16515 train-loss: 0.010559394231273068\n",
      "[LOG 20200502-14:55:35] epoch: 16516 train-loss: 0.010559389367699623\n",
      "[LOG 20200502-14:55:35] epoch: 16517 train-loss: 0.010559383986724747\n",
      "[LOG 20200502-14:55:35] epoch: 16518 train-loss: 0.010559379330111874\n",
      "[LOG 20200502-14:55:35] epoch: 16519 train-loss: 0.010559373638696141\n",
      "[LOG 20200502-14:55:35] epoch: 16520 train-loss: 0.010559369085563554\n",
      "[LOG 20200502-14:55:36] epoch: 16521 train-loss: 0.01055936287674639\n",
      "[LOG 20200502-14:55:36] epoch: 16522 train-loss: 0.0105593575992518\n",
      "[LOG 20200502-14:55:36] epoch: 16523 train-loss: 0.010559351907836067\n",
      "[LOG 20200502-14:55:36] epoch: 16524 train-loss: 0.010559345078137185\n",
      "[LOG 20200502-14:55:37] epoch: 16525 train-loss: 0.010559338765839735\n",
      "[LOG 20200502-14:55:37] epoch: 16526 train-loss: 0.010559331211778853\n",
      "[LOG 20200502-14:55:37] epoch: 16527 train-loss: 0.010559323864678541\n",
      "[LOG 20200502-14:55:37] epoch: 16528 train-loss: 0.010559316414097944\n",
      "[LOG 20200502-14:55:38] epoch: 16529 train-loss: 0.01055930813567506\n",
      "[LOG 20200502-14:55:38] epoch: 16530 train-loss: 0.010559299650291601\n",
      "[LOG 20200502-14:55:38] epoch: 16531 train-loss: 0.010559290854467286\n",
      "[LOG 20200502-14:55:38] epoch: 16532 train-loss: 0.010559281644721826\n",
      "[LOG 20200502-14:55:39] epoch: 16533 train-loss: 0.01055927233149608\n",
      "[LOG 20200502-14:55:39] epoch: 16534 train-loss: 0.010559262190428045\n",
      "[LOG 20200502-14:55:39] epoch: 16535 train-loss: 0.010559251428478293\n",
      "[LOG 20200502-14:55:39] epoch: 16536 train-loss: 0.010559241183929972\n",
      "[LOG 20200502-14:55:40] epoch: 16537 train-loss: 0.010559229594137933\n",
      "[LOG 20200502-14:55:40] epoch: 16538 train-loss: 0.010559218211306466\n",
      "[LOG 20200502-14:55:40] epoch: 16539 train-loss: 0.010559206724994712\n",
      "[LOG 20200502-14:55:40] epoch: 16540 train-loss: 0.010559194307360385\n",
      "[LOG 20200502-14:55:41] epoch: 16541 train-loss: 0.010559182200166915\n",
      "[LOG 20200502-14:55:41] epoch: 16542 train-loss: 0.010559169265131155\n",
      "[LOG 20200502-14:55:41] epoch: 16543 train-loss: 0.010559155916174253\n",
      "[LOG 20200502-14:55:41] epoch: 16544 train-loss: 0.010559142360256778\n",
      "[LOG 20200502-14:55:42] epoch: 16545 train-loss: 0.010559129114780162\n",
      "[LOG 20200502-14:55:42] epoch: 16546 train-loss: 0.010559115248421827\n",
      "[LOG 20200502-14:55:42] epoch: 16547 train-loss: 0.010559101692504354\n",
      "[LOG 20200502-14:55:42] epoch: 16548 train-loss: 0.010559088033106592\n",
      "[LOG 20200502-14:55:42] epoch: 16549 train-loss: 0.0105590738563074\n",
      "[LOG 20200502-14:55:43] epoch: 16550 train-loss: 0.01055906019690964\n",
      "[LOG 20200502-14:55:43] epoch: 16551 train-loss: 0.010559046330551306\n",
      "[LOG 20200502-14:55:43] epoch: 16552 train-loss: 0.010559032257232401\n",
      "[LOG 20200502-14:55:43] epoch: 16553 train-loss: 0.01055901859783464\n",
      "[LOG 20200502-14:55:44] epoch: 16554 train-loss: 0.010559004731476307\n",
      "[LOG 20200502-14:55:44] epoch: 16555 train-loss: 0.010558991072078546\n",
      "[LOG 20200502-14:55:44] epoch: 16556 train-loss: 0.010558977619641356\n",
      "[LOG 20200502-14:55:44] epoch: 16557 train-loss: 0.010558964477645027\n",
      "[LOG 20200502-14:55:45] epoch: 16558 train-loss: 0.01055895123216841\n",
      "[LOG 20200502-14:55:45] epoch: 16559 train-loss: 0.010558938193652365\n",
      "[LOG 20200502-14:55:45] epoch: 16560 train-loss: 0.010558925051656034\n",
      "[LOG 20200502-14:55:45] epoch: 16561 train-loss: 0.01055891253054142\n",
      "[LOG 20200502-14:55:46] epoch: 16562 train-loss: 0.010558899698985947\n",
      "[LOG 20200502-14:55:46] epoch: 16563 train-loss: 0.010558887281351619\n",
      "[LOG 20200502-14:55:46] epoch: 16564 train-loss: 0.010558875484599007\n",
      "[LOG 20200502-14:55:46] epoch: 16565 train-loss: 0.010558863170444965\n",
      "[LOG 20200502-14:55:47] epoch: 16566 train-loss: 0.010558851787613498\n",
      "[LOG 20200502-14:55:47] epoch: 16567 train-loss: 0.010558839473459456\n",
      "[LOG 20200502-14:55:47] epoch: 16568 train-loss: 0.01055882829758856\n",
      "[LOG 20200502-14:55:47] epoch: 16569 train-loss: 0.010558816604316235\n",
      "[LOG 20200502-14:55:47] epoch: 16570 train-loss: 0.01055880511800448\n",
      "[LOG 20200502-14:55:48] epoch: 16571 train-loss: 0.010558793942133585\n",
      "[LOG 20200502-14:55:48] epoch: 16572 train-loss: 0.010558782869742976\n",
      "[LOG 20200502-14:55:48] epoch: 16573 train-loss: 0.010558771383431222\n",
      "[LOG 20200502-14:55:48] epoch: 16574 train-loss: 0.010558760000599755\n",
      "[LOG 20200502-14:55:49] epoch: 16575 train-loss: 0.010558748617768288\n",
      "[LOG 20200502-14:55:49] epoch: 16576 train-loss: 0.010558736821015676\n",
      "[LOG 20200502-14:55:49] epoch: 16577 train-loss: 0.010558725438184209\n",
      "[LOG 20200502-14:55:49] epoch: 16578 train-loss: 0.010558713951872455\n",
      "[LOG 20200502-14:55:50] epoch: 16579 train-loss: 0.010558702258600129\n",
      "[LOG 20200502-14:55:50] epoch: 16580 train-loss: 0.010558690151406659\n",
      "[LOG 20200502-14:55:50] epoch: 16581 train-loss: 0.010558678354654048\n",
      "[LOG 20200502-14:55:50] epoch: 16582 train-loss: 0.010558666350940863\n",
      "[LOG 20200502-14:55:51] epoch: 16583 train-loss: 0.01055865351938539\n",
      "[LOG 20200502-14:55:51] epoch: 16584 train-loss: 0.010558640480869345\n",
      "[LOG 20200502-14:55:51] epoch: 16585 train-loss: 0.0105586274423533\n",
      "[LOG 20200502-14:55:51] epoch: 16586 train-loss: 0.010558614817758402\n",
      "[LOG 20200502-14:55:52] epoch: 16587 train-loss: 0.010558601054880355\n",
      "[LOG 20200502-14:55:52] epoch: 16588 train-loss: 0.01055858718852202\n",
      "[LOG 20200502-14:55:52] epoch: 16589 train-loss: 0.010558572494321398\n",
      "[LOG 20200502-14:55:52] epoch: 16590 train-loss: 0.010558557800120778\n",
      "[LOG 20200502-14:55:53] epoch: 16591 train-loss: 0.010558542278077867\n",
      "[LOG 20200502-14:55:53] epoch: 16592 train-loss: 0.01055852696299553\n",
      "[LOG 20200502-14:55:53] epoch: 16593 train-loss: 0.010558510923551189\n",
      "[LOG 20200502-14:55:53] epoch: 16594 train-loss: 0.01055849426322513\n",
      "[LOG 20200502-14:55:54] epoch: 16595 train-loss: 0.010558477809859646\n",
      "[LOG 20200502-14:55:54] epoch: 16596 train-loss: 0.010558460425171588\n",
      "[LOG 20200502-14:55:54] epoch: 16597 train-loss: 0.01055844273004267\n",
      "[LOG 20200502-14:55:54] epoch: 16598 train-loss: 0.010558425241874324\n",
      "[LOG 20200502-14:55:55] epoch: 16599 train-loss: 0.010558406615422832\n",
      "[LOG 20200502-14:55:55] epoch: 16600 train-loss: 0.010558388092451625\n",
      "[LOG 20200502-14:55:55] epoch: 16601 train-loss: 0.010558369362519847\n",
      "[LOG 20200502-14:55:55] epoch: 16602 train-loss: 0.010558350218666924\n",
      "[LOG 20200502-14:55:56] epoch: 16603 train-loss: 0.01055833086785343\n",
      "[LOG 20200502-14:55:56] epoch: 16604 train-loss: 0.010558311206599077\n",
      "[LOG 20200502-14:55:56] epoch: 16605 train-loss: 0.01055829164882501\n",
      "[LOG 20200502-14:55:56] epoch: 16606 train-loss: 0.010558271780610085\n",
      "[LOG 20200502-14:55:56] epoch: 16607 train-loss: 0.010558251498474015\n",
      "[LOG 20200502-14:55:57] epoch: 16608 train-loss: 0.010558231526778804\n",
      "[LOG 20200502-14:55:57] epoch: 16609 train-loss: 0.010558211658563878\n",
      "[LOG 20200502-14:55:57] epoch: 16610 train-loss: 0.010558191479908096\n",
      "[LOG 20200502-14:55:57] epoch: 16611 train-loss: 0.0105581721290946\n",
      "[LOG 20200502-14:55:58] epoch: 16612 train-loss: 0.010558152364359962\n",
      "[LOG 20200502-14:55:58] epoch: 16613 train-loss: 0.010558133841388755\n",
      "[LOG 20200502-14:55:58] epoch: 16614 train-loss: 0.010558113869693544\n",
      "[LOG 20200502-14:55:58] epoch: 16615 train-loss: 0.010558094622360336\n",
      "[LOG 20200502-14:55:59] epoch: 16616 train-loss: 0.010558075995908843\n",
      "[LOG 20200502-14:55:59] epoch: 16617 train-loss: 0.01055805767989821\n",
      "[LOG 20200502-14:55:59] epoch: 16618 train-loss: 0.010558039674328433\n",
      "[LOG 20200502-14:55:59] epoch: 16619 train-loss: 0.010558022496600946\n",
      "[LOG 20200502-14:56:00] epoch: 16620 train-loss: 0.010558004697991742\n",
      "[LOG 20200502-14:56:00] epoch: 16621 train-loss: 0.010557988555067115\n",
      "[LOG 20200502-14:56:00] epoch: 16622 train-loss: 0.010557972412142489\n",
      "[LOG 20200502-14:56:00] epoch: 16623 train-loss: 0.010557957200540436\n",
      "[LOG 20200502-14:56:01] epoch: 16624 train-loss: 0.01055794157501724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:56:01] epoch: 16625 train-loss: 0.010557926880816618\n",
      "[LOG 20200502-14:56:01] epoch: 16626 train-loss: 0.010557911876175139\n",
      "[LOG 20200502-14:56:01] epoch: 16627 train-loss: 0.010557898837659094\n",
      "[LOG 20200502-14:56:02] epoch: 16628 train-loss: 0.01055788579914305\n",
      "[LOG 20200502-14:56:02] epoch: 16629 train-loss: 0.01055787286410729\n",
      "[LOG 20200502-14:56:02] epoch: 16630 train-loss: 0.010557860860394107\n",
      "[LOG 20200502-14:56:02] epoch: 16631 train-loss: 0.010557848856680922\n",
      "[LOG 20200502-14:56:03] epoch: 16632 train-loss: 0.010557836956448026\n",
      "[LOG 20200502-14:56:03] epoch: 16633 train-loss: 0.010557826194498274\n",
      "[LOG 20200502-14:56:03] epoch: 16634 train-loss: 0.01055781553602881\n",
      "[LOG 20200502-14:56:03] epoch: 16635 train-loss: 0.010557805394960774\n",
      "[LOG 20200502-14:56:03] epoch: 16636 train-loss: 0.01055779525389274\n",
      "[LOG 20200502-14:56:04] epoch: 16637 train-loss: 0.010557785216304991\n",
      "[LOG 20200502-14:56:04] epoch: 16638 train-loss: 0.01055777600655953\n",
      "[LOG 20200502-14:56:04] epoch: 16639 train-loss: 0.010557766486373212\n",
      "[LOG 20200502-14:56:04] epoch: 16640 train-loss: 0.010557757380108038\n",
      "[LOG 20200502-14:56:05] epoch: 16641 train-loss: 0.010557748894724581\n",
      "[LOG 20200502-14:56:05] epoch: 16642 train-loss: 0.010557740305860838\n",
      "[LOG 20200502-14:56:05] epoch: 16643 train-loss: 0.010557731096115377\n",
      "[LOG 20200502-14:56:05] epoch: 16644 train-loss: 0.010557722300291061\n",
      "[LOG 20200502-14:56:06] epoch: 16645 train-loss: 0.010557713814907603\n",
      "[LOG 20200502-14:56:06] epoch: 16646 train-loss: 0.010557705433004431\n",
      "[LOG 20200502-14:56:06] epoch: 16647 train-loss: 0.01055769705110126\n",
      "[LOG 20200502-14:56:06] epoch: 16648 train-loss: 0.010557688462237516\n",
      "[LOG 20200502-14:56:07] epoch: 16649 train-loss: 0.01055767997685406\n",
      "[LOG 20200502-14:56:07] epoch: 16650 train-loss: 0.010557670353187455\n",
      "[LOG 20200502-14:56:07] epoch: 16651 train-loss: 0.010557661039961709\n",
      "[LOG 20200502-14:56:07] epoch: 16652 train-loss: 0.010557652347617678\n",
      "[LOG 20200502-14:56:08] epoch: 16653 train-loss: 0.010557642723951075\n",
      "[LOG 20200502-14:56:08] epoch: 16654 train-loss: 0.01055763310028447\n",
      "[LOG 20200502-14:56:08] epoch: 16655 train-loss: 0.010557623787058724\n",
      "[LOG 20200502-14:56:08] epoch: 16656 train-loss: 0.010557613335549831\n",
      "[LOG 20200502-14:56:09] epoch: 16657 train-loss: 0.010557602780560652\n",
      "[LOG 20200502-14:56:09] epoch: 16658 train-loss: 0.010557591604689756\n",
      "[LOG 20200502-14:56:09] epoch: 16659 train-loss: 0.010557580842740007\n",
      "[LOG 20200502-14:56:09] epoch: 16660 train-loss: 0.01055756914946768\n",
      "[LOG 20200502-14:56:09] epoch: 16661 train-loss: 0.010557556938793924\n",
      "[LOG 20200502-14:56:10] epoch: 16662 train-loss: 0.010557545349001884\n",
      "[LOG 20200502-14:56:10] epoch: 16663 train-loss: 0.010557532207005553\n",
      "[LOG 20200502-14:56:10] epoch: 16664 train-loss: 0.010557518444127507\n",
      "[LOG 20200502-14:56:10] epoch: 16665 train-loss: 0.01055750519865089\n",
      "[LOG 20200502-14:56:11] epoch: 16666 train-loss: 0.0105574917462137\n",
      "[LOG 20200502-14:56:11] epoch: 16667 train-loss: 0.010557477465934224\n",
      "[LOG 20200502-14:56:11] epoch: 16668 train-loss: 0.010557462875213888\n",
      "[LOG 20200502-14:56:11] epoch: 16669 train-loss: 0.010557447663611837\n",
      "[LOG 20200502-14:56:12] epoch: 16670 train-loss: 0.010557432865930928\n",
      "[LOG 20200502-14:56:12] epoch: 16671 train-loss: 0.010557417136927446\n",
      "[LOG 20200502-14:56:12] epoch: 16672 train-loss: 0.010557400890522532\n",
      "[LOG 20200502-14:56:12] epoch: 16673 train-loss: 0.010557384747597907\n",
      "[LOG 20200502-14:56:13] epoch: 16674 train-loss: 0.010557368915114138\n",
      "[LOG 20200502-14:56:13] epoch: 16675 train-loss: 0.010557351840866936\n",
      "[LOG 20200502-14:56:13] epoch: 16676 train-loss: 0.01055733518054088\n",
      "[LOG 20200502-14:56:13] epoch: 16677 train-loss: 0.010557318106293678\n",
      "[LOG 20200502-14:56:14] epoch: 16678 train-loss: 0.010557300928566191\n",
      "[LOG 20200502-14:56:14] epoch: 16679 train-loss: 0.010557283440397846\n",
      "[LOG 20200502-14:56:14] epoch: 16680 train-loss: 0.010557266159190072\n",
      "[LOG 20200502-14:56:14] epoch: 16681 train-loss: 0.010557248877982298\n",
      "[LOG 20200502-14:56:15] epoch: 16682 train-loss: 0.010557231493294239\n",
      "[LOG 20200502-14:56:15] epoch: 16683 train-loss: 0.010557214419047037\n",
      "[LOG 20200502-14:56:15] epoch: 16684 train-loss: 0.010557196930878691\n",
      "[LOG 20200502-14:56:15] epoch: 16685 train-loss: 0.010557179546190633\n",
      "[LOG 20200502-14:56:16] epoch: 16686 train-loss: 0.010557162678904004\n",
      "[LOG 20200502-14:56:16] epoch: 16687 train-loss: 0.010557145811617374\n",
      "[LOG 20200502-14:56:16] epoch: 16688 train-loss: 0.010557128944330745\n",
      "[LOG 20200502-14:56:16] epoch: 16689 train-loss: 0.010557112077044116\n",
      "[LOG 20200502-14:56:16] epoch: 16690 train-loss: 0.010557095727158917\n",
      "[LOG 20200502-14:56:17] epoch: 16691 train-loss: 0.010557079791194864\n",
      "[LOG 20200502-14:56:17] epoch: 16692 train-loss: 0.010557063130868806\n",
      "[LOG 20200502-14:56:17] epoch: 16693 train-loss: 0.010557047505345609\n",
      "[LOG 20200502-14:56:17] epoch: 16694 train-loss: 0.010557032293743558\n",
      "[LOG 20200502-14:56:18] epoch: 16695 train-loss: 0.010557016357779503\n",
      "[LOG 20200502-14:56:18] epoch: 16696 train-loss: 0.010557000628776021\n",
      "[LOG 20200502-14:56:18] epoch: 16697 train-loss: 0.010556985417173969\n",
      "[LOG 20200502-14:56:18] epoch: 16698 train-loss: 0.010556970516012775\n",
      "[LOG 20200502-14:56:19] epoch: 16699 train-loss: 0.010556955821812153\n",
      "[LOG 20200502-14:56:19] epoch: 16700 train-loss: 0.010556940713690387\n",
      "[LOG 20200502-14:56:19] epoch: 16701 train-loss: 0.010556925812529193\n",
      "[LOG 20200502-14:56:19] epoch: 16702 train-loss: 0.010556911325289143\n",
      "[LOG 20200502-14:56:20] epoch: 16703 train-loss: 0.010556897045009665\n",
      "[LOG 20200502-14:56:20] epoch: 16704 train-loss: 0.010556883075171046\n",
      "[LOG 20200502-14:56:20] epoch: 16705 train-loss: 0.010556869001852142\n",
      "[LOG 20200502-14:56:20] epoch: 16706 train-loss: 0.010556854100690948\n",
      "[LOG 20200502-14:56:21] epoch: 16707 train-loss: 0.010556839923891757\n",
      "[LOG 20200502-14:56:21] epoch: 16708 train-loss: 0.010556825643612279\n",
      "[LOG 20200502-14:56:21] epoch: 16709 train-loss: 0.010556811466813087\n",
      "[LOG 20200502-14:56:21] epoch: 16710 train-loss: 0.010556796772612466\n",
      "[LOG 20200502-14:56:22] epoch: 16711 train-loss: 0.01055678218189213\n",
      "[LOG 20200502-14:56:22] epoch: 16712 train-loss: 0.010556767280730937\n",
      "[LOG 20200502-14:56:22] epoch: 16713 train-loss: 0.010556752793490887\n",
      "[LOG 20200502-14:56:22] epoch: 16714 train-loss: 0.010556736857526831\n",
      "[LOG 20200502-14:56:22] epoch: 16715 train-loss: 0.010556721335483922\n",
      "[LOG 20200502-14:56:23] epoch: 16716 train-loss: 0.010556705399519868\n",
      "[LOG 20200502-14:56:23] epoch: 16717 train-loss: 0.010556689877476957\n",
      "[LOG 20200502-14:56:23] epoch: 16718 train-loss: 0.010556673424111472\n",
      "[LOG 20200502-14:56:23] epoch: 16719 train-loss: 0.010556656142903699\n",
      "[LOG 20200502-14:56:24] epoch: 16720 train-loss: 0.010556639586057927\n",
      "[LOG 20200502-14:56:24] epoch: 16721 train-loss: 0.010556622097889582\n",
      "[LOG 20200502-14:56:24] epoch: 16722 train-loss: 0.010556604092319807\n",
      "[LOG 20200502-14:56:24] epoch: 16723 train-loss: 0.010556585776309172\n",
      "[LOG 20200502-14:56:25] epoch: 16724 train-loss: 0.010556566735936536\n",
      "[LOG 20200502-14:56:25] epoch: 16725 train-loss: 0.010556547592083612\n",
      "[LOG 20200502-14:56:25] epoch: 16726 train-loss: 0.010556528137789832\n",
      "[LOG 20200502-14:56:25] epoch: 16727 train-loss: 0.01055650795913405\n",
      "[LOG 20200502-14:56:26] epoch: 16728 train-loss: 0.010556488090919124\n",
      "[LOG 20200502-14:56:26] epoch: 16729 train-loss: 0.010556466567019621\n",
      "[LOG 20200502-14:56:26] epoch: 16730 train-loss: 0.010556445457041264\n",
      "[LOG 20200502-14:56:26] epoch: 16731 train-loss: 0.010556423519220617\n",
      "[LOG 20200502-14:56:27] epoch: 16732 train-loss: 0.010556402202281687\n",
      "[LOG 20200502-14:56:27] epoch: 16733 train-loss: 0.010556380057500469\n",
      "[LOG 20200502-14:56:27] epoch: 16734 train-loss: 0.010556357809238963\n",
      "[LOG 20200502-14:56:27] epoch: 16735 train-loss: 0.010556334526174597\n",
      "[LOG 20200502-14:56:28] epoch: 16736 train-loss: 0.010556311450070806\n",
      "[LOG 20200502-14:56:28] epoch: 16737 train-loss: 0.010556288477447297\n",
      "[LOG 20200502-14:56:28] epoch: 16738 train-loss: 0.010556265090902647\n",
      "[LOG 20200502-14:56:28] epoch: 16739 train-loss: 0.010556242428719997\n",
      "[LOG 20200502-14:56:29] epoch: 16740 train-loss: 0.010556218628254201\n",
      "[LOG 20200502-14:56:29] epoch: 16741 train-loss: 0.010556195448670123\n",
      "[LOG 20200502-14:56:29] epoch: 16742 train-loss: 0.010556171855164899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:56:29] epoch: 16743 train-loss: 0.010556149089501964\n",
      "[LOG 20200502-14:56:30] epoch: 16744 train-loss: 0.01055612601339817\n",
      "[LOG 20200502-14:56:30] epoch: 16745 train-loss: 0.010556103247735236\n",
      "[LOG 20200502-14:56:30] epoch: 16746 train-loss: 0.010556081102954017\n",
      "[LOG 20200502-14:56:30] epoch: 16747 train-loss: 0.010556058544251654\n",
      "[LOG 20200502-14:56:31] epoch: 16748 train-loss: 0.010556037020352151\n",
      "[LOG 20200502-14:56:31] epoch: 16749 train-loss: 0.010556015392972363\n",
      "[LOG 20200502-14:56:31] epoch: 16750 train-loss: 0.010555994076033434\n",
      "[LOG 20200502-14:56:31] epoch: 16751 train-loss: 0.01055597389737765\n",
      "[LOG 20200502-14:56:32] epoch: 16752 train-loss: 0.010555953511761295\n",
      "[LOG 20200502-14:56:32] epoch: 16753 train-loss: 0.010555934057467513\n",
      "[LOG 20200502-14:56:32] epoch: 16754 train-loss: 0.010555915120575163\n",
      "[LOG 20200502-14:56:32] epoch: 16755 train-loss: 0.010555895769761669\n",
      "[LOG 20200502-14:56:33] epoch: 16756 train-loss: 0.010555877971152464\n",
      "[LOG 20200502-14:56:33] epoch: 16757 train-loss: 0.010555860482984118\n",
      "[LOG 20200502-14:56:33] epoch: 16758 train-loss: 0.0105558427878552\n",
      "[LOG 20200502-14:56:33] epoch: 16759 train-loss: 0.010555826024048857\n",
      "[LOG 20200502-14:56:34] epoch: 16760 train-loss: 0.010555810502005948\n",
      "[LOG 20200502-14:56:34] epoch: 16761 train-loss: 0.010555794255601035\n",
      "[LOG 20200502-14:56:34] epoch: 16762 train-loss: 0.010555778733558126\n",
      "[LOG 20200502-14:56:34] epoch: 16763 train-loss: 0.010555763728916645\n",
      "[LOG 20200502-14:56:35] epoch: 16764 train-loss: 0.010555749241676595\n",
      "[LOG 20200502-14:56:35] epoch: 16765 train-loss: 0.010555735789239407\n",
      "[LOG 20200502-14:56:35] epoch: 16766 train-loss: 0.010555721819400787\n",
      "[LOG 20200502-14:56:35] epoch: 16767 train-loss: 0.01055570888436503\n",
      "[LOG 20200502-14:56:35] epoch: 16768 train-loss: 0.01055569543192784\n",
      "[LOG 20200502-14:56:36] epoch: 16769 train-loss: 0.010555683014293512\n",
      "[LOG 20200502-14:56:36] epoch: 16770 train-loss: 0.010555670907100042\n",
      "[LOG 20200502-14:56:36] epoch: 16771 train-loss: 0.010555658385985427\n",
      "[LOG 20200502-14:56:36] epoch: 16772 train-loss: 0.010555646175311672\n",
      "[LOG 20200502-14:56:37] epoch: 16773 train-loss: 0.010555634999440776\n",
      "[LOG 20200502-14:56:37] epoch: 16774 train-loss: 0.010555622892247306\n",
      "[LOG 20200502-14:56:37] epoch: 16775 train-loss: 0.010555611509415839\n",
      "[LOG 20200502-14:56:37] epoch: 16776 train-loss: 0.01055559960918294\n",
      "[LOG 20200502-14:56:38] epoch: 16777 train-loss: 0.010555588640272617\n",
      "[LOG 20200502-14:56:38] epoch: 16778 train-loss: 0.010555577050480578\n",
      "[LOG 20200502-14:56:38] epoch: 16779 train-loss: 0.010555566392011113\n",
      "[LOG 20200502-14:56:38] epoch: 16780 train-loss: 0.010555554181337357\n",
      "[LOG 20200502-14:56:39] epoch: 16781 train-loss: 0.010555542384584745\n",
      "[LOG 20200502-14:56:39] epoch: 16782 train-loss: 0.010555530898272991\n",
      "[LOG 20200502-14:56:39] epoch: 16783 train-loss: 0.010555519308480952\n",
      "[LOG 20200502-14:56:39] epoch: 16784 train-loss: 0.010555506580405764\n",
      "[LOG 20200502-14:56:40] epoch: 16785 train-loss: 0.01055549488713344\n",
      "[LOG 20200502-14:56:40] epoch: 16786 train-loss: 0.01055548226253854\n",
      "[LOG 20200502-14:56:40] epoch: 16787 train-loss: 0.01055546984490421\n",
      "[LOG 20200502-14:56:40] epoch: 16788 train-loss: 0.010555457116829025\n",
      "[LOG 20200502-14:56:41] epoch: 16789 train-loss: 0.01055544356091155\n",
      "[LOG 20200502-14:56:41] epoch: 16790 train-loss: 0.010555429798033502\n",
      "[LOG 20200502-14:56:41] epoch: 16791 train-loss: 0.010555416242116027\n",
      "[LOG 20200502-14:56:41] epoch: 16792 train-loss: 0.010555401651395692\n",
      "[LOG 20200502-14:56:42] epoch: 16793 train-loss: 0.010555387060675357\n",
      "[LOG 20200502-14:56:42] epoch: 16794 train-loss: 0.010555372366474735\n",
      "[LOG 20200502-14:56:42] epoch: 16795 train-loss: 0.01055535694791211\n",
      "[LOG 20200502-14:56:42] epoch: 16796 train-loss: 0.010555341218908628\n",
      "[LOG 20200502-14:56:43] epoch: 16797 train-loss: 0.010555325075984001\n",
      "[LOG 20200502-14:56:43] epoch: 16798 train-loss: 0.01055530903653966\n",
      "[LOG 20200502-14:56:43] epoch: 16799 train-loss: 0.010555291962292459\n",
      "[LOG 20200502-14:56:43] epoch: 16800 train-loss: 0.010555274681084685\n",
      "[LOG 20200502-14:56:44] epoch: 16801 train-loss: 0.010555256882475482\n",
      "[LOG 20200502-14:56:44] epoch: 16802 train-loss: 0.010555238980385993\n",
      "[LOG 20200502-14:56:44] epoch: 16803 train-loss: 0.010555220974816216\n",
      "[LOG 20200502-14:56:44] epoch: 16804 train-loss: 0.01055520245184501\n",
      "[LOG 20200502-14:56:45] epoch: 16805 train-loss: 0.010555183721913232\n",
      "[LOG 20200502-14:56:45] epoch: 16806 train-loss: 0.01055516478502088\n",
      "[LOG 20200502-14:56:45] epoch: 16807 train-loss: 0.0105551453307271\n",
      "[LOG 20200502-14:56:45] epoch: 16808 train-loss: 0.010555125462512175\n",
      "[LOG 20200502-14:56:46] epoch: 16809 train-loss: 0.010555106008218395\n",
      "[LOG 20200502-14:56:46] epoch: 16810 train-loss: 0.010555085933042897\n",
      "[LOG 20200502-14:56:46] epoch: 16811 train-loss: 0.010555065443946255\n",
      "[LOG 20200502-14:56:46] epoch: 16812 train-loss: 0.010555045161810186\n",
      "[LOG 20200502-14:56:47] epoch: 16813 train-loss: 0.010555025397075547\n",
      "[LOG 20200502-14:56:47] epoch: 16814 train-loss: 0.010555005011459192\n",
      "[LOG 20200502-14:56:47] epoch: 16815 train-loss: 0.010554984936283695\n",
      "[LOG 20200502-14:56:47] epoch: 16816 train-loss: 0.010554964550667338\n",
      "[LOG 20200502-14:56:48] epoch: 16817 train-loss: 0.010554944889412986\n",
      "[LOG 20200502-14:56:48] epoch: 16818 train-loss: 0.010554924607276917\n",
      "[LOG 20200502-14:56:48] epoch: 16819 train-loss: 0.010554904428621134\n",
      "[LOG 20200502-14:56:48] epoch: 16820 train-loss: 0.010554884043004777\n",
      "[LOG 20200502-14:56:49] epoch: 16821 train-loss: 0.010554864071309566\n",
      "[LOG 20200502-14:56:49] epoch: 16822 train-loss: 0.010554845548338361\n",
      "[LOG 20200502-14:56:49] epoch: 16823 train-loss: 0.010554825369682577\n",
      "[LOG 20200502-14:56:49] epoch: 16824 train-loss: 0.010554806536270512\n",
      "[LOG 20200502-14:56:49] epoch: 16825 train-loss: 0.010554787495897876\n",
      "[LOG 20200502-14:56:50] epoch: 16826 train-loss: 0.010554768765966097\n",
      "[LOG 20200502-14:56:50] epoch: 16827 train-loss: 0.010554750346475177\n",
      "[LOG 20200502-14:56:50] epoch: 16828 train-loss: 0.0105547316165434\n",
      "[LOG 20200502-14:56:50] epoch: 16829 train-loss: 0.010554713610973623\n",
      "[LOG 20200502-14:56:51] epoch: 16830 train-loss: 0.010554696329765849\n",
      "[LOG 20200502-14:56:51] epoch: 16831 train-loss: 0.01055467842767636\n",
      "[LOG 20200502-14:56:51] epoch: 16832 train-loss: 0.010554660732547442\n",
      "[LOG 20200502-14:56:51] epoch: 16833 train-loss: 0.010554642520017095\n",
      "[LOG 20200502-14:56:52] epoch: 16834 train-loss: 0.010554625445769893\n",
      "[LOG 20200502-14:56:52] epoch: 16835 train-loss: 0.010554608371522691\n",
      "[LOG 20200502-14:56:52] epoch: 16836 train-loss: 0.010554591400755776\n",
      "[LOG 20200502-14:56:52] epoch: 16837 train-loss: 0.010554574533469148\n",
      "[LOG 20200502-14:56:53] epoch: 16838 train-loss: 0.010554557562702231\n",
      "[LOG 20200502-14:56:53] epoch: 16839 train-loss: 0.010554540798895888\n",
      "[LOG 20200502-14:56:53] epoch: 16840 train-loss: 0.010554523828128973\n",
      "[LOG 20200502-14:56:53] epoch: 16841 train-loss: 0.010554507167802917\n",
      "[LOG 20200502-14:56:54] epoch: 16842 train-loss: 0.010554489783114858\n",
      "[LOG 20200502-14:56:54] epoch: 16843 train-loss: 0.010554472915828228\n",
      "[LOG 20200502-14:56:54] epoch: 16844 train-loss: 0.010554456048541598\n",
      "[LOG 20200502-14:56:54] epoch: 16845 train-loss: 0.010554438249932395\n",
      "[LOG 20200502-14:56:55] epoch: 16846 train-loss: 0.010554420865244336\n",
      "[LOG 20200502-14:56:55] epoch: 16847 train-loss: 0.01055440285967456\n",
      "[LOG 20200502-14:56:55] epoch: 16848 train-loss: 0.010554385474986501\n",
      "[LOG 20200502-14:56:55] epoch: 16849 train-loss: 0.010554367158975866\n",
      "[LOG 20200502-14:56:55] epoch: 16850 train-loss: 0.010554348946445517\n",
      "[LOG 20200502-14:56:56] epoch: 16851 train-loss: 0.010554330319994025\n",
      "[LOG 20200502-14:56:56] epoch: 16852 train-loss: 0.010554311176141104\n",
      "[LOG 20200502-14:56:56] epoch: 16853 train-loss: 0.010554291721847322\n",
      "[LOG 20200502-14:56:56] epoch: 16854 train-loss: 0.010554271336230967\n",
      "[LOG 20200502-14:56:57] epoch: 16855 train-loss: 0.010554251468016041\n",
      "[LOG 20200502-14:56:57] epoch: 16856 train-loss: 0.010554230771958828\n",
      "[LOG 20200502-14:56:57] epoch: 16857 train-loss: 0.010554209972421328\n",
      "[LOG 20200502-14:56:57] epoch: 16858 train-loss: 0.010554188552002111\n",
      "[LOG 20200502-14:56:58] epoch: 16859 train-loss: 0.010554165993299749\n",
      "[LOG 20200502-14:56:58] epoch: 16860 train-loss: 0.010554144055479102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:56:58] epoch: 16861 train-loss: 0.010554121186335882\n",
      "[LOG 20200502-14:56:58] epoch: 16862 train-loss: 0.010554098006751802\n",
      "[LOG 20200502-14:56:59] epoch: 16863 train-loss: 0.010554074309766293\n",
      "[LOG 20200502-14:56:59] epoch: 16864 train-loss: 0.010554050612780783\n",
      "[LOG 20200502-14:56:59] epoch: 16865 train-loss: 0.010554025880992413\n",
      "[LOG 20200502-14:56:59] epoch: 16866 train-loss: 0.01055400177008576\n",
      "[LOG 20200502-14:57:00] epoch: 16867 train-loss: 0.0105539762104551\n",
      "[LOG 20200502-14:57:00] epoch: 16868 train-loss: 0.01055395065082444\n",
      "[LOG 20200502-14:57:00] epoch: 16869 train-loss: 0.010553925401634641\n",
      "[LOG 20200502-14:57:00] epoch: 16870 train-loss: 0.010553899738523696\n",
      "[LOG 20200502-14:57:01] epoch: 16871 train-loss: 0.01055387407541275\n",
      "[LOG 20200502-14:57:01] epoch: 16872 train-loss: 0.010553848205341233\n",
      "[LOG 20200502-14:57:01] epoch: 16873 train-loss: 0.010553822335269716\n",
      "[LOG 20200502-14:57:01] epoch: 16874 train-loss: 0.010553796051277054\n",
      "[LOG 20200502-14:57:02] epoch: 16875 train-loss: 0.010553770491646396\n",
      "[LOG 20200502-14:57:02] epoch: 16876 train-loss: 0.010553744725055166\n",
      "[LOG 20200502-14:57:02] epoch: 16877 train-loss: 0.010553718751503361\n",
      "[LOG 20200502-14:57:02] epoch: 16878 train-loss: 0.010553693502313562\n",
      "[LOG 20200502-14:57:03] epoch: 16879 train-loss: 0.010553668046163188\n",
      "[LOG 20200502-14:57:03] epoch: 16880 train-loss: 0.010553643831776248\n",
      "[LOG 20200502-14:57:03] epoch: 16881 train-loss: 0.010553618686066734\n",
      "[LOG 20200502-14:57:03] epoch: 16882 train-loss: 0.010553594471679794\n",
      "[LOG 20200502-14:57:03] epoch: 16883 train-loss: 0.010553570671213998\n",
      "[LOG 20200502-14:57:04] epoch: 16884 train-loss: 0.010553546870748201\n",
      "[LOG 20200502-14:57:04] epoch: 16885 train-loss: 0.010553523794644408\n",
      "[LOG 20200502-14:57:04] epoch: 16886 train-loss: 0.010553501856823763\n",
      "[LOG 20200502-14:57:04] epoch: 16887 train-loss: 0.0105534792981214\n",
      "[LOG 20200502-14:57:05] epoch: 16888 train-loss: 0.010553457877702184\n",
      "[LOG 20200502-14:57:05] epoch: 16889 train-loss: 0.010553436974684397\n",
      "[LOG 20200502-14:57:05] epoch: 16890 train-loss: 0.010553416485587755\n",
      "[LOG 20200502-14:57:05] epoch: 16891 train-loss: 0.01055339641041226\n",
      "[LOG 20200502-14:57:06] epoch: 16892 train-loss: 0.010553376852638192\n",
      "[LOG 20200502-14:57:06] epoch: 16893 train-loss: 0.010553358329666985\n",
      "[LOG 20200502-14:57:06] epoch: 16894 train-loss: 0.010553339496254921\n",
      "[LOG 20200502-14:57:06] epoch: 16895 train-loss: 0.010553321697645716\n",
      "[LOG 20200502-14:57:07] epoch: 16896 train-loss: 0.010553303795556227\n",
      "[LOG 20200502-14:57:07] epoch: 16897 train-loss: 0.01055328713523017\n",
      "[LOG 20200502-14:57:07] epoch: 16898 train-loss: 0.010553270681864686\n",
      "[LOG 20200502-14:57:07] epoch: 16899 train-loss: 0.0105532542284992\n",
      "[LOG 20200502-14:57:08] epoch: 16900 train-loss: 0.010553238396015432\n",
      "[LOG 20200502-14:57:08] epoch: 16901 train-loss: 0.010553222977452807\n",
      "[LOG 20200502-14:57:08] epoch: 16902 train-loss: 0.010553207972811328\n",
      "[LOG 20200502-14:57:08] epoch: 16903 train-loss: 0.010553192864689562\n",
      "[LOG 20200502-14:57:08] epoch: 16904 train-loss: 0.010553178273969226\n",
      "[LOG 20200502-14:57:09] epoch: 16905 train-loss: 0.010553163786729177\n",
      "[LOG 20200502-14:57:09] epoch: 16906 train-loss: 0.010553150230811702\n",
      "[LOG 20200502-14:57:09] epoch: 16907 train-loss: 0.010553136260973083\n",
      "[LOG 20200502-14:57:09] epoch: 16908 train-loss: 0.010553122187654177\n",
      "[LOG 20200502-14:57:10] epoch: 16909 train-loss: 0.010553108735216988\n",
      "[LOG 20200502-14:57:10] epoch: 16910 train-loss: 0.010553095386260085\n",
      "[LOG 20200502-14:57:10] epoch: 16911 train-loss: 0.010553081726862324\n",
      "[LOG 20200502-14:57:10] epoch: 16912 train-loss: 0.010553068274425136\n",
      "[LOG 20200502-14:57:11] epoch: 16913 train-loss: 0.010553055339389376\n",
      "[LOG 20200502-14:57:11] epoch: 16914 train-loss: 0.01055304157651133\n",
      "[LOG 20200502-14:57:11] epoch: 16915 train-loss: 0.010553027710152997\n",
      "[LOG 20200502-14:57:11] epoch: 16916 train-loss: 0.010553014568156667\n",
      "[LOG 20200502-14:57:12] epoch: 16917 train-loss: 0.01055300080527862\n",
      "[LOG 20200502-14:57:12] epoch: 16918 train-loss: 0.010552987042400572\n",
      "[LOG 20200502-14:57:12] epoch: 16919 train-loss: 0.010552972555160522\n",
      "[LOG 20200502-14:57:12] epoch: 16920 train-loss: 0.010552958378361331\n",
      "[LOG 20200502-14:57:13] epoch: 16921 train-loss: 0.010552944098081853\n",
      "[LOG 20200502-14:57:13] epoch: 16922 train-loss: 0.010552929610841803\n",
      "[LOG 20200502-14:57:13] epoch: 16923 train-loss: 0.010552914295759466\n",
      "[LOG 20200502-14:57:13] epoch: 16924 train-loss: 0.010552898877196841\n",
      "[LOG 20200502-14:57:13] epoch: 16925 train-loss: 0.010552883458634218\n",
      "[LOG 20200502-14:57:14] epoch: 16926 train-loss: 0.010552867419189878\n",
      "[LOG 20200502-14:57:14] epoch: 16927 train-loss: 0.010552851379745536\n",
      "[LOG 20200502-14:57:14] epoch: 16928 train-loss: 0.010552834926380051\n",
      "[LOG 20200502-14:57:14] epoch: 16929 train-loss: 0.010552818162573708\n",
      "[LOG 20200502-14:57:15] epoch: 16930 train-loss: 0.010552801398767365\n",
      "[LOG 20200502-14:57:15] epoch: 16931 train-loss: 0.010552783289717304\n",
      "[LOG 20200502-14:57:15] epoch: 16932 train-loss: 0.010552765698068671\n",
      "[LOG 20200502-14:57:15] epoch: 16933 train-loss: 0.010552747278577752\n",
      "[LOG 20200502-14:57:16] epoch: 16934 train-loss: 0.010552728755606545\n",
      "[LOG 20200502-14:57:16] epoch: 16935 train-loss: 0.010552709818714194\n",
      "[LOG 20200502-14:57:16] epoch: 16936 train-loss: 0.010552690674861273\n",
      "[LOG 20200502-14:57:16] epoch: 16937 train-loss: 0.010552671013606919\n",
      "[LOG 20200502-14:57:17] epoch: 16938 train-loss: 0.010552650938431421\n",
      "[LOG 20200502-14:57:17] epoch: 16939 train-loss: 0.010552631070216497\n",
      "[LOG 20200502-14:57:17] epoch: 16940 train-loss: 0.010552610995041\n",
      "[LOG 20200502-14:57:17] epoch: 16941 train-loss: 0.010552589574621784\n",
      "[LOG 20200502-14:57:18] epoch: 16942 train-loss: 0.010552569189005427\n",
      "[LOG 20200502-14:57:18] epoch: 16943 train-loss: 0.010552547975546785\n",
      "[LOG 20200502-14:57:18] epoch: 16944 train-loss: 0.010552526969048712\n",
      "[LOG 20200502-14:57:18] epoch: 16945 train-loss: 0.010552505755590068\n",
      "[LOG 20200502-14:57:19] epoch: 16946 train-loss: 0.010552483817769421\n",
      "[LOG 20200502-14:57:19] epoch: 16947 train-loss: 0.010552461879948774\n",
      "[LOG 20200502-14:57:19] epoch: 16948 train-loss: 0.010552440873450704\n",
      "[LOG 20200502-14:57:19] epoch: 16949 train-loss: 0.010552419246070914\n",
      "[LOG 20200502-14:57:19] epoch: 16950 train-loss: 0.010552397618691126\n",
      "[LOG 20200502-14:57:20] epoch: 16951 train-loss: 0.010552376405232482\n",
      "[LOG 20200502-14:57:20] epoch: 16952 train-loss: 0.01055235488133298\n",
      "[LOG 20200502-14:57:20] epoch: 16953 train-loss: 0.010552333874834908\n",
      "[LOG 20200502-14:57:20] epoch: 16954 train-loss: 0.010552312143974833\n",
      "[LOG 20200502-14:57:21] epoch: 16955 train-loss: 0.010552291240957048\n",
      "[LOG 20200502-14:57:21] epoch: 16956 train-loss: 0.010552270234458976\n",
      "[LOG 20200502-14:57:21] epoch: 16957 train-loss: 0.010552249641882049\n",
      "[LOG 20200502-14:57:21] epoch: 16958 train-loss: 0.010552229152785407\n",
      "[LOG 20200502-14:57:22] epoch: 16959 train-loss: 0.010552208974129625\n",
      "[LOG 20200502-14:57:22] epoch: 16960 train-loss: 0.01055218879547384\n",
      "[LOG 20200502-14:57:22] epoch: 16961 train-loss: 0.010552169030739201\n",
      "[LOG 20200502-14:57:22] epoch: 16962 train-loss: 0.010552149472965134\n",
      "[LOG 20200502-14:57:23] epoch: 16963 train-loss: 0.010552129708230495\n",
      "[LOG 20200502-14:57:23] epoch: 16964 train-loss: 0.01055211066785786\n",
      "[LOG 20200502-14:57:23] epoch: 16965 train-loss: 0.01055209142052465\n",
      "[LOG 20200502-14:57:23] epoch: 16966 train-loss: 0.010552072690592872\n",
      "[LOG 20200502-14:57:24] epoch: 16967 train-loss: 0.010552054685023095\n",
      "[LOG 20200502-14:57:24] epoch: 16968 train-loss: 0.010552036575973034\n",
      "[LOG 20200502-14:57:24] epoch: 16969 train-loss: 0.010552018466922972\n",
      "[LOG 20200502-14:57:24] epoch: 16970 train-loss: 0.010551999943951765\n",
      "[LOG 20200502-14:57:25] epoch: 16971 train-loss: 0.010551982145342562\n",
      "[LOG 20200502-14:57:25] epoch: 16972 train-loss: 0.010551964450213645\n",
      "[LOG 20200502-14:57:25] epoch: 16973 train-loss: 0.010551946755084727\n",
      "[LOG 20200502-14:57:25] epoch: 16974 train-loss: 0.010551929370396666\n",
      "[LOG 20200502-14:57:25] epoch: 16975 train-loss: 0.01055191115786632\n",
      "[LOG 20200502-14:57:26] epoch: 16976 train-loss: 0.010551893876658546\n",
      "[LOG 20200502-14:57:26] epoch: 16977 train-loss: 0.010551875974569056\n",
      "[LOG 20200502-14:57:26] epoch: 16978 train-loss: 0.010551859003802141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:57:26] epoch: 16979 train-loss: 0.010551840998232365\n",
      "[LOG 20200502-14:57:27] epoch: 16980 train-loss: 0.01055182319962316\n",
      "[LOG 20200502-14:57:27] epoch: 16981 train-loss: 0.01055180529753367\n",
      "[LOG 20200502-14:57:27] epoch: 16982 train-loss: 0.01055178687804275\n",
      "[LOG 20200502-14:57:27] epoch: 16983 train-loss: 0.010551768355071545\n",
      "[LOG 20200502-14:57:28] epoch: 16984 train-loss: 0.010551749418179194\n",
      "[LOG 20200502-14:57:28] epoch: 16985 train-loss: 0.01055173058476713\n",
      "[LOG 20200502-14:57:28] epoch: 16986 train-loss: 0.010551712165276209\n",
      "[LOG 20200502-14:57:28] epoch: 16987 train-loss: 0.010551691986620426\n",
      "[LOG 20200502-14:57:29] epoch: 16988 train-loss: 0.010551671807964643\n",
      "[LOG 20200502-14:57:29] epoch: 16989 train-loss: 0.010551652457151148\n",
      "[LOG 20200502-14:57:29] epoch: 16990 train-loss: 0.01055163134717279\n",
      "[LOG 20200502-14:57:29] epoch: 16991 train-loss: 0.010551610651115576\n",
      "[LOG 20200502-14:57:30] epoch: 16992 train-loss: 0.010551589644617505\n",
      "[LOG 20200502-14:57:30] epoch: 16993 train-loss: 0.01055156791375743\n",
      "[LOG 20200502-14:57:30] epoch: 16994 train-loss: 0.010551545768976212\n",
      "[LOG 20200502-14:57:31] epoch: 16995 train-loss: 0.010551523313754134\n",
      "[LOG 20200502-14:57:31] epoch: 16996 train-loss: 0.010551500134170055\n",
      "[LOG 20200502-14:57:31] epoch: 16997 train-loss: 0.010551476644145118\n",
      "[LOG 20200502-14:57:31] epoch: 16998 train-loss: 0.010551453464561038\n",
      "[LOG 20200502-14:57:32] epoch: 16999 train-loss: 0.01055142893973324\n",
      "[LOG 20200502-14:57:32] epoch: 17000 train-loss: 0.010551404828826586\n",
      "[LOG 20200502-14:57:32] epoch: 17001 train-loss: 0.0105513794761565\n",
      "[LOG 20200502-14:57:32] epoch: 17002 train-loss: 0.010551354951328702\n",
      "[LOG 20200502-14:57:33] epoch: 17003 train-loss: 0.010551328977776898\n",
      "[LOG 20200502-14:57:33] epoch: 17004 train-loss: 0.010551303211185668\n",
      "[LOG 20200502-14:57:33] epoch: 17005 train-loss: 0.010551277444594435\n",
      "[LOG 20200502-14:57:33] epoch: 17006 train-loss: 0.01055125105712149\n",
      "[LOG 20200502-14:57:33] epoch: 17007 train-loss: 0.010551225600971116\n",
      "[LOG 20200502-14:57:34] epoch: 17008 train-loss: 0.010551199523939027\n",
      "[LOG 20200502-14:57:34] epoch: 17009 train-loss: 0.010551173239946365\n",
      "[LOG 20200502-14:57:34] epoch: 17010 train-loss: 0.010551147162914276\n",
      "[LOG 20200502-14:57:34] epoch: 17011 train-loss: 0.010551121292842759\n",
      "[LOG 20200502-14:57:35] epoch: 17012 train-loss: 0.010551095526251528\n",
      "[LOG 20200502-14:57:35] epoch: 17013 train-loss: 0.010551070070101155\n",
      "[LOG 20200502-14:57:35] epoch: 17014 train-loss: 0.010551044096549353\n",
      "[LOG 20200502-14:57:35] epoch: 17015 train-loss: 0.010551019054320123\n",
      "[LOG 20200502-14:57:36] epoch: 17016 train-loss: 0.01055099442601204\n",
      "[LOG 20200502-14:57:36] epoch: 17017 train-loss: 0.010550969176822238\n",
      "[LOG 20200502-14:57:36] epoch: 17018 train-loss: 0.010550945790277587\n",
      "[LOG 20200502-14:57:36] epoch: 17019 train-loss: 0.010550922507213222\n",
      "[LOG 20200502-14:57:37] epoch: 17020 train-loss: 0.010550898499786854\n",
      "[LOG 20200502-14:57:37] epoch: 17021 train-loss: 0.010550876148045063\n",
      "[LOG 20200502-14:57:37] epoch: 17022 train-loss: 0.010550854313704703\n",
      "[LOG 20200502-14:57:37] epoch: 17023 train-loss: 0.010550832065443197\n",
      "[LOG 20200502-14:57:38] epoch: 17024 train-loss: 0.010550811058945127\n",
      "[LOG 20200502-14:57:38] epoch: 17025 train-loss: 0.010550789948966768\n",
      "[LOG 20200502-14:57:38] epoch: 17026 train-loss: 0.010550769977271557\n",
      "[LOG 20200502-14:57:38] epoch: 17027 train-loss: 0.01055075041949749\n",
      "[LOG 20200502-14:57:39] epoch: 17028 train-loss: 0.010550731586085426\n",
      "[LOG 20200502-14:57:39] epoch: 17029 train-loss: 0.010550713063114218\n",
      "[LOG 20200502-14:57:39] epoch: 17030 train-loss: 0.010550694850583872\n",
      "[LOG 20200502-14:57:39] epoch: 17031 train-loss: 0.010550677155454954\n",
      "[LOG 20200502-14:57:40] epoch: 17032 train-loss: 0.010550660184688039\n",
      "[LOG 20200502-14:57:40] epoch: 17033 train-loss: 0.010550643420881696\n",
      "[LOG 20200502-14:57:40] epoch: 17034 train-loss: 0.010550627691878213\n",
      "[LOG 20200502-14:57:40] epoch: 17035 train-loss: 0.010550611859394444\n",
      "[LOG 20200502-14:57:41] epoch: 17036 train-loss: 0.010550596130390963\n",
      "[LOG 20200502-14:57:41] epoch: 17037 train-loss: 0.010550581022269197\n",
      "[LOG 20200502-14:57:41] epoch: 17038 train-loss: 0.01055056643154886\n",
      "[LOG 20200502-14:57:41] epoch: 17039 train-loss: 0.010550552358229956\n",
      "[LOG 20200502-14:57:42] epoch: 17040 train-loss: 0.010550538491871621\n",
      "[LOG 20200502-14:57:42] epoch: 17041 train-loss: 0.010550524522033002\n",
      "[LOG 20200502-14:57:42] epoch: 17042 train-loss: 0.01055051065567467\n",
      "[LOG 20200502-14:57:42] epoch: 17043 train-loss: 0.010550497203237481\n",
      "[LOG 20200502-14:57:43] epoch: 17044 train-loss: 0.010550484785603153\n",
      "[LOG 20200502-14:57:43] epoch: 17045 train-loss: 0.010550471229685677\n",
      "[LOG 20200502-14:57:43] epoch: 17046 train-loss: 0.010550458191169633\n",
      "[LOG 20200502-14:57:44] epoch: 17047 train-loss: 0.010550445463094447\n",
      "[LOG 20200502-14:57:44] epoch: 17048 train-loss: 0.010550432528058687\n",
      "[LOG 20200502-14:57:44] epoch: 17049 train-loss: 0.01055041959302293\n",
      "[LOG 20200502-14:57:44] epoch: 17050 train-loss: 0.010550406864947744\n",
      "[LOG 20200502-14:57:45] epoch: 17051 train-loss: 0.010550393722951412\n",
      "[LOG 20200502-14:57:45] epoch: 17052 train-loss: 0.010550380994876226\n",
      "[LOG 20200502-14:57:45] epoch: 17053 train-loss: 0.010550367852879895\n",
      "[LOG 20200502-14:57:45] epoch: 17054 train-loss: 0.01055035481436385\n",
      "[LOG 20200502-14:57:46] epoch: 17055 train-loss: 0.010550341465406947\n",
      "[LOG 20200502-14:57:46] epoch: 17056 train-loss: 0.010550327599048615\n",
      "[LOG 20200502-14:57:46] epoch: 17057 train-loss: 0.010550313939650854\n",
      "[LOG 20200502-14:57:47] epoch: 17058 train-loss: 0.010550299969812235\n",
      "[LOG 20200502-14:57:47] epoch: 17059 train-loss: 0.010550285689532757\n",
      "[LOG 20200502-14:57:47] epoch: 17060 train-loss: 0.010550271098812422\n",
      "[LOG 20200502-14:57:47] epoch: 17061 train-loss: 0.010550256301131513\n",
      "[LOG 20200502-14:57:48] epoch: 17062 train-loss: 0.010550240986049175\n",
      "[LOG 20200502-14:57:48] epoch: 17063 train-loss: 0.010550225257045694\n",
      "[LOG 20200502-14:57:48] epoch: 17064 train-loss: 0.010550210045443641\n",
      "[LOG 20200502-14:57:48] epoch: 17065 train-loss: 0.01055019348859787\n",
      "[LOG 20200502-14:57:49] epoch: 17066 train-loss: 0.010550177552633815\n",
      "[LOG 20200502-14:57:49] epoch: 17067 train-loss: 0.010550161202748617\n",
      "[LOG 20200502-14:57:49] epoch: 17068 train-loss: 0.010550143404139413\n",
      "[LOG 20200502-14:57:49] epoch: 17069 train-loss: 0.01055012560553021\n",
      "[LOG 20200502-14:57:50] epoch: 17070 train-loss: 0.010550108324322436\n",
      "[LOG 20200502-14:57:50] epoch: 17071 train-loss: 0.010550089697870944\n",
      "[LOG 20200502-14:57:50] epoch: 17072 train-loss: 0.010550071795781454\n",
      "[LOG 20200502-14:57:50] epoch: 17073 train-loss: 0.010550052755408816\n",
      "[LOG 20200502-14:57:51] epoch: 17074 train-loss: 0.010550033404595323\n",
      "[LOG 20200502-14:57:51] epoch: 17075 train-loss: 0.010550014053781828\n",
      "[LOG 20200502-14:57:51] epoch: 17076 train-loss: 0.010549994909928905\n",
      "[LOG 20200502-14:57:51] epoch: 17077 train-loss: 0.010549975041713979\n",
      "[LOG 20200502-14:57:52] epoch: 17078 train-loss: 0.010549954966538481\n",
      "[LOG 20200502-14:57:52] epoch: 17079 train-loss: 0.010549934684402414\n",
      "[LOG 20200502-14:57:52] epoch: 17080 train-loss: 0.010549914505746629\n",
      "[LOG 20200502-14:57:52] epoch: 17081 train-loss: 0.010549894327090846\n",
      "[LOG 20200502-14:57:53] epoch: 17082 train-loss: 0.010549873837994205\n",
      "[LOG 20200502-14:57:53] epoch: 17083 train-loss: 0.010549853141936991\n",
      "[LOG 20200502-14:57:53] epoch: 17084 train-loss: 0.010549832342399491\n",
      "[LOG 20200502-14:57:53] epoch: 17085 train-loss: 0.010549811956783136\n",
      "[LOG 20200502-14:57:53] epoch: 17086 train-loss: 0.01054979105376535\n",
      "[LOG 20200502-14:57:54] epoch: 17087 train-loss: 0.010549770875109566\n",
      "[LOG 20200502-14:57:54] epoch: 17088 train-loss: 0.010549750386012925\n",
      "[LOG 20200502-14:57:54] epoch: 17089 train-loss: 0.010549730207357142\n",
      "[LOG 20200502-14:57:54] epoch: 17090 train-loss: 0.0105497097182605\n",
      "[LOG 20200502-14:57:55] epoch: 17091 train-loss: 0.010549689850045575\n",
      "[LOG 20200502-14:57:55] epoch: 17092 train-loss: 0.010549670085310936\n",
      "[LOG 20200502-14:57:55] epoch: 17093 train-loss: 0.010549650424056582\n",
      "[LOG 20200502-14:57:55] epoch: 17094 train-loss: 0.010549630866282515\n",
      "[LOG 20200502-14:57:56] epoch: 17095 train-loss: 0.010549612136350738\n",
      "[LOG 20200502-14:57:56] epoch: 17096 train-loss: 0.010549593199458387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:57:56] epoch: 17097 train-loss: 0.010549574262566037\n",
      "[LOG 20200502-14:57:56] epoch: 17098 train-loss: 0.010549555325673686\n",
      "[LOG 20200502-14:57:57] epoch: 17099 train-loss: 0.010549537734025054\n",
      "[LOG 20200502-14:57:57] epoch: 17100 train-loss: 0.01054951941801442\n",
      "[LOG 20200502-14:57:57] epoch: 17101 train-loss: 0.010549502136806646\n",
      "[LOG 20200502-14:57:57] epoch: 17102 train-loss: 0.010549484234717157\n",
      "[LOG 20200502-14:57:58] epoch: 17103 train-loss: 0.01054946705698967\n",
      "[LOG 20200502-14:57:58] epoch: 17104 train-loss: 0.01054945018970304\n",
      "[LOG 20200502-14:57:58] epoch: 17105 train-loss: 0.01054943311545584\n",
      "[LOG 20200502-14:57:58] epoch: 17106 train-loss: 0.01054941624816921\n",
      "[LOG 20200502-14:57:59] epoch: 17107 train-loss: 0.010549399794803726\n",
      "[LOG 20200502-14:57:59] epoch: 17108 train-loss: 0.010549383134477668\n",
      "[LOG 20200502-14:57:59] epoch: 17109 train-loss: 0.010549366991553042\n",
      "[LOG 20200502-14:57:59] epoch: 17110 train-loss: 0.010549350848628415\n",
      "[LOG 20200502-14:58:00] epoch: 17111 train-loss: 0.010549334291782644\n",
      "[LOG 20200502-14:58:00] epoch: 17112 train-loss: 0.010549318459298875\n",
      "[LOG 20200502-14:58:00] epoch: 17113 train-loss: 0.01054930148853196\n",
      "[LOG 20200502-14:58:00] epoch: 17114 train-loss: 0.01054928544908762\n",
      "[LOG 20200502-14:58:01] epoch: 17115 train-loss: 0.01054926940964328\n",
      "[LOG 20200502-14:58:01] epoch: 17116 train-loss: 0.010549252956277795\n",
      "[LOG 20200502-14:58:01] epoch: 17117 train-loss: 0.010549236192471452\n",
      "[LOG 20200502-14:58:01] epoch: 17118 train-loss: 0.010549220049546825\n",
      "[LOG 20200502-14:58:02] epoch: 17119 train-loss: 0.01054920307877991\n",
      "[LOG 20200502-14:58:02] epoch: 17120 train-loss: 0.010549185694091849\n",
      "[LOG 20200502-14:58:02] epoch: 17121 train-loss: 0.010549168826805221\n",
      "[LOG 20200502-14:58:02] epoch: 17122 train-loss: 0.010549151338636875\n",
      "[LOG 20200502-14:58:03] epoch: 17123 train-loss: 0.010549133022626242\n",
      "[LOG 20200502-14:58:03] epoch: 17124 train-loss: 0.010549115120536752\n",
      "[LOG 20200502-14:58:03] epoch: 17125 train-loss: 0.01054909649408526\n",
      "[LOG 20200502-14:58:03] epoch: 17126 train-loss: 0.010549077660673194\n",
      "[LOG 20200502-14:58:03] epoch: 17127 train-loss: 0.01054905882726113\n",
      "[LOG 20200502-14:58:04] epoch: 17128 train-loss: 0.010549038855565919\n",
      "[LOG 20200502-14:58:04] epoch: 17129 train-loss: 0.010549019194311567\n",
      "[LOG 20200502-14:58:04] epoch: 17130 train-loss: 0.010548998705214925\n",
      "[LOG 20200502-14:58:04] epoch: 17131 train-loss: 0.010548978216118284\n",
      "[LOG 20200502-14:58:05] epoch: 17132 train-loss: 0.010548957416580783\n",
      "[LOG 20200502-14:58:05] epoch: 17133 train-loss: 0.010548935478760136\n",
      "[LOG 20200502-14:58:05] epoch: 17134 train-loss: 0.010548913333978917\n",
      "[LOG 20200502-14:58:05] epoch: 17135 train-loss: 0.01054889170659913\n",
      "[LOG 20200502-14:58:06] epoch: 17136 train-loss: 0.010548868733975623\n",
      "[LOG 20200502-14:58:06] epoch: 17137 train-loss: 0.010548846382233832\n",
      "[LOG 20200502-14:58:06] epoch: 17138 train-loss: 0.010548822685248323\n",
      "[LOG 20200502-14:58:06] epoch: 17139 train-loss: 0.010548799091743099\n",
      "[LOG 20200502-14:58:07] epoch: 17140 train-loss: 0.010548775601718161\n",
      "[LOG 20200502-14:58:07] epoch: 17141 train-loss: 0.010548751801252365\n",
      "[LOG 20200502-14:58:07] epoch: 17142 train-loss: 0.010548727069463994\n",
      "[LOG 20200502-14:58:07] epoch: 17143 train-loss: 0.010548703475958772\n",
      "[LOG 20200502-14:58:08] epoch: 17144 train-loss: 0.010548679468532404\n",
      "[LOG 20200502-14:58:08] epoch: 17145 train-loss: 0.010548655047184892\n",
      "[LOG 20200502-14:58:08] epoch: 17146 train-loss: 0.010548630522357093\n",
      "[LOG 20200502-14:58:08] epoch: 17147 train-loss: 0.010548606411450438\n",
      "[LOG 20200502-14:58:09] epoch: 17148 train-loss: 0.010548581990102926\n",
      "[LOG 20200502-14:58:09] epoch: 17149 train-loss: 0.010548557775715986\n",
      "[LOG 20200502-14:58:09] epoch: 17150 train-loss: 0.010548533768289618\n",
      "[LOG 20200502-14:58:09] epoch: 17151 train-loss: 0.010548510485225253\n",
      "[LOG 20200502-14:58:10] epoch: 17152 train-loss: 0.010548487305641174\n",
      "[LOG 20200502-14:58:10] epoch: 17153 train-loss: 0.010548464126057096\n",
      "[LOG 20200502-14:58:10] epoch: 17154 train-loss: 0.01054844187779559\n",
      "[LOG 20200502-14:58:10] epoch: 17155 train-loss: 0.010548419629534086\n",
      "[LOG 20200502-14:58:11] epoch: 17156 train-loss: 0.010548397484752867\n",
      "[LOG 20200502-14:58:11] epoch: 17157 train-loss: 0.010548376581735082\n",
      "[LOG 20200502-14:58:11] epoch: 17158 train-loss: 0.010548355678717295\n",
      "[LOG 20200502-14:58:11] epoch: 17159 train-loss: 0.01054833581050237\n",
      "[LOG 20200502-14:58:12] epoch: 17160 train-loss: 0.010548315631846586\n",
      "[LOG 20200502-14:58:12] epoch: 17160 new best train-loss: 0.010548315631846586 found\n",
      "[LOG 20200502-14:58:12] epoch: 17161 train-loss: 0.010548296487993665\n",
      "[LOG 20200502-14:58:12] epoch: 17162 train-loss: 0.010548277758061886\n",
      "[LOG 20200502-14:58:12] epoch: 17163 train-loss: 0.010548259545531537\n",
      "[LOG 20200502-14:58:13] epoch: 17164 train-loss: 0.010548241229520904\n",
      "[LOG 20200502-14:58:13] epoch: 17165 train-loss: 0.010548224672675133\n",
      "[LOG 20200502-14:58:13] epoch: 17166 train-loss: 0.010548208012349077\n",
      "[LOG 20200502-14:58:13] epoch: 17167 train-loss: 0.01054819186942445\n",
      "[LOG 20200502-14:58:14] epoch: 17168 train-loss: 0.010548176140420966\n",
      "[LOG 20200502-14:58:14] epoch: 17169 train-loss: 0.010548161446220346\n",
      "[LOG 20200502-14:58:14] epoch: 17170 train-loss: 0.010548146131138006\n",
      "[LOG 20200502-14:58:14] epoch: 17170 new best train-loss: 0.010548146131138006 found\n",
      "[LOG 20200502-14:58:14] epoch: 17171 train-loss: 0.010548132264779674\n",
      "[LOG 20200502-14:58:14] epoch: 17172 train-loss: 0.010548118501901627\n",
      "[LOG 20200502-14:58:15] epoch: 17173 train-loss: 0.01054810525642501\n",
      "[LOG 20200502-14:58:15] epoch: 17174 train-loss: 0.010548092010948393\n",
      "[LOG 20200502-14:58:15] epoch: 17175 train-loss: 0.01054807948983378\n",
      "[LOG 20200502-14:58:15] epoch: 17176 train-loss: 0.010548067589600882\n",
      "[LOG 20200502-14:58:16] epoch: 17177 train-loss: 0.01054805496500598\n",
      "[LOG 20200502-14:58:16] epoch: 17178 train-loss: 0.01054804316825337\n",
      "[LOG 20200502-14:58:16] epoch: 17179 train-loss: 0.010548031268020472\n",
      "[LOG 20200502-14:58:16] epoch: 17180 train-loss: 0.010548019885189004\n",
      "[LOG 20200502-14:58:16] epoch: 17180 new best train-loss: 0.010548019885189004 found\n",
      "[LOG 20200502-14:58:17] epoch: 17181 train-loss: 0.010548008709318109\n",
      "[LOG 20200502-14:58:17] epoch: 17182 train-loss: 0.010547997740407785\n",
      "[LOG 20200502-14:58:17] epoch: 17183 train-loss: 0.010547987081938319\n",
      "[LOG 20200502-14:58:17] epoch: 17184 train-loss: 0.010547976423468854\n",
      "[LOG 20200502-14:58:18] epoch: 17185 train-loss: 0.010547965144117674\n",
      "[LOG 20200502-14:58:18] epoch: 17186 train-loss: 0.010547954692608781\n",
      "[LOG 20200502-14:58:18] epoch: 17187 train-loss: 0.01054794362021817\n",
      "[LOG 20200502-14:58:18] epoch: 17188 train-loss: 0.010547932237386703\n",
      "[LOG 20200502-14:58:19] epoch: 17189 train-loss: 0.010547921578917239\n",
      "[LOG 20200502-14:58:19] epoch: 17190 train-loss: 0.0105479107134872\n",
      "[LOG 20200502-14:58:19] epoch: 17190 new best train-loss: 0.0105479107134872 found\n",
      "[LOG 20200502-14:58:19] epoch: 17191 train-loss: 0.01054789995153745\n",
      "[LOG 20200502-14:58:19] epoch: 17192 train-loss: 0.01054788887914684\n",
      "[LOG 20200502-14:58:20] epoch: 17193 train-loss: 0.010547877496315373\n",
      "[LOG 20200502-14:58:20] epoch: 17194 train-loss: 0.010547865182161331\n",
      "[LOG 20200502-14:58:20] epoch: 17195 train-loss: 0.010547853695849577\n",
      "[LOG 20200502-14:58:20] epoch: 17196 train-loss: 0.010547841692136394\n",
      "[LOG 20200502-14:58:21] epoch: 17197 train-loss: 0.010547829377982352\n",
      "[LOG 20200502-14:58:21] epoch: 17198 train-loss: 0.010547816753387451\n",
      "[LOG 20200502-14:58:21] epoch: 17199 train-loss: 0.01054780361139112\n",
      "[LOG 20200502-14:58:21] epoch: 17200 train-loss: 0.010547790158953931\n",
      "[LOG 20200502-14:58:21] epoch: 17200 new best train-loss: 0.010547790158953931 found\n",
      "[LOG 20200502-14:58:22] epoch: 17201 train-loss: 0.010547777016957601\n",
      "[LOG 20200502-14:58:22] epoch: 17202 train-loss: 0.010547763461040126\n",
      "[LOG 20200502-14:58:22] epoch: 17203 train-loss: 0.010547749284240935\n",
      "[LOG 20200502-14:58:22] epoch: 17204 train-loss: 0.010547734693520598\n",
      "[LOG 20200502-14:58:23] epoch: 17205 train-loss: 0.010547719792359404\n",
      "[LOG 20200502-14:58:23] epoch: 17206 train-loss: 0.010547704063355923\n",
      "[LOG 20200502-14:58:23] epoch: 17207 train-loss: 0.010547689162194729\n",
      "[LOG 20200502-14:58:23] epoch: 17208 train-loss: 0.010547673536671532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:58:24] epoch: 17209 train-loss: 0.01054765729026662\n",
      "[LOG 20200502-14:58:24] epoch: 17210 train-loss: 0.010547640836901136\n",
      "[LOG 20200502-14:58:24] epoch: 17210 new best train-loss: 0.010547640836901136 found\n",
      "[LOG 20200502-14:58:24] epoch: 17211 train-loss: 0.010547624590496222\n",
      "[LOG 20200502-14:58:24] epoch: 17212 train-loss: 0.010547607102327876\n",
      "[LOG 20200502-14:58:25] epoch: 17213 train-loss: 0.010547590235041248\n",
      "[LOG 20200502-14:58:25] epoch: 17214 train-loss: 0.01054757253991233\n",
      "[LOG 20200502-14:58:25] epoch: 17215 train-loss: 0.010547555155224271\n",
      "[LOG 20200502-14:58:25] epoch: 17216 train-loss: 0.01054753725313478\n",
      "[LOG 20200502-14:58:25] epoch: 17217 train-loss: 0.010547519764966436\n",
      "[LOG 20200502-14:58:26] epoch: 17218 train-loss: 0.010547501345475515\n",
      "[LOG 20200502-14:58:26] epoch: 17219 train-loss: 0.01054748354686631\n",
      "[LOG 20200502-14:58:26] epoch: 17220 train-loss: 0.010547465334335962\n",
      "[LOG 20200502-14:58:26] epoch: 17220 new best train-loss: 0.010547465334335962 found\n",
      "[LOG 20200502-14:58:26] epoch: 17221 train-loss: 0.010547446914845042\n",
      "[LOG 20200502-14:58:27] epoch: 17222 train-loss: 0.010547429426676698\n",
      "[LOG 20200502-14:58:27] epoch: 17223 train-loss: 0.010547410593264632\n",
      "[LOG 20200502-14:58:27] epoch: 17224 train-loss: 0.010547393105096288\n",
      "[LOG 20200502-14:58:27] epoch: 17225 train-loss: 0.010547374789085653\n",
      "[LOG 20200502-14:58:28] epoch: 17226 train-loss: 0.01054735699047645\n",
      "[LOG 20200502-14:58:28] epoch: 17227 train-loss: 0.010547339398827817\n",
      "[LOG 20200502-14:58:28] epoch: 17228 train-loss: 0.010547321910659472\n",
      "[LOG 20200502-14:58:28] epoch: 17229 train-loss: 0.01054730431901084\n",
      "[LOG 20200502-14:58:29] epoch: 17230 train-loss: 0.010547287141283354\n",
      "[LOG 20200502-14:58:29] epoch: 17230 new best train-loss: 0.010547287141283354 found\n",
      "[LOG 20200502-14:58:29] epoch: 17231 train-loss: 0.01054727037747701\n",
      "[LOG 20200502-14:58:29] epoch: 17232 train-loss: 0.010547253406710096\n",
      "[LOG 20200502-14:58:29] epoch: 17233 train-loss: 0.010547237056824896\n",
      "[LOG 20200502-14:58:30] epoch: 17234 train-loss: 0.010547220810419984\n",
      "[LOG 20200502-14:58:30] epoch: 17235 train-loss: 0.010547204770975642\n",
      "[LOG 20200502-14:58:30] epoch: 17236 train-loss: 0.010547188835011588\n",
      "[LOG 20200502-14:58:30] epoch: 17237 train-loss: 0.010547173830370108\n",
      "[LOG 20200502-14:58:31] epoch: 17238 train-loss: 0.01054715799788634\n",
      "[LOG 20200502-14:58:31] epoch: 17239 train-loss: 0.01054714299324486\n",
      "[LOG 20200502-14:58:31] epoch: 17240 train-loss: 0.010547128299044238\n",
      "[LOG 20200502-14:58:31] epoch: 17240 new best train-loss: 0.010547128299044238 found\n",
      "[LOG 20200502-14:58:31] epoch: 17241 train-loss: 0.010547113915284475\n",
      "[LOG 20200502-14:58:32] epoch: 17242 train-loss: 0.010547099635004997\n",
      "[LOG 20200502-14:58:32] epoch: 17243 train-loss: 0.010547085458205806\n",
      "[LOG 20200502-14:58:32] epoch: 17244 train-loss: 0.010547071177926328\n",
      "[LOG 20200502-14:58:32] epoch: 17245 train-loss: 0.010547057518528568\n",
      "[LOG 20200502-14:58:33] epoch: 17246 train-loss: 0.010547043859130807\n",
      "[LOG 20200502-14:58:33] epoch: 17247 train-loss: 0.010547029785811901\n",
      "[LOG 20200502-14:58:33] epoch: 17248 train-loss: 0.01054701612641414\n",
      "[LOG 20200502-14:58:33] epoch: 17249 train-loss: 0.010547002570496665\n",
      "[LOG 20200502-14:58:33] epoch: 17250 train-loss: 0.010546988911098905\n",
      "[LOG 20200502-14:58:34] epoch: 17250 new best train-loss: 0.010546988911098905 found\n",
      "[LOG 20200502-14:58:34] epoch: 17251 train-loss: 0.010546975458661715\n",
      "[LOG 20200502-14:58:34] epoch: 17252 train-loss: 0.010546962627106242\n",
      "[LOG 20200502-14:58:34] epoch: 17253 train-loss: 0.010546948243346479\n",
      "[LOG 20200502-14:58:34] epoch: 17254 train-loss: 0.010546934894389577\n",
      "[LOG 20200502-14:58:35] epoch: 17255 train-loss: 0.0105469206141101\n",
      "[LOG 20200502-14:58:35] epoch: 17256 train-loss: 0.01054690664427148\n",
      "[LOG 20200502-14:58:35] epoch: 17257 train-loss: 0.010546892260511717\n",
      "[LOG 20200502-14:58:35] epoch: 17258 train-loss: 0.010546877980232239\n",
      "[LOG 20200502-14:58:36] epoch: 17259 train-loss: 0.010546863286031617\n",
      "[LOG 20200502-14:58:36] epoch: 17260 train-loss: 0.010546848384870423\n",
      "[LOG 20200502-14:58:36] epoch: 17260 new best train-loss: 0.010546848384870423 found\n",
      "[LOG 20200502-14:58:36] epoch: 17261 train-loss: 0.010546833690669801\n",
      "[LOG 20200502-14:58:36] epoch: 17262 train-loss: 0.010546817858186033\n",
      "[LOG 20200502-14:58:37] epoch: 17263 train-loss: 0.010546801715261407\n",
      "[LOG 20200502-14:58:37] epoch: 17264 train-loss: 0.01054678557233678\n",
      "[LOG 20200502-14:58:37] epoch: 17265 train-loss: 0.010546768912010722\n",
      "[LOG 20200502-14:58:38] epoch: 17266 train-loss: 0.010546752044724094\n",
      "[LOG 20200502-14:58:38] epoch: 17267 train-loss: 0.010546734349595176\n",
      "[LOG 20200502-14:58:38] epoch: 17268 train-loss: 0.010546717068387402\n",
      "[LOG 20200502-14:58:38] epoch: 17269 train-loss: 0.01054669895933734\n",
      "[LOG 20200502-14:58:39] epoch: 17270 train-loss: 0.010546679712004133\n",
      "[LOG 20200502-14:58:39] epoch: 17270 new best train-loss: 0.010546679712004133 found\n",
      "[LOG 20200502-14:58:39] epoch: 17271 train-loss: 0.010546660982072353\n",
      "[LOG 20200502-14:58:39] epoch: 17272 train-loss: 0.010546642045180002\n",
      "[LOG 20200502-14:58:39] epoch: 17273 train-loss: 0.010546622176965078\n",
      "[LOG 20200502-14:58:40] epoch: 17274 train-loss: 0.01054660241223044\n",
      "[LOG 20200502-14:58:40] epoch: 17275 train-loss: 0.010546581923133798\n",
      "[LOG 20200502-14:58:40] epoch: 17276 train-loss: 0.010546561537517441\n",
      "[LOG 20200502-14:58:40] epoch: 17277 train-loss: 0.010546540737979941\n",
      "[LOG 20200502-14:58:41] epoch: 17278 train-loss: 0.010546519524521299\n",
      "[LOG 20200502-14:58:41] epoch: 17279 train-loss: 0.010546498000621796\n",
      "[LOG 20200502-14:58:41] epoch: 17280 train-loss: 0.010546477304564582\n",
      "[LOG 20200502-14:58:41] epoch: 17280 new best train-loss: 0.010546477304564582 found\n",
      "[LOG 20200502-14:58:41] epoch: 17281 train-loss: 0.010546455573704507\n",
      "[LOG 20200502-14:58:42] epoch: 17282 train-loss: 0.010546433428923288\n",
      "[LOG 20200502-14:58:42] epoch: 17283 train-loss: 0.010546412732866075\n",
      "[LOG 20200502-14:58:42] epoch: 17284 train-loss: 0.010546390484604571\n",
      "[LOG 20200502-14:58:42] epoch: 17285 train-loss: 0.010546369271145927\n",
      "[LOG 20200502-14:58:43] epoch: 17286 train-loss: 0.010546348057687283\n",
      "[LOG 20200502-14:58:43] epoch: 17287 train-loss: 0.01054632705118921\n",
      "[LOG 20200502-14:58:43] epoch: 17288 train-loss: 0.010546306355131997\n",
      "[LOG 20200502-14:58:43] epoch: 17289 train-loss: 0.010546285659074783\n",
      "[LOG 20200502-14:58:44] epoch: 17290 train-loss: 0.010546265687379573\n",
      "[LOG 20200502-14:58:44] epoch: 17290 new best train-loss: 0.010546265687379573 found\n",
      "[LOG 20200502-14:58:44] epoch: 17291 train-loss: 0.010546245405243503\n",
      "[LOG 20200502-14:58:44] epoch: 17292 train-loss: 0.010546225950949721\n",
      "[LOG 20200502-14:58:44] epoch: 17293 train-loss: 0.010546207117537657\n",
      "[LOG 20200502-14:58:45] epoch: 17294 train-loss: 0.010546188284125593\n",
      "[LOG 20200502-14:58:45] epoch: 17295 train-loss: 0.010546169968114959\n",
      "[LOG 20200502-14:58:45] epoch: 17296 train-loss: 0.010546152790387472\n",
      "[LOG 20200502-14:58:45] epoch: 17297 train-loss: 0.01054613519873884\n",
      "[LOG 20200502-14:58:45] epoch: 17298 train-loss: 0.010546118952333927\n",
      "[LOG 20200502-14:58:46] epoch: 17299 train-loss: 0.010546102912889587\n",
      "[LOG 20200502-14:58:46] epoch: 17300 train-loss: 0.010546086976925531\n",
      "[LOG 20200502-14:58:46] epoch: 17300 new best train-loss: 0.010546086976925531 found\n",
      "[LOG 20200502-14:58:46] epoch: 17301 train-loss: 0.01054607228272491\n",
      "[LOG 20200502-14:58:47] epoch: 17302 train-loss: 0.010546057692004574\n",
      "[LOG 20200502-14:58:47] epoch: 17303 train-loss: 0.010546043722165955\n",
      "[LOG 20200502-14:58:47] epoch: 17304 train-loss: 0.010546030476689339\n",
      "[LOG 20200502-14:58:47] epoch: 17305 train-loss: 0.010546017438173294\n",
      "[LOG 20200502-14:58:47] epoch: 17306 train-loss: 0.010546005020538965\n",
      "[LOG 20200502-14:58:48] epoch: 17307 train-loss: 0.010545993120306067\n",
      "[LOG 20200502-14:58:48] epoch: 17308 train-loss: 0.010545981427033743\n",
      "[LOG 20200502-14:58:48] epoch: 17309 train-loss: 0.010545970768564276\n",
      "[LOG 20200502-14:58:48] epoch: 17310 train-loss: 0.010545960110094812\n",
      "[LOG 20200502-14:58:48] epoch: 17310 new best train-loss: 0.010545960110094812 found\n",
      "[LOG 20200502-14:58:49] epoch: 17311 train-loss: 0.010545949658585919\n",
      "[LOG 20200502-14:58:49] epoch: 17312 train-loss: 0.010545939931439029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:58:49] epoch: 17313 train-loss: 0.010545930411252711\n",
      "[LOG 20200502-14:58:49] epoch: 17314 train-loss: 0.010545921408467822\n",
      "[LOG 20200502-14:58:50] epoch: 17315 train-loss: 0.010545912509163221\n",
      "[LOG 20200502-14:58:50] epoch: 17316 train-loss: 0.010545903609858619\n",
      "[LOG 20200502-14:58:50] epoch: 17317 train-loss: 0.010545895331435733\n",
      "[LOG 20200502-14:58:50] epoch: 17318 train-loss: 0.010545887053012848\n",
      "[LOG 20200502-14:58:51] epoch: 17319 train-loss: 0.010545878464149104\n",
      "[LOG 20200502-14:58:51] epoch: 17320 train-loss: 0.01054587091008822\n",
      "[LOG 20200502-14:58:51] epoch: 17320 new best train-loss: 0.01054587091008822 found\n",
      "[LOG 20200502-14:58:51] epoch: 17321 train-loss: 0.010545862942106195\n",
      "[LOG 20200502-14:58:51] epoch: 17322 train-loss: 0.010545855181084739\n",
      "[LOG 20200502-14:58:52] epoch: 17323 train-loss: 0.01054584752354357\n",
      "[LOG 20200502-14:58:52] epoch: 17324 train-loss: 0.010545839866002401\n",
      "[LOG 20200502-14:58:52] epoch: 17325 train-loss: 0.010545831587579515\n",
      "[LOG 20200502-14:58:52] epoch: 17326 train-loss: 0.010545824240479205\n",
      "[LOG 20200502-14:58:53] epoch: 17327 train-loss: 0.010545816065536605\n",
      "[LOG 20200502-14:58:53] epoch: 17328 train-loss: 0.010545808511475721\n",
      "[LOG 20200502-14:58:53] epoch: 17329 train-loss: 0.010545800440013409\n",
      "[LOG 20200502-14:58:53] epoch: 17330 train-loss: 0.01054579195462995\n",
      "[LOG 20200502-14:58:53] epoch: 17330 new best train-loss: 0.01054579195462995 found\n",
      "[LOG 20200502-14:58:54] epoch: 17331 train-loss: 0.010545783676207066\n",
      "[LOG 20200502-14:58:54] epoch: 17332 train-loss: 0.01054577488038275\n",
      "[LOG 20200502-14:58:54] epoch: 17333 train-loss: 0.010545765463676717\n",
      "[LOG 20200502-14:58:54] epoch: 17334 train-loss: 0.01054575697829326\n",
      "[LOG 20200502-14:58:55] epoch: 17335 train-loss: 0.010545747872028086\n",
      "[LOG 20200502-14:58:55] epoch: 17336 train-loss: 0.010545738248361481\n",
      "[LOG 20200502-14:58:55] epoch: 17337 train-loss: 0.010545728624694876\n",
      "[LOG 20200502-14:58:55] epoch: 17338 train-loss: 0.01054571858710713\n",
      "[LOG 20200502-14:58:56] epoch: 17339 train-loss: 0.010545707928637663\n",
      "[LOG 20200502-14:58:56] epoch: 17340 train-loss: 0.010545696856247054\n",
      "[LOG 20200502-14:58:56] epoch: 17340 new best train-loss: 0.010545696856247054 found\n",
      "[LOG 20200502-14:58:56] epoch: 17341 train-loss: 0.01054568619777759\n",
      "[LOG 20200502-14:58:56] epoch: 17342 train-loss: 0.010545674711465836\n",
      "[LOG 20200502-14:58:57] epoch: 17343 train-loss: 0.010545662707752652\n",
      "[LOG 20200502-14:58:57] epoch: 17344 train-loss: 0.010545651117960611\n",
      "[LOG 20200502-14:58:57] epoch: 17345 train-loss: 0.010545638907286856\n",
      "[LOG 20200502-14:58:57] epoch: 17346 train-loss: 0.010545626282691956\n",
      "[LOG 20200502-14:58:58] epoch: 17347 train-loss: 0.010545613761577342\n",
      "[LOG 20200502-14:58:58] epoch: 17348 train-loss: 0.010545600412620438\n",
      "[LOG 20200502-14:58:58] epoch: 17349 train-loss: 0.010545587270624109\n",
      "[LOG 20200502-14:58:58] epoch: 17350 train-loss: 0.010545573507746061\n",
      "[LOG 20200502-14:58:58] epoch: 17350 new best train-loss: 0.010545573507746061 found\n",
      "[LOG 20200502-14:58:58] epoch: 17351 train-loss: 0.010545559434427155\n",
      "[LOG 20200502-14:58:59] epoch: 17352 train-loss: 0.010545545154147677\n",
      "[LOG 20200502-14:58:59] epoch: 17353 train-loss: 0.010545530770387914\n",
      "[LOG 20200502-14:58:59] epoch: 17354 train-loss: 0.010545516386628151\n",
      "[LOG 20200502-14:59:00] epoch: 17355 train-loss: 0.01054550220982896\n",
      "[LOG 20200502-14:59:00] epoch: 17356 train-loss: 0.010545486998226907\n",
      "[LOG 20200502-14:59:00] epoch: 17357 train-loss: 0.01054547271794743\n",
      "[LOG 20200502-14:59:00] epoch: 17358 train-loss: 0.010545457920266522\n",
      "[LOG 20200502-14:59:00] epoch: 17359 train-loss: 0.010545442812144756\n",
      "[LOG 20200502-14:59:01] epoch: 17360 train-loss: 0.01054542801446385\n",
      "[LOG 20200502-14:59:01] epoch: 17360 new best train-loss: 0.01054542801446385 found\n",
      "[LOG 20200502-14:59:01] epoch: 17361 train-loss: 0.0105454135272238\n",
      "[LOG 20200502-14:59:01] epoch: 17362 train-loss: 0.010545398419102034\n",
      "[LOG 20200502-14:59:01] epoch: 17363 train-loss: 0.010545384242302842\n",
      "[LOG 20200502-14:59:02] epoch: 17364 train-loss: 0.010545369237661362\n",
      "[LOG 20200502-14:59:02] epoch: 17365 train-loss: 0.010545354853901599\n",
      "[LOG 20200502-14:59:02] epoch: 17366 train-loss: 0.010545341091023551\n",
      "[LOG 20200502-14:59:02] epoch: 17367 train-loss: 0.010545326707263788\n",
      "[LOG 20200502-14:59:03] epoch: 17368 train-loss: 0.010545313047866026\n",
      "[LOG 20200502-14:59:03] epoch: 17369 train-loss: 0.010545299595428837\n",
      "[LOG 20200502-14:59:03] epoch: 17370 train-loss: 0.010545285936031077\n",
      "[LOG 20200502-14:59:03] epoch: 17370 new best train-loss: 0.010545285936031077 found\n",
      "[LOG 20200502-14:59:03] epoch: 17371 train-loss: 0.01054527320795589\n",
      "[LOG 20200502-14:59:04] epoch: 17372 train-loss: 0.010545260479880704\n",
      "[LOG 20200502-14:59:04] epoch: 17373 train-loss: 0.010545247855285803\n",
      "[LOG 20200502-14:59:04] epoch: 17374 train-loss: 0.010545235230690904\n",
      "[LOG 20200502-14:59:04] epoch: 17375 train-loss: 0.010545223020017147\n",
      "[LOG 20200502-14:59:05] epoch: 17376 train-loss: 0.010545211119784249\n",
      "[LOG 20200502-14:59:05] epoch: 17377 train-loss: 0.01054520004739364\n",
      "[LOG 20200502-14:59:05] epoch: 17378 train-loss: 0.010545188664562173\n",
      "[LOG 20200502-14:59:05] epoch: 17379 train-loss: 0.010545177281730704\n",
      "[LOG 20200502-14:59:06] epoch: 17380 train-loss: 0.010545166002379524\n",
      "[LOG 20200502-14:59:06] epoch: 17380 new best train-loss: 0.010545166002379524 found\n",
      "[LOG 20200502-14:59:06] epoch: 17381 train-loss: 0.010545155343910059\n",
      "[LOG 20200502-14:59:06] epoch: 17382 train-loss: 0.010545144892401166\n",
      "[LOG 20200502-14:59:06] epoch: 17383 train-loss: 0.010545134544372559\n",
      "[LOG 20200502-14:59:07] epoch: 17384 train-loss: 0.010545124610265097\n",
      "[LOG 20200502-14:59:07] epoch: 17385 train-loss: 0.01054511395179563\n",
      "[LOG 20200502-14:59:07] epoch: 17386 train-loss: 0.01054510422464874\n",
      "[LOG 20200502-14:59:07] epoch: 17387 train-loss: 0.01054509398010042\n",
      "[LOG 20200502-14:59:07] epoch: 17388 train-loss: 0.010545083632071814\n",
      "[LOG 20200502-14:59:08] epoch: 17389 train-loss: 0.010545074008405209\n",
      "[LOG 20200502-14:59:08] epoch: 17390 train-loss: 0.010545063763856888\n",
      "[LOG 20200502-14:59:08] epoch: 17390 new best train-loss: 0.010545063763856888 found\n",
      "[LOG 20200502-14:59:08] epoch: 17391 train-loss: 0.010545053105387423\n",
      "[LOG 20200502-14:59:08] epoch: 17392 train-loss: 0.010545043274760246\n",
      "[LOG 20200502-14:59:09] epoch: 17393 train-loss: 0.01054503240933021\n",
      "[LOG 20200502-14:59:09] epoch: 17394 train-loss: 0.010545022785663605\n",
      "[LOG 20200502-14:59:09] epoch: 17395 train-loss: 0.010545011816753281\n",
      "[LOG 20200502-14:59:09] epoch: 17396 train-loss: 0.01054500157220496\n",
      "[LOG 20200502-14:59:10] epoch: 17397 train-loss: 0.010544989568491777\n",
      "[LOG 20200502-14:59:10] epoch: 17398 train-loss: 0.010544978910022311\n",
      "[LOG 20200502-14:59:10] epoch: 17399 train-loss: 0.010544966699348556\n",
      "[LOG 20200502-14:59:10] epoch: 17400 train-loss: 0.010544954695635371\n",
      "[LOG 20200502-14:59:10] epoch: 17400 new best train-loss: 0.010544954695635371 found\n",
      "[LOG 20200502-14:59:11] epoch: 17401 train-loss: 0.01054494289888276\n",
      "[LOG 20200502-14:59:11] epoch: 17402 train-loss: 0.010544930067327287\n",
      "[LOG 20200502-14:59:11] epoch: 17403 train-loss: 0.010544916614890099\n",
      "[LOG 20200502-14:59:11] epoch: 17404 train-loss: 0.010544903058972623\n",
      "[LOG 20200502-14:59:12] epoch: 17405 train-loss: 0.010544889916976294\n",
      "[LOG 20200502-14:59:12] epoch: 17406 train-loss: 0.010544875740177102\n",
      "[LOG 20200502-14:59:12] epoch: 17407 train-loss: 0.010544861252937052\n",
      "[LOG 20200502-14:59:12] epoch: 17408 train-loss: 0.010544846248295572\n",
      "[LOG 20200502-14:59:13] epoch: 17409 train-loss: 0.010544830726252662\n",
      "[LOG 20200502-14:59:13] epoch: 17410 train-loss: 0.010544815411170324\n",
      "[LOG 20200502-14:59:13] epoch: 17410 new best train-loss: 0.010544815411170324 found\n",
      "[LOG 20200502-14:59:13] epoch: 17411 train-loss: 0.010544798854324553\n",
      "[LOG 20200502-14:59:13] epoch: 17412 train-loss: 0.010544782918360498\n",
      "[LOG 20200502-14:59:14] epoch: 17413 train-loss: 0.010544765326711867\n",
      "[LOG 20200502-14:59:14] epoch: 17414 train-loss: 0.010544748976826668\n",
      "[LOG 20200502-14:59:14] epoch: 17415 train-loss: 0.01054473128169775\n",
      "[LOG 20200502-14:59:14] epoch: 17416 train-loss: 0.010544714000489976\n",
      "[LOG 20200502-14:59:15] epoch: 17417 train-loss: 0.010544695891439915\n",
      "[LOG 20200502-14:59:15] epoch: 17418 train-loss: 0.010544677885870138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:59:15] epoch: 17419 train-loss: 0.010544659983780649\n",
      "[LOG 20200502-14:59:15] epoch: 17420 train-loss: 0.010544642392132018\n",
      "[LOG 20200502-14:59:15] epoch: 17420 new best train-loss: 0.010544642392132018 found\n",
      "[LOG 20200502-14:59:16] epoch: 17421 train-loss: 0.010544623972641097\n",
      "[LOG 20200502-14:59:16] epoch: 17422 train-loss: 0.010544606174031893\n",
      "[LOG 20200502-14:59:16] epoch: 17423 train-loss: 0.010544588064981831\n",
      "[LOG 20200502-14:59:16] epoch: 17424 train-loss: 0.010544570680293772\n",
      "[LOG 20200502-14:59:17] epoch: 17425 train-loss: 0.010544553295605712\n",
      "[LOG 20200502-14:59:17] epoch: 17426 train-loss: 0.01054453601439794\n",
      "[LOG 20200502-14:59:17] epoch: 17427 train-loss: 0.010544519250591597\n",
      "[LOG 20200502-14:59:17] epoch: 17428 train-loss: 0.010544502900706397\n",
      "[LOG 20200502-14:59:18] epoch: 17429 train-loss: 0.010544486136900054\n",
      "[LOG 20200502-14:59:18] epoch: 17430 train-loss: 0.010544470407896571\n",
      "[LOG 20200502-14:59:18] epoch: 17430 new best train-loss: 0.010544470407896571 found\n",
      "[LOG 20200502-14:59:18] epoch: 17431 train-loss: 0.010544454782373376\n",
      "[LOG 20200502-14:59:18] epoch: 17432 train-loss: 0.01054443915685018\n",
      "[LOG 20200502-14:59:19] epoch: 17433 train-loss: 0.010544425600932704\n",
      "[LOG 20200502-14:59:19] epoch: 17434 train-loss: 0.01054441101021237\n",
      "[LOG 20200502-14:59:19] epoch: 17435 train-loss: 0.010544397247334322\n",
      "[LOG 20200502-14:59:19] epoch: 17436 train-loss: 0.010544385140140852\n",
      "[LOG 20200502-14:59:20] epoch: 17437 train-loss: 0.010544372205105092\n",
      "[LOG 20200502-14:59:20] epoch: 17438 train-loss: 0.010544360408352481\n",
      "[LOG 20200502-14:59:20] epoch: 17439 train-loss: 0.010544349129001299\n",
      "[LOG 20200502-14:59:20] epoch: 17440 train-loss: 0.010544338263571262\n",
      "[LOG 20200502-14:59:20] epoch: 17440 new best train-loss: 0.010544338263571262 found\n",
      "[LOG 20200502-14:59:21] epoch: 17441 train-loss: 0.0105443283294638\n",
      "[LOG 20200502-14:59:21] epoch: 17442 train-loss: 0.010544318602316909\n",
      "[LOG 20200502-14:59:21] epoch: 17443 train-loss: 0.010544309185610877\n",
      "[LOG 20200502-14:59:21] epoch: 17444 train-loss: 0.010544299975865416\n",
      "[LOG 20200502-14:59:22] epoch: 17445 train-loss: 0.01054429200788339\n",
      "[LOG 20200502-14:59:22] epoch: 17446 train-loss: 0.010544284350342222\n",
      "[LOG 20200502-14:59:22] epoch: 17447 train-loss: 0.010544276899761625\n",
      "[LOG 20200502-14:59:22] epoch: 17448 train-loss: 0.010544269863102172\n",
      "[LOG 20200502-14:59:23] epoch: 17449 train-loss: 0.01054426334384415\n",
      "[LOG 20200502-14:59:23] epoch: 17450 train-loss: 0.010544257135026984\n",
      "[LOG 20200502-14:59:23] epoch: 17450 new best train-loss: 0.010544257135026984 found\n",
      "[LOG 20200502-14:59:23] epoch: 17451 train-loss: 0.01054425092620982\n",
      "[LOG 20200502-14:59:23] epoch: 17452 train-loss: 0.010544245959156089\n",
      "[LOG 20200502-14:59:24] epoch: 17453 train-loss: 0.010544240060779784\n",
      "[LOG 20200502-14:59:24] epoch: 17454 train-loss: 0.010544235300686624\n",
      "[LOG 20200502-14:59:24] epoch: 17455 train-loss: 0.010544230333632894\n",
      "[LOG 20200502-14:59:24] epoch: 17456 train-loss: 0.010544225573539734\n",
      "[LOG 20200502-14:59:25] epoch: 17457 train-loss: 0.010544221537808577\n",
      "[LOG 20200502-14:59:25] epoch: 17458 train-loss: 0.010544217191636562\n",
      "[LOG 20200502-14:59:25] epoch: 17459 train-loss: 0.01054421253502369\n",
      "[LOG 20200502-14:59:25] epoch: 17460 train-loss: 0.010544209016693963\n",
      "[LOG 20200502-14:59:25] epoch: 17460 new best train-loss: 0.010544209016693963 found\n",
      "[LOG 20200502-14:59:26] epoch: 17461 train-loss: 0.01054420518792338\n",
      "[LOG 20200502-14:59:26] epoch: 17462 train-loss: 0.010544200634790791\n",
      "[LOG 20200502-14:59:26] epoch: 17463 train-loss: 0.010544196909500493\n",
      "[LOG 20200502-14:59:26] epoch: 17464 train-loss: 0.010544192666808764\n",
      "[LOG 20200502-14:59:27] epoch: 17465 train-loss: 0.01054418832063675\n",
      "[LOG 20200502-14:59:27] epoch: 17466 train-loss: 0.010544184284905592\n",
      "[LOG 20200502-14:59:27] epoch: 17467 train-loss: 0.010544180042213865\n",
      "[LOG 20200502-14:59:27] epoch: 17468 train-loss: 0.010544175489081277\n",
      "[LOG 20200502-14:59:28] epoch: 17469 train-loss: 0.010544170625507832\n",
      "[LOG 20200502-14:59:28] epoch: 17470 train-loss: 0.010544165244532956\n",
      "[LOG 20200502-14:59:28] epoch: 17470 new best train-loss: 0.010544165244532956 found\n",
      "[LOG 20200502-14:59:28] epoch: 17471 train-loss: 0.010544161105321513\n",
      "[LOG 20200502-14:59:28] epoch: 17472 train-loss: 0.010544155517386066\n",
      "[LOG 20200502-14:59:28] epoch: 17473 train-loss: 0.010544150239891477\n",
      "[LOG 20200502-14:59:29] epoch: 17474 train-loss: 0.010544144238034884\n",
      "[LOG 20200502-14:59:29] epoch: 17475 train-loss: 0.010544138650099436\n",
      "[LOG 20200502-14:59:29] epoch: 17476 train-loss: 0.010544131820400557\n",
      "[LOG 20200502-14:59:29] epoch: 17477 train-loss: 0.010544125508103106\n",
      "[LOG 20200502-14:59:30] epoch: 17478 train-loss: 0.01054411857492394\n",
      "[LOG 20200502-14:59:30] epoch: 17479 train-loss: 0.010544111745225059\n",
      "[LOG 20200502-14:59:30] epoch: 17480 train-loss: 0.010544103777243031\n",
      "[LOG 20200502-14:59:30] epoch: 17480 new best train-loss: 0.010544103777243031 found\n",
      "[LOG 20200502-14:59:30] epoch: 17481 train-loss: 0.010544096016221575\n",
      "[LOG 20200502-14:59:31] epoch: 17482 train-loss: 0.010544088669121265\n",
      "[LOG 20200502-14:59:31] epoch: 17483 train-loss: 0.010544079562856091\n",
      "[LOG 20200502-14:59:31] epoch: 17484 train-loss: 0.010544071077472635\n",
      "[LOG 20200502-14:59:31] epoch: 17485 train-loss: 0.010544062178168032\n",
      "[LOG 20200502-14:59:32] epoch: 17486 train-loss: 0.010544053071902858\n",
      "[LOG 20200502-14:59:32] epoch: 17487 train-loss: 0.010544043758677112\n",
      "[LOG 20200502-14:59:32] epoch: 17488 train-loss: 0.010544034341971079\n",
      "[LOG 20200502-14:59:32] epoch: 17489 train-loss: 0.010544023786981901\n",
      "[LOG 20200502-14:59:33] epoch: 17490 train-loss: 0.010544014163315296\n",
      "[LOG 20200502-14:59:33] epoch: 17490 new best train-loss: 0.010544014163315296 found\n",
      "[LOG 20200502-14:59:33] epoch: 17491 train-loss: 0.010544004022247262\n",
      "[LOG 20200502-14:59:33] epoch: 17492 train-loss: 0.01054399326029751\n",
      "[LOG 20200502-14:59:34] epoch: 17493 train-loss: 0.010543982912268903\n",
      "[LOG 20200502-14:59:34] epoch: 17494 train-loss: 0.010543972253799438\n",
      "[LOG 20200502-14:59:34] epoch: 17495 train-loss: 0.010543960974448256\n",
      "[LOG 20200502-14:59:34] epoch: 17496 train-loss: 0.01054395062641965\n",
      "[LOG 20200502-14:59:35] epoch: 17497 train-loss: 0.010543939657509327\n",
      "[LOG 20200502-14:59:35] epoch: 17498 train-loss: 0.01054392879207929\n",
      "[LOG 20200502-14:59:35] epoch: 17499 train-loss: 0.01054391771968868\n",
      "[LOG 20200502-14:59:35] epoch: 17500 train-loss: 0.01054390747514036\n",
      "[LOG 20200502-14:59:35] epoch: 17500 new best train-loss: 0.01054390747514036 found\n",
      "[LOG 20200502-14:59:36] epoch: 17501 train-loss: 0.01054389640274975\n",
      "[LOG 20200502-14:59:36] epoch: 17502 train-loss: 0.010543885951240858\n",
      "[LOG 20200502-14:59:36] epoch: 17503 train-loss: 0.010543875292771392\n",
      "[LOG 20200502-14:59:36] epoch: 17504 train-loss: 0.010543865151703358\n",
      "[LOG 20200502-14:59:37] epoch: 17505 train-loss: 0.010543854907155037\n",
      "[LOG 20200502-14:59:37] epoch: 17506 train-loss: 0.01054384507652786\n",
      "[LOG 20200502-14:59:37] epoch: 17507 train-loss: 0.010543835245900683\n",
      "[LOG 20200502-14:59:37] epoch: 17508 train-loss: 0.010543826243115796\n",
      "[LOG 20200502-14:59:38] epoch: 17509 train-loss: 0.010543816722929478\n",
      "[LOG 20200502-14:59:38] epoch: 17510 train-loss: 0.010543807306223445\n",
      "[LOG 20200502-14:59:38] epoch: 17510 new best train-loss: 0.010543807306223445 found\n",
      "[LOG 20200502-14:59:38] epoch: 17511 train-loss: 0.01054379902780056\n",
      "[LOG 20200502-14:59:38] epoch: 17512 train-loss: 0.010543789921535386\n",
      "[LOG 20200502-14:59:38] epoch: 17513 train-loss: 0.01054378216051393\n",
      "[LOG 20200502-14:59:39] epoch: 17514 train-loss: 0.010543774089051617\n",
      "[LOG 20200502-14:59:39] epoch: 17515 train-loss: 0.010543766224549877\n",
      "[LOG 20200502-14:59:39] epoch: 17516 train-loss: 0.01054375877396928\n",
      "[LOG 20200502-14:59:39] epoch: 17517 train-loss: 0.010543751840790113\n",
      "[LOG 20200502-14:59:40] epoch: 17518 train-loss: 0.010543744597170088\n",
      "[LOG 20200502-14:59:40] epoch: 17519 train-loss: 0.010543737663990922\n",
      "[LOG 20200502-14:59:40] epoch: 17520 train-loss: 0.010543731041252613\n",
      "[LOG 20200502-14:59:40] epoch: 17520 new best train-loss: 0.010543731041252613 found\n",
      "[LOG 20200502-14:59:40] epoch: 17521 train-loss: 0.010543724211553732\n",
      "[LOG 20200502-14:59:41] epoch: 17522 train-loss: 0.010543718002736568\n",
      "[LOG 20200502-14:59:41] epoch: 17523 train-loss: 0.010543711793919405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-14:59:41] epoch: 17524 train-loss: 0.010543705792062812\n",
      "[LOG 20200502-14:59:41] epoch: 17525 train-loss: 0.010543699583245648\n",
      "[LOG 20200502-14:59:42] epoch: 17526 train-loss: 0.010543693684869342\n",
      "[LOG 20200502-14:59:42] epoch: 17527 train-loss: 0.010543687889973322\n",
      "[LOG 20200502-14:59:42] epoch: 17528 train-loss: 0.010543681474195587\n",
      "[LOG 20200502-14:59:42] epoch: 17529 train-loss: 0.010543675782779852\n",
      "[LOG 20200502-14:59:43] epoch: 17530 train-loss: 0.010543669470482402\n",
      "[LOG 20200502-14:59:43] epoch: 17530 new best train-loss: 0.010543669470482402 found\n",
      "[LOG 20200502-14:59:43] epoch: 17531 train-loss: 0.010543663261665238\n",
      "[LOG 20200502-14:59:43] epoch: 17532 train-loss: 0.010543656845887503\n",
      "[LOG 20200502-14:59:43] epoch: 17533 train-loss: 0.010543650430109765\n",
      "[LOG 20200502-14:59:44] epoch: 17534 train-loss: 0.010543642979529168\n",
      "[LOG 20200502-14:59:44] epoch: 17535 train-loss: 0.010543636356790861\n",
      "[LOG 20200502-14:59:44] epoch: 17536 train-loss: 0.01054362900969055\n",
      "[LOG 20200502-14:59:44] epoch: 17537 train-loss: 0.010543621973031096\n",
      "[LOG 20200502-14:59:45] epoch: 17538 train-loss: 0.010543613798088498\n",
      "[LOG 20200502-14:59:45] epoch: 17539 train-loss: 0.010543605519665612\n",
      "[LOG 20200502-14:59:45] epoch: 17540 train-loss: 0.010543596827321582\n",
      "[LOG 20200502-14:59:45] epoch: 17540 new best train-loss: 0.010543596827321582 found\n",
      "[LOG 20200502-14:59:45] epoch: 17541 train-loss: 0.010543588652378984\n",
      "[LOG 20200502-14:59:46] epoch: 17542 train-loss: 0.010543579442633523\n",
      "[LOG 20200502-14:59:46] epoch: 17543 train-loss: 0.010543569301565489\n",
      "[LOG 20200502-14:59:46] epoch: 17544 train-loss: 0.010543559574418597\n",
      "[LOG 20200502-14:59:47] epoch: 17545 train-loss: 0.010543548915949132\n",
      "[LOG 20200502-14:59:47] epoch: 17546 train-loss: 0.010543538050519096\n",
      "[LOG 20200502-14:59:47] epoch: 17547 train-loss: 0.0105435268746482\n",
      "[LOG 20200502-14:59:47] epoch: 17548 train-loss: 0.010543515284856161\n",
      "[LOG 20200502-14:59:47] epoch: 17549 train-loss: 0.010543503281142976\n",
      "[LOG 20200502-14:59:48] epoch: 17550 train-loss: 0.010543491277429793\n",
      "[LOG 20200502-14:59:48] epoch: 17550 new best train-loss: 0.010543491277429793 found\n",
      "[LOG 20200502-14:59:48] epoch: 17551 train-loss: 0.010543478859795464\n",
      "[LOG 20200502-14:59:48] epoch: 17552 train-loss: 0.010543465924759706\n",
      "[LOG 20200502-14:59:48] epoch: 17553 train-loss: 0.01054345267928309\n",
      "[LOG 20200502-14:59:49] epoch: 17554 train-loss: 0.010543438916405043\n",
      "[LOG 20200502-14:59:49] epoch: 17555 train-loss: 0.010543425360487567\n",
      "[LOG 20200502-14:59:49] epoch: 17556 train-loss: 0.010543412011530664\n",
      "[LOG 20200502-14:59:49] epoch: 17557 train-loss: 0.010543397834731473\n",
      "[LOG 20200502-14:59:50] epoch: 17558 train-loss: 0.01054338396837314\n",
      "[LOG 20200502-14:59:50] epoch: 17559 train-loss: 0.01054337051593595\n",
      "[LOG 20200502-14:59:50] epoch: 17560 train-loss: 0.010543356132176187\n",
      "[LOG 20200502-14:59:50] epoch: 17560 new best train-loss: 0.010543356132176187 found\n",
      "[LOG 20200502-14:59:50] epoch: 17561 train-loss: 0.01054334236929814\n",
      "[LOG 20200502-14:59:51] epoch: 17562 train-loss: 0.010543328916860951\n",
      "[LOG 20200502-14:59:51] epoch: 17563 train-loss: 0.010543316085305478\n",
      "[LOG 20200502-14:59:51] epoch: 17564 train-loss: 0.010543302322427431\n",
      "[LOG 20200502-14:59:51] epoch: 17565 train-loss: 0.010543289387391673\n",
      "[LOG 20200502-14:59:52] epoch: 17566 train-loss: 0.010543277280198203\n",
      "[LOG 20200502-14:59:52] epoch: 17567 train-loss: 0.01054326444864273\n",
      "[LOG 20200502-14:59:52] epoch: 17568 train-loss: 0.010543252755370405\n",
      "[LOG 20200502-14:59:52] epoch: 17569 train-loss: 0.010543241062098078\n",
      "[LOG 20200502-14:59:53] epoch: 17570 train-loss: 0.010543230093187757\n",
      "[LOG 20200502-14:59:53] epoch: 17570 new best train-loss: 0.010543230093187757 found\n",
      "[LOG 20200502-14:59:53] epoch: 17571 train-loss: 0.010543219641678862\n",
      "[LOG 20200502-14:59:53] epoch: 17572 train-loss: 0.010543209604091115\n",
      "[LOG 20200502-14:59:53] epoch: 17573 train-loss: 0.010543200083904795\n",
      "[LOG 20200502-14:59:54] epoch: 17574 train-loss: 0.01054319128808048\n",
      "[LOG 20200502-14:59:54] epoch: 17575 train-loss: 0.010543182699216736\n",
      "[LOG 20200502-14:59:54] epoch: 17576 train-loss: 0.010543174834714996\n",
      "[LOG 20200502-14:59:54] epoch: 17577 train-loss: 0.01054316707369354\n",
      "[LOG 20200502-14:59:55] epoch: 17578 train-loss: 0.010543160657915805\n",
      "[LOG 20200502-14:59:55] epoch: 17579 train-loss: 0.01054315393169721\n",
      "[LOG 20200502-14:59:55] epoch: 17580 train-loss: 0.010543148033320904\n",
      "[LOG 20200502-14:59:55] epoch: 17580 new best train-loss: 0.010543148033320904 found\n",
      "[LOG 20200502-14:59:55] epoch: 17581 train-loss: 0.010543143066267172\n",
      "[LOG 20200502-14:59:56] epoch: 17582 train-loss: 0.010543137685292296\n",
      "[LOG 20200502-14:59:56] epoch: 17583 train-loss: 0.010543133856521713\n",
      "[LOG 20200502-14:59:56] epoch: 17584 train-loss: 0.0105431302347117\n",
      "[LOG 20200502-14:59:56] epoch: 17585 train-loss: 0.010543126302460829\n",
      "[LOG 20200502-14:59:57] epoch: 17586 train-loss: 0.010543123715453677\n",
      "[LOG 20200502-14:59:57] epoch: 17587 train-loss: 0.010543120714525381\n",
      "[LOG 20200502-14:59:57] epoch: 17588 train-loss: 0.010543118541439375\n",
      "[LOG 20200502-14:59:57] epoch: 17589 train-loss: 0.010543116575313939\n",
      "[LOG 20200502-14:59:57] epoch: 17590 train-loss: 0.010543114919629362\n",
      "[LOG 20200502-14:59:57] epoch: 17590 new best train-loss: 0.010543114919629362 found\n",
      "[LOG 20200502-14:59:58] epoch: 17591 train-loss: 0.010543113781346215\n",
      "[LOG 20200502-14:59:58] epoch: 17592 train-loss: 0.010543113263944784\n",
      "[LOG 20200502-14:59:58] epoch: 17593 train-loss: 0.010543112229141925\n",
      "[LOG 20200502-14:59:59] epoch: 17594 train-loss: 0.010543111608260207\n",
      "[LOG 20200502-14:59:59] epoch: 17595 train-loss: 0.010543111090858778\n",
      "[LOG 20200502-14:59:59] epoch: 17596 train-loss: 0.01054311078041792\n",
      "[LOG 20200502-14:59:59] epoch: 17597 train-loss: 0.010543110883898206\n",
      "[LOG 20200502-15:00:00] epoch: 17598 train-loss: 0.010543110366496775\n",
      "[LOG 20200502-15:00:00] epoch: 17599 train-loss: 0.010543110573457347\n",
      "[LOG 20200502-15:00:00] epoch: 17600 train-loss: 0.01054311078041792\n",
      "[LOG 20200502-15:00:00] epoch: 17600 new best train-loss: 0.01054311078041792 found\n",
      "[LOG 20200502-15:00:00] epoch: 17601 train-loss: 0.010543110676937632\n",
      "[LOG 20200502-15:00:01] epoch: 17602 train-loss: 0.010543110056055916\n",
      "[LOG 20200502-15:00:01] epoch: 17603 train-loss: 0.01054311046997706\n",
      "[LOG 20200502-15:00:01] epoch: 17604 train-loss: 0.010543110573457347\n",
      "[LOG 20200502-15:00:01] epoch: 17605 train-loss: 0.010543110263016488\n",
      "[LOG 20200502-15:00:02] epoch: 17606 train-loss: 0.010543110263016488\n",
      "[LOG 20200502-15:00:02] epoch: 17607 train-loss: 0.010543109849095345\n",
      "[LOG 20200502-15:00:02] epoch: 17608 train-loss: 0.01054310891777277\n",
      "[LOG 20200502-15:00:02] epoch: 17609 train-loss: 0.01054310840037134\n",
      "[LOG 20200502-15:00:03] epoch: 17610 train-loss: 0.01054310736556848\n",
      "[LOG 20200502-15:00:03] epoch: 17610 new best train-loss: 0.01054310736556848 found\n",
      "[LOG 20200502-15:00:03] epoch: 17611 train-loss: 0.01054310602032476\n",
      "[LOG 20200502-15:00:03] epoch: 17612 train-loss: 0.010543104468120469\n",
      "[LOG 20200502-15:00:03] epoch: 17613 train-loss: 0.010543103226357035\n",
      "[LOG 20200502-15:00:04] epoch: 17614 train-loss: 0.010543100949790742\n",
      "[LOG 20200502-15:00:04] epoch: 17615 train-loss: 0.010543098880185021\n",
      "[LOG 20200502-15:00:04] epoch: 17616 train-loss: 0.010543096086217297\n",
      "[LOG 20200502-15:00:04] epoch: 17617 train-loss: 0.01054309287832843\n",
      "[LOG 20200502-15:00:05] epoch: 17618 train-loss: 0.010543090291321278\n",
      "[LOG 20200502-15:00:05] epoch: 17619 train-loss: 0.010543086876471838\n",
      "[LOG 20200502-15:00:05] epoch: 17620 train-loss: 0.010543083047701253\n",
      "[LOG 20200502-15:00:05] epoch: 17620 new best train-loss: 0.010543083047701253 found\n",
      "[LOG 20200502-15:00:05] epoch: 17621 train-loss: 0.010543079218930669\n",
      "[LOG 20200502-15:00:05] epoch: 17622 train-loss: 0.010543074562317796\n",
      "[LOG 20200502-15:00:06] epoch: 17623 train-loss: 0.010543070319626067\n",
      "[LOG 20200502-15:00:06] epoch: 17624 train-loss: 0.010543065559532907\n",
      "[LOG 20200502-15:00:06] epoch: 17625 train-loss: 0.010543060385518603\n",
      "[LOG 20200502-15:00:06] epoch: 17626 train-loss: 0.010543055108024014\n",
      "[LOG 20200502-15:00:07] epoch: 17627 train-loss: 0.01054304993400971\n",
      "[LOG 20200502-15:00:07] epoch: 17628 train-loss: 0.010543044035633406\n",
      "[LOG 20200502-15:00:07] epoch: 17629 train-loss: 0.010543037723335955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-15:00:07] epoch: 17630 train-loss: 0.01054303151451879\n",
      "[LOG 20200502-15:00:07] epoch: 17630 new best train-loss: 0.01054303151451879 found\n",
      "[LOG 20200502-15:00:08] epoch: 17631 train-loss: 0.010543025305701626\n",
      "[LOG 20200502-15:00:08] epoch: 17632 train-loss: 0.010543019303845035\n",
      "[LOG 20200502-15:00:08] epoch: 17633 train-loss: 0.010543012784587013\n",
      "[LOG 20200502-15:00:08] epoch: 17634 train-loss: 0.010543006368809275\n",
      "[LOG 20200502-15:00:09] epoch: 17635 train-loss: 0.010543000056511827\n",
      "[LOG 20200502-15:00:09] epoch: 17636 train-loss: 0.010542993123332659\n",
      "[LOG 20200502-15:00:09] epoch: 17637 train-loss: 0.010542986811035208\n",
      "[LOG 20200502-15:00:09] epoch: 17638 train-loss: 0.010542980188296901\n",
      "[LOG 20200502-15:00:10] epoch: 17639 train-loss: 0.010542973669038879\n",
      "[LOG 20200502-15:00:10] epoch: 17640 train-loss: 0.010542967770662572\n",
      "[LOG 20200502-15:00:10] epoch: 17640 new best train-loss: 0.010542967770662572 found\n",
      "[LOG 20200502-15:00:10] epoch: 17641 train-loss: 0.010542961665325694\n",
      "[LOG 20200502-15:00:10] epoch: 17642 train-loss: 0.01054295545650853\n",
      "[LOG 20200502-15:00:11] epoch: 17643 train-loss: 0.010542949247691367\n",
      "[LOG 20200502-15:00:11] epoch: 17644 train-loss: 0.010542943763236204\n",
      "[LOG 20200502-15:00:11] epoch: 17645 train-loss: 0.010542938589221902\n",
      "[LOG 20200502-15:00:11] epoch: 17646 train-loss: 0.01054293279432588\n",
      "[LOG 20200502-15:00:12] epoch: 17647 train-loss: 0.010542928034232723\n",
      "[LOG 20200502-15:00:12] epoch: 17648 train-loss: 0.010542922756738134\n",
      "[LOG 20200502-15:00:12] epoch: 17649 train-loss: 0.010542918203605546\n",
      "[LOG 20200502-15:00:12] epoch: 17650 train-loss: 0.010542914374834962\n",
      "[LOG 20200502-15:00:12] epoch: 17650 new best train-loss: 0.010542914374834962 found\n",
      "[LOG 20200502-15:00:13] epoch: 17651 train-loss: 0.010542909718222089\n",
      "[LOG 20200502-15:00:13] epoch: 17652 train-loss: 0.01054290599293179\n",
      "[LOG 20200502-15:00:13] epoch: 17653 train-loss: 0.010542902992003493\n",
      "[LOG 20200502-15:00:13] epoch: 17654 train-loss: 0.010542899059752623\n",
      "[LOG 20200502-15:00:14] epoch: 17655 train-loss: 0.010542896369265186\n",
      "[LOG 20200502-15:00:14] epoch: 17656 train-loss: 0.01054289336833689\n",
      "[LOG 20200502-15:00:14] epoch: 17657 train-loss: 0.010542890884810023\n",
      "[LOG 20200502-15:00:14] epoch: 17658 train-loss: 0.0105428880908423\n",
      "[LOG 20200502-15:00:14] epoch: 17659 train-loss: 0.010542885814276006\n",
      "[LOG 20200502-15:00:15] epoch: 17660 train-loss: 0.010542884158591429\n",
      "[LOG 20200502-15:00:15] epoch: 17660 new best train-loss: 0.010542884158591429 found\n",
      "[LOG 20200502-15:00:15] epoch: 17661 train-loss: 0.010542881985505423\n",
      "[LOG 20200502-15:00:15] epoch: 17662 train-loss: 0.010542879708939128\n",
      "[LOG 20200502-15:00:15] epoch: 17663 train-loss: 0.010542878156734837\n",
      "[LOG 20200502-15:00:16] epoch: 17664 train-loss: 0.010542876708010832\n",
      "[LOG 20200502-15:00:16] epoch: 17665 train-loss: 0.010542874431444539\n",
      "[LOG 20200502-15:00:16] epoch: 17666 train-loss: 0.010542872775759961\n",
      "[LOG 20200502-15:00:16] epoch: 17667 train-loss: 0.010542871223555671\n",
      "[LOG 20200502-15:00:17] epoch: 17668 train-loss: 0.010542869257430235\n",
      "[LOG 20200502-15:00:17] epoch: 17669 train-loss: 0.010542867084344229\n",
      "[LOG 20200502-15:00:17] epoch: 17670 train-loss: 0.010542865325179365\n",
      "[LOG 20200502-15:00:17] epoch: 17670 new best train-loss: 0.010542865325179365 found\n",
      "[LOG 20200502-15:00:17] epoch: 17671 train-loss: 0.010542863048613071\n",
      "[LOG 20200502-15:00:18] epoch: 17672 train-loss: 0.01054286015116506\n",
      "[LOG 20200502-15:00:18] epoch: 17673 train-loss: 0.010542857460677624\n",
      "[LOG 20200502-15:00:18] epoch: 17674 train-loss: 0.010542854873670472\n",
      "[LOG 20200502-15:00:18] epoch: 17675 train-loss: 0.010542851872742176\n",
      "[LOG 20200502-15:00:19] epoch: 17676 train-loss: 0.01054284804397159\n",
      "[LOG 20200502-15:00:19] epoch: 17677 train-loss: 0.010542844525641866\n",
      "[LOG 20200502-15:00:19] epoch: 17678 train-loss: 0.010542840489910709\n",
      "[LOG 20200502-15:00:19] epoch: 17679 train-loss: 0.010542836143738694\n",
      "[LOG 20200502-15:00:20] epoch: 17680 train-loss: 0.010542831176684963\n",
      "[LOG 20200502-15:00:20] epoch: 17680 new best train-loss: 0.010542831176684963 found\n",
      "[LOG 20200502-15:00:20] epoch: 17681 train-loss: 0.010542826002670659\n",
      "[LOG 20200502-15:00:20] epoch: 17682 train-loss: 0.01054281989733378\n",
      "[LOG 20200502-15:00:20] epoch: 17683 train-loss: 0.010542814516358905\n",
      "[LOG 20200502-15:00:21] epoch: 17684 train-loss: 0.01054280779014031\n",
      "[LOG 20200502-15:00:21] epoch: 17685 train-loss: 0.010542801063921716\n",
      "[LOG 20200502-15:00:21] epoch: 17686 train-loss: 0.010542794027262263\n",
      "[LOG 20200502-15:00:21] epoch: 17687 train-loss: 0.010542786162760522\n",
      "[LOG 20200502-15:00:21] epoch: 17688 train-loss: 0.010542778194778495\n",
      "[LOG 20200502-15:00:22] epoch: 17689 train-loss: 0.010542770123316182\n",
      "[LOG 20200502-15:00:22] epoch: 17690 train-loss: 0.010542761430972152\n",
      "[LOG 20200502-15:00:22] epoch: 17690 new best train-loss: 0.010542761430972152 found\n",
      "[LOG 20200502-15:00:22] epoch: 17691 train-loss: 0.010542752531667551\n",
      "[LOG 20200502-15:00:23] epoch: 17692 train-loss: 0.010542743425402377\n",
      "[LOG 20200502-15:00:23] epoch: 17693 train-loss: 0.01054273411217663\n",
      "[LOG 20200502-15:00:23] epoch: 17694 train-loss: 0.010542724695470598\n",
      "[LOG 20200502-15:00:23] epoch: 17695 train-loss: 0.01054271517528428\n",
      "[LOG 20200502-15:00:23] epoch: 17696 train-loss: 0.010542705551617675\n",
      "[LOG 20200502-15:00:24] epoch: 17697 train-loss: 0.010542696031431356\n",
      "[LOG 20200502-15:00:24] epoch: 17698 train-loss: 0.010542686511245038\n",
      "[LOG 20200502-15:00:24] epoch: 17699 train-loss: 0.010542676784098148\n",
      "[LOG 20200502-15:00:24] epoch: 17700 train-loss: 0.010542667781313261\n",
      "[LOG 20200502-15:00:24] epoch: 17700 new best train-loss: 0.010542667781313261 found\n",
      "[LOG 20200502-15:00:25] epoch: 17701 train-loss: 0.010542659192449517\n",
      "[LOG 20200502-15:00:25] epoch: 17702 train-loss: 0.010542649568782913\n",
      "[LOG 20200502-15:00:25] epoch: 17703 train-loss: 0.010542640462517738\n",
      "[LOG 20200502-15:00:25] epoch: 17704 train-loss: 0.010542632391055426\n",
      "[LOG 20200502-15:00:26] epoch: 17705 train-loss: 0.01054262411263254\n",
      "[LOG 20200502-15:00:26] epoch: 17706 train-loss: 0.010542616662051942\n",
      "[LOG 20200502-15:00:26] epoch: 17707 train-loss: 0.010542609418431917\n",
      "[LOG 20200502-15:00:26] epoch: 17708 train-loss: 0.010542602174811892\n",
      "[LOG 20200502-15:00:27] epoch: 17709 train-loss: 0.010542595862514444\n",
      "[LOG 20200502-15:00:27] epoch: 17710 train-loss: 0.010542590481539568\n",
      "[LOG 20200502-15:00:27] epoch: 17710 new best train-loss: 0.010542590481539568 found\n",
      "[LOG 20200502-15:00:27] epoch: 17711 train-loss: 0.010542584790123833\n",
      "[LOG 20200502-15:00:27] epoch: 17712 train-loss: 0.010542579926550388\n",
      "[LOG 20200502-15:00:28] epoch: 17713 train-loss: 0.01054257620126009\n",
      "[LOG 20200502-15:00:28] epoch: 17714 train-loss: 0.010542572062048648\n",
      "[LOG 20200502-15:00:28] epoch: 17715 train-loss: 0.010542568440238634\n",
      "[LOG 20200502-15:00:28] epoch: 17716 train-loss: 0.010542565956711769\n",
      "[LOG 20200502-15:00:29] epoch: 17717 train-loss: 0.010542563369704617\n",
      "[LOG 20200502-15:00:29] epoch: 17718 train-loss: 0.010542562334901757\n",
      "[LOG 20200502-15:00:29] epoch: 17719 train-loss: 0.010542560989658037\n",
      "[LOG 20200502-15:00:29] epoch: 17720 train-loss: 0.010542560058335463\n",
      "[LOG 20200502-15:00:29] epoch: 17720 new best train-loss: 0.010542560058335463 found\n",
      "[LOG 20200502-15:00:30] epoch: 17721 train-loss: 0.010542559747894606\n",
      "[LOG 20200502-15:00:30] epoch: 17722 train-loss: 0.010542560058335463\n",
      "[LOG 20200502-15:00:30] epoch: 17723 train-loss: 0.010542560886177752\n",
      "[LOG 20200502-15:00:30] epoch: 17724 train-loss: 0.010542561817500327\n",
      "[LOG 20200502-15:00:30] epoch: 17725 train-loss: 0.010542563680145476\n",
      "[LOG 20200502-15:00:31] epoch: 17726 train-loss: 0.010542564921908908\n",
      "[LOG 20200502-15:00:31] epoch: 17727 train-loss: 0.010542567715876631\n",
      "[LOG 20200502-15:00:31] epoch: 17728 train-loss: 0.01054256988896264\n",
      "[LOG 20200502-15:00:31] epoch: 17729 train-loss: 0.01054257278641065\n",
      "[LOG 20200502-15:00:32] epoch: 17730 train-loss: 0.010542576304740377\n",
      "[LOG 20200502-15:00:32] epoch: 17731 train-loss: 0.010542579823070101\n",
      "[LOG 20200502-15:00:32] epoch: 17732 train-loss: 0.010542583444880115\n",
      "[LOG 20200502-15:00:32] epoch: 17733 train-loss: 0.010542587066690126\n",
      "[LOG 20200502-15:00:33] epoch: 17734 train-loss: 0.010542591102421284\n",
      "[LOG 20200502-15:00:33] epoch: 17735 train-loss: 0.010542595034672154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-15:00:33] epoch: 17736 train-loss: 0.010542599794765314\n",
      "[LOG 20200502-15:00:33] epoch: 17737 train-loss: 0.010542603416575326\n",
      "[LOG 20200502-15:00:34] epoch: 17738 train-loss: 0.010542608176668486\n",
      "[LOG 20200502-15:00:34] epoch: 17739 train-loss: 0.010542612626320787\n",
      "[LOG 20200502-15:00:34] epoch: 17740 train-loss: 0.010542616351611085\n",
      "[LOG 20200502-15:00:34] epoch: 17741 train-loss: 0.010542620594302813\n",
      "[LOG 20200502-15:00:35] epoch: 17742 train-loss: 0.010542624940474829\n",
      "[LOG 20200502-15:00:35] epoch: 17743 train-loss: 0.010542629079686271\n",
      "[LOG 20200502-15:00:35] epoch: 17744 train-loss: 0.010542633218897713\n",
      "[LOG 20200502-15:00:35] epoch: 17745 train-loss: 0.01054263725462887\n",
      "[LOG 20200502-15:00:36] epoch: 17746 train-loss: 0.010542640255557166\n",
      "[LOG 20200502-15:00:36] epoch: 17747 train-loss: 0.010542644291288324\n",
      "[LOG 20200502-15:00:36] epoch: 17748 train-loss: 0.010542647085256047\n",
      "[LOG 20200502-15:00:36] epoch: 17749 train-loss: 0.010542650293144915\n",
      "[LOG 20200502-15:00:37] epoch: 17750 train-loss: 0.010542652983632352\n",
      "[LOG 20200502-15:00:37] epoch: 17751 train-loss: 0.010542655363678932\n",
      "[LOG 20200502-15:00:37] epoch: 17752 train-loss: 0.010542658261126943\n",
      "[LOG 20200502-15:00:37] epoch: 17753 train-loss: 0.010542659916811518\n",
      "[LOG 20200502-15:00:38] epoch: 17754 train-loss: 0.010542661779456668\n",
      "[LOG 20200502-15:00:38] epoch: 17755 train-loss: 0.010542663124700388\n",
      "[LOG 20200502-15:00:38] epoch: 17756 train-loss: 0.010542664676904678\n",
      "[LOG 20200502-15:00:38] epoch: 17757 train-loss: 0.010542664987345537\n",
      "[LOG 20200502-15:00:39] epoch: 17758 train-loss: 0.010542665297786394\n",
      "[LOG 20200502-15:00:39] epoch: 17759 train-loss: 0.010542666022148397\n",
      "[LOG 20200502-15:00:39] epoch: 17760 train-loss: 0.010542666022148397\n",
      "[LOG 20200502-15:00:39] epoch: 17761 train-loss: 0.010542665194306109\n",
      "[LOG 20200502-15:00:40] epoch: 17762 train-loss: 0.010542664573424392\n",
      "[LOG 20200502-15:00:40] epoch: 17763 train-loss: 0.010542664159503248\n",
      "[LOG 20200502-15:00:40] epoch: 17764 train-loss: 0.010542662917739816\n",
      "[LOG 20200502-15:00:40] epoch: 17765 train-loss: 0.010542661882936954\n",
      "[LOG 20200502-15:00:41] epoch: 17766 train-loss: 0.010542660537693236\n",
      "[LOG 20200502-15:00:41] epoch: 17767 train-loss: 0.010542658364607228\n",
      "[LOG 20200502-15:00:41] epoch: 17768 train-loss: 0.01054265670892265\n",
      "[LOG 20200502-15:00:41] epoch: 17769 train-loss: 0.010542654535836644\n",
      "[LOG 20200502-15:00:42] epoch: 17770 train-loss: 0.010542652983632352\n",
      "[LOG 20200502-15:00:42] epoch: 17771 train-loss: 0.010542650603585772\n",
      "[LOG 20200502-15:00:42] epoch: 17772 train-loss: 0.010542648120058907\n",
      "[LOG 20200502-15:00:42] epoch: 17773 train-loss: 0.010542645740012327\n",
      "[LOG 20200502-15:00:43] epoch: 17774 train-loss: 0.010542643566926321\n",
      "[LOG 20200502-15:00:43] epoch: 17775 train-loss: 0.0105426414973206\n",
      "[LOG 20200502-15:00:43] epoch: 17776 train-loss: 0.010542639013793733\n",
      "[LOG 20200502-15:00:43] epoch: 17777 train-loss: 0.01054263725462887\n",
      "[LOG 20200502-15:00:44] epoch: 17778 train-loss: 0.010542635081542863\n",
      "[LOG 20200502-15:00:44] epoch: 17779 train-loss: 0.01054263280497657\n",
      "[LOG 20200502-15:00:44] epoch: 17780 train-loss: 0.010542631252772279\n",
      "[LOG 20200502-15:00:44] epoch: 17781 train-loss: 0.01054262939012713\n",
      "[LOG 20200502-15:00:45] epoch: 17782 train-loss: 0.010542628251843982\n",
      "[LOG 20200502-15:00:45] epoch: 17783 train-loss: 0.010542627113560835\n",
      "[LOG 20200502-15:00:45] epoch: 17784 train-loss: 0.010542626078757975\n",
      "[LOG 20200502-15:00:45] epoch: 17785 train-loss: 0.010542625561356544\n",
      "[LOG 20200502-15:00:45] epoch: 17786 train-loss: 0.010542624526553683\n",
      "[LOG 20200502-15:00:46] epoch: 17787 train-loss: 0.01054262463003397\n",
      "[LOG 20200502-15:00:46] epoch: 17788 train-loss: 0.01054262411263254\n",
      "[LOG 20200502-15:00:46] epoch: 17789 train-loss: 0.010542624526553683\n",
      "[LOG 20200502-15:00:46] epoch: 17790 train-loss: 0.010542625043955114\n",
      "[LOG 20200502-15:00:47] epoch: 17791 train-loss: 0.010542625250915686\n",
      "[LOG 20200502-15:00:47] epoch: 17792 train-loss: 0.010542626803119978\n",
      "[LOG 20200502-15:00:47] epoch: 17793 train-loss: 0.010542627837922838\n",
      "[LOG 20200502-15:00:47] epoch: 17794 train-loss: 0.010542629493607415\n",
      "[LOG 20200502-15:00:48] epoch: 17795 train-loss: 0.01054263094233142\n",
      "[LOG 20200502-15:00:48] epoch: 17796 train-loss: 0.010542632287575139\n",
      "[LOG 20200502-15:00:48] epoch: 17797 train-loss: 0.010542634253700575\n",
      "[LOG 20200502-15:00:48] epoch: 17798 train-loss: 0.010542636530266868\n",
      "[LOG 20200502-15:00:49] epoch: 17799 train-loss: 0.010542638392912017\n",
      "[LOG 20200502-15:00:49] epoch: 17800 train-loss: 0.010542640048596594\n",
      "[LOG 20200502-15:00:49] epoch: 17801 train-loss: 0.010542642739084031\n",
      "[LOG 20200502-15:00:49] epoch: 17802 train-loss: 0.01054264491217004\n",
      "[LOG 20200502-15:00:50] epoch: 17803 train-loss: 0.010542647499177191\n",
      "[LOG 20200502-15:00:50] epoch: 17804 train-loss: 0.010542649879223771\n",
      "[LOG 20200502-15:00:50] epoch: 17805 train-loss: 0.010542652052309778\n",
      "[LOG 20200502-15:00:50] epoch: 17806 train-loss: 0.010542654432356358\n",
      "[LOG 20200502-15:00:51] epoch: 17807 train-loss: 0.010542656605442366\n",
      "[LOG 20200502-15:00:51] epoch: 17808 train-loss: 0.010542658675048087\n",
      "[LOG 20200502-15:00:51] epoch: 17809 train-loss: 0.010542660330732664\n",
      "[LOG 20200502-15:00:51] epoch: 17810 train-loss: 0.010542661675976383\n",
      "[LOG 20200502-15:00:52] epoch: 17811 train-loss: 0.010542662814259529\n",
      "[LOG 20200502-15:00:52] epoch: 17812 train-loss: 0.01054266436646382\n",
      "[LOG 20200502-15:00:52] epoch: 17813 train-loss: 0.010542664780384965\n",
      "[LOG 20200502-15:00:52] epoch: 17814 train-loss: 0.010542665194306109\n",
      "[LOG 20200502-15:00:53] epoch: 17815 train-loss: 0.010542665090825822\n",
      "[LOG 20200502-15:00:53] epoch: 17816 train-loss: 0.010542664780384965\n",
      "[LOG 20200502-15:00:53] epoch: 17817 train-loss: 0.01054266436646382\n",
      "[LOG 20200502-15:00:53] epoch: 17818 train-loss: 0.010542663021220101\n",
      "[LOG 20200502-15:00:54] epoch: 17819 train-loss: 0.010542662296858098\n",
      "[LOG 20200502-15:00:54] epoch: 17820 train-loss: 0.010542660330732664\n",
      "[LOG 20200502-15:00:54] epoch: 17821 train-loss: 0.010542658157646656\n",
      "[LOG 20200502-15:00:54] epoch: 17822 train-loss: 0.010542655363678932\n",
      "[LOG 20200502-15:00:55] epoch: 17823 train-loss: 0.010542652466230922\n",
      "[LOG 20200502-15:00:55] epoch: 17824 train-loss: 0.010542649051381482\n",
      "[LOG 20200502-15:00:55] epoch: 17825 train-loss: 0.01054264542957147\n",
      "[LOG 20200502-15:00:55] epoch: 17826 train-loss: 0.010542641600800885\n",
      "[LOG 20200502-15:00:56] epoch: 17827 train-loss: 0.01054263673722744\n",
      "[LOG 20200502-15:00:56] epoch: 17828 train-loss: 0.010542632598015998\n",
      "[LOG 20200502-15:00:56] epoch: 17829 train-loss: 0.010542627113560835\n",
      "[LOG 20200502-15:00:56] epoch: 17830 train-loss: 0.010542621836066246\n",
      "[LOG 20200502-15:00:57] epoch: 17831 train-loss: 0.010542616558571657\n",
      "[LOG 20200502-15:00:57] epoch: 17832 train-loss: 0.010542611281077066\n",
      "[LOG 20200502-15:00:57] epoch: 17833 train-loss: 0.01054260517574019\n",
      "[LOG 20200502-15:00:57] epoch: 17834 train-loss: 0.010542600105206171\n",
      "[LOG 20200502-15:00:57] epoch: 17835 train-loss: 0.010542594517270723\n",
      "[LOG 20200502-15:00:58] epoch: 17836 train-loss: 0.010542588929335276\n",
      "[LOG 20200502-15:00:58] epoch: 17837 train-loss: 0.010542583134439256\n",
      "[LOG 20200502-15:00:58] epoch: 17838 train-loss: 0.010542577546503808\n",
      "[LOG 20200502-15:00:58] epoch: 17839 train-loss: 0.010542573096851507\n",
      "[LOG 20200502-15:00:59] epoch: 17840 train-loss: 0.01054256802631749\n",
      "[LOG 20200502-15:00:59] epoch: 17841 train-loss: 0.010542562955783473\n",
      "[LOG 20200502-15:00:59] epoch: 17842 train-loss: 0.010542558920052316\n",
      "[LOG 20200502-15:00:59] epoch: 17843 train-loss: 0.010542555194762018\n",
      "[LOG 20200502-15:01:00] epoch: 17844 train-loss: 0.01054255198687315\n",
      "[LOG 20200502-15:01:00] epoch: 17845 train-loss: 0.010542548675503995\n",
      "[LOG 20200502-15:01:00] epoch: 17846 train-loss: 0.010542545881536271\n",
      "[LOG 20200502-15:01:00] epoch: 17847 train-loss: 0.010542543604969978\n",
      "[LOG 20200502-15:01:01] epoch: 17848 train-loss: 0.010542541535364257\n",
      "[LOG 20200502-15:01:01] epoch: 17849 train-loss: 0.010542540707521968\n",
      "[LOG 20200502-15:01:01] epoch: 17850 train-loss: 0.010542540086640252\n",
      "[LOG 20200502-15:01:01] epoch: 17850 new best train-loss: 0.010542540086640252 found\n",
      "[LOG 20200502-15:01:01] epoch: 17851 train-loss: 0.010542540086640252\n",
      "[LOG 20200502-15:01:02] epoch: 17852 train-loss: 0.010542540707521968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-15:01:02] epoch: 17853 train-loss: 0.010542541638844542\n",
      "[LOG 20200502-15:01:02] epoch: 17854 train-loss: 0.010542542984088263\n",
      "[LOG 20200502-15:01:02] epoch: 17855 train-loss: 0.01054254536413484\n",
      "[LOG 20200502-15:01:03] epoch: 17856 train-loss: 0.010542547847661708\n",
      "[LOG 20200502-15:01:03] epoch: 17857 train-loss: 0.010542550745109716\n",
      "[LOG 20200502-15:01:03] epoch: 17858 train-loss: 0.010542554677360587\n",
      "[LOG 20200502-15:01:03] epoch: 17859 train-loss: 0.010542558402650885\n",
      "[LOG 20200502-15:01:04] epoch: 17860 train-loss: 0.010542563059263758\n",
      "[LOG 20200502-15:01:04] epoch: 17861 train-loss: 0.010542567819356918\n",
      "[LOG 20200502-15:01:04] epoch: 17862 train-loss: 0.010542572889890935\n",
      "[LOG 20200502-15:01:04] epoch: 17863 train-loss: 0.010542578788267242\n",
      "[LOG 20200502-15:01:05] epoch: 17864 train-loss: 0.010542585307525264\n",
      "[LOG 20200502-15:01:05] epoch: 17865 train-loss: 0.010542591412862143\n",
      "[LOG 20200502-15:01:05] epoch: 17866 train-loss: 0.01054259803560045\n",
      "[LOG 20200502-15:01:05] epoch: 17867 train-loss: 0.01054260517574019\n",
      "[LOG 20200502-15:01:06] epoch: 17868 train-loss: 0.010542612833281359\n",
      "[LOG 20200502-15:01:06] epoch: 17869 train-loss: 0.01054262038734224\n",
      "[LOG 20200502-15:01:06] epoch: 17870 train-loss: 0.010542627630962266\n",
      "[LOG 20200502-15:01:06] epoch: 17871 train-loss: 0.010542635805904865\n",
      "[LOG 20200502-15:01:07] epoch: 17872 train-loss: 0.010542643980847465\n",
      "[LOG 20200502-15:01:07] epoch: 17873 train-loss: 0.010542652155790064\n",
      "[LOG 20200502-15:01:07] epoch: 17874 train-loss: 0.010542659916811518\n",
      "[LOG 20200502-15:01:07] epoch: 17875 train-loss: 0.010542668402194977\n",
      "[LOG 20200502-15:01:08] epoch: 17876 train-loss: 0.010542676680617862\n",
      "[LOG 20200502-15:01:08] epoch: 17877 train-loss: 0.010542684855560461\n",
      "[LOG 20200502-15:01:08] epoch: 17878 train-loss: 0.010542692616581917\n",
      "[LOG 20200502-15:01:08] epoch: 17879 train-loss: 0.01054270068804423\n",
      "[LOG 20200502-15:01:08] epoch: 17880 train-loss: 0.01054270938038826\n",
      "[LOG 20200502-15:01:09] epoch: 17881 train-loss: 0.01054271672748857\n",
      "[LOG 20200502-15:01:09] epoch: 17882 train-loss: 0.010542724695470598\n",
      "[LOG 20200502-15:01:09] epoch: 17883 train-loss: 0.010542732249531481\n",
      "[LOG 20200502-15:01:09] epoch: 17884 train-loss: 0.01054273907923036\n",
      "[LOG 20200502-15:01:10] epoch: 17885 train-loss: 0.010542746943732103\n",
      "[LOG 20200502-15:01:10] epoch: 17886 train-loss: 0.01054275387691127\n",
      "[LOG 20200502-15:01:10] epoch: 17887 train-loss: 0.010542760085728433\n",
      "[LOG 20200502-15:01:10] epoch: 17888 train-loss: 0.010542766915427314\n",
      "[LOG 20200502-15:01:11] epoch: 17889 train-loss: 0.01054277281380362\n",
      "[LOG 20200502-15:01:11] epoch: 17890 train-loss: 0.010542778298258781\n",
      "[LOG 20200502-15:01:11] epoch: 17891 train-loss: 0.010542784093154801\n",
      "[LOG 20200502-15:01:11] epoch: 17892 train-loss: 0.010542789474129677\n",
      "[LOG 20200502-15:01:12] epoch: 17893 train-loss: 0.010542794751624266\n",
      "[LOG 20200502-15:01:12] epoch: 17894 train-loss: 0.010542799511717426\n",
      "[LOG 20200502-15:01:12] epoch: 17895 train-loss: 0.010542803547448583\n",
      "[LOG 20200502-15:01:12] epoch: 17896 train-loss: 0.010542807376219166\n",
      "[LOG 20200502-15:01:13] epoch: 17897 train-loss: 0.010542811101509465\n",
      "[LOG 20200502-15:01:13] epoch: 17898 train-loss: 0.010542814309398333\n",
      "[LOG 20200502-15:01:13] epoch: 17899 train-loss: 0.010542818241649203\n",
      "[LOG 20200502-15:01:13] epoch: 17900 train-loss: 0.010542821553018358\n",
      "[LOG 20200502-15:01:14] epoch: 17901 train-loss: 0.010542823622624079\n",
      "[LOG 20200502-15:01:14] epoch: 17902 train-loss: 0.0105428256922298\n",
      "[LOG 20200502-15:01:14] epoch: 17903 train-loss: 0.010542828175756667\n",
      "[LOG 20200502-15:01:14] epoch: 17904 train-loss: 0.010542830245362388\n",
      "[LOG 20200502-15:01:15] epoch: 17905 train-loss: 0.010542831901046965\n",
      "[LOG 20200502-15:01:15] epoch: 17906 train-loss: 0.0105428338671724\n",
      "[LOG 20200502-15:01:15] epoch: 17907 train-loss: 0.010542835419376692\n",
      "[LOG 20200502-15:01:15] epoch: 17908 train-loss: 0.010542836971580982\n",
      "[LOG 20200502-15:01:16] epoch: 17909 train-loss: 0.010542837902903557\n",
      "[LOG 20200502-15:01:16] epoch: 17910 train-loss: 0.01054283966206842\n",
      "[LOG 20200502-15:01:16] epoch: 17911 train-loss: 0.010542841110792425\n",
      "[LOG 20200502-15:01:16] epoch: 17912 train-loss: 0.01054284255951643\n",
      "[LOG 20200502-15:01:17] epoch: 17913 train-loss: 0.01054284411172072\n",
      "[LOG 20200502-15:01:17] epoch: 17914 train-loss: 0.010542845353484154\n",
      "[LOG 20200502-15:01:17] epoch: 17915 train-loss: 0.010542847009168731\n",
      "[LOG 20200502-15:01:17] epoch: 17916 train-loss: 0.010542848975294165\n",
      "[LOG 20200502-15:01:17] epoch: 17917 train-loss: 0.010542851044899888\n",
      "[LOG 20200502-15:01:18] epoch: 17918 train-loss: 0.010542853011025323\n",
      "[LOG 20200502-15:01:18] epoch: 17919 train-loss: 0.010542855287591616\n",
      "[LOG 20200502-15:01:18] epoch: 17920 train-loss: 0.010542857978079054\n",
      "[LOG 20200502-15:01:19] epoch: 17921 train-loss: 0.010542860668566491\n",
      "[LOG 20200502-15:01:19] epoch: 17922 train-loss: 0.010542863669494787\n",
      "[LOG 20200502-15:01:19] epoch: 17923 train-loss: 0.010542867084344229\n",
      "[LOG 20200502-15:01:19] epoch: 17924 train-loss: 0.010542870499193668\n",
      "[LOG 20200502-15:01:20] epoch: 17925 train-loss: 0.010542874534924826\n",
      "[LOG 20200502-15:01:20] epoch: 17926 train-loss: 0.010542878156734837\n",
      "[LOG 20200502-15:01:20] epoch: 17927 train-loss: 0.010542882502906852\n",
      "[LOG 20200502-15:01:20] epoch: 17928 train-loss: 0.010542886952559153\n",
      "[LOG 20200502-15:01:21] epoch: 17929 train-loss: 0.010542891298731169\n",
      "[LOG 20200502-15:01:21] epoch: 17930 train-loss: 0.010542896265784899\n",
      "[LOG 20200502-15:01:21] epoch: 17931 train-loss: 0.010542901750240061\n",
      "[LOG 20200502-15:01:21] epoch: 17932 train-loss: 0.01054290651033322\n",
      "[LOG 20200502-15:01:22] epoch: 17933 train-loss: 0.010542911994788382\n",
      "[LOG 20200502-15:01:22] epoch: 17934 train-loss: 0.010542917168802686\n",
      "[LOG 20200502-15:01:22] epoch: 17935 train-loss: 0.01054292337761985\n",
      "[LOG 20200502-15:01:22] epoch: 17936 train-loss: 0.01054292917251587\n",
      "[LOG 20200502-15:01:22] epoch: 17937 train-loss: 0.010542934967411889\n",
      "[LOG 20200502-15:01:23] epoch: 17938 train-loss: 0.010542940658827623\n",
      "[LOG 20200502-15:01:23] epoch: 17939 train-loss: 0.010542946660684215\n",
      "[LOG 20200502-15:01:23] epoch: 17940 train-loss: 0.010542952455580235\n",
      "[LOG 20200502-15:01:24] epoch: 17941 train-loss: 0.010542958043515682\n",
      "[LOG 20200502-15:01:24] epoch: 17942 train-loss: 0.010542963838411702\n",
      "[LOG 20200502-15:01:24] epoch: 17943 train-loss: 0.01054296942634715\n",
      "[LOG 20200502-15:01:24] epoch: 17944 train-loss: 0.010542975428203741\n",
      "[LOG 20200502-15:01:24] epoch: 17945 train-loss: 0.01054298049873776\n",
      "[LOG 20200502-15:01:25] epoch: 17946 train-loss: 0.010542985362311205\n",
      "[LOG 20200502-15:01:25] epoch: 17947 train-loss: 0.010542990536325507\n",
      "[LOG 20200502-15:01:25] epoch: 17948 train-loss: 0.01054299519293838\n",
      "[LOG 20200502-15:01:25] epoch: 17949 train-loss: 0.010542999228669537\n",
      "[LOG 20200502-15:01:26] epoch: 17950 train-loss: 0.010543003988762697\n",
      "[LOG 20200502-15:01:26] epoch: 17951 train-loss: 0.010543007610572709\n",
      "[LOG 20200502-15:01:26] epoch: 17952 train-loss: 0.010543011335863007\n",
      "[LOG 20200502-15:01:26] epoch: 17953 train-loss: 0.010543014026350446\n",
      "[LOG 20200502-15:01:27] epoch: 17954 train-loss: 0.010543016716837883\n",
      "[LOG 20200502-15:01:27] epoch: 17955 train-loss: 0.010543018786443604\n",
      "[LOG 20200502-15:01:27] epoch: 17956 train-loss: 0.010543020545608468\n",
      "[LOG 20200502-15:01:27] epoch: 17957 train-loss: 0.010543021580411328\n",
      "[LOG 20200502-15:01:28] epoch: 17958 train-loss: 0.010543022718694475\n",
      "[LOG 20200502-15:01:28] epoch: 17959 train-loss: 0.010543023546536764\n",
      "[LOG 20200502-15:01:28] epoch: 17960 train-loss: 0.010543023236095905\n",
      "[LOG 20200502-15:01:28] epoch: 17961 train-loss: 0.010543023753497336\n",
      "[LOG 20200502-15:01:29] epoch: 17962 train-loss: 0.01054302261521419\n",
      "[LOG 20200502-15:01:29] epoch: 17963 train-loss: 0.010543022201293044\n",
      "[LOG 20200502-15:01:29] epoch: 17964 train-loss: 0.01054302075256904\n",
      "[LOG 20200502-15:01:29] epoch: 17965 train-loss: 0.010543018476002745\n",
      "[LOG 20200502-15:01:30] epoch: 17966 train-loss: 0.010543017441199886\n",
      "[LOG 20200502-15:01:30] epoch: 17967 train-loss: 0.010543015268113878\n",
      "[LOG 20200502-15:01:30] epoch: 17968 train-loss: 0.010543013715909587\n",
      "[LOG 20200502-15:01:30] epoch: 17969 train-loss: 0.010543011232382722\n",
      "[LOG 20200502-15:01:31] epoch: 17970 train-loss: 0.010543009059296714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-15:01:31] epoch: 17971 train-loss: 0.010543006368809275\n",
      "[LOG 20200502-15:01:31] epoch: 17972 train-loss: 0.010543003988762697\n",
      "[LOG 20200502-15:01:31] epoch: 17973 train-loss: 0.01054300181567669\n",
      "[LOG 20200502-15:01:32] epoch: 17974 train-loss: 0.010542999746070968\n",
      "[LOG 20200502-15:01:32] epoch: 17975 train-loss: 0.010542997779945532\n",
      "[LOG 20200502-15:01:32] epoch: 17976 train-loss: 0.01054299602078067\n",
      "[LOG 20200502-15:01:32] epoch: 17977 train-loss: 0.010542994468576379\n",
      "[LOG 20200502-15:01:33] epoch: 17978 train-loss: 0.010542993123332659\n",
      "[LOG 20200502-15:01:33] epoch: 17979 train-loss: 0.010542992502450943\n",
      "[LOG 20200502-15:01:33] epoch: 17980 train-loss: 0.010542991571128368\n",
      "[LOG 20200502-15:01:33] epoch: 17981 train-loss: 0.01054299177808894\n",
      "[LOG 20200502-15:01:34] epoch: 17982 train-loss: 0.010542992295490371\n",
      "[LOG 20200502-15:01:34] epoch: 17983 train-loss: 0.01054299333029323\n",
      "[LOG 20200502-15:01:34] epoch: 17984 train-loss: 0.010542994365096092\n",
      "[LOG 20200502-15:01:34] epoch: 17985 train-loss: 0.010542996848622957\n",
      "[LOG 20200502-15:01:34] epoch: 17986 train-loss: 0.010542999021708965\n",
      "[LOG 20200502-15:01:35] epoch: 17987 train-loss: 0.010543001401755545\n",
      "[LOG 20200502-15:01:35] epoch: 17988 train-loss: 0.0105430054374867\n",
      "[LOG 20200502-15:01:35] epoch: 17989 train-loss: 0.010543009162777\n",
      "[LOG 20200502-15:01:35] epoch: 17990 train-loss: 0.01054301444027159\n",
      "[LOG 20200502-15:01:36] epoch: 17991 train-loss: 0.010543018889923891\n",
      "[LOG 20200502-15:01:36] epoch: 17992 train-loss: 0.010543024788300196\n",
      "[LOG 20200502-15:01:36] epoch: 17993 train-loss: 0.010543031100597646\n",
      "[LOG 20200502-15:01:36] epoch: 17994 train-loss: 0.010543037930296527\n",
      "[LOG 20200502-15:01:37] epoch: 17995 train-loss: 0.010543045277396837\n",
      "[LOG 20200502-15:01:37] epoch: 17996 train-loss: 0.010543052727977434\n",
      "[LOG 20200502-15:01:37] epoch: 17997 train-loss: 0.010543060592479177\n",
      "[LOG 20200502-15:01:37] epoch: 17998 train-loss: 0.010543069284823205\n",
      "[LOG 20200502-15:01:38] epoch: 17999 train-loss: 0.010543078494568666\n",
      "[LOG 20200502-15:01:38] epoch: 18000 train-loss: 0.010543087497353554\n",
      "[LOG 20200502-15:01:38] epoch: 18001 train-loss: 0.01054309732798073\n",
      "[LOG 20200502-15:01:38] epoch: 18002 train-loss: 0.010543107469048765\n",
      "[LOG 20200502-15:01:39] epoch: 18003 train-loss: 0.0105431176101168\n",
      "[LOG 20200502-15:01:39] epoch: 18004 train-loss: 0.010543128268586265\n",
      "[LOG 20200502-15:01:39] epoch: 18005 train-loss: 0.01054313892705573\n",
      "[LOG 20200502-15:01:39] epoch: 18006 train-loss: 0.010543150309887197\n",
      "[LOG 20200502-15:01:40] epoch: 18007 train-loss: 0.010543160554435518\n",
      "[LOG 20200502-15:01:40] epoch: 18008 train-loss: 0.01054317286858956\n",
      "[LOG 20200502-15:01:40] epoch: 18009 train-loss: 0.01054318342357874\n",
      "[LOG 20200502-15:01:40] epoch: 18010 train-loss: 0.010543195634252496\n",
      "[LOG 20200502-15:01:40] epoch: 18011 train-loss: 0.010543206603162818\n",
      "[LOG 20200502-15:01:41] epoch: 18012 train-loss: 0.010543218089474572\n",
      "[LOG 20200502-15:01:41] epoch: 18013 train-loss: 0.01054322947230604\n",
      "[LOG 20200502-15:01:41] epoch: 18014 train-loss: 0.010543241269058652\n",
      "[LOG 20200502-15:01:41] epoch: 18015 train-loss: 0.010543252444929548\n",
      "[LOG 20200502-15:01:42] epoch: 18016 train-loss: 0.010543263724280728\n",
      "[LOG 20200502-15:01:42] epoch: 18017 train-loss: 0.010543274693191051\n",
      "[LOG 20200502-15:01:42] epoch: 18018 train-loss: 0.01054328576558166\n",
      "[LOG 20200502-15:01:42] epoch: 18019 train-loss: 0.010543297251893414\n",
      "[LOG 20200502-15:01:43] epoch: 18020 train-loss: 0.01054330739296145\n",
      "[LOG 20200502-15:01:43] epoch: 18021 train-loss: 0.010543317430549197\n",
      "[LOG 20200502-15:01:43] epoch: 18022 train-loss: 0.01054332736465666\n",
      "[LOG 20200502-15:01:43] epoch: 18023 train-loss: 0.010543337609204981\n",
      "[LOG 20200502-15:01:44] epoch: 18024 train-loss: 0.010543347025911013\n",
      "[LOG 20200502-15:01:44] epoch: 18025 train-loss: 0.010543356132176187\n",
      "[LOG 20200502-15:01:44] epoch: 18026 train-loss: 0.010543365445401933\n",
      "[LOG 20200502-15:01:44] epoch: 18027 train-loss: 0.01054337372382482\n",
      "[LOG 20200502-15:01:45] epoch: 18028 train-loss: 0.010543382105727991\n",
      "[LOG 20200502-15:01:45] epoch: 18029 train-loss: 0.01054339028067059\n",
      "[LOG 20200502-15:01:45] epoch: 18030 train-loss: 0.010543397524290614\n",
      "[LOG 20200502-15:01:45] epoch: 18031 train-loss: 0.010543405181831784\n",
      "[LOG 20200502-15:01:45] epoch: 18032 train-loss: 0.010543412425451808\n",
      "[LOG 20200502-15:01:46] epoch: 18033 train-loss: 0.01054341894470983\n",
      "[LOG 20200502-15:01:46] epoch: 18034 train-loss: 0.01054342525700728\n",
      "[LOG 20200502-15:01:46] epoch: 18035 train-loss: 0.010543431465824446\n",
      "[LOG 20200502-15:01:47] epoch: 18036 train-loss: 0.010543437467681037\n",
      "[LOG 20200502-15:01:47] epoch: 18037 train-loss: 0.01054344315909677\n",
      "[LOG 20200502-15:01:47] epoch: 18038 train-loss: 0.010543447712229358\n",
      "[LOG 20200502-15:01:47] epoch: 18039 train-loss: 0.010543453196684519\n",
      "[LOG 20200502-15:01:47] epoch: 18040 train-loss: 0.01054345816373825\n",
      "[LOG 20200502-15:01:48] epoch: 18041 train-loss: 0.010543463130791983\n",
      "[LOG 20200502-15:01:48] epoch: 18042 train-loss: 0.010543467787404856\n",
      "[LOG 20200502-15:01:48] epoch: 18043 train-loss: 0.010543471926616298\n",
      "[LOG 20200502-15:01:48] epoch: 18044 train-loss: 0.010543476583229171\n",
      "[LOG 20200502-15:01:49] epoch: 18045 train-loss: 0.010543481136361757\n",
      "[LOG 20200502-15:01:49] epoch: 18046 train-loss: 0.010543485689494345\n",
      "[LOG 20200502-15:01:49] epoch: 18047 train-loss: 0.0105434897252255\n",
      "[LOG 20200502-15:01:49] epoch: 18048 train-loss: 0.010543494071397517\n",
      "[LOG 20200502-15:01:50] epoch: 18049 train-loss: 0.010543498417569531\n",
      "[LOG 20200502-15:01:50] epoch: 18050 train-loss: 0.01054350266026126\n",
      "[LOG 20200502-15:01:50] epoch: 18051 train-loss: 0.010543507834275564\n",
      "[LOG 20200502-15:01:50] epoch: 18052 train-loss: 0.010543512594368722\n",
      "[LOG 20200502-15:01:51] epoch: 18053 train-loss: 0.010543517354461882\n",
      "[LOG 20200502-15:01:51] epoch: 18054 train-loss: 0.010543522838917043\n",
      "[LOG 20200502-15:01:51] epoch: 18055 train-loss: 0.010543527599010203\n",
      "[LOG 20200502-15:01:51] epoch: 18056 train-loss: 0.010543533290425936\n",
      "[LOG 20200502-15:01:52] epoch: 18057 train-loss: 0.010543539292282529\n",
      "[LOG 20200502-15:01:52] epoch: 18058 train-loss: 0.010543544362816546\n",
      "[LOG 20200502-15:01:52] epoch: 18059 train-loss: 0.010543550882074568\n",
      "[LOG 20200502-15:01:52] epoch: 18060 train-loss: 0.010543557815253735\n",
      "[LOG 20200502-15:01:53] epoch: 18061 train-loss: 0.01054356423103147\n",
      "[LOG 20200502-15:01:53] epoch: 18062 train-loss: 0.010543571578131782\n",
      "[LOG 20200502-15:01:53] epoch: 18063 train-loss: 0.010543578097389804\n",
      "[LOG 20200502-15:01:53] epoch: 18064 train-loss: 0.010543585547970401\n",
      "[LOG 20200502-15:01:54] epoch: 18065 train-loss: 0.010543592895070711\n",
      "[LOG 20200502-15:01:54] epoch: 18066 train-loss: 0.010543600759572454\n",
      "[LOG 20200502-15:01:54] epoch: 18067 train-loss: 0.010543609037995338\n",
      "[LOG 20200502-15:01:54] epoch: 18068 train-loss: 0.010543617730339369\n",
      "[LOG 20200502-15:01:55] epoch: 18069 train-loss: 0.010543625180919966\n",
      "[LOG 20200502-15:01:55] epoch: 18070 train-loss: 0.010543633562823137\n",
      "[LOG 20200502-15:01:55] epoch: 18071 train-loss: 0.010543642772568597\n",
      "[LOG 20200502-15:01:55] epoch: 18072 train-loss: 0.01054365084403091\n",
      "[LOG 20200502-15:01:56] epoch: 18073 train-loss: 0.01054365953637494\n",
      "[LOG 20200502-15:01:56] epoch: 18074 train-loss: 0.010543667918278111\n",
      "[LOG 20200502-15:01:56] epoch: 18075 train-loss: 0.010543677024543285\n",
      "[LOG 20200502-15:01:56] epoch: 18076 train-loss: 0.010543685509926744\n",
      "[LOG 20200502-15:01:57] epoch: 18077 train-loss: 0.0105436939953102\n",
      "[LOG 20200502-15:01:57] epoch: 18078 train-loss: 0.010543702480693659\n",
      "[LOG 20200502-15:01:57] epoch: 18079 train-loss: 0.010543710966077115\n",
      "[LOG 20200502-15:01:57] epoch: 18080 train-loss: 0.010543719141019715\n",
      "[LOG 20200502-15:01:58] epoch: 18081 train-loss: 0.01054372690204117\n",
      "[LOG 20200502-15:01:58] epoch: 18082 train-loss: 0.01054373455958234\n",
      "[LOG 20200502-15:01:58] epoch: 18083 train-loss: 0.01054374190668265\n",
      "[LOG 20200502-15:01:58] epoch: 18084 train-loss: 0.010543749253782961\n",
      "[LOG 20200502-15:01:59] epoch: 18085 train-loss: 0.01054375691132413\n",
      "[LOG 20200502-15:01:59] epoch: 18086 train-loss: 0.010543762499259578\n",
      "[LOG 20200502-15:01:59] epoch: 18087 train-loss: 0.01054376850111617\n",
      "[LOG 20200502-15:01:59] epoch: 18088 train-loss: 0.01054377429601219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-15:02:00] epoch: 18089 train-loss: 0.010543779883947637\n",
      "[LOG 20200502-15:02:00] epoch: 18090 train-loss: 0.010543784851001369\n",
      "[LOG 20200502-15:02:00] epoch: 18091 train-loss: 0.010543789507614242\n",
      "[LOG 20200502-15:02:00] epoch: 18092 train-loss: 0.010543793853786256\n",
      "[LOG 20200502-15:02:01] epoch: 18093 train-loss: 0.010543796854714552\n",
      "[LOG 20200502-15:02:01] epoch: 18094 train-loss: 0.010543800269563993\n",
      "[LOG 20200502-15:02:01] epoch: 18095 train-loss: 0.010543803580933146\n",
      "[LOG 20200502-15:02:01] epoch: 18096 train-loss: 0.010543806064460013\n",
      "[LOG 20200502-15:02:02] epoch: 18097 train-loss: 0.010543808341026306\n",
      "[LOG 20200502-15:02:02] epoch: 18098 train-loss: 0.010543809479309453\n",
      "[LOG 20200502-15:02:02] epoch: 18099 train-loss: 0.010543811238474317\n",
      "[LOG 20200502-15:02:02] epoch: 18100 train-loss: 0.01054381165239546\n",
      "[LOG 20200502-15:02:03] epoch: 18101 train-loss: 0.010543813204599751\n",
      "[LOG 20200502-15:02:03] epoch: 18102 train-loss: 0.010543813411560323\n",
      "[LOG 20200502-15:02:03] epoch: 18103 train-loss: 0.010543813722001182\n",
      "[LOG 20200502-15:02:03] epoch: 18104 train-loss: 0.010543813928961754\n",
      "[LOG 20200502-15:02:04] epoch: 18105 train-loss: 0.01054381454984347\n",
      "[LOG 20200502-15:02:04] epoch: 18106 train-loss: 0.01054381454984347\n",
      "[LOG 20200502-15:02:04] epoch: 18107 train-loss: 0.010543814446363185\n",
      "[LOG 20200502-15:02:04] epoch: 18108 train-loss: 0.010543814135922326\n",
      "[LOG 20200502-15:02:05] epoch: 18109 train-loss: 0.010543814756804042\n",
      "[LOG 20200502-15:02:05] epoch: 18110 train-loss: 0.010543815170725187\n",
      "[LOG 20200502-15:02:05] epoch: 18111 train-loss: 0.010543815688126616\n",
      "[LOG 20200502-15:02:05] epoch: 18112 train-loss: 0.010543816205528047\n",
      "[LOG 20200502-15:02:06] epoch: 18113 train-loss: 0.01054381744729148\n",
      "[LOG 20200502-15:02:06] epoch: 18114 train-loss: 0.010543818171653483\n",
      "[LOG 20200502-15:02:06] epoch: 18115 train-loss: 0.010543820448219776\n",
      "[LOG 20200502-15:02:06] epoch: 18116 train-loss: 0.010543822310864925\n",
      "[LOG 20200502-15:02:06] epoch: 18117 train-loss: 0.010543825001352362\n",
      "[LOG 20200502-15:02:07] epoch: 18118 train-loss: 0.010543828416201804\n",
      "[LOG 20200502-15:02:07] epoch: 18119 train-loss: 0.01054383193453153\n",
      "[LOG 20200502-15:02:07] epoch: 18120 train-loss: 0.010543836073742973\n",
      "[LOG 20200502-15:02:08] epoch: 18121 train-loss: 0.010543840626875559\n",
      "[LOG 20200502-15:02:08] epoch: 18122 train-loss: 0.01054384559392929\n",
      "[LOG 20200502-15:02:08] epoch: 18123 train-loss: 0.010543851595785882\n",
      "[LOG 20200502-15:02:08] epoch: 18124 train-loss: 0.010543857390681902\n",
      "[LOG 20200502-15:02:08] epoch: 18125 train-loss: 0.01054386380645964\n",
      "[LOG 20200502-15:02:09] epoch: 18126 train-loss: 0.01054387167096138\n",
      "[LOG 20200502-15:02:09] epoch: 18127 train-loss: 0.010543879638943408\n",
      "[LOG 20200502-15:02:09] epoch: 18128 train-loss: 0.010543887503445148\n",
      "[LOG 20200502-15:02:10] epoch: 18129 train-loss: 0.010543896713190608\n",
      "[LOG 20200502-15:02:10] epoch: 18130 train-loss: 0.010543905715975497\n",
      "[LOG 20200502-15:02:10] epoch: 18131 train-loss: 0.010543915753563246\n",
      "[LOG 20200502-15:02:10] epoch: 18132 train-loss: 0.01054392589463128\n",
      "[LOG 20200502-15:02:10] epoch: 18133 train-loss: 0.010543936760061316\n",
      "[LOG 20200502-15:02:11] epoch: 18134 train-loss: 0.010543948142892785\n",
      "[LOG 20200502-15:02:11] epoch: 18135 train-loss: 0.010543959939645397\n",
      "[LOG 20200502-15:02:11] epoch: 18136 train-loss: 0.010543971736398008\n",
      "[LOG 20200502-15:02:11] epoch: 18137 train-loss: 0.01054398353315062\n",
      "[LOG 20200502-15:02:12] epoch: 18138 train-loss: 0.010543996468186378\n",
      "[LOG 20200502-15:02:12] epoch: 18139 train-loss: 0.010544009196261564\n",
      "[LOG 20200502-15:02:12] epoch: 18140 train-loss: 0.010544022648698755\n",
      "[LOG 20200502-15:02:12] epoch: 18141 train-loss: 0.010544035894175371\n",
      "[LOG 20200502-15:02:13] epoch: 18142 train-loss: 0.010544049139651988\n",
      "[LOG 20200502-15:02:13] epoch: 18143 train-loss: 0.010544063109490607\n",
      "[LOG 20200502-15:02:13] epoch: 18144 train-loss: 0.010544076561927795\n",
      "[LOG 20200502-15:02:13] epoch: 18145 train-loss: 0.010544089807404412\n",
      "[LOG 20200502-15:02:14] epoch: 18146 train-loss: 0.010544103880723318\n",
      "[LOG 20200502-15:02:14] epoch: 18147 train-loss: 0.010544117643601365\n",
      "[LOG 20200502-15:02:14] epoch: 18148 train-loss: 0.010544131820400557\n",
      "[LOG 20200502-15:02:14] epoch: 18149 train-loss: 0.010544145479798317\n",
      "[LOG 20200502-15:02:15] epoch: 18150 train-loss: 0.010544159242676364\n",
      "[LOG 20200502-15:02:15] epoch: 18151 train-loss: 0.010544172695113553\n",
      "[LOG 20200502-15:02:15] epoch: 18152 train-loss: 0.010544185837109884\n",
      "[LOG 20200502-15:02:15] epoch: 18153 train-loss: 0.010544199186066786\n",
      "[LOG 20200502-15:02:16] epoch: 18154 train-loss: 0.010544212741984261\n",
      "[LOG 20200502-15:02:16] epoch: 18155 train-loss: 0.010544225366579162\n",
      "[LOG 20200502-15:02:16] epoch: 18156 train-loss: 0.01054423778421349\n",
      "[LOG 20200502-15:02:16] epoch: 18157 train-loss: 0.010544250305328105\n",
      "[LOG 20200502-15:02:16] epoch: 18158 train-loss: 0.010544262102080716\n",
      "[LOG 20200502-15:02:17] epoch: 18159 train-loss: 0.010544274312754473\n",
      "[LOG 20200502-15:02:17] epoch: 18160 train-loss: 0.010544285592105653\n",
      "[LOG 20200502-15:02:17] epoch: 18161 train-loss: 0.010544296664496263\n",
      "[LOG 20200502-15:02:18] epoch: 18162 train-loss: 0.010544307426446013\n",
      "[LOG 20200502-15:02:18] epoch: 18163 train-loss: 0.010544318602316909\n",
      "[LOG 20200502-15:02:18] epoch: 18164 train-loss: 0.0105443283294638\n",
      "[LOG 20200502-15:02:18] epoch: 18165 train-loss: 0.01054433805661069\n",
      "[LOG 20200502-15:02:18] epoch: 18166 train-loss: 0.010544347576797009\n",
      "[LOG 20200502-15:02:19] epoch: 18167 train-loss: 0.010544357096983327\n",
      "[LOG 20200502-15:02:19] epoch: 18168 train-loss: 0.010544365478886498\n",
      "[LOG 20200502-15:02:19] epoch: 18169 train-loss: 0.010544373964269957\n",
      "[LOG 20200502-15:02:20] epoch: 18170 train-loss: 0.010544381828771697\n",
      "[LOG 20200502-15:02:20] epoch: 18171 train-loss: 0.01054438990023401\n",
      "[LOG 20200502-15:02:20] epoch: 18172 train-loss: 0.010544397350814607\n",
      "[LOG 20200502-15:02:20] epoch: 18173 train-loss: 0.010544404801395204\n",
      "[LOG 20200502-15:02:21] epoch: 18174 train-loss: 0.010544411941534944\n",
      "[LOG 20200502-15:02:21] epoch: 18175 train-loss: 0.01054441939211554\n",
      "[LOG 20200502-15:02:21] epoch: 18176 train-loss: 0.010544425807893276\n",
      "[LOG 20200502-15:02:21] epoch: 18177 train-loss: 0.010544432637592157\n",
      "[LOG 20200502-15:02:22] epoch: 18178 train-loss: 0.010544439053369893\n",
      "[LOG 20200502-15:02:22] epoch: 18179 train-loss: 0.010544445365667343\n",
      "[LOG 20200502-15:02:22] epoch: 18180 train-loss: 0.01054445126404365\n",
      "[LOG 20200502-15:02:22] epoch: 18181 train-loss: 0.010544457679821385\n",
      "[LOG 20200502-15:02:23] epoch: 18182 train-loss: 0.010544463578197692\n",
      "[LOG 20200502-15:02:23] epoch: 18183 train-loss: 0.010544470511376858\n",
      "[LOG 20200502-15:02:23] epoch: 18184 train-loss: 0.010544476720194021\n",
      "[LOG 20200502-15:02:23] epoch: 18185 train-loss: 0.010544482515090041\n",
      "[LOG 20200502-15:02:24] epoch: 18186 train-loss: 0.010544489344788922\n",
      "[LOG 20200502-15:02:24] epoch: 18187 train-loss: 0.010544495864046944\n",
      "[LOG 20200502-15:02:24] epoch: 18188 train-loss: 0.010544503004186682\n",
      "[LOG 20200502-15:02:24] epoch: 18189 train-loss: 0.010544510247806707\n",
      "[LOG 20200502-15:02:25] epoch: 18190 train-loss: 0.010544517077505589\n",
      "[LOG 20200502-15:02:25] epoch: 18191 train-loss: 0.010544524528086185\n",
      "[LOG 20200502-15:02:25] epoch: 18192 train-loss: 0.010544532392587926\n",
      "[LOG 20200502-15:02:25] epoch: 18193 train-loss: 0.010544540153609382\n",
      "[LOG 20200502-15:02:26] epoch: 18194 train-loss: 0.010544548328551982\n",
      "[LOG 20200502-15:02:26] epoch: 18195 train-loss: 0.010544556606974866\n",
      "[LOG 20200502-15:02:26] epoch: 18196 train-loss: 0.010544565092358325\n",
      "[LOG 20200502-15:02:26] epoch: 18197 train-loss: 0.01054457440558407\n",
      "[LOG 20200502-15:02:27] epoch: 18198 train-loss: 0.010544583304888673\n",
      "[LOG 20200502-15:02:27] epoch: 18199 train-loss: 0.01054459179027213\n",
      "[LOG 20200502-15:02:27] epoch: 18200 train-loss: 0.010544601931340165\n",
      "[LOG 20200502-15:02:27] epoch: 18201 train-loss: 0.010544611968927912\n",
      "[LOG 20200502-15:02:28] epoch: 18202 train-loss: 0.010544621696074804\n",
      "[LOG 20200502-15:02:28] epoch: 18203 train-loss: 0.010544631319741407\n",
      "[LOG 20200502-15:02:28] epoch: 18204 train-loss: 0.010544641253848871\n",
      "[LOG 20200502-15:02:28] epoch: 18205 train-loss: 0.010544651912318336\n",
      "[LOG 20200502-15:02:29] epoch: 18206 train-loss: 0.010544662260346942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-15:02:29] epoch: 18207 train-loss: 0.010544672090974119\n",
      "[LOG 20200502-15:02:29] epoch: 18208 train-loss: 0.010544683059884442\n",
      "[LOG 20200502-15:02:29] epoch: 18209 train-loss: 0.01054469340791305\n",
      "[LOG 20200502-15:02:30] epoch: 18210 train-loss: 0.010544703859421942\n",
      "[LOG 20200502-15:02:30] epoch: 18211 train-loss: 0.01054471441441112\n",
      "[LOG 20200502-15:02:30] epoch: 18212 train-loss: 0.010544724762439728\n",
      "[LOG 20200502-15:02:30] epoch: 18213 train-loss: 0.010544734903507762\n",
      "[LOG 20200502-15:02:30] epoch: 18214 train-loss: 0.010544744734134939\n",
      "[LOG 20200502-15:02:31] epoch: 18215 train-loss: 0.010544754357801544\n",
      "[LOG 20200502-15:02:31] epoch: 18216 train-loss: 0.01054476418842872\n",
      "[LOG 20200502-15:02:31] epoch: 18217 train-loss: 0.010544773087733321\n",
      "[LOG 20200502-15:02:32] epoch: 18218 train-loss: 0.010544782504439354\n",
      "[LOG 20200502-15:02:32] epoch: 18219 train-loss: 0.010544790989822812\n",
      "[LOG 20200502-15:02:32] epoch: 18220 train-loss: 0.010544799371725984\n",
      "[LOG 20200502-15:02:32] epoch: 18221 train-loss: 0.010544807132747438\n",
      "[LOG 20200502-15:02:33] epoch: 18222 train-loss: 0.010544814790288607\n",
      "[LOG 20200502-15:02:33] epoch: 18223 train-loss: 0.01054482182694806\n",
      "[LOG 20200502-15:02:33] epoch: 18224 train-loss: 0.010544828967087798\n",
      "[LOG 20200502-15:02:33] epoch: 18225 train-loss: 0.010544835072424676\n",
      "[LOG 20200502-15:02:34] epoch: 18226 train-loss: 0.010544840970800983\n",
      "[LOG 20200502-15:02:34] epoch: 18227 train-loss: 0.010544846662216716\n",
      "[LOG 20200502-15:02:34] epoch: 18228 train-loss: 0.01054485080142816\n",
      "[LOG 20200502-15:02:34] epoch: 18229 train-loss: 0.01054485576848189\n",
      "[LOG 20200502-15:02:35] epoch: 18230 train-loss: 0.010544859804213047\n",
      "[LOG 20200502-15:02:35] epoch: 18231 train-loss: 0.010544864046904776\n",
      "[LOG 20200502-15:02:35] epoch: 18232 train-loss: 0.010544866737392213\n",
      "[LOG 20200502-15:02:35] epoch: 18233 train-loss: 0.010544869841800796\n",
      "[LOG 20200502-15:02:36] epoch: 18234 train-loss: 0.010544872428807948\n",
      "[LOG 20200502-15:02:36] epoch: 18235 train-loss: 0.010544874912334813\n",
      "[LOG 20200502-15:02:36] epoch: 18236 train-loss: 0.010544876878460249\n",
      "[LOG 20200502-15:02:36] epoch: 18237 train-loss: 0.010544879155026542\n",
      "[LOG 20200502-15:02:37] epoch: 18238 train-loss: 0.010544880707230832\n",
      "[LOG 20200502-15:02:37] epoch: 18239 train-loss: 0.010544882466395697\n",
      "[LOG 20200502-15:02:37] epoch: 18240 train-loss: 0.010544883811639415\n",
      "[LOG 20200502-15:02:37] epoch: 18241 train-loss: 0.010544884949922562\n",
      "[LOG 20200502-15:02:38] epoch: 18242 train-loss: 0.010544885674284564\n",
      "[LOG 20200502-15:02:38] epoch: 18243 train-loss: 0.010544887433449427\n",
      "[LOG 20200502-15:02:38] epoch: 18244 train-loss: 0.010544888571732573\n",
      "[LOG 20200502-15:02:38] epoch: 18245 train-loss: 0.01054489022741715\n",
      "[LOG 20200502-15:02:39] epoch: 18246 train-loss: 0.010544892297022872\n",
      "[LOG 20200502-15:02:39] epoch: 18247 train-loss: 0.010544894677069452\n",
      "[LOG 20200502-15:02:39] epoch: 18248 train-loss: 0.010544896229273744\n",
      "[LOG 20200502-15:02:39] epoch: 18249 train-loss: 0.010544899437162612\n",
      "[LOG 20200502-15:02:39] epoch: 18250 train-loss: 0.01054490233461062\n",
      "[LOG 20200502-15:02:40] epoch: 18251 train-loss: 0.010544905645979775\n",
      "[LOG 20200502-15:02:40] epoch: 18252 train-loss: 0.01054490999215179\n",
      "[LOG 20200502-15:02:40] epoch: 18253 train-loss: 0.010544914545284377\n",
      "[LOG 20200502-15:02:41] epoch: 18254 train-loss: 0.010544919408857822\n",
      "[LOG 20200502-15:02:41] epoch: 18255 train-loss: 0.010544924582872126\n",
      "[LOG 20200502-15:02:41] epoch: 18256 train-loss: 0.01054493079168929\n",
      "[LOG 20200502-15:02:41] epoch: 18257 train-loss: 0.010544937517907884\n",
      "[LOG 20200502-15:02:42] epoch: 18258 train-loss: 0.010544944244126478\n",
      "[LOG 20200502-15:02:42] epoch: 18259 train-loss: 0.01054495159122679\n",
      "[LOG 20200502-15:02:42] epoch: 18260 train-loss: 0.010544959662689103\n",
      "[LOG 20200502-15:02:42] epoch: 18261 train-loss: 0.010544968561993705\n",
      "[LOG 20200502-15:02:43] epoch: 18262 train-loss: 0.010544977771739164\n",
      "[LOG 20200502-15:02:43] epoch: 18263 train-loss: 0.010544987705846628\n",
      "[LOG 20200502-15:02:43] epoch: 18264 train-loss: 0.010544998260835806\n",
      "[LOG 20200502-15:02:43] epoch: 18265 train-loss: 0.010545008505384127\n",
      "[LOG 20200502-15:02:44] epoch: 18266 train-loss: 0.01054502030213674\n",
      "[LOG 20200502-15:02:44] epoch: 18267 train-loss: 0.010545032098889351\n",
      "[LOG 20200502-15:02:44] epoch: 18268 train-loss: 0.010545044102602534\n",
      "[LOG 20200502-15:02:44] epoch: 18269 train-loss: 0.01054505662371715\n",
      "[LOG 20200502-15:02:45] epoch: 18270 train-loss: 0.01054506976571348\n",
      "[LOG 20200502-15:02:45] epoch: 18271 train-loss: 0.010545083011190096\n",
      "[LOG 20200502-15:02:45] epoch: 18272 train-loss: 0.010545095739265284\n",
      "[LOG 20200502-15:02:45] epoch: 18273 train-loss: 0.010545110123025047\n",
      "[LOG 20200502-15:02:46] epoch: 18274 train-loss: 0.010545124092863666\n",
      "[LOG 20200502-15:02:46] epoch: 18275 train-loss: 0.010545138062702285\n",
      "[LOG 20200502-15:02:46] epoch: 18276 train-loss: 0.010545153067343764\n",
      "[LOG 20200502-15:02:46] epoch: 18277 train-loss: 0.0105451676580641\n",
      "[LOG 20200502-15:02:47] epoch: 18278 train-loss: 0.010545182352264723\n",
      "[LOG 20200502-15:02:47] epoch: 18279 train-loss: 0.010545197253425917\n",
      "[LOG 20200502-15:02:47] epoch: 18280 train-loss: 0.010545212361547682\n",
      "[LOG 20200502-15:02:47] epoch: 18281 train-loss: 0.010545226745307446\n",
      "[LOG 20200502-15:02:48] epoch: 18282 train-loss: 0.01054524216387007\n",
      "[LOG 20200502-15:02:48] epoch: 18283 train-loss: 0.010545257065031264\n",
      "[LOG 20200502-15:02:48] epoch: 18284 train-loss: 0.010545271448791027\n",
      "[LOG 20200502-15:02:48] epoch: 18285 train-loss: 0.010545286142991649\n",
      "[LOG 20200502-15:02:48] epoch: 18286 train-loss: 0.01054530031979084\n",
      "[LOG 20200502-15:02:49] epoch: 18287 train-loss: 0.010545314910511175\n",
      "[LOG 20200502-15:02:49] epoch: 18288 train-loss: 0.010545329087310366\n",
      "[LOG 20200502-15:02:49] epoch: 18289 train-loss: 0.010545343057148986\n",
      "[LOG 20200502-15:02:49] epoch: 18290 train-loss: 0.01054535692350732\n",
      "[LOG 20200502-15:02:50] epoch: 18291 train-loss: 0.010545370479424795\n",
      "[LOG 20200502-15:02:50] epoch: 18292 train-loss: 0.010545383104019694\n",
      "[LOG 20200502-15:02:50] epoch: 18293 train-loss: 0.010545395832094882\n",
      "[LOG 20200502-15:02:50] epoch: 18294 train-loss: 0.010545408870610926\n",
      "[LOG 20200502-15:02:51] epoch: 18295 train-loss: 0.01054542087432411\n",
      "[LOG 20200502-15:02:51] epoch: 18296 train-loss: 0.01054543298151758\n",
      "[LOG 20200502-15:02:51] epoch: 18297 train-loss: 0.010545444260868762\n",
      "[LOG 20200502-15:02:51] epoch: 18298 train-loss: 0.010545455022818513\n",
      "[LOG 20200502-15:02:52] epoch: 18299 train-loss: 0.010545466612610552\n",
      "[LOG 20200502-15:02:52] epoch: 18300 train-loss: 0.010545476753678586\n",
      "[LOG 20200502-15:02:52] epoch: 18301 train-loss: 0.010545486894746622\n",
      "[LOG 20200502-15:02:52] epoch: 18302 train-loss: 0.010545496518413225\n",
      "[LOG 20200502-15:02:53] epoch: 18303 train-loss: 0.010545506038599543\n",
      "[LOG 20200502-15:02:53] epoch: 18304 train-loss: 0.01054551483442386\n",
      "[LOG 20200502-15:02:53] epoch: 18305 train-loss: 0.010545524251129892\n",
      "[LOG 20200502-15:02:53] epoch: 18306 train-loss: 0.010545532529552778\n",
      "[LOG 20200502-15:02:54] epoch: 18307 train-loss: 0.010545540807975663\n",
      "[LOG 20200502-15:02:54] epoch: 18308 train-loss: 0.010545548672477404\n",
      "[LOG 20200502-15:02:54] epoch: 18309 train-loss: 0.010545556847420003\n",
      "[LOG 20200502-15:02:54] epoch: 18310 train-loss: 0.010545563987559743\n",
      "[LOG 20200502-15:02:55] epoch: 18311 train-loss: 0.010545571024219194\n",
      "[LOG 20200502-15:02:55] epoch: 18312 train-loss: 0.010545578578280078\n",
      "[LOG 20200502-15:02:55] epoch: 18313 train-loss: 0.010545586028860675\n",
      "[LOG 20200502-15:02:55] epoch: 18314 train-loss: 0.010545592548118697\n",
      "[LOG 20200502-15:02:55] epoch: 18315 train-loss: 0.010545599688258436\n",
      "[LOG 20200502-15:02:56] epoch: 18316 train-loss: 0.010545606621437602\n",
      "[LOG 20200502-15:02:56] epoch: 18317 train-loss: 0.010545613451136483\n",
      "[LOG 20200502-15:02:56] epoch: 18318 train-loss: 0.010545620694756508\n",
      "[LOG 20200502-15:02:56] epoch: 18319 train-loss: 0.010545627524455389\n",
      "[LOG 20200502-15:02:57] epoch: 18320 train-loss: 0.010545634457634555\n",
      "[LOG 20200502-15:02:57] epoch: 18321 train-loss: 0.01054564170125458\n",
      "[LOG 20200502-15:02:57] epoch: 18322 train-loss: 0.010545649151835177\n",
      "[LOG 20200502-15:02:57] epoch: 18323 train-loss: 0.010545656085014343\n",
      "[LOG 20200502-15:02:58] epoch: 18324 train-loss: 0.010545664673878087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-15:02:58] epoch: 18325 train-loss: 0.01054567222793897\n",
      "[LOG 20200502-15:02:58] epoch: 18326 train-loss: 0.010545680299401283\n",
      "[LOG 20200502-15:02:58] epoch: 18327 train-loss: 0.010545688577824168\n",
      "[LOG 20200502-15:02:59] epoch: 18328 train-loss: 0.010545697684089342\n",
      "[LOG 20200502-15:02:59] epoch: 18329 train-loss: 0.010545705962512229\n",
      "[LOG 20200502-15:02:59] epoch: 18330 train-loss: 0.010545714965297116\n",
      "[LOG 20200502-15:03:00] epoch: 18331 train-loss: 0.010545724692444006\n",
      "[LOG 20200502-15:03:00] epoch: 18332 train-loss: 0.010545734316110611\n",
      "[LOG 20200502-15:03:00] epoch: 18333 train-loss: 0.010545743732816644\n",
      "[LOG 20200502-15:03:00] epoch: 18334 train-loss: 0.010545753563443819\n",
      "[LOG 20200502-15:03:01] epoch: 18335 train-loss: 0.010545763497551283\n",
      "[LOG 20200502-15:03:01] epoch: 18336 train-loss: 0.01054577436298132\n",
      "[LOG 20200502-15:03:01] epoch: 18337 train-loss: 0.010545784504049353\n",
      "[LOG 20200502-15:03:01] epoch: 18338 train-loss: 0.010545794541637102\n",
      "[LOG 20200502-15:03:02] epoch: 18339 train-loss: 0.010545805820988284\n",
      "[LOG 20200502-15:03:02] epoch: 18340 train-loss: 0.010545816272497177\n",
      "[LOG 20200502-15:03:02] epoch: 18341 train-loss: 0.010545826827486357\n",
      "[LOG 20200502-15:03:02] epoch: 18342 train-loss: 0.01054583779639668\n",
      "[LOG 20200502-15:03:03] epoch: 18343 train-loss: 0.010545848868787289\n",
      "[LOG 20200502-15:03:03] epoch: 18344 train-loss: 0.010545859423776468\n",
      "[LOG 20200502-15:03:03] epoch: 18345 train-loss: 0.010545869668324789\n",
      "[LOG 20200502-15:03:03] epoch: 18346 train-loss: 0.010545880637235112\n",
      "[LOG 20200502-15:03:04] epoch: 18347 train-loss: 0.010545890571342574\n",
      "[LOG 20200502-15:03:04] epoch: 18348 train-loss: 0.010545901229812039\n",
      "[LOG 20200502-15:03:04] epoch: 18349 train-loss: 0.010545911681320932\n",
      "[LOG 20200502-15:03:04] epoch: 18350 train-loss: 0.010545921304987537\n",
      "[LOG 20200502-15:03:04] epoch: 18351 train-loss: 0.010545931135614714\n",
      "[LOG 20200502-15:03:05] epoch: 18352 train-loss: 0.010545940345360173\n",
      "[LOG 20200502-15:03:05] epoch: 18353 train-loss: 0.010545949555105634\n",
      "[LOG 20200502-15:03:05] epoch: 18354 train-loss: 0.010545958454410235\n",
      "[LOG 20200502-15:03:05] epoch: 18355 train-loss: 0.010545966422392262\n",
      "[LOG 20200502-15:03:06] epoch: 18356 train-loss: 0.01054597439037429\n",
      "[LOG 20200502-15:03:06] epoch: 18357 train-loss: 0.010545982047915459\n",
      "[LOG 20200502-15:03:06] epoch: 18358 train-loss: 0.010545989188055197\n",
      "[LOG 20200502-15:03:06] epoch: 18359 train-loss: 0.010545995396872362\n",
      "[LOG 20200502-15:03:07] epoch: 18360 train-loss: 0.010546002433531813\n",
      "[LOG 20200502-15:03:07] epoch: 18361 train-loss: 0.01054600833190812\n",
      "[LOG 20200502-15:03:07] epoch: 18362 train-loss: 0.010546013402442137\n",
      "[LOG 20200502-15:03:07] epoch: 18363 train-loss: 0.010546018369495869\n",
      "[LOG 20200502-15:03:08] epoch: 18364 train-loss: 0.01054602281914817\n",
      "[LOG 20200502-15:03:08] epoch: 18365 train-loss: 0.010546027165320184\n",
      "[LOG 20200502-15:03:08] epoch: 18366 train-loss: 0.01054603099409077\n",
      "[LOG 20200502-15:03:08] epoch: 18367 train-loss: 0.01054603409849935\n",
      "[LOG 20200502-15:03:09] epoch: 18368 train-loss: 0.01054603730638822\n",
      "[LOG 20200502-15:03:09] epoch: 18369 train-loss: 0.010546039686434798\n",
      "[LOG 20200502-15:03:09] epoch: 18370 train-loss: 0.01054604227344195\n",
      "[LOG 20200502-15:03:09] epoch: 18371 train-loss: 0.010546044446527958\n",
      "[LOG 20200502-15:03:10] epoch: 18372 train-loss: 0.010546046309173107\n",
      "[LOG 20200502-15:03:10] epoch: 18373 train-loss: 0.010546048171818256\n",
      "[LOG 20200502-15:03:10] epoch: 18374 train-loss: 0.01054604941358169\n",
      "[LOG 20200502-15:03:10] epoch: 18375 train-loss: 0.010546051379707124\n",
      "[LOG 20200502-15:03:11] epoch: 18376 train-loss: 0.010546052207549414\n",
      "[LOG 20200502-15:03:11] epoch: 18377 train-loss: 0.010546053656273417\n",
      "[LOG 20200502-15:03:11] epoch: 18378 train-loss: 0.010546055415438281\n",
      "[LOG 20200502-15:03:11] epoch: 18379 train-loss: 0.010546056760682\n",
      "[LOG 20200502-15:03:12] epoch: 18380 train-loss: 0.010546058933768008\n",
      "[LOG 20200502-15:03:12] epoch: 18381 train-loss: 0.010546060485972298\n",
      "[LOG 20200502-15:03:12] epoch: 18382 train-loss: 0.01054606255557802\n",
      "[LOG 20200502-15:03:12] epoch: 18383 train-loss: 0.010546065246065458\n",
      "[LOG 20200502-15:03:13] epoch: 18384 train-loss: 0.010546068040033182\n",
      "[LOG 20200502-15:03:13] epoch: 18385 train-loss: 0.010546071558362909\n",
      "[LOG 20200502-15:03:13] epoch: 18386 train-loss: 0.010546075697574351\n",
      "[LOG 20200502-15:03:13] epoch: 18387 train-loss: 0.010546079319384363\n",
      "[LOG 20200502-15:03:14] epoch: 18388 train-loss: 0.01054608438991838\n",
      "[LOG 20200502-15:03:14] epoch: 18389 train-loss: 0.010546089253491826\n",
      "[LOG 20200502-15:03:14] epoch: 18390 train-loss: 0.010546095151868131\n",
      "[LOG 20200502-15:03:14] epoch: 18391 train-loss: 0.010546100843283866\n",
      "[LOG 20200502-15:03:15] epoch: 18392 train-loss: 0.010546107672982745\n",
      "[LOG 20200502-15:03:15] epoch: 18393 train-loss: 0.010546115123563342\n",
      "[LOG 20200502-15:03:15] epoch: 18394 train-loss: 0.010546122470663654\n",
      "[LOG 20200502-15:03:15] epoch: 18395 train-loss: 0.01054613095604711\n",
      "[LOG 20200502-15:03:16] epoch: 18396 train-loss: 0.010546139958832\n",
      "[LOG 20200502-15:03:16] epoch: 18397 train-loss: 0.010546149065097174\n",
      "[LOG 20200502-15:03:16] epoch: 18398 train-loss: 0.010546158792244064\n",
      "[LOG 20200502-15:03:16] epoch: 18399 train-loss: 0.010546169243752956\n",
      "[LOG 20200502-15:03:17] epoch: 18400 train-loss: 0.010546180109182993\n",
      "[LOG 20200502-15:03:17] epoch: 18401 train-loss: 0.010546191181573603\n",
      "[LOG 20200502-15:03:17] epoch: 18402 train-loss: 0.010546203081806501\n",
      "[LOG 20200502-15:03:17] epoch: 18403 train-loss: 0.010546215292480256\n",
      "[LOG 20200502-15:03:18] epoch: 18404 train-loss: 0.010546227399673726\n",
      "[LOG 20200502-15:03:18] epoch: 18405 train-loss: 0.01054623992078834\n",
      "[LOG 20200502-15:03:18] epoch: 18406 train-loss: 0.010546253269745244\n",
      "[LOG 20200502-15:03:18] epoch: 18407 train-loss: 0.010546266308261288\n",
      "[LOG 20200502-15:03:19] epoch: 18408 train-loss: 0.010546280278099908\n",
      "[LOG 20200502-15:03:19] epoch: 18409 train-loss: 0.010546293523576524\n",
      "[LOG 20200502-15:03:19] epoch: 18410 train-loss: 0.010546308114296861\n",
      "[LOG 20200502-15:03:19] epoch: 18411 train-loss: 0.01054632208413548\n",
      "[LOG 20200502-15:03:19] epoch: 18412 train-loss: 0.010546336467895243\n",
      "[LOG 20200502-15:03:20] epoch: 18413 train-loss: 0.010546350851655006\n",
      "[LOG 20200502-15:03:20] epoch: 18414 train-loss: 0.01054636523541477\n",
      "[LOG 20200502-15:03:20] epoch: 18415 train-loss: 0.010546379205253389\n",
      "[LOG 20200502-15:03:20] epoch: 18416 train-loss: 0.010546394002934298\n",
      "[LOG 20200502-15:03:21] epoch: 18417 train-loss: 0.01054640838669406\n",
      "[LOG 20200502-15:03:21] epoch: 18418 train-loss: 0.010546422977414396\n",
      "[LOG 20200502-15:03:21] epoch: 18419 train-loss: 0.010546437361174159\n",
      "[LOG 20200502-15:03:21] epoch: 18420 train-loss: 0.010546451641453637\n",
      "[LOG 20200502-15:03:22] epoch: 18421 train-loss: 0.01054646499041054\n",
      "[LOG 20200502-15:03:22] epoch: 18422 train-loss: 0.010546479374170303\n",
      "[LOG 20200502-15:03:22] epoch: 18423 train-loss: 0.010546492723127207\n",
      "[LOG 20200502-15:03:22] epoch: 18424 train-loss: 0.010546505968603823\n",
      "[LOG 20200502-15:03:23] epoch: 18425 train-loss: 0.010546519007119868\n",
      "[LOG 20200502-15:03:23] epoch: 18426 train-loss: 0.010546531528234482\n",
      "[LOG 20200502-15:03:23] epoch: 18427 train-loss: 0.010546544359789954\n",
      "[LOG 20200502-15:03:23] epoch: 18428 train-loss: 0.01054655657046371\n",
      "[LOG 20200502-15:03:24] epoch: 18429 train-loss: 0.010546568367216323\n",
      "[LOG 20200502-15:03:24] epoch: 18430 train-loss: 0.010546579853528075\n",
      "[LOG 20200502-15:03:24] epoch: 18431 train-loss: 0.01054659102939897\n",
      "[LOG 20200502-15:03:24] epoch: 18432 train-loss: 0.010546601894829009\n",
      "[LOG 20200502-15:03:25] epoch: 18433 train-loss: 0.010546612242857615\n",
      "[LOG 20200502-15:03:25] epoch: 18434 train-loss: 0.010546622073484791\n",
      "[LOG 20200502-15:03:25] epoch: 18435 train-loss: 0.010546632421513399\n",
      "[LOG 20200502-15:03:25] epoch: 18436 train-loss: 0.010546641010377143\n",
      "[LOG 20200502-15:03:26] epoch: 18437 train-loss: 0.010546650634043746\n",
      "[LOG 20200502-15:03:26] epoch: 18438 train-loss: 0.010546658602025773\n",
      "[LOG 20200502-15:03:26] epoch: 18439 train-loss: 0.010546667190889517\n",
      "[LOG 20200502-15:03:26] epoch: 18440 train-loss: 0.010546675262351831\n",
      "[LOG 20200502-15:03:27] epoch: 18441 train-loss: 0.010546683023373285\n",
      "[LOG 20200502-15:03:27] epoch: 18442 train-loss: 0.01054669026699331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-15:03:27] epoch: 18443 train-loss: 0.010546697821054194\n",
      "[LOG 20200502-15:03:27] epoch: 18444 train-loss: 0.010546704340312216\n",
      "[LOG 20200502-15:03:28] epoch: 18445 train-loss: 0.01054671106653081\n",
      "[LOG 20200502-15:03:28] epoch: 18446 train-loss: 0.010546717999709977\n",
      "[LOG 20200502-15:03:28] epoch: 18447 train-loss: 0.010546723898086283\n",
      "[LOG 20200502-15:03:28] epoch: 18448 train-loss: 0.010546730003423162\n",
      "[LOG 20200502-15:03:29] epoch: 18449 train-loss: 0.010546736419200897\n",
      "[LOG 20200502-15:03:29] epoch: 18450 train-loss: 0.010546742834978633\n",
      "[LOG 20200502-15:03:29] epoch: 18451 train-loss: 0.010546749043795798\n",
      "[LOG 20200502-15:03:29] epoch: 18452 train-loss: 0.010546754631731246\n",
      "[LOG 20200502-15:03:30] epoch: 18453 train-loss: 0.010546760944028696\n",
      "[LOG 20200502-15:03:30] epoch: 18454 train-loss: 0.010546767049365573\n",
      "[LOG 20200502-15:03:30] epoch: 18455 train-loss: 0.010546773361663023\n",
      "[LOG 20200502-15:03:30] epoch: 18456 train-loss: 0.010546780087881617\n",
      "[LOG 20200502-15:03:31] epoch: 18457 train-loss: 0.010546786503659355\n",
      "[LOG 20200502-15:03:31] epoch: 18458 train-loss: 0.01054679374727938\n",
      "[LOG 20200502-15:03:31] epoch: 18459 train-loss: 0.010546800576978259\n",
      "[LOG 20200502-15:03:31] epoch: 18460 train-loss: 0.010546808131039143\n",
      "[LOG 20200502-15:03:31] epoch: 18461 train-loss: 0.010546814857257737\n",
      "[LOG 20200502-15:03:32] epoch: 18462 train-loss: 0.01054682241131862\n",
      "[LOG 20200502-15:03:32] epoch: 18463 train-loss: 0.010546830379300647\n",
      "[LOG 20200502-15:03:32] epoch: 18464 train-loss: 0.010546838140322102\n",
      "[LOG 20200502-15:03:32] epoch: 18465 train-loss: 0.010546846211784415\n",
      "[LOG 20200502-15:03:33] epoch: 18466 train-loss: 0.010546854490207301\n",
      "[LOG 20200502-15:03:33] epoch: 18467 train-loss: 0.010546863492992189\n",
      "[LOG 20200502-15:03:33] epoch: 18468 train-loss: 0.010546872495777078\n",
      "[LOG 20200502-15:03:33] epoch: 18469 train-loss: 0.01054688056723939\n",
      "[LOG 20200502-15:03:34] epoch: 18470 train-loss: 0.010546889983945422\n",
      "[LOG 20200502-15:03:34] epoch: 18471 train-loss: 0.010546898676289452\n",
      "[LOG 20200502-15:03:34] epoch: 18472 train-loss: 0.010546908299956057\n",
      "[LOG 20200502-15:03:34] epoch: 18473 train-loss: 0.01054691823406352\n",
      "[LOG 20200502-15:03:35] epoch: 18474 train-loss: 0.010546927650769552\n",
      "[LOG 20200502-15:03:35] epoch: 18475 train-loss: 0.010546937067475583\n",
      "[LOG 20200502-15:03:35] epoch: 18476 train-loss: 0.01054694689810276\n",
      "[LOG 20200502-15:03:35] epoch: 18477 train-loss: 0.01054695610784822\n",
      "[LOG 20200502-15:03:36] epoch: 18478 train-loss: 0.01054696635239654\n",
      "[LOG 20200502-15:03:36] epoch: 18479 train-loss: 0.01054697587258286\n",
      "[LOG 20200502-15:03:36] epoch: 18480 train-loss: 0.010546985703210035\n",
      "[LOG 20200502-15:03:36] epoch: 18481 train-loss: 0.010546994499034353\n",
      "[LOG 20200502-15:03:37] epoch: 18482 train-loss: 0.010547003812260099\n",
      "[LOG 20200502-15:03:37] epoch: 18483 train-loss: 0.010547012608084414\n",
      "[LOG 20200502-15:03:37] epoch: 18484 train-loss: 0.010547021507389016\n",
      "[LOG 20200502-15:03:37] epoch: 18485 train-loss: 0.010547029889292188\n",
      "[LOG 20200502-15:03:38] epoch: 18486 train-loss: 0.010547038581636217\n",
      "[LOG 20200502-15:03:38] epoch: 18487 train-loss: 0.010547047067019675\n",
      "[LOG 20200502-15:03:38] epoch: 18488 train-loss: 0.010547054103679128\n",
      "[LOG 20200502-15:03:38] epoch: 18489 train-loss: 0.010547061864700582\n",
      "[LOG 20200502-15:03:39] epoch: 18490 train-loss: 0.010547068383958604\n",
      "[LOG 20200502-15:03:39] epoch: 18491 train-loss: 0.010547075420618057\n",
      "[LOG 20200502-15:03:39] epoch: 18492 train-loss: 0.010547081215514077\n",
      "[LOG 20200502-15:03:39] epoch: 18493 train-loss: 0.010547086906929811\n",
      "[LOG 20200502-15:03:40] epoch: 18494 train-loss: 0.010547092701825831\n",
      "[LOG 20200502-15:03:40] epoch: 18495 train-loss: 0.010547097565399276\n",
      "[LOG 20200502-15:03:40] epoch: 18496 train-loss: 0.01054710222201215\n",
      "[LOG 20200502-15:03:40] epoch: 18497 train-loss: 0.010547106568184163\n",
      "[LOG 20200502-15:03:41] epoch: 18498 train-loss: 0.01054711008651389\n",
      "[LOG 20200502-15:03:41] epoch: 18499 train-loss: 0.010547113708323903\n",
      "[LOG 20200502-15:03:41] epoch: 18500 train-loss: 0.01054711639881134\n",
      "[LOG 20200502-15:03:41] epoch: 18501 train-loss: 0.01054711877885792\n",
      "[LOG 20200502-15:03:42] epoch: 18502 train-loss: 0.010547120744983355\n",
      "[LOG 20200502-15:03:42] epoch: 18503 train-loss: 0.010547122814589076\n",
      "[LOG 20200502-15:03:42] epoch: 18504 train-loss: 0.01054712374591165\n",
      "[LOG 20200502-15:03:42] epoch: 18505 train-loss: 0.010547124780714512\n",
      "[LOG 20200502-15:03:43] epoch: 18506 train-loss: 0.010547125401596228\n",
      "[LOG 20200502-15:03:43] epoch: 18507 train-loss: 0.010547125815517373\n",
      "[LOG 20200502-15:03:43] epoch: 18508 train-loss: 0.010547126643359661\n",
      "[LOG 20200502-15:03:43] epoch: 18509 train-loss: 0.010547126850320233\n",
      "[LOG 20200502-15:03:44] epoch: 18510 train-loss: 0.010547126850320233\n",
      "[LOG 20200502-15:03:44] epoch: 18511 train-loss: 0.010547127264241377\n",
      "[LOG 20200502-15:03:44] epoch: 18512 train-loss: 0.010547127160761092\n",
      "[LOG 20200502-15:03:44] epoch: 18513 train-loss: 0.010547127160761092\n",
      "[LOG 20200502-15:03:45] epoch: 18514 train-loss: 0.01054712695380052\n",
      "[LOG 20200502-15:03:45] epoch: 18515 train-loss: 0.010547127781642808\n",
      "[LOG 20200502-15:03:45] epoch: 18516 train-loss: 0.010547128816445669\n",
      "[LOG 20200502-15:03:45] epoch: 18517 train-loss: 0.010547129230366813\n",
      "[LOG 20200502-15:03:46] epoch: 18518 train-loss: 0.010547130679090818\n",
      "[LOG 20200502-15:03:46] epoch: 18519 train-loss: 0.010547131817373965\n",
      "[LOG 20200502-15:03:46] epoch: 18520 train-loss: 0.010547133680019114\n",
      "[LOG 20200502-15:03:46] epoch: 18521 train-loss: 0.010547135335703691\n",
      "[LOG 20200502-15:03:47] epoch: 18522 train-loss: 0.010547138129671415\n",
      "[LOG 20200502-15:03:47] epoch: 18523 train-loss: 0.010547141027119424\n",
      "[LOG 20200502-15:03:47] epoch: 18524 train-loss: 0.01054714454544915\n",
      "[LOG 20200502-15:03:47] epoch: 18525 train-loss: 0.010547148374219736\n",
      "[LOG 20200502-15:03:48] epoch: 18526 train-loss: 0.010547153548234038\n",
      "[LOG 20200502-15:03:48] epoch: 18527 train-loss: 0.010547157894406054\n",
      "[LOG 20200502-15:03:48] epoch: 18528 train-loss: 0.010547164206703505\n",
      "[LOG 20200502-15:03:48] epoch: 18529 train-loss: 0.010547169898119237\n",
      "[LOG 20200502-15:03:49] epoch: 18530 train-loss: 0.01054717641737726\n",
      "[LOG 20200502-15:03:49] epoch: 18531 train-loss: 0.010547183454036713\n",
      "[LOG 20200502-15:03:49] epoch: 18532 train-loss: 0.010547191215058168\n",
      "[LOG 20200502-15:03:49] epoch: 18533 train-loss: 0.01054719959696134\n",
      "[LOG 20200502-15:03:50] epoch: 18534 train-loss: 0.010547207875384225\n",
      "[LOG 20200502-15:03:50] epoch: 18535 train-loss: 0.010547217395570543\n",
      "[LOG 20200502-15:03:50] epoch: 18536 train-loss: 0.010547226398355432\n",
      "[LOG 20200502-15:03:50] epoch: 18537 train-loss: 0.010547236228982607\n",
      "[LOG 20200502-15:03:51] epoch: 18538 train-loss: 0.010547246577011215\n",
      "[LOG 20200502-15:03:51] epoch: 18539 train-loss: 0.01054725723548068\n",
      "[LOG 20200502-15:03:51] epoch: 18540 train-loss: 0.010547268204391003\n",
      "[LOG 20200502-15:03:51] epoch: 18541 train-loss: 0.010547279483742185\n",
      "[LOG 20200502-15:03:52] epoch: 18542 train-loss: 0.010547291590935655\n",
      "[LOG 20200502-15:03:52] epoch: 18543 train-loss: 0.010547303698129125\n",
      "[LOG 20200502-15:03:52] epoch: 18544 train-loss: 0.010547315494881736\n",
      "[LOG 20200502-15:03:52] epoch: 18545 train-loss: 0.010547327912516065\n",
      "[LOG 20200502-15:03:53] epoch: 18546 train-loss: 0.01054734012318982\n",
      "[LOG 20200502-15:03:53] epoch: 18547 train-loss: 0.010547352954745293\n",
      "[LOG 20200502-15:03:53] epoch: 18548 train-loss: 0.010547365372379621\n",
      "[LOG 20200502-15:03:53] epoch: 18549 train-loss: 0.010547378410895666\n",
      "[LOG 20200502-15:03:53] epoch: 18550 train-loss: 0.010547391035490565\n",
      "[LOG 20200502-15:03:54] epoch: 18551 train-loss: 0.010547403660085466\n",
      "[LOG 20200502-15:03:54] epoch: 18552 train-loss: 0.010547416284680367\n",
      "[LOG 20200502-15:03:54] epoch: 18553 train-loss: 0.010547428702314695\n",
      "[LOG 20200502-15:03:55] epoch: 18554 train-loss: 0.01054744174083074\n",
      "[LOG 20200502-15:03:55] epoch: 18555 train-loss: 0.010547453537583351\n",
      "[LOG 20200502-15:03:55] epoch: 18556 train-loss: 0.010547465230855677\n",
      "[LOG 20200502-15:03:55] epoch: 18557 train-loss: 0.010547478165891435\n",
      "[LOG 20200502-15:03:56] epoch: 18558 train-loss: 0.010547489238282045\n",
      "[LOG 20200502-15:03:56] epoch: 18559 train-loss: 0.010547500828074085\n",
      "[LOG 20200502-15:03:56] epoch: 18560 train-loss: 0.010547511693504121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-15:03:56] epoch: 18561 train-loss: 0.0105475222484933\n",
      "[LOG 20200502-15:03:56] epoch: 18562 train-loss: 0.010547533113923337\n",
      "[LOG 20200502-15:03:57] epoch: 18563 train-loss: 0.010547544186313948\n",
      "[LOG 20200502-15:03:57] epoch: 18564 train-loss: 0.010547553189098835\n",
      "[LOG 20200502-15:03:57] epoch: 18565 train-loss: 0.010547562916245725\n",
      "[LOG 20200502-15:03:57] epoch: 18566 train-loss: 0.01054757171207004\n",
      "[LOG 20200502-15:03:58] epoch: 18567 train-loss: 0.01054758040441407\n",
      "[LOG 20200502-15:03:58] epoch: 18568 train-loss: 0.010547589407198958\n",
      "[LOG 20200502-15:03:58] epoch: 18569 train-loss: 0.01054759675429927\n",
      "[LOG 20200502-15:03:58] epoch: 18570 train-loss: 0.01054760410139958\n",
      "[LOG 20200502-15:03:59] epoch: 18571 train-loss: 0.010547611551980177\n",
      "[LOG 20200502-15:03:59] epoch: 18572 train-loss: 0.0105476180712382\n",
      "[LOG 20200502-15:03:59] epoch: 18573 train-loss: 0.010547624590496222\n",
      "[LOG 20200502-15:03:59] epoch: 18574 train-loss: 0.0105476306958331\n",
      "[LOG 20200502-15:04:00] epoch: 18575 train-loss: 0.010547636904650264\n",
      "[LOG 20200502-15:04:00] epoch: 18576 train-loss: 0.010547642078664567\n",
      "[LOG 20200502-15:04:00] epoch: 18577 train-loss: 0.0105476470457183\n",
      "[LOG 20200502-15:04:00] epoch: 18578 train-loss: 0.010547652012772031\n",
      "[LOG 20200502-15:04:01] epoch: 18579 train-loss: 0.010547656255463759\n",
      "[LOG 20200502-15:04:01] epoch: 18580 train-loss: 0.010547661119037204\n",
      "[LOG 20200502-15:04:01] epoch: 18581 train-loss: 0.010547665361728933\n",
      "[LOG 20200502-15:04:01] epoch: 18582 train-loss: 0.010547669087019231\n",
      "[LOG 20200502-15:04:02] epoch: 18583 train-loss: 0.010547673743632104\n",
      "[LOG 20200502-15:04:02] epoch: 18584 train-loss: 0.010547677468922403\n",
      "[LOG 20200502-15:04:02] epoch: 18585 train-loss: 0.01054768150465356\n",
      "[LOG 20200502-15:04:02] epoch: 18586 train-loss: 0.010547685333424144\n",
      "[LOG 20200502-15:04:03] epoch: 18587 train-loss: 0.010547688748273585\n",
      "[LOG 20200502-15:04:03] epoch: 18588 train-loss: 0.0105476930944456\n",
      "[LOG 20200502-15:04:03] epoch: 18589 train-loss: 0.010547696923216185\n",
      "[LOG 20200502-15:04:03] epoch: 18590 train-loss: 0.010547701372868486\n",
      "[LOG 20200502-15:04:04] epoch: 18591 train-loss: 0.010547705615560213\n",
      "[LOG 20200502-15:04:04] epoch: 18592 train-loss: 0.01054770996173223\n",
      "[LOG 20200502-15:04:04] epoch: 18593 train-loss: 0.010547714307904243\n",
      "[LOG 20200502-15:04:04] epoch: 18594 train-loss: 0.010547719067997403\n",
      "[LOG 20200502-15:04:05] epoch: 18595 train-loss: 0.010547723724610276\n",
      "[LOG 20200502-15:04:05] epoch: 18596 train-loss: 0.010547729209065437\n",
      "[LOG 20200502-15:04:05] epoch: 18597 train-loss: 0.010547734072638882\n",
      "[LOG 20200502-15:04:05] epoch: 18598 train-loss: 0.010547739453613758\n",
      "[LOG 20200502-15:04:05] epoch: 18599 train-loss: 0.010547745248509778\n",
      "[LOG 20200502-15:04:06] epoch: 18600 train-loss: 0.010547750836445225\n",
      "[LOG 20200502-15:04:06] epoch: 18601 train-loss: 0.010547756734821532\n",
      "[LOG 20200502-15:04:06] epoch: 18602 train-loss: 0.010547763047118982\n",
      "[LOG 20200502-15:04:07] epoch: 18603 train-loss: 0.01054776966985729\n",
      "[LOG 20200502-15:04:07] epoch: 18604 train-loss: 0.010547776292595599\n",
      "[LOG 20200502-15:04:07] epoch: 18605 train-loss: 0.010547782915333906\n",
      "[LOG 20200502-15:04:07] epoch: 18606 train-loss: 0.010547789848513074\n",
      "[LOG 20200502-15:04:08] epoch: 18607 train-loss: 0.010547797092133097\n",
      "[LOG 20200502-15:04:08] epoch: 18608 train-loss: 0.010547803507910835\n",
      "[LOG 20200502-15:04:08] epoch: 18609 train-loss: 0.010547810958491432\n",
      "[LOG 20200502-15:04:08] epoch: 18610 train-loss: 0.01054781809863117\n",
      "[LOG 20200502-15:04:09] epoch: 18611 train-loss: 0.010547825549211767\n",
      "[LOG 20200502-15:04:09] epoch: 18612 train-loss: 0.010547832068469789\n",
      "[LOG 20200502-15:04:09] epoch: 18613 train-loss: 0.010547839312089814\n",
      "[LOG 20200502-15:04:09] epoch: 18614 train-loss: 0.010547846141788695\n",
      "[LOG 20200502-15:04:10] epoch: 18615 train-loss: 0.010547853074967861\n",
      "[LOG 20200502-15:04:10] epoch: 18616 train-loss: 0.010547859490745597\n",
      "[LOG 20200502-15:04:10] epoch: 18617 train-loss: 0.010547866216964193\n",
      "[LOG 20200502-15:04:10] epoch: 18618 train-loss: 0.010547872943182787\n",
      "[LOG 20200502-15:04:11] epoch: 18619 train-loss: 0.010547878841559092\n",
      "[LOG 20200502-15:04:11] epoch: 18620 train-loss: 0.010547884326014254\n",
      "[LOG 20200502-15:04:11] epoch: 18621 train-loss: 0.010547890224390559\n",
      "[LOG 20200502-15:04:11] epoch: 18622 train-loss: 0.010547895708845722\n",
      "[LOG 20200502-15:04:11] epoch: 18623 train-loss: 0.010547900261978308\n",
      "[LOG 20200502-15:04:12] epoch: 18624 train-loss: 0.010547905435992612\n",
      "[LOG 20200502-15:04:12] epoch: 18625 train-loss: 0.01054790916128291\n",
      "[LOG 20200502-15:04:12] epoch: 18626 train-loss: 0.010547913507454924\n",
      "[LOG 20200502-15:04:12] epoch: 18627 train-loss: 0.010547916922304366\n",
      "[LOG 20200502-15:04:13] epoch: 18628 train-loss: 0.010547919612791803\n",
      "[LOG 20200502-15:04:13] epoch: 18629 train-loss: 0.010547922717200385\n",
      "[LOG 20200502-15:04:13] epoch: 18630 train-loss: 0.010547924890286393\n",
      "[LOG 20200502-15:04:13] epoch: 18631 train-loss: 0.0105479270633724\n",
      "[LOG 20200502-15:04:14] epoch: 18632 train-loss: 0.010547927994694974\n",
      "[LOG 20200502-15:04:14] epoch: 18633 train-loss: 0.010547928719056977\n",
      "[LOG 20200502-15:04:14] epoch: 18634 train-loss: 0.01054792913297812\n",
      "[LOG 20200502-15:04:14] epoch: 18635 train-loss: 0.010547929546899266\n",
      "[LOG 20200502-15:04:15] epoch: 18636 train-loss: 0.01054792913297812\n",
      "[LOG 20200502-15:04:15] epoch: 18637 train-loss: 0.01054792758077383\n",
      "[LOG 20200502-15:04:15] epoch: 18638 train-loss: 0.010547926959892115\n",
      "[LOG 20200502-15:04:15] epoch: 18639 train-loss: 0.010547925304207537\n",
      "[LOG 20200502-15:04:16] epoch: 18640 train-loss: 0.010547923545042673\n",
      "[LOG 20200502-15:04:16] epoch: 18641 train-loss: 0.010547921682397524\n",
      "[LOG 20200502-15:04:16] epoch: 18642 train-loss: 0.0105479188884298\n",
      "[LOG 20200502-15:04:16] epoch: 18643 train-loss: 0.010547916404902935\n",
      "[LOG 20200502-15:04:17] epoch: 18644 train-loss: 0.01054791309353378\n",
      "[LOG 20200502-15:04:17] epoch: 18645 train-loss: 0.010547910816967487\n",
      "[LOG 20200502-15:04:17] epoch: 18646 train-loss: 0.010547908022999763\n",
      "[LOG 20200502-15:04:17] epoch: 18647 train-loss: 0.010547905022071468\n",
      "[LOG 20200502-15:04:18] epoch: 18648 train-loss: 0.010547902021143172\n",
      "[LOG 20200502-15:04:18] epoch: 18649 train-loss: 0.010547899641096592\n",
      "[LOG 20200502-15:04:18] epoch: 18650 train-loss: 0.01054789705408944\n",
      "[LOG 20200502-15:04:18] epoch: 18651 train-loss: 0.010547894156641431\n",
      "[LOG 20200502-15:04:19] epoch: 18652 train-loss: 0.010547892293996282\n",
      "[LOG 20200502-15:04:19] epoch: 18653 train-loss: 0.010547889810469415\n",
      "[LOG 20200502-15:04:19] epoch: 18654 train-loss: 0.010547888154784838\n",
      "[LOG 20200502-15:04:19] epoch: 18655 train-loss: 0.010547887223462263\n",
      "[LOG 20200502-15:04:20] epoch: 18656 train-loss: 0.010547885878218545\n",
      "[LOG 20200502-15:04:20] epoch: 18657 train-loss: 0.010547885360817114\n",
      "[LOG 20200502-15:04:20] epoch: 18658 train-loss: 0.01054788494689597\n",
      "[LOG 20200502-15:04:20] epoch: 18659 train-loss: 0.010547885567777686\n",
      "[LOG 20200502-15:04:20] epoch: 18660 train-loss: 0.010547886395619975\n",
      "[LOG 20200502-15:04:21] epoch: 18661 train-loss: 0.01054788784434398\n",
      "[LOG 20200502-15:04:21] epoch: 18662 train-loss: 0.01054788970698913\n",
      "[LOG 20200502-15:04:21] epoch: 18663 train-loss: 0.010547891880075136\n",
      "[LOG 20200502-15:04:21] epoch: 18664 train-loss: 0.01054789467404286\n",
      "[LOG 20200502-15:04:22] epoch: 18665 train-loss: 0.010547897985412015\n",
      "[LOG 20200502-15:04:22] epoch: 18666 train-loss: 0.010547901917662885\n",
      "[LOG 20200502-15:04:22] epoch: 18667 train-loss: 0.010547906884716617\n",
      "[LOG 20200502-15:04:22] epoch: 18668 train-loss: 0.010547911127408346\n",
      "[LOG 20200502-15:04:23] epoch: 18669 train-loss: 0.010547916818824079\n",
      "[LOG 20200502-15:04:23] epoch: 18670 train-loss: 0.010547922717200385\n",
      "[LOG 20200502-15:04:23] epoch: 18671 train-loss: 0.010547928305135833\n",
      "[LOG 20200502-15:04:23] epoch: 18672 train-loss: 0.010547935134834714\n",
      "[LOG 20200502-15:04:24] epoch: 18673 train-loss: 0.010547942792375883\n",
      "[LOG 20200502-15:04:24] epoch: 18674 train-loss: 0.010547950346436765\n",
      "[LOG 20200502-15:04:24] epoch: 18675 train-loss: 0.010547957693537077\n",
      "[LOG 20200502-15:04:24] epoch: 18676 train-loss: 0.010547966385881105\n",
      "[LOG 20200502-15:04:25] epoch: 18677 train-loss: 0.010547974560823705\n",
      "[LOG 20200502-15:04:25] epoch: 18678 train-loss: 0.010547983874049451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-15:04:25] epoch: 18679 train-loss: 0.010547992462913195\n",
      "[LOG 20200502-15:04:25] epoch: 18680 train-loss: 0.010548001465698084\n",
      "[LOG 20200502-15:04:26] epoch: 18681 train-loss: 0.010548010985884402\n",
      "[LOG 20200502-15:04:26] epoch: 18682 train-loss: 0.01054801998866929\n",
      "[LOG 20200502-15:04:26] epoch: 18683 train-loss: 0.010548029301895035\n",
      "[LOG 20200502-15:04:26] epoch: 18684 train-loss: 0.010548039236002497\n",
      "[LOG 20200502-15:04:26] epoch: 18685 train-loss: 0.010548048342267672\n",
      "[LOG 20200502-15:04:27] epoch: 18686 train-loss: 0.01054805837985542\n",
      "[LOG 20200502-15:04:27] epoch: 18687 train-loss: 0.010548067693081167\n",
      "[LOG 20200502-15:04:27] epoch: 18688 train-loss: 0.010548077523708344\n",
      "[LOG 20200502-15:04:27] epoch: 18689 train-loss: 0.010548085905611515\n",
      "[LOG 20200502-15:04:28] epoch: 18690 train-loss: 0.010548095218837261\n",
      "[LOG 20200502-15:04:28] epoch: 18691 train-loss: 0.010548104014661577\n",
      "[LOG 20200502-15:04:28] epoch: 18692 train-loss: 0.01054811312092675\n",
      "[LOG 20200502-15:04:28] epoch: 18693 train-loss: 0.010548120985428492\n",
      "[LOG 20200502-15:04:29] epoch: 18694 train-loss: 0.010548129056890806\n",
      "[LOG 20200502-15:04:29] epoch: 18695 train-loss: 0.010548137024872832\n",
      "[LOG 20200502-15:04:29] epoch: 18696 train-loss: 0.010548144268492857\n",
      "[LOG 20200502-15:04:29] epoch: 18697 train-loss: 0.010548151201672025\n",
      "[LOG 20200502-15:04:30] epoch: 18698 train-loss: 0.010548158134851191\n",
      "[LOG 20200502-15:04:30] epoch: 18699 train-loss: 0.010548164550628927\n",
      "[LOG 20200502-15:04:30] epoch: 18700 train-loss: 0.010548170138564374\n",
      "[LOG 20200502-15:04:30] epoch: 18701 train-loss: 0.01054817603694068\n",
      "[LOG 20200502-15:04:31] epoch: 18702 train-loss: 0.010548180797033839\n",
      "[LOG 20200502-15:04:31] epoch: 18703 train-loss: 0.010548185867567858\n",
      "[LOG 20200502-15:04:31] epoch: 18704 train-loss: 0.0105481900067793\n",
      "[LOG 20200502-15:04:31] epoch: 18705 train-loss: 0.01054819342162874\n",
      "[LOG 20200502-15:04:32] epoch: 18706 train-loss: 0.010548196939958466\n",
      "[LOG 20200502-15:04:32] epoch: 18707 train-loss: 0.010548199940886762\n",
      "[LOG 20200502-15:04:32] epoch: 18708 train-loss: 0.010548202320933342\n",
      "[LOG 20200502-15:04:32] epoch: 18709 train-loss: 0.010548205114901066\n",
      "[LOG 20200502-15:04:33] epoch: 18710 train-loss: 0.010548206563625071\n",
      "[LOG 20200502-15:04:33] epoch: 18711 train-loss: 0.010548207805388503\n",
      "[LOG 20200502-15:04:33] epoch: 18712 train-loss: 0.010548209047151936\n",
      "[LOG 20200502-15:04:33] epoch: 18713 train-loss: 0.010548210185435083\n",
      "[LOG 20200502-15:04:34] epoch: 18714 train-loss: 0.010548211116757657\n",
      "[LOG 20200502-15:04:34] epoch: 18715 train-loss: 0.010548211220237944\n",
      "[LOG 20200502-15:04:34] epoch: 18716 train-loss: 0.01054821184111966\n",
      "[LOG 20200502-15:04:34] epoch: 18717 train-loss: 0.010548211944599947\n",
      "[LOG 20200502-15:04:34] epoch: 18718 train-loss: 0.010548211944599947\n",
      "[LOG 20200502-15:04:35] epoch: 18719 train-loss: 0.010548211944599947\n",
      "[LOG 20200502-15:04:35] epoch: 18720 train-loss: 0.01054821132371823\n",
      "[LOG 20200502-15:04:35] epoch: 18721 train-loss: 0.01054821132371823\n",
      "[LOG 20200502-15:04:36] epoch: 18722 train-loss: 0.010548211013277372\n",
      "[LOG 20200502-15:04:36] epoch: 18723 train-loss: 0.010548211530678801\n",
      "[LOG 20200502-15:04:36] epoch: 18724 train-loss: 0.010548211427198516\n",
      "[LOG 20200502-15:04:36] epoch: 18725 train-loss: 0.010548211427198516\n",
      "[LOG 20200502-15:04:37] epoch: 18726 train-loss: 0.010548211737639375\n",
      "[LOG 20200502-15:04:37] epoch: 18727 train-loss: 0.010548211944599947\n",
      "[LOG 20200502-15:04:37] epoch: 18728 train-loss: 0.01054821266896195\n",
      "[LOG 20200502-15:04:37] epoch: 18729 train-loss: 0.010548213289843665\n",
      "[LOG 20200502-15:04:38] epoch: 18730 train-loss: 0.010548214324646525\n",
      "[LOG 20200502-15:04:38] epoch: 18731 train-loss: 0.010548215462929673\n",
      "[LOG 20200502-15:04:38] epoch: 18732 train-loss: 0.010548216497732533\n",
      "[LOG 20200502-15:04:38] epoch: 18733 train-loss: 0.010548217946456538\n",
      "[LOG 20200502-15:04:38] epoch: 18734 train-loss: 0.010548219705621401\n",
      "[LOG 20200502-15:04:39] epoch: 18735 train-loss: 0.010548221775227122\n",
      "[LOG 20200502-15:04:39] epoch: 18736 train-loss: 0.010548223741352558\n",
      "[LOG 20200502-15:04:39] epoch: 18737 train-loss: 0.010548225603997707\n",
      "[LOG 20200502-15:04:39] epoch: 18738 train-loss: 0.010548228397965431\n",
      "[LOG 20200502-15:04:40] epoch: 18739 train-loss: 0.010548230674531724\n",
      "[LOG 20200502-15:04:40] epoch: 18740 train-loss: 0.010548233261538876\n",
      "[LOG 20200502-15:04:40] epoch: 18741 train-loss: 0.010548236262467172\n",
      "[LOG 20200502-15:04:40] epoch: 18742 train-loss: 0.010548239263395468\n",
      "[LOG 20200502-15:04:41] epoch: 18743 train-loss: 0.010548242057363192\n",
      "[LOG 20200502-15:04:41] epoch: 18744 train-loss: 0.010548244333929487\n",
      "[LOG 20200502-15:04:41] epoch: 18745 train-loss: 0.010548247955739498\n",
      "[LOG 20200502-15:04:41] epoch: 18746 train-loss: 0.010548251267108653\n",
      "[LOG 20200502-15:04:42] epoch: 18747 train-loss: 0.010548253543674946\n",
      "[LOG 20200502-15:04:42] epoch: 18748 train-loss: 0.010548256441122957\n",
      "[LOG 20200502-15:04:42] epoch: 18749 train-loss: 0.010548259131610394\n",
      "[LOG 20200502-15:04:42] epoch: 18750 train-loss: 0.01054826182209783\n",
      "[LOG 20200502-15:04:43] epoch: 18751 train-loss: 0.010548264409104982\n",
      "[LOG 20200502-15:04:43] epoch: 18752 train-loss: 0.010548266375230419\n",
      "[LOG 20200502-15:04:43] epoch: 18753 train-loss: 0.01054826896223757\n",
      "[LOG 20200502-15:04:43] epoch: 18754 train-loss: 0.010548270410961576\n",
      "[LOG 20200502-15:04:44] epoch: 18755 train-loss: 0.010548272480567297\n",
      "[LOG 20200502-15:04:44] epoch: 18756 train-loss: 0.010548273722330729\n",
      "[LOG 20200502-15:04:44] epoch: 18757 train-loss: 0.01054827475713359\n",
      "[LOG 20200502-15:04:44] epoch: 18758 train-loss: 0.010548275067574449\n",
      "[LOG 20200502-15:04:45] epoch: 18759 train-loss: 0.010548276205857595\n",
      "[LOG 20200502-15:04:45] epoch: 18760 train-loss: 0.010548275895416737\n",
      "[LOG 20200502-15:04:45] epoch: 18761 train-loss: 0.010548275584975878\n",
      "[LOG 20200502-15:04:45] epoch: 18762 train-loss: 0.010548274964094162\n",
      "[LOG 20200502-15:04:46] epoch: 18763 train-loss: 0.010548273618850443\n",
      "[LOG 20200502-15:04:46] epoch: 18764 train-loss: 0.010548272480567297\n",
      "[LOG 20200502-15:04:46] epoch: 18765 train-loss: 0.010548269686599573\n",
      "[LOG 20200502-15:04:46] epoch: 18766 train-loss: 0.010548267306552993\n",
      "[LOG 20200502-15:04:47] epoch: 18767 train-loss: 0.010548264098664125\n",
      "[LOG 20200502-15:04:47] epoch: 18768 train-loss: 0.010548261304696402\n",
      "[LOG 20200502-15:04:47] epoch: 18769 train-loss: 0.010548257372445531\n",
      "[LOG 20200502-15:04:47] epoch: 18770 train-loss: 0.010548253336714374\n",
      "[LOG 20200502-15:04:48] epoch: 18771 train-loss: 0.0105482486801015\n",
      "[LOG 20200502-15:04:48] epoch: 18772 train-loss: 0.010548243609567484\n",
      "[LOG 20200502-15:04:48] epoch: 18773 train-loss: 0.010548238642513752\n",
      "[LOG 20200502-15:04:48] epoch: 18774 train-loss: 0.010548232847617732\n",
      "[LOG 20200502-15:04:49] epoch: 18775 train-loss: 0.010548226742280854\n",
      "[LOG 20200502-15:04:49] epoch: 18776 train-loss: 0.010548220326503118\n",
      "[LOG 20200502-15:04:49] epoch: 18777 train-loss: 0.010548213910725381\n",
      "[LOG 20200502-15:04:49] epoch: 18778 train-loss: 0.010548207184506787\n",
      "[LOG 20200502-15:04:50] epoch: 18779 train-loss: 0.01054820056176848\n",
      "[LOG 20200502-15:04:50] epoch: 18780 train-loss: 0.01054819342162874\n",
      "[LOG 20200502-15:04:50] epoch: 18781 train-loss: 0.01054818659192986\n",
      "[LOG 20200502-15:04:50] epoch: 18782 train-loss: 0.010548179555270407\n",
      "[LOG 20200502-15:04:50] epoch: 18783 train-loss: 0.010548172311650382\n",
      "[LOG 20200502-15:04:51] epoch: 18784 train-loss: 0.010548165171510644\n",
      "[LOG 20200502-15:04:51] epoch: 18785 train-loss: 0.010548158134851191\n",
      "[LOG 20200502-15:04:51] epoch: 18786 train-loss: 0.010548151719073454\n",
      "[LOG 20200502-15:04:51] epoch: 18787 train-loss: 0.01054814499285486\n",
      "[LOG 20200502-15:04:52] epoch: 18788 train-loss: 0.010548138990998268\n",
      "[LOG 20200502-15:04:52] epoch: 18789 train-loss: 0.01054813288566139\n",
      "[LOG 20200502-15:04:52] epoch: 18790 train-loss: 0.010548127401206229\n",
      "[LOG 20200502-15:04:52] epoch: 18791 train-loss: 0.010548121399349637\n",
      "[LOG 20200502-15:04:53] epoch: 18792 train-loss: 0.010548117053177621\n",
      "[LOG 20200502-15:04:53] epoch: 18793 train-loss: 0.010548111775683032\n",
      "[LOG 20200502-15:04:53] epoch: 18794 train-loss: 0.010548107946912447\n",
      "[LOG 20200502-15:04:53] epoch: 18795 train-loss: 0.010548104221622149\n",
      "[LOG 20200502-15:04:54] epoch: 18796 train-loss: 0.010548101220693853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-15:04:54] epoch: 18797 train-loss: 0.010548098219765557\n",
      "[LOG 20200502-15:04:54] epoch: 18798 train-loss: 0.010548096460600695\n",
      "[LOG 20200502-15:04:54] epoch: 18799 train-loss: 0.010548094287514687\n",
      "[LOG 20200502-15:04:55] epoch: 18800 train-loss: 0.010548093252711825\n",
      "[LOG 20200502-15:04:55] epoch: 18801 train-loss: 0.010548092424869537\n",
      "[LOG 20200502-15:04:55] epoch: 18802 train-loss: 0.010548092942270968\n",
      "[LOG 20200502-15:04:55] epoch: 18803 train-loss: 0.010548092838790681\n",
      "[LOG 20200502-15:04:56] epoch: 18804 train-loss: 0.010548093770113256\n",
      "[LOG 20200502-15:04:56] epoch: 18805 train-loss: 0.010548094908396402\n",
      "[LOG 20200502-15:04:56] epoch: 18806 train-loss: 0.010548096771041552\n",
      "[LOG 20200502-15:04:56] epoch: 18807 train-loss: 0.010548099254568418\n",
      "[LOG 20200502-15:04:57] epoch: 18808 train-loss: 0.01054810184157557\n",
      "[LOG 20200502-15:04:57] epoch: 18809 train-loss: 0.010548105049464438\n",
      "[LOG 20200502-15:04:57] epoch: 18810 train-loss: 0.010548108257353306\n",
      "[LOG 20200502-15:04:57] epoch: 18811 train-loss: 0.010548112086123891\n",
      "[LOG 20200502-15:04:57] epoch: 18812 train-loss: 0.010548116328815619\n",
      "[LOG 20200502-15:04:58] epoch: 18813 train-loss: 0.010548120674987635\n",
      "[LOG 20200502-15:04:58] epoch: 18814 train-loss: 0.010548124917679362\n",
      "[LOG 20200502-15:04:58] epoch: 18815 train-loss: 0.010548130402134525\n",
      "[LOG 20200502-15:04:58] epoch: 18816 train-loss: 0.010548135369188257\n",
      "[LOG 20200502-15:04:59] epoch: 18817 train-loss: 0.010548139922320843\n",
      "[LOG 20200502-15:04:59] epoch: 18818 train-loss: 0.01054814582069715\n",
      "[LOG 20200502-15:04:59] epoch: 18819 train-loss: 0.01054815027034945\n",
      "[LOG 20200502-15:04:59] epoch: 18820 train-loss: 0.010548155340883467\n",
      "[LOG 20200502-15:05:00] epoch: 18821 train-loss: 0.010548160825338628\n",
      "[LOG 20200502-15:05:00] epoch: 18822 train-loss: 0.010548166102833219\n",
      "[LOG 20200502-15:05:00] epoch: 18823 train-loss: 0.010548171069886949\n",
      "[LOG 20200502-15:05:00] epoch: 18824 train-loss: 0.01054817603694068\n",
      "[LOG 20200502-15:05:01] epoch: 18825 train-loss: 0.010548180797033839\n",
      "[LOG 20200502-15:05:01] epoch: 18826 train-loss: 0.010548185557126999\n",
      "[LOG 20200502-15:05:01] epoch: 18827 train-loss: 0.010548189799818728\n",
      "[LOG 20200502-15:05:01] epoch: 18828 train-loss: 0.01054819424947103\n",
      "[LOG 20200502-15:05:02] epoch: 18829 train-loss: 0.010548197457359897\n",
      "[LOG 20200502-15:05:02] epoch: 18830 train-loss: 0.010548201182650195\n",
      "[LOG 20200502-15:05:02] epoch: 18831 train-loss: 0.010548204183578491\n",
      "[LOG 20200502-15:05:02] epoch: 18832 train-loss: 0.010548207184506787\n",
      "[LOG 20200502-15:05:03] epoch: 18833 train-loss: 0.010548209150632223\n",
      "[LOG 20200502-15:05:03] epoch: 18834 train-loss: 0.010548211116757657\n",
      "[LOG 20200502-15:05:03] epoch: 18835 train-loss: 0.010548212565481663\n",
      "[LOG 20200502-15:05:03] epoch: 18836 train-loss: 0.010548213910725381\n",
      "[LOG 20200502-15:05:03] epoch: 18837 train-loss: 0.01054821422116624\n",
      "[LOG 20200502-15:05:04] epoch: 18838 train-loss: 0.010548214324646525\n",
      "[LOG 20200502-15:05:04] epoch: 18839 train-loss: 0.010548214531607099\n",
      "[LOG 20200502-15:05:04] epoch: 18840 train-loss: 0.01054821339332395\n",
      "[LOG 20200502-15:05:04] epoch: 18841 train-loss: 0.010548212462001376\n",
      "[LOG 20200502-15:05:05] epoch: 18842 train-loss: 0.010548211427198516\n",
      "[LOG 20200502-15:05:05] epoch: 18843 train-loss: 0.010548209047151936\n",
      "[LOG 20200502-15:05:05] epoch: 18844 train-loss: 0.010548206667105356\n",
      "[LOG 20200502-15:05:05] epoch: 18845 train-loss: 0.010548203769657347\n",
      "[LOG 20200502-15:05:06] epoch: 18846 train-loss: 0.010548201182650195\n",
      "[LOG 20200502-15:05:06] epoch: 18847 train-loss: 0.010548197767800756\n",
      "[LOG 20200502-15:05:06] epoch: 18848 train-loss: 0.01054819424947103\n",
      "[LOG 20200502-15:05:06] epoch: 18849 train-loss: 0.0105481900067793\n",
      "[LOG 20200502-15:05:07] epoch: 18850 train-loss: 0.01054818607452843\n",
      "[LOG 20200502-15:05:07] epoch: 18851 train-loss: 0.010548181728356414\n",
      "[LOG 20200502-15:05:07] epoch: 18852 train-loss: 0.010548177796105543\n",
      "[LOG 20200502-15:05:07] epoch: 18853 train-loss: 0.01054817313949267\n",
      "[LOG 20200502-15:05:08] epoch: 18854 train-loss: 0.010548168793320656\n",
      "[LOG 20200502-15:05:08] epoch: 18855 train-loss: 0.010548163722786639\n",
      "[LOG 20200502-15:05:08] epoch: 18856 train-loss: 0.01054815999749634\n",
      "[LOG 20200502-15:05:08] epoch: 18857 train-loss: 0.010548155340883467\n",
      "[LOG 20200502-15:05:09] epoch: 18858 train-loss: 0.010548151098191738\n",
      "[LOG 20200502-15:05:09] epoch: 18859 train-loss: 0.010548146234618293\n",
      "[LOG 20200502-15:05:09] epoch: 18860 train-loss: 0.010548142716288567\n",
      "[LOG 20200502-15:05:09] epoch: 18861 train-loss: 0.010548138266636265\n",
      "[LOG 20200502-15:05:09] epoch: 18862 train-loss: 0.010548134230905108\n",
      "[LOG 20200502-15:05:10] epoch: 18863 train-loss: 0.010548130609095097\n",
      "[LOG 20200502-15:05:10] epoch: 18864 train-loss: 0.010548126987285085\n",
      "[LOG 20200502-15:05:10] epoch: 18865 train-loss: 0.010548123468955358\n",
      "[LOG 20200502-15:05:10] epoch: 18866 train-loss: 0.010548120157586204\n",
      "[LOG 20200502-15:05:11] epoch: 18867 train-loss: 0.010548117053177621\n",
      "[LOG 20200502-15:05:11] epoch: 18868 train-loss: 0.010548114569650756\n",
      "[LOG 20200502-15:05:11] epoch: 18869 train-loss: 0.010548112086123891\n",
      "[LOG 20200502-15:05:12] epoch: 18870 train-loss: 0.010548110119998455\n",
      "[LOG 20200502-15:05:12] epoch: 18871 train-loss: 0.010548107429511018\n",
      "[LOG 20200502-15:05:12] epoch: 18872 train-loss: 0.010548105152944723\n",
      "[LOG 20200502-15:05:12] epoch: 18873 train-loss: 0.010548103290299574\n",
      "[LOG 20200502-15:05:12] epoch: 18874 train-loss: 0.010548101531134712\n",
      "[LOG 20200502-15:05:13] epoch: 18875 train-loss: 0.010548099771969847\n",
      "[LOG 20200502-15:05:13] epoch: 18876 train-loss: 0.010548098219765557\n",
      "[LOG 20200502-15:05:13] epoch: 18877 train-loss: 0.010548097184962697\n",
      "[LOG 20200502-15:05:13] epoch: 18878 train-loss: 0.010548095736238692\n",
      "[LOG 20200502-15:05:14] epoch: 18879 train-loss: 0.010548094597955545\n",
      "[LOG 20200502-15:05:14] epoch: 18880 train-loss: 0.010548092735310396\n",
      "[LOG 20200502-15:05:14] epoch: 18881 train-loss: 0.010548091390066676\n",
      "[LOG 20200502-15:05:14] epoch: 18882 train-loss: 0.010548090148303244\n",
      "[LOG 20200502-15:05:15] epoch: 18883 train-loss: 0.010548088182177808\n",
      "[LOG 20200502-15:05:15] epoch: 18884 train-loss: 0.010548087147374948\n",
      "[LOG 20200502-15:05:15] epoch: 18885 train-loss: 0.010548085388210084\n",
      "[LOG 20200502-15:05:15] epoch: 18886 train-loss: 0.010548083939486079\n",
      "[LOG 20200502-15:05:16] epoch: 18887 train-loss: 0.01054808207684093\n",
      "[LOG 20200502-15:05:16] epoch: 18888 train-loss: 0.010548079696794352\n",
      "[LOG 20200502-15:05:16] epoch: 18889 train-loss: 0.010548077523708344\n",
      "[LOG 20200502-15:05:16] epoch: 18890 train-loss: 0.010548074936701192\n",
      "[LOG 20200502-15:05:17] epoch: 18891 train-loss: 0.010548071521851752\n",
      "[LOG 20200502-15:05:17] epoch: 18892 train-loss: 0.010548069141805172\n",
      "[LOG 20200502-15:05:17] epoch: 18893 train-loss: 0.010548065416514874\n",
      "[LOG 20200502-15:05:17] epoch: 18894 train-loss: 0.010548061794704862\n",
      "[LOG 20200502-15:05:18] epoch: 18895 train-loss: 0.010548057758973705\n",
      "[LOG 20200502-15:05:18] epoch: 18896 train-loss: 0.010548053309321404\n",
      "[LOG 20200502-15:05:18] epoch: 18897 train-loss: 0.010548048445747958\n",
      "[LOG 20200502-15:05:18] epoch: 18898 train-loss: 0.010548043478694227\n",
      "[LOG 20200502-15:05:19] epoch: 18899 train-loss: 0.010548037890758779\n",
      "[LOG 20200502-15:05:19] epoch: 18900 train-loss: 0.010548032406303618\n",
      "[LOG 20200502-15:05:19] epoch: 18901 train-loss: 0.010548025473124452\n",
      "[LOG 20200502-15:05:19] epoch: 18902 train-loss: 0.010548019264307287\n",
      "[LOG 20200502-15:05:19] epoch: 18903 train-loss: 0.010548011606766118\n",
      "[LOG 20200502-15:05:20] epoch: 18904 train-loss: 0.010548004570106665\n",
      "[LOG 20200502-15:05:20] epoch: 18905 train-loss: 0.01054799629168378\n",
      "[LOG 20200502-15:05:20] epoch: 18906 train-loss: 0.010547988530662324\n",
      "[LOG 20200502-15:05:20] epoch: 18907 train-loss: 0.010547979217436578\n",
      "[LOG 20200502-15:05:21] epoch: 18908 train-loss: 0.010547970421612263\n",
      "[LOG 20200502-15:05:21] epoch: 18909 train-loss: 0.010547961418827375\n",
      "[LOG 20200502-15:05:21] epoch: 18910 train-loss: 0.010547952209081914\n",
      "[LOG 20200502-15:05:21] epoch: 18911 train-loss: 0.010547942274974452\n",
      "[LOG 20200502-15:05:22] epoch: 18912 train-loss: 0.01054793234086699\n",
      "[LOG 20200502-15:05:22] epoch: 18913 train-loss: 0.010547922199798955\n",
      "[LOG 20200502-15:05:22] epoch: 18914 train-loss: 0.010547912162211206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-15:05:22] epoch: 18915 train-loss: 0.010547900779379739\n",
      "[LOG 20200502-15:05:23] epoch: 18916 train-loss: 0.010547890948752562\n",
      "[LOG 20200502-15:05:23] epoch: 18917 train-loss: 0.010547880083322525\n",
      "[LOG 20200502-15:05:23] epoch: 18918 train-loss: 0.01054786942485306\n",
      "[LOG 20200502-15:05:23] epoch: 18919 train-loss: 0.010547859076824453\n",
      "[LOG 20200502-15:05:23] epoch: 18920 train-loss: 0.010547848211394416\n",
      "[LOG 20200502-15:05:24] epoch: 18921 train-loss: 0.010547838484247526\n",
      "[LOG 20200502-15:05:24] epoch: 18922 train-loss: 0.010547828550140062\n",
      "[LOG 20200502-15:05:24] epoch: 18923 train-loss: 0.010547818305591742\n",
      "[LOG 20200502-15:05:24] epoch: 18924 train-loss: 0.010547809095846282\n",
      "[LOG 20200502-15:05:25] epoch: 18925 train-loss: 0.010547799472179677\n",
      "[LOG 20200502-15:05:25] epoch: 18926 train-loss: 0.01054779046939479\n",
      "[LOG 20200502-15:05:25] epoch: 18927 train-loss: 0.010547781259649329\n",
      "[LOG 20200502-15:05:25] epoch: 18928 train-loss: 0.010547773084706731\n",
      "[LOG 20200502-15:05:26] epoch: 18929 train-loss: 0.010547765220204989\n",
      "[LOG 20200502-15:05:26] epoch: 18930 train-loss: 0.010547757873104678\n",
      "[LOG 20200502-15:05:26] epoch: 18931 train-loss: 0.010547750526004367\n",
      "[LOG 20200502-15:05:26] epoch: 18932 train-loss: 0.010547743696305487\n",
      "[LOG 20200502-15:05:27] epoch: 18933 train-loss: 0.010547738522291183\n",
      "[LOG 20200502-15:05:27] epoch: 18934 train-loss: 0.010547732416954305\n",
      "[LOG 20200502-15:05:27] epoch: 18935 train-loss: 0.01054772755338086\n",
      "[LOG 20200502-15:05:27] epoch: 18936 train-loss: 0.01054772331068913\n",
      "[LOG 20200502-15:05:28] epoch: 18937 train-loss: 0.010547718550595973\n",
      "[LOG 20200502-15:05:28] epoch: 18938 train-loss: 0.010547715653147962\n",
      "[LOG 20200502-15:05:28] epoch: 18939 train-loss: 0.010547712238298522\n",
      "[LOG 20200502-15:05:28] epoch: 18940 train-loss: 0.01054770996173223\n",
      "[LOG 20200502-15:05:29] epoch: 18941 train-loss: 0.010547707685165934\n",
      "[LOG 20200502-15:05:29] epoch: 18942 train-loss: 0.010547706132961644\n",
      "[LOG 20200502-15:05:29] epoch: 18943 train-loss: 0.01054770520163907\n",
      "[LOG 20200502-15:05:29] epoch: 18944 train-loss: 0.010547703649434779\n",
      "[LOG 20200502-15:05:30] epoch: 18945 train-loss: 0.010547703442474207\n",
      "[LOG 20200502-15:05:30] epoch: 18946 train-loss: 0.010547703235513635\n",
      "[LOG 20200502-15:05:30] epoch: 18947 train-loss: 0.010547703545954492\n",
      "[LOG 20200502-15:05:30] epoch: 18948 train-loss: 0.01054770385639535\n",
      "[LOG 20200502-15:05:31] epoch: 18949 train-loss: 0.010547704373796781\n",
      "[LOG 20200502-15:05:31] epoch: 18950 train-loss: 0.01054770489119821\n",
      "[LOG 20200502-15:05:31] epoch: 18951 train-loss: 0.010547706339922216\n",
      "[LOG 20200502-15:05:31] epoch: 18952 train-loss: 0.010547707892126508\n",
      "[LOG 20200502-15:05:32] epoch: 18953 train-loss: 0.010547709030409655\n",
      "[LOG 20200502-15:05:32] epoch: 18954 train-loss: 0.010547710168692801\n",
      "[LOG 20200502-15:05:32] epoch: 18955 train-loss: 0.010547712134818235\n",
      "[LOG 20200502-15:05:32] epoch: 18956 train-loss: 0.010547713376581669\n",
      "[LOG 20200502-15:05:33] epoch: 18957 train-loss: 0.010547714721825387\n",
      "[LOG 20200502-15:05:33] epoch: 18958 train-loss: 0.01054771596358882\n",
      "[LOG 20200502-15:05:33] epoch: 18959 train-loss: 0.010547717515793111\n",
      "[LOG 20200502-15:05:33] epoch: 18960 train-loss: 0.010547719067997403\n",
      "[LOG 20200502-15:05:33] epoch: 18961 train-loss: 0.010547719792359404\n",
      "[LOG 20200502-15:05:34] epoch: 18962 train-loss: 0.010547720102800263\n",
      "[LOG 20200502-15:05:34] epoch: 18963 train-loss: 0.010547720930642553\n",
      "[LOG 20200502-15:05:34] epoch: 18964 train-loss: 0.010547721137603125\n",
      "[LOG 20200502-15:05:34] epoch: 18965 train-loss: 0.010547720930642553\n",
      "[LOG 20200502-15:05:35] epoch: 18966 train-loss: 0.010547720413241122\n",
      "[LOG 20200502-15:05:35] epoch: 18967 train-loss: 0.010547719895839691\n",
      "[LOG 20200502-15:05:35] epoch: 18968 train-loss: 0.010547719171477688\n",
      "[LOG 20200502-15:05:35] epoch: 18969 train-loss: 0.010547717515793111\n",
      "[LOG 20200502-15:05:36] epoch: 18970 train-loss: 0.01054771544618739\n",
      "[LOG 20200502-15:05:36] epoch: 18971 train-loss: 0.010547712859180238\n",
      "[LOG 20200502-15:05:36] epoch: 18972 train-loss: 0.010547710479133658\n",
      "[LOG 20200502-15:05:37] epoch: 18973 train-loss: 0.010547707167764505\n",
      "[LOG 20200502-15:05:37] epoch: 18974 train-loss: 0.010547704270316495\n",
      "[LOG 20200502-15:05:37] epoch: 18975 train-loss: 0.010547699820664194\n",
      "[LOG 20200502-15:05:37] epoch: 18976 train-loss: 0.01054769516405132\n",
      "[LOG 20200502-15:05:37] epoch: 18977 train-loss: 0.010547691335280737\n",
      "[LOG 20200502-15:05:38] epoch: 18978 train-loss: 0.010547685333424144\n",
      "[LOG 20200502-15:05:38] epoch: 18979 train-loss: 0.010547680159409841\n",
      "[LOG 20200502-15:05:38] epoch: 18980 train-loss: 0.010547674157553248\n",
      "[LOG 20200502-15:05:38] epoch: 18981 train-loss: 0.010547668052216371\n",
      "[LOG 20200502-15:05:39] epoch: 18982 train-loss: 0.010547661946879493\n",
      "[LOG 20200502-15:05:39] epoch: 18983 train-loss: 0.010547654806739755\n",
      "[LOG 20200502-15:05:39] epoch: 18984 train-loss: 0.010547648184001446\n",
      "[LOG 20200502-15:05:39] epoch: 18985 train-loss: 0.010547640526460277\n",
      "[LOG 20200502-15:05:40] epoch: 18986 train-loss: 0.010547633489800824\n",
      "[LOG 20200502-15:05:40] epoch: 18987 train-loss: 0.010547625314858224\n",
      "[LOG 20200502-15:05:40] epoch: 18988 train-loss: 0.0105476180712382\n",
      "[LOG 20200502-15:05:40] epoch: 18989 train-loss: 0.01054761072413789\n",
      "[LOG 20200502-15:05:41] epoch: 18990 train-loss: 0.010547602756155862\n",
      "[LOG 20200502-15:05:41] epoch: 18991 train-loss: 0.010547594477732977\n",
      "[LOG 20200502-15:05:41] epoch: 18992 train-loss: 0.010547587337593237\n",
      "[LOG 20200502-15:05:41] epoch: 18993 train-loss: 0.01054757988701264\n",
      "[LOG 20200502-15:05:42] epoch: 18994 train-loss: 0.010547572229471471\n",
      "[LOG 20200502-15:05:42] epoch: 18995 train-loss: 0.010547564571930302\n",
      "[LOG 20200502-15:05:42] epoch: 18996 train-loss: 0.010547557224829992\n",
      "[LOG 20200502-15:05:42] epoch: 18997 train-loss: 0.01054754987772968\n",
      "[LOG 20200502-15:05:42] epoch: 18998 train-loss: 0.010547542737589942\n",
      "[LOG 20200502-15:05:43] epoch: 18999 train-loss: 0.010547535907891061\n",
      "[LOG 20200502-15:05:43] epoch: 19000 train-loss: 0.010547529078192182\n",
      "[LOG 20200502-15:05:43] epoch: 19001 train-loss: 0.010547522455453873\n",
      "[LOG 20200502-15:05:43] epoch: 19002 train-loss: 0.010547516557077566\n",
      "[LOG 20200502-15:05:44] epoch: 19003 train-loss: 0.010547509830858972\n",
      "[LOG 20200502-15:05:44] epoch: 19004 train-loss: 0.010547504449884096\n",
      "[LOG 20200502-15:05:44] epoch: 19005 train-loss: 0.010547498861948649\n",
      "[LOG 20200502-15:05:44] epoch: 19006 train-loss: 0.010547492756611772\n",
      "[LOG 20200502-15:05:45] epoch: 19007 train-loss: 0.010547487893038325\n",
      "[LOG 20200502-15:05:45] epoch: 19008 train-loss: 0.010547482408583164\n",
      "[LOG 20200502-15:05:45] epoch: 19009 train-loss: 0.010547476924128003\n",
      "[LOG 20200502-15:05:45] epoch: 19010 train-loss: 0.010547471957074272\n",
      "[LOG 20200502-15:05:46] epoch: 19011 train-loss: 0.010547467610902257\n",
      "[LOG 20200502-15:05:46] epoch: 19012 train-loss: 0.010547461919486523\n",
      "[LOG 20200502-15:05:46] epoch: 19013 train-loss: 0.010547457676794793\n",
      "[LOG 20200502-15:05:46] epoch: 19014 train-loss: 0.01054745333062278\n",
      "[LOG 20200502-15:05:47] epoch: 19015 train-loss: 0.010547448674009906\n",
      "[LOG 20200502-15:05:47] epoch: 19016 train-loss: 0.010547443396515317\n",
      "[LOG 20200502-15:05:47] epoch: 19017 train-loss: 0.010547439153823588\n",
      "[LOG 20200502-15:05:47] epoch: 19018 train-loss: 0.010547434497210715\n",
      "[LOG 20200502-15:05:47] epoch: 19019 train-loss: 0.010547429323196411\n",
      "[LOG 20200502-15:05:48] epoch: 19020 train-loss: 0.010547424459622966\n",
      "[LOG 20200502-15:05:48] epoch: 19021 train-loss: 0.010547419699529806\n",
      "[LOG 20200502-15:05:48] epoch: 19022 train-loss: 0.010547414732476076\n",
      "[LOG 20200502-15:05:48] epoch: 19023 train-loss: 0.01054740883409977\n",
      "[LOG 20200502-15:05:49] epoch: 19024 train-loss: 0.010547403660085466\n",
      "[LOG 20200502-15:05:49] epoch: 19025 train-loss: 0.010547397451268302\n",
      "[LOG 20200502-15:05:49] epoch: 19026 train-loss: 0.010547391552891996\n",
      "[LOG 20200502-15:05:49] epoch: 19027 train-loss: 0.010547385861476263\n",
      "[LOG 20200502-15:05:50] epoch: 19028 train-loss: 0.010547379031777382\n",
      "[LOG 20200502-15:05:50] epoch: 19029 train-loss: 0.010547371788157357\n",
      "[LOG 20200502-15:05:50] epoch: 19030 train-loss: 0.01054736516541905\n",
      "[LOG 20200502-15:05:50] epoch: 19031 train-loss: 0.010547357507877879\n",
      "[LOG 20200502-15:05:51] epoch: 19032 train-loss: 0.010547349643376138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-15:05:51] epoch: 19033 train-loss: 0.01054734167539411\n",
      "[LOG 20200502-15:05:51] epoch: 19034 train-loss: 0.010547333190010654\n",
      "[LOG 20200502-15:05:51] epoch: 19035 train-loss: 0.01054732408374548\n",
      "[LOG 20200502-15:05:52] epoch: 19036 train-loss: 0.010547314667039447\n",
      "[LOG 20200502-15:05:52] epoch: 19037 train-loss: 0.0105473053538137\n",
      "[LOG 20200502-15:05:52] epoch: 19038 train-loss: 0.01054729510926538\n",
      "[LOG 20200502-15:05:52] epoch: 19039 train-loss: 0.010547284450795915\n",
      "[LOG 20200502-15:05:53] epoch: 19040 train-loss: 0.010547273688846164\n",
      "[LOG 20200502-15:05:53] epoch: 19041 train-loss: 0.010547262616455555\n",
      "[LOG 20200502-15:05:53] epoch: 19042 train-loss: 0.010547250716222657\n",
      "[LOG 20200502-15:05:53] epoch: 19043 train-loss: 0.010547239229910903\n",
      "[LOG 20200502-15:05:54] epoch: 19044 train-loss: 0.010547227329678006\n",
      "[LOG 20200502-15:05:54] epoch: 19045 train-loss: 0.010547214187681675\n",
      "[LOG 20200502-15:05:54] epoch: 19046 train-loss: 0.010547201563086774\n",
      "[LOG 20200502-15:05:54] epoch: 19047 train-loss: 0.010547188628051016\n",
      "[LOG 20200502-15:05:55] epoch: 19048 train-loss: 0.010547175899975829\n",
      "[LOG 20200502-15:05:55] epoch: 19049 train-loss: 0.010547162551018927\n",
      "[LOG 20200502-15:05:55] epoch: 19050 train-loss: 0.010547149409022596\n",
      "[LOG 20200502-15:05:55] epoch: 19051 train-loss: 0.010547135646144548\n",
      "[LOG 20200502-15:05:56] epoch: 19052 train-loss: 0.010547122297187647\n",
      "[LOG 20200502-15:05:56] epoch: 19053 train-loss: 0.010547108741270171\n",
      "[LOG 20200502-15:05:56] epoch: 19054 train-loss: 0.010547095806234412\n",
      "[LOG 20200502-15:05:56] epoch: 19055 train-loss: 0.010547082043356366\n",
      "[LOG 20200502-15:05:56] epoch: 19056 train-loss: 0.010547069108320607\n",
      "[LOG 20200502-15:05:57] epoch: 19057 train-loss: 0.010547056173284849\n",
      "[LOG 20200502-15:05:57] epoch: 19058 train-loss: 0.010547043445209662\n",
      "[LOG 20200502-15:05:57] epoch: 19059 train-loss: 0.010547030820614763\n",
      "[LOG 20200502-15:05:57] epoch: 19060 train-loss: 0.010547018196019862\n",
      "[LOG 20200502-15:05:58] epoch: 19061 train-loss: 0.010547006709708108\n",
      "[LOG 20200502-15:05:58] epoch: 19062 train-loss: 0.010546994602514638\n",
      "[LOG 20200502-15:05:58] epoch: 19063 train-loss: 0.010546983840564886\n",
      "[LOG 20200502-15:05:58] epoch: 19064 train-loss: 0.010546972871654563\n",
      "[LOG 20200502-15:05:59] epoch: 19065 train-loss: 0.010546962627106242\n",
      "[LOG 20200502-15:05:59] epoch: 19066 train-loss: 0.01054695269299878\n",
      "[LOG 20200502-15:05:59] epoch: 19067 train-loss: 0.010546943069332175\n",
      "[LOG 20200502-15:05:59] epoch: 19068 train-loss: 0.010546934376988146\n",
      "[LOG 20200502-15:06:00] epoch: 19069 train-loss: 0.01054692609856526\n",
      "[LOG 20200502-15:06:00] epoch: 19070 train-loss: 0.010546918337543806\n",
      "[LOG 20200502-15:06:00] epoch: 19071 train-loss: 0.010546911197404066\n",
      "[LOG 20200502-15:06:00] epoch: 19072 train-loss: 0.010546903436382612\n",
      "[LOG 20200502-15:06:01] epoch: 19073 train-loss: 0.010546897434526019\n",
      "[LOG 20200502-15:06:01] epoch: 19074 train-loss: 0.010546891329189142\n",
      "[LOG 20200502-15:06:01] epoch: 19075 train-loss: 0.01054688584473398\n",
      "[LOG 20200502-15:06:01] epoch: 19076 train-loss: 0.01054688087768025\n",
      "[LOG 20200502-15:06:02] epoch: 19077 train-loss: 0.010546876531508233\n",
      "[LOG 20200502-15:06:02] epoch: 19078 train-loss: 0.010546872185336219\n",
      "[LOG 20200502-15:06:02] epoch: 19079 train-loss: 0.01054686846004592\n",
      "[LOG 20200502-15:06:02] epoch: 19080 train-loss: 0.010546865045196481\n",
      "[LOG 20200502-15:06:02] epoch: 19081 train-loss: 0.010546862561669614\n",
      "[LOG 20200502-15:06:03] epoch: 19082 train-loss: 0.010546859353780746\n",
      "[LOG 20200502-15:06:03] epoch: 19083 train-loss: 0.010546856973734167\n",
      "[LOG 20200502-15:06:03] epoch: 19084 train-loss: 0.010546855214569304\n",
      "[LOG 20200502-15:06:04] epoch: 19085 train-loss: 0.010546853144963583\n",
      "[LOG 20200502-15:06:04] epoch: 19086 train-loss: 0.010546851489279006\n",
      "[LOG 20200502-15:06:04] epoch: 19087 train-loss: 0.01054684952315357\n",
      "[LOG 20200502-15:06:04] epoch: 19088 train-loss: 0.010546848384870423\n",
      "[LOG 20200502-15:06:05] epoch: 19089 train-loss: 0.010546846936146418\n",
      "[LOG 20200502-15:06:05] epoch: 19090 train-loss: 0.010546845383942127\n",
      "[LOG 20200502-15:06:05] epoch: 19091 train-loss: 0.010546844142178694\n",
      "[LOG 20200502-15:06:05] epoch: 19092 train-loss: 0.010546842486494117\n",
      "[LOG 20200502-15:06:05] epoch: 19093 train-loss: 0.01054684134821097\n",
      "[LOG 20200502-15:06:06] epoch: 19094 train-loss: 0.01054683979600668\n",
      "[LOG 20200502-15:06:06] epoch: 19095 train-loss: 0.010546837829881243\n",
      "[LOG 20200502-15:06:06] epoch: 19096 train-loss: 0.01054683638115724\n",
      "[LOG 20200502-15:06:07] epoch: 19097 train-loss: 0.01054683400111066\n",
      "[LOG 20200502-15:06:07] epoch: 19098 train-loss: 0.01054683162106408\n",
      "[LOG 20200502-15:06:07] epoch: 19099 train-loss: 0.010546828827096356\n",
      "[LOG 20200502-15:06:07] epoch: 19100 train-loss: 0.010546825722687773\n",
      "[LOG 20200502-15:06:07] epoch: 19101 train-loss: 0.01054682241131862\n",
      "[LOG 20200502-15:06:08] epoch: 19102 train-loss: 0.010546819099949466\n",
      "[LOG 20200502-15:06:08] epoch: 19103 train-loss: 0.010546815064218309\n",
      "[LOG 20200502-15:06:08] epoch: 19104 train-loss: 0.010546810511085723\n",
      "[LOG 20200502-15:06:08] epoch: 19105 train-loss: 0.010546806061433421\n",
      "[LOG 20200502-15:06:09] epoch: 19106 train-loss: 0.010546800576978259\n",
      "[LOG 20200502-15:06:09] epoch: 19107 train-loss: 0.010546795092523098\n",
      "[LOG 20200502-15:06:09] epoch: 19108 train-loss: 0.010546788883705934\n",
      "[LOG 20200502-15:06:09] epoch: 19109 train-loss: 0.010546782778369056\n",
      "[LOG 20200502-15:06:10] epoch: 19110 train-loss: 0.01054677584518989\n",
      "[LOG 20200502-15:06:10] epoch: 19111 train-loss: 0.010546768498089578\n",
      "[LOG 20200502-15:06:10] epoch: 19112 train-loss: 0.010546761461430125\n",
      "[LOG 20200502-15:06:10] epoch: 19113 train-loss: 0.010546752872566382\n",
      "[LOG 20200502-15:06:11] epoch: 19114 train-loss: 0.010546744594143497\n",
      "[LOG 20200502-15:06:11] epoch: 19115 train-loss: 0.010546736005279753\n",
      "[LOG 20200502-15:06:11] epoch: 19116 train-loss: 0.010546727002494864\n",
      "[LOG 20200502-15:06:11] epoch: 19117 train-loss: 0.010546717896229692\n",
      "[LOG 20200502-15:06:12] epoch: 19118 train-loss: 0.010546708065602515\n",
      "[LOG 20200502-15:06:12] epoch: 19119 train-loss: 0.010546698545416197\n",
      "[LOG 20200502-15:06:12] epoch: 19120 train-loss: 0.010546688611308733\n",
      "[LOG 20200502-15:06:12] epoch: 19121 train-loss: 0.010546678366760412\n",
      "[LOG 20200502-15:06:13] epoch: 19122 train-loss: 0.010546667811771234\n",
      "[LOG 20200502-15:06:13] epoch: 19123 train-loss: 0.010546658084624343\n",
      "[LOG 20200502-15:06:13] epoch: 19124 train-loss: 0.01054664763311545\n",
      "[LOG 20200502-15:06:13] epoch: 19125 train-loss: 0.010546636974645985\n",
      "[LOG 20200502-15:06:14] epoch: 19126 train-loss: 0.010546627040538523\n",
      "[LOG 20200502-15:06:14] epoch: 19127 train-loss: 0.010546616795990203\n",
      "[LOG 20200502-15:06:14] epoch: 19128 train-loss: 0.010546606654922167\n",
      "[LOG 20200502-15:06:14] epoch: 19129 train-loss: 0.010546596410373846\n",
      "[LOG 20200502-15:06:15] epoch: 19130 train-loss: 0.010546586269305812\n",
      "[LOG 20200502-15:06:15] epoch: 19131 train-loss: 0.010546576438678635\n",
      "[LOG 20200502-15:06:15] epoch: 19132 train-loss: 0.010546566918492317\n",
      "[LOG 20200502-15:06:15] epoch: 19133 train-loss: 0.01054655708786514\n",
      "[LOG 20200502-15:06:16] epoch: 19134 train-loss: 0.010546547981599966\n",
      "[LOG 20200502-15:06:16] epoch: 19135 train-loss: 0.010546538564893935\n",
      "[LOG 20200502-15:06:16] epoch: 19136 train-loss: 0.010546528837747045\n",
      "[LOG 20200502-15:06:17] epoch: 19137 train-loss: 0.010546520869765017\n",
      "[LOG 20200502-15:06:17] epoch: 19138 train-loss: 0.010546511970460415\n",
      "[LOG 20200502-15:06:17] epoch: 19139 train-loss: 0.010546503898998102\n",
      "[LOG 20200502-15:06:17] epoch: 19140 train-loss: 0.010546495413614644\n",
      "[LOG 20200502-15:06:18] epoch: 19141 train-loss: 0.010546486928231187\n",
      "[LOG 20200502-15:06:18] epoch: 19142 train-loss: 0.010546479374170303\n",
      "[LOG 20200502-15:06:18] epoch: 19143 train-loss: 0.010546471613148848\n",
      "[LOG 20200502-15:06:18] epoch: 19144 train-loss: 0.010546463955607679\n",
      "[LOG 20200502-15:06:19] epoch: 19145 train-loss: 0.01054645681546794\n",
      "[LOG 20200502-15:06:19] epoch: 19146 train-loss: 0.010546448847485913\n",
      "[LOG 20200502-15:06:19] epoch: 19147 train-loss: 0.010546441189944744\n",
      "[LOG 20200502-15:06:19] epoch: 19148 train-loss: 0.010546434670686722\n",
      "[LOG 20200502-15:06:20] epoch: 19149 train-loss: 0.010546426806184981\n",
      "[LOG 20200502-15:06:20] epoch: 19150 train-loss: 0.010546419769525528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-15:06:20] epoch: 19151 train-loss: 0.010546412318944931\n",
      "[LOG 20200502-15:06:20] epoch: 19152 train-loss: 0.010546405282285478\n",
      "[LOG 20200502-15:06:21] epoch: 19153 train-loss: 0.010546397831704881\n",
      "[LOG 20200502-15:06:21] epoch: 19154 train-loss: 0.010546390484604571\n",
      "[LOG 20200502-15:06:21] epoch: 19155 train-loss: 0.010546382723583115\n",
      "[LOG 20200502-15:06:21] epoch: 19156 train-loss: 0.01054637496256166\n",
      "[LOG 20200502-15:06:22] epoch: 19157 train-loss: 0.010546367201540206\n",
      "[LOG 20200502-15:06:22] epoch: 19158 train-loss: 0.010546359130077891\n",
      "[LOG 20200502-15:06:22] epoch: 19159 train-loss: 0.010546350851655006\n",
      "[LOG 20200502-15:06:22] epoch: 19160 train-loss: 0.010546342573232122\n",
      "[LOG 20200502-15:06:23] epoch: 19161 train-loss: 0.01054633367392752\n",
      "[LOG 20200502-15:06:23] epoch: 19162 train-loss: 0.010546324567662345\n",
      "[LOG 20200502-15:06:23] epoch: 19163 train-loss: 0.010546315150956312\n",
      "[LOG 20200502-15:06:23] epoch: 19164 train-loss: 0.01054630521684885\n",
      "[LOG 20200502-15:06:24] epoch: 19165 train-loss: 0.010546295800142817\n",
      "[LOG 20200502-15:06:24] epoch: 19166 train-loss: 0.01054628524515364\n",
      "[LOG 20200502-15:06:24] epoch: 19167 train-loss: 0.010546274276243316\n",
      "[LOG 20200502-15:06:24] epoch: 19168 train-loss: 0.01054626413517528\n",
      "[LOG 20200502-15:06:25] epoch: 19169 train-loss: 0.010546252131462097\n",
      "[LOG 20200502-15:06:25] epoch: 19170 train-loss: 0.010546240024268627\n",
      "[LOG 20200502-15:06:25] epoch: 19171 train-loss: 0.010546227917075157\n",
      "[LOG 20200502-15:06:25] epoch: 19172 train-loss: 0.010546215395960543\n",
      "[LOG 20200502-15:06:26] epoch: 19173 train-loss: 0.010546203185286786\n",
      "[LOG 20200502-15:06:26] epoch: 19174 train-loss: 0.010546189629369311\n",
      "[LOG 20200502-15:06:26] epoch: 19175 train-loss: 0.010546176176932123\n",
      "[LOG 20200502-15:06:26] epoch: 19176 train-loss: 0.010546162414054075\n",
      "[LOG 20200502-15:06:27] epoch: 19177 train-loss: 0.010546148340735171\n",
      "[LOG 20200502-15:06:27] epoch: 19178 train-loss: 0.010546133646534549\n",
      "[LOG 20200502-15:06:27] epoch: 19179 train-loss: 0.010546119159294499\n",
      "[LOG 20200502-15:06:27] epoch: 19180 train-loss: 0.010546104258133305\n",
      "[LOG 20200502-15:06:27] epoch: 19181 train-loss: 0.010546089046531253\n",
      "[LOG 20200502-15:06:28] epoch: 19182 train-loss: 0.010546073317527771\n",
      "[LOG 20200502-15:06:28] epoch: 19183 train-loss: 0.010546058105925718\n",
      "[LOG 20200502-15:06:28] epoch: 19184 train-loss: 0.010546042376922237\n",
      "[LOG 20200502-15:06:28] epoch: 19185 train-loss: 0.01054602675139904\n",
      "[LOG 20200502-15:06:29] epoch: 19186 train-loss: 0.010546011332836416\n",
      "[LOG 20200502-15:06:29] epoch: 19187 train-loss: 0.010545995707313219\n",
      "[LOG 20200502-15:06:29] epoch: 19188 train-loss: 0.010545979978309738\n",
      "[LOG 20200502-15:06:29] epoch: 19189 train-loss: 0.010545964456266828\n",
      "[LOG 20200502-15:06:30] epoch: 19190 train-loss: 0.010545949141184488\n",
      "[LOG 20200502-15:06:30] epoch: 19191 train-loss: 0.01054593413654301\n",
      "[LOG 20200502-15:06:30] epoch: 19192 train-loss: 0.010545918924940957\n",
      "[LOG 20200502-15:06:30] epoch: 19193 train-loss: 0.010545904230740335\n",
      "[LOG 20200502-15:06:31] epoch: 19194 train-loss: 0.01054589015742143\n",
      "[LOG 20200502-15:06:31] epoch: 19195 train-loss: 0.010545876291063096\n",
      "[LOG 20200502-15:06:31] epoch: 19196 train-loss: 0.010545862735145621\n",
      "[LOG 20200502-15:06:31] epoch: 19197 train-loss: 0.010545849489669004\n",
      "[LOG 20200502-15:06:32] epoch: 19198 train-loss: 0.010545836554633247\n",
      "[LOG 20200502-15:06:32] epoch: 19199 train-loss: 0.010545823826558061\n",
      "[LOG 20200502-15:06:32] epoch: 19200 train-loss: 0.010545811822844876\n",
      "[LOG 20200502-15:06:32] epoch: 19201 train-loss: 0.01054580064697398\n",
      "[LOG 20200502-15:06:33] epoch: 19202 train-loss: 0.010545790298945375\n",
      "[LOG 20200502-15:06:33] epoch: 19203 train-loss: 0.01054577984743648\n",
      "[LOG 20200502-15:06:33] epoch: 19204 train-loss: 0.010545769809848733\n",
      "[LOG 20200502-15:06:33] epoch: 19205 train-loss: 0.0105457603931427\n",
      "[LOG 20200502-15:06:34] epoch: 19206 train-loss: 0.01054575170079867\n",
      "[LOG 20200502-15:06:34] epoch: 19207 train-loss: 0.01054574352585607\n",
      "[LOG 20200502-15:06:34] epoch: 19208 train-loss: 0.010545735557874044\n",
      "[LOG 20200502-15:06:34] epoch: 19209 train-loss: 0.010545727382931445\n",
      "[LOG 20200502-15:06:35] epoch: 19210 train-loss: 0.01054572169151571\n",
      "[LOG 20200502-15:06:35] epoch: 19211 train-loss: 0.010545714551375972\n",
      "[LOG 20200502-15:06:35] epoch: 19212 train-loss: 0.010545708342558809\n",
      "[LOG 20200502-15:06:35] epoch: 19213 train-loss: 0.010545702547662787\n",
      "[LOG 20200502-15:06:36] epoch: 19214 train-loss: 0.010545697270168198\n",
      "[LOG 20200502-15:06:36] epoch: 19215 train-loss: 0.010545692820515897\n",
      "[LOG 20200502-15:06:36] epoch: 19216 train-loss: 0.010545688577824168\n",
      "[LOG 20200502-15:06:36] epoch: 19217 train-loss: 0.010545684128171869\n",
      "[LOG 20200502-15:06:37] epoch: 19218 train-loss: 0.010545680092440711\n",
      "[LOG 20200502-15:06:37] epoch: 19219 train-loss: 0.01054567564278841\n",
      "[LOG 20200502-15:06:37] epoch: 19220 train-loss: 0.010545672124458684\n",
      "[LOG 20200502-15:06:37] epoch: 19221 train-loss: 0.010545668813089529\n",
      "[LOG 20200502-15:06:38] epoch: 19222 train-loss: 0.01054566508779923\n",
      "[LOG 20200502-15:06:38] epoch: 19223 train-loss: 0.010545661879910363\n",
      "[LOG 20200502-15:06:38] epoch: 19224 train-loss: 0.010545658258100351\n",
      "[LOG 20200502-15:06:38] epoch: 19225 train-loss: 0.010545655257172055\n",
      "[LOG 20200502-15:06:38] epoch: 19226 train-loss: 0.010545651635362042\n",
      "[LOG 20200502-15:06:39] epoch: 19227 train-loss: 0.010545648323992888\n",
      "[LOG 20200502-15:06:39] epoch: 19228 train-loss: 0.010545644288261732\n",
      "[LOG 20200502-15:06:39] epoch: 19229 train-loss: 0.010545640666451719\n",
      "[LOG 20200502-15:06:40] epoch: 19230 train-loss: 0.010545637562043138\n",
      "[LOG 20200502-15:06:40] epoch: 19231 train-loss: 0.010545632594989406\n",
      "[LOG 20200502-15:06:40] epoch: 19232 train-loss: 0.010545628145337105\n",
      "[LOG 20200502-15:06:40] epoch: 19233 train-loss: 0.010545623385243945\n",
      "[LOG 20200502-15:06:40] epoch: 19234 train-loss: 0.010545618728631072\n",
      "[LOG 20200502-15:06:41] epoch: 19235 train-loss: 0.010545613347656198\n",
      "[LOG 20200502-15:06:41] epoch: 19236 train-loss: 0.010545607345799604\n",
      "[LOG 20200502-15:06:41] epoch: 19237 train-loss: 0.010545601033502154\n",
      "[LOG 20200502-15:06:41] epoch: 19238 train-loss: 0.010545594514244132\n",
      "[LOG 20200502-15:06:42] epoch: 19239 train-loss: 0.01054558799498611\n",
      "[LOG 20200502-15:06:42] epoch: 19240 train-loss: 0.010545580233964656\n",
      "[LOG 20200502-15:06:42] epoch: 19241 train-loss: 0.010545572576423487\n",
      "[LOG 20200502-15:06:42] epoch: 19242 train-loss: 0.010545564504961172\n",
      "[LOG 20200502-15:06:43] epoch: 19243 train-loss: 0.010545555916097429\n",
      "[LOG 20200502-15:06:43] epoch: 19244 train-loss: 0.010545547430713972\n",
      "[LOG 20200502-15:06:43] epoch: 19245 train-loss: 0.010545537393126223\n",
      "[LOG 20200502-15:06:43] epoch: 19246 train-loss: 0.01054552745901876\n",
      "[LOG 20200502-15:06:44] epoch: 19247 train-loss: 0.010545517938832441\n",
      "[LOG 20200502-15:06:44] epoch: 19248 train-loss: 0.010545507590803836\n",
      "[LOG 20200502-15:06:44] epoch: 19249 train-loss: 0.010545496104492081\n",
      "[LOG 20200502-15:06:44] epoch: 19250 train-loss: 0.010545485756463475\n",
      "[LOG 20200502-15:06:45] epoch: 19251 train-loss: 0.010545474477112293\n",
      "[LOG 20200502-15:06:45] epoch: 19252 train-loss: 0.010545463197761111\n",
      "[LOG 20200502-15:06:45] epoch: 19253 train-loss: 0.010545450883607069\n",
      "[LOG 20200502-15:06:45] epoch: 19254 train-loss: 0.010545439707736174\n",
      "[LOG 20200502-15:06:46] epoch: 19255 train-loss: 0.010545427807503276\n",
      "[LOG 20200502-15:06:46] epoch: 19256 train-loss: 0.010545416010750664\n",
      "[LOG 20200502-15:06:46] epoch: 19257 train-loss: 0.010545403282675479\n",
      "[LOG 20200502-15:06:46] epoch: 19258 train-loss: 0.010545391796363724\n",
      "[LOG 20200502-15:06:47] epoch: 19259 train-loss: 0.01054537927524911\n",
      "[LOG 20200502-15:06:47] epoch: 19260 train-loss: 0.010545367685457071\n",
      "[LOG 20200502-15:06:47] epoch: 19261 train-loss: 0.010545355888704458\n",
      "[LOG 20200502-15:06:47] epoch: 19262 train-loss: 0.010545343884991275\n",
      "[LOG 20200502-15:06:48] epoch: 19263 train-loss: 0.010545332502159808\n",
      "[LOG 20200502-15:06:48] epoch: 19264 train-loss: 0.010545320808887482\n",
      "[LOG 20200502-15:06:48] epoch: 19265 train-loss: 0.010545309736496873\n",
      "[LOG 20200502-15:06:48] epoch: 19266 train-loss: 0.010545298250185119\n",
      "[LOG 20200502-15:06:48] epoch: 19267 train-loss: 0.010545287902156511\n",
      "[LOG 20200502-15:06:49] epoch: 19268 train-loss: 0.0105452761054039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-15:06:49] epoch: 19269 train-loss: 0.010545265757375293\n",
      "[LOG 20200502-15:06:49] epoch: 19270 train-loss: 0.0105452553058664\n",
      "[LOG 20200502-15:06:49] epoch: 19271 train-loss: 0.010545245475239225\n",
      "[LOG 20200502-15:06:50] epoch: 19272 train-loss: 0.010545234920250045\n",
      "[LOG 20200502-15:06:50] epoch: 19273 train-loss: 0.01054522529658344\n",
      "[LOG 20200502-15:06:50] epoch: 19274 train-loss: 0.01054521556943655\n",
      "[LOG 20200502-15:06:50] epoch: 19275 train-loss: 0.010545205945769945\n",
      "[LOG 20200502-15:06:51] epoch: 19276 train-loss: 0.010545196322103342\n",
      "[LOG 20200502-15:06:51] epoch: 19277 train-loss: 0.010545187215838168\n",
      "[LOG 20200502-15:06:51] epoch: 19278 train-loss: 0.010545178109572994\n",
      "[LOG 20200502-15:06:51] epoch: 19279 train-loss: 0.01054516900330782\n",
      "[LOG 20200502-15:06:52] epoch: 19280 train-loss: 0.010545159690082073\n",
      "[LOG 20200502-15:06:52] epoch: 19281 train-loss: 0.010545150583816899\n",
      "[LOG 20200502-15:06:52] epoch: 19282 train-loss: 0.010545141477551725\n",
      "[LOG 20200502-15:06:52] epoch: 19283 train-loss: 0.010545132785207696\n",
      "[LOG 20200502-15:06:53] epoch: 19284 train-loss: 0.010545123885903094\n",
      "[LOG 20200502-15:06:53] epoch: 19285 train-loss: 0.01054511477963792\n",
      "[LOG 20200502-15:06:53] epoch: 19286 train-loss: 0.010545105673372746\n",
      "[LOG 20200502-15:06:53] epoch: 19287 train-loss: 0.010545096567107571\n",
      "[LOG 20200502-15:06:54] epoch: 19288 train-loss: 0.010545087460842397\n",
      "[LOG 20200502-15:06:54] epoch: 19289 train-loss: 0.010545077526734935\n",
      "[LOG 20200502-15:06:54] epoch: 19290 train-loss: 0.010545068316989474\n",
      "[LOG 20200502-15:06:54] epoch: 19291 train-loss: 0.010545058900283443\n",
      "[LOG 20200502-15:06:55] epoch: 19292 train-loss: 0.010545048655735122\n",
      "[LOG 20200502-15:06:55] epoch: 19293 train-loss: 0.010545038100745942\n",
      "[LOG 20200502-15:06:55] epoch: 19294 train-loss: 0.010545028373599052\n",
      "[LOG 20200502-15:06:55] epoch: 19295 train-loss: 0.010545017404688729\n",
      "[LOG 20200502-15:06:56] epoch: 19296 train-loss: 0.010545006228817834\n",
      "[LOG 20200502-15:06:56] epoch: 19297 train-loss: 0.010544995156427225\n",
      "[LOG 20200502-15:06:56] epoch: 19298 train-loss: 0.01054498398055633\n",
      "[LOG 20200502-15:06:56] epoch: 19299 train-loss: 0.01054497187336286\n",
      "[LOG 20200502-15:06:56] epoch: 19300 train-loss: 0.010544959973129962\n",
      "[LOG 20200502-15:06:57] epoch: 19301 train-loss: 0.010544947555495633\n",
      "[LOG 20200502-15:06:57] epoch: 19302 train-loss: 0.010544934516979588\n",
      "[LOG 20200502-15:06:57] epoch: 19303 train-loss: 0.010544921685424116\n",
      "[LOG 20200502-15:06:57] epoch: 19304 train-loss: 0.010544908026026355\n",
      "[LOG 20200502-15:06:58] epoch: 19305 train-loss: 0.010544894780549739\n",
      "[LOG 20200502-15:06:58] epoch: 19306 train-loss: 0.01054488050027026\n",
      "[LOG 20200502-15:06:58] epoch: 19307 train-loss: 0.010544866116510497\n",
      "[LOG 20200502-15:06:58] epoch: 19308 train-loss: 0.010544851525790162\n",
      "[LOG 20200502-15:06:59] epoch: 19309 train-loss: 0.010544836728109254\n",
      "[LOG 20200502-15:06:59] epoch: 19310 train-loss: 0.010544821206066344\n",
      "[LOG 20200502-15:06:59] epoch: 19311 train-loss: 0.010544805684023432\n",
      "[LOG 20200502-15:07:00] epoch: 19312 train-loss: 0.010544790368941095\n",
      "[LOG 20200502-15:07:00] epoch: 19313 train-loss: 0.010544774329496754\n",
      "[LOG 20200502-15:07:00] epoch: 19314 train-loss: 0.0105447583935327\n",
      "[LOG 20200502-15:07:00] epoch: 19315 train-loss: 0.010544742043647502\n",
      "[LOG 20200502-15:07:01] epoch: 19316 train-loss: 0.010544726107683446\n",
      "[LOG 20200502-15:07:01] epoch: 19317 train-loss: 0.01054470944735739\n",
      "[LOG 20200502-15:07:01] epoch: 19318 train-loss: 0.010544692683551047\n",
      "[LOG 20200502-15:07:01] epoch: 19319 train-loss: 0.010544676333665848\n",
      "[LOG 20200502-15:07:02] epoch: 19320 train-loss: 0.010544659983780649\n",
      "[LOG 20200502-15:07:02] epoch: 19321 train-loss: 0.010544643737375736\n",
      "[LOG 20200502-15:07:02] epoch: 19322 train-loss: 0.01054462811185254\n",
      "[LOG 20200502-15:07:02] epoch: 19323 train-loss: 0.010544611658487055\n",
      "[LOG 20200502-15:07:03] epoch: 19324 train-loss: 0.010544596446885003\n",
      "[LOG 20200502-15:07:03] epoch: 19325 train-loss: 0.010544580407440662\n",
      "[LOG 20200502-15:07:03] epoch: 19326 train-loss: 0.010544564988878038\n",
      "[LOG 20200502-15:07:03] epoch: 19327 train-loss: 0.010544550398157703\n",
      "[LOG 20200502-15:07:04] epoch: 19328 train-loss: 0.010544535600476794\n",
      "[LOG 20200502-15:07:04] epoch: 19329 train-loss: 0.010544521216717031\n",
      "[LOG 20200502-15:07:04] epoch: 19330 train-loss: 0.01054450703991784\n",
      "[LOG 20200502-15:07:04] epoch: 19331 train-loss: 0.010544493587480651\n",
      "[LOG 20200502-15:07:05] epoch: 19332 train-loss: 0.010544480548964607\n",
      "[LOG 20200502-15:07:05] epoch: 19333 train-loss: 0.010544467406968275\n",
      "[LOG 20200502-15:07:05] epoch: 19334 train-loss: 0.010544455403255092\n",
      "[LOG 20200502-15:07:05] epoch: 19335 train-loss: 0.010544443916943338\n",
      "[LOG 20200502-15:07:06] epoch: 19336 train-loss: 0.010544433051513301\n",
      "[LOG 20200502-15:07:06] epoch: 19337 train-loss: 0.010544421565201547\n",
      "[LOG 20200502-15:07:06] epoch: 19338 train-loss: 0.010544411941534944\n",
      "[LOG 20200502-15:07:06] epoch: 19339 train-loss: 0.010544401800466908\n",
      "[LOG 20200502-15:07:07] epoch: 19340 train-loss: 0.010544392487241162\n",
      "[LOG 20200502-15:07:07] epoch: 19341 train-loss: 0.010544383484456275\n",
      "[LOG 20200502-15:07:07] epoch: 19342 train-loss: 0.010544375206033388\n",
      "[LOG 20200502-15:07:07] epoch: 19343 train-loss: 0.010544366720649932\n",
      "[LOG 20200502-15:07:08] epoch: 19344 train-loss: 0.010544359994431337\n",
      "[LOG 20200502-15:07:08] epoch: 19345 train-loss: 0.010544352233409882\n",
      "[LOG 20200502-15:07:08] epoch: 19346 train-loss: 0.010544346128073003\n",
      "[LOG 20200502-15:07:08] epoch: 19347 train-loss: 0.010544339815775553\n",
      "[LOG 20200502-15:07:09] epoch: 19348 train-loss: 0.01054433329651753\n",
      "[LOG 20200502-15:07:09] epoch: 19349 train-loss: 0.010544327605101798\n",
      "[LOG 20200502-15:07:09] epoch: 19350 train-loss: 0.010544322327607207\n",
      "[LOG 20200502-15:07:09] epoch: 19351 train-loss: 0.01054431725707319\n",
      "[LOG 20200502-15:07:09] epoch: 19352 train-loss: 0.010544312393499745\n",
      "[LOG 20200502-15:07:10] epoch: 19353 train-loss: 0.010544307322965728\n",
      "[LOG 20200502-15:07:10] epoch: 19354 train-loss: 0.010544302873313427\n",
      "[LOG 20200502-15:07:10] epoch: 19355 train-loss: 0.010544297699299123\n",
      "[LOG 20200502-15:07:11] epoch: 19356 train-loss: 0.010544293663567968\n",
      "[LOG 20200502-15:07:11] epoch: 19357 train-loss: 0.010544289213915667\n",
      "[LOG 20200502-15:07:11] epoch: 19358 train-loss: 0.010544284453822507\n",
      "[LOG 20200502-15:07:11] epoch: 19359 train-loss: 0.010544279693729348\n",
      "[LOG 20200502-15:07:11] epoch: 19360 train-loss: 0.01054427545103762\n",
      "[LOG 20200502-15:07:12] epoch: 19361 train-loss: 0.01054427069094446\n",
      "[LOG 20200502-15:07:12] epoch: 19362 train-loss: 0.010544266034331586\n",
      "[LOG 20200502-15:07:12] epoch: 19363 train-loss: 0.010544260446396139\n",
      "[LOG 20200502-15:07:12] epoch: 19364 train-loss: 0.010544255479342408\n",
      "[LOG 20200502-15:07:13] epoch: 19365 train-loss: 0.010544250098367533\n",
      "[LOG 20200502-15:07:13] epoch: 19366 train-loss: 0.010544243786070082\n",
      "[LOG 20200502-15:07:13] epoch: 19367 train-loss: 0.010544237680733204\n",
      "[LOG 20200502-15:07:13] epoch: 19368 train-loss: 0.010544231264955468\n",
      "[LOG 20200502-15:07:14] epoch: 19369 train-loss: 0.01054422464221716\n",
      "[LOG 20200502-15:07:14] epoch: 19370 train-loss: 0.010544217191636562\n",
      "[LOG 20200502-15:07:14] epoch: 19371 train-loss: 0.010544209637575679\n",
      "[LOG 20200502-15:07:14] epoch: 19372 train-loss: 0.010544202083514797\n",
      "[LOG 20200502-15:07:15] epoch: 19373 train-loss: 0.010544193701611625\n",
      "[LOG 20200502-15:07:15] epoch: 19374 train-loss: 0.01054418459534645\n",
      "[LOG 20200502-15:07:15] epoch: 19375 train-loss: 0.010544175489081277\n",
      "[LOG 20200502-15:07:15] epoch: 19376 train-loss: 0.01054416617585553\n",
      "[LOG 20200502-15:07:16] epoch: 19377 train-loss: 0.010544156552188926\n",
      "[LOG 20200502-15:07:16] epoch: 19378 train-loss: 0.010544146825042035\n",
      "[LOG 20200502-15:07:16] epoch: 19379 train-loss: 0.010544135545690855\n",
      "[LOG 20200502-15:07:16] epoch: 19380 train-loss: 0.010544124473300245\n",
      "[LOG 20200502-15:07:17] epoch: 19381 train-loss: 0.010544113918311067\n",
      "[LOG 20200502-15:07:17] epoch: 19382 train-loss: 0.010544102535479598\n",
      "[LOG 20200502-15:07:17] epoch: 19383 train-loss: 0.010544090738726987\n",
      "[LOG 20200502-15:07:17] epoch: 19384 train-loss: 0.010544078735013803\n",
      "[LOG 20200502-15:07:17] epoch: 19385 train-loss: 0.01054406724870205\n",
      "[LOG 20200502-15:07:18] epoch: 19386 train-loss: 0.01054405514150858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-15:07:18] epoch: 19387 train-loss: 0.010544042930834822\n",
      "[LOG 20200502-15:07:18] epoch: 19388 train-loss: 0.010544030099279351\n",
      "[LOG 20200502-15:07:18] epoch: 19389 train-loss: 0.010544017888605595\n",
      "[LOG 20200502-15:07:19] epoch: 19390 train-loss: 0.010544005781412125\n",
      "[LOG 20200502-15:07:19] epoch: 19391 train-loss: 0.010543993467258083\n",
      "[LOG 20200502-15:07:19] epoch: 19392 train-loss: 0.01054398063570261\n",
      "[LOG 20200502-15:07:19] epoch: 19393 train-loss: 0.010543968631989427\n",
      "[LOG 20200502-15:07:20] epoch: 19394 train-loss: 0.010543956628276242\n",
      "[LOG 20200502-15:07:20] epoch: 19395 train-loss: 0.010543944521082772\n",
      "[LOG 20200502-15:07:20] epoch: 19396 train-loss: 0.010543932103448443\n",
      "[LOG 20200502-15:07:20] epoch: 19397 train-loss: 0.010543920927577548\n",
      "[LOG 20200502-15:07:21] epoch: 19398 train-loss: 0.010543908716903793\n",
      "[LOG 20200502-15:07:21] epoch: 19399 train-loss: 0.010543897334072325\n",
      "[LOG 20200502-15:07:21] epoch: 19400 train-loss: 0.010543885744280286\n",
      "[LOG 20200502-15:07:21] epoch: 19401 train-loss: 0.01054387508581082\n",
      "[LOG 20200502-15:07:22] epoch: 19402 train-loss: 0.01054386380645964\n",
      "[LOG 20200502-15:07:22] epoch: 19403 train-loss: 0.010543852837549316\n",
      "[LOG 20200502-15:07:22] epoch: 19404 train-loss: 0.010543842282560136\n",
      "[LOG 20200502-15:07:22] epoch: 19405 train-loss: 0.010543832244972387\n",
      "[LOG 20200502-15:07:23] epoch: 19406 train-loss: 0.01054382137954235\n",
      "[LOG 20200502-15:07:23] epoch: 19407 train-loss: 0.010543811445434889\n",
      "[LOG 20200502-15:07:23] epoch: 19408 train-loss: 0.010543801511327425\n",
      "[LOG 20200502-15:07:23] epoch: 19409 train-loss: 0.01054379116329882\n",
      "[LOG 20200502-15:07:24] epoch: 19410 train-loss: 0.010543781850073073\n",
      "[LOG 20200502-15:07:24] epoch: 19411 train-loss: 0.010543771812485324\n",
      "[LOG 20200502-15:07:24] epoch: 19412 train-loss: 0.010543762292299006\n",
      "[LOG 20200502-15:07:24] epoch: 19413 train-loss: 0.010543752668632401\n",
      "[LOG 20200502-15:07:25] epoch: 19414 train-loss: 0.010543743044965796\n",
      "[LOG 20200502-15:07:25] epoch: 19415 train-loss: 0.010543733110858334\n",
      "[LOG 20200502-15:07:25] epoch: 19416 train-loss: 0.010543723590672016\n",
      "[LOG 20200502-15:07:25] epoch: 19417 train-loss: 0.010543714173965983\n",
      "[LOG 20200502-15:07:26] epoch: 19418 train-loss: 0.010543704136378236\n",
      "[LOG 20200502-15:07:26] epoch: 19419 train-loss: 0.010543694719672203\n",
      "[LOG 20200502-15:07:26] epoch: 19420 train-loss: 0.010543684682084454\n",
      "[LOG 20200502-15:07:26] epoch: 19421 train-loss: 0.010543674437536133\n",
      "[LOG 20200502-15:07:27] epoch: 19422 train-loss: 0.010543664399948385\n",
      "[LOG 20200502-15:07:27] epoch: 19423 train-loss: 0.010543654362360636\n",
      "[LOG 20200502-15:07:27] epoch: 19424 train-loss: 0.010543643600410886\n",
      "[LOG 20200502-15:07:27] epoch: 19425 train-loss: 0.01054363294194142\n",
      "[LOG 20200502-15:07:28] epoch: 19426 train-loss: 0.010543621869550811\n",
      "[LOG 20200502-15:07:28] epoch: 19427 train-loss: 0.010543610900640488\n",
      "[LOG 20200502-15:07:28] epoch: 19428 train-loss: 0.010543599931730164\n",
      "[LOG 20200502-15:07:28] epoch: 19429 train-loss: 0.010543588134977553\n",
      "[LOG 20200502-15:07:29] epoch: 19430 train-loss: 0.01054357633822494\n",
      "[LOG 20200502-15:07:29] epoch: 19431 train-loss: 0.010543564644952616\n",
      "[LOG 20200502-15:07:29] epoch: 19432 train-loss: 0.010543551502956284\n",
      "[LOG 20200502-15:07:29] epoch: 19433 train-loss: 0.010543539188802242\n",
      "[LOG 20200502-15:07:30] epoch: 19434 train-loss: 0.010543525943325626\n",
      "[LOG 20200502-15:07:30] epoch: 19435 train-loss: 0.010543512076967292\n",
      "[LOG 20200502-15:07:30] epoch: 19436 train-loss: 0.010543498934970962\n",
      "[LOG 20200502-15:07:30] epoch: 19437 train-loss: 0.010543485172092915\n",
      "[LOG 20200502-15:07:31] epoch: 19438 train-loss: 0.010543471305734582\n",
      "[LOG 20200502-15:07:31] epoch: 19439 train-loss: 0.010543456197612815\n",
      "[LOG 20200502-15:07:31] epoch: 19440 train-loss: 0.01054344212429391\n",
      "[LOG 20200502-15:07:31] epoch: 19441 train-loss: 0.010543427533573575\n",
      "[LOG 20200502-15:07:32] epoch: 19442 train-loss: 0.01054341159760952\n",
      "[LOG 20200502-15:07:32] epoch: 19443 train-loss: 0.010543396282527182\n",
      "[LOG 20200502-15:07:32] epoch: 19444 train-loss: 0.010543380967444844\n",
      "[LOG 20200502-15:07:32] epoch: 19445 train-loss: 0.010543365238441361\n",
      "[LOG 20200502-15:07:33] epoch: 19446 train-loss: 0.01054334950943788\n",
      "[LOG 20200502-15:07:33] epoch: 19447 train-loss: 0.010543333676954111\n",
      "[LOG 20200502-15:07:33] epoch: 19448 train-loss: 0.010543317430549197\n",
      "[LOG 20200502-15:07:33] epoch: 19449 train-loss: 0.010543301598065428\n",
      "[LOG 20200502-15:07:33] epoch: 19450 train-loss: 0.010543285144699944\n",
      "[LOG 20200502-15:07:34] epoch: 19451 train-loss: 0.01054326920873589\n",
      "[LOG 20200502-15:07:34] epoch: 19452 train-loss: 0.010543253583212694\n",
      "[LOG 20200502-15:07:34] epoch: 19453 train-loss: 0.010543237233327495\n",
      "[LOG 20200502-15:07:34] epoch: 19454 train-loss: 0.01054322129736344\n",
      "[LOG 20200502-15:07:35] epoch: 19455 train-loss: 0.010543205464879671\n",
      "[LOG 20200502-15:07:35] epoch: 19456 train-loss: 0.010543190563718477\n",
      "[LOG 20200502-15:07:35] epoch: 19457 train-loss: 0.010543174834714996\n",
      "[LOG 20200502-15:07:35] epoch: 19458 train-loss: 0.010543159623112943\n",
      "[LOG 20200502-15:07:36] epoch: 19459 train-loss: 0.01054314523935318\n",
      "[LOG 20200502-15:07:36] epoch: 19460 train-loss: 0.010543130648632845\n",
      "[LOG 20200502-15:07:36] epoch: 19461 train-loss: 0.010543116678794226\n",
      "[LOG 20200502-15:07:36] epoch: 19462 train-loss: 0.01054310312287675\n",
      "[LOG 20200502-15:07:37] epoch: 19463 train-loss: 0.010543088946077559\n",
      "[LOG 20200502-15:07:37] epoch: 19464 train-loss: 0.010543076424962945\n",
      "[LOG 20200502-15:07:37] epoch: 19465 train-loss: 0.010543064110808902\n",
      "[LOG 20200502-15:07:37] epoch: 19466 train-loss: 0.01054305231405629\n",
      "[LOG 20200502-15:07:38] epoch: 19467 train-loss: 0.010543040310343107\n",
      "[LOG 20200502-15:07:38] epoch: 19468 train-loss: 0.010543029030991925\n",
      "[LOG 20200502-15:07:38] epoch: 19469 train-loss: 0.010543018269042173\n",
      "[LOG 20200502-15:07:38] epoch: 19470 train-loss: 0.010543008231454425\n",
      "[LOG 20200502-15:07:39] epoch: 19471 train-loss: 0.010542998193866678\n",
      "[LOG 20200502-15:07:39] epoch: 19472 train-loss: 0.010542989087601503\n",
      "[LOG 20200502-15:07:39] epoch: 19473 train-loss: 0.010542979774375757\n",
      "[LOG 20200502-15:07:39] epoch: 19474 train-loss: 0.010542972013354301\n",
      "[LOG 20200502-15:07:40] epoch: 19475 train-loss: 0.010542963424490558\n",
      "[LOG 20200502-15:07:40] epoch: 19476 train-loss: 0.01054295628435082\n",
      "[LOG 20200502-15:07:40] epoch: 19477 train-loss: 0.010542949247691367\n",
      "[LOG 20200502-15:07:40] epoch: 19478 train-loss: 0.010542942004071342\n",
      "[LOG 20200502-15:07:41] epoch: 19479 train-loss: 0.010542935070892176\n",
      "[LOG 20200502-15:07:41] epoch: 19480 train-loss: 0.010542929069035582\n",
      "[LOG 20200502-15:07:41] epoch: 19481 train-loss: 0.010542923481100135\n",
      "[LOG 20200502-15:07:41] epoch: 19482 train-loss: 0.010542917686204115\n",
      "[LOG 20200502-15:07:42] epoch: 19483 train-loss: 0.010542911994788382\n",
      "[LOG 20200502-15:07:42] epoch: 19484 train-loss: 0.010542906717293792\n",
      "[LOG 20200502-15:07:42] epoch: 19485 train-loss: 0.010542901439799203\n",
      "[LOG 20200502-15:07:42] epoch: 19486 train-loss: 0.010542896679706044\n",
      "[LOG 20200502-15:07:42] epoch: 19487 train-loss: 0.010542891298731169\n",
      "[LOG 20200502-15:07:43] epoch: 19488 train-loss: 0.010542886435157724\n",
      "[LOG 20200502-15:07:43] epoch: 19489 train-loss: 0.010542881468103992\n",
      "[LOG 20200502-15:07:43] epoch: 19490 train-loss: 0.010542877121931978\n",
      "[LOG 20200502-15:07:44] epoch: 19491 train-loss: 0.010542871844437387\n",
      "[LOG 20200502-15:07:44] epoch: 19492 train-loss: 0.010542866980863942\n",
      "[LOG 20200502-15:07:44] epoch: 19493 train-loss: 0.010542861703369353\n",
      "[LOG 20200502-15:07:44] epoch: 19494 train-loss: 0.01054285621891419\n",
      "[LOG 20200502-15:07:44] epoch: 19495 train-loss: 0.010542850630978743\n",
      "[LOG 20200502-15:07:45] epoch: 19496 train-loss: 0.010542845353484154\n",
      "[LOG 20200502-15:07:45] epoch: 19497 train-loss: 0.010542839455107847\n",
      "[LOG 20200502-15:07:45] epoch: 19498 train-loss: 0.010542833142810397\n",
      "[LOG 20200502-15:07:45] epoch: 19499 train-loss: 0.01054282652007209\n",
      "[LOG 20200502-15:07:46] epoch: 19500 train-loss: 0.01054281989733378\n",
      "[LOG 20200502-15:07:46] epoch: 19501 train-loss: 0.010542812757194042\n",
      "[LOG 20200502-15:07:46] epoch: 19502 train-loss: 0.010542804996172586\n",
      "[LOG 20200502-15:07:46] epoch: 19503 train-loss: 0.01054279754559199\n",
      "[LOG 20200502-15:07:47] epoch: 19504 train-loss: 0.010542789474129677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-15:07:47] epoch: 19505 train-loss: 0.01054278047134479\n",
      "[LOG 20200502-15:07:47] epoch: 19506 train-loss: 0.010542772089441618\n",
      "[LOG 20200502-15:07:47] epoch: 19507 train-loss: 0.010542762776215872\n",
      "[LOG 20200502-15:07:48] epoch: 19508 train-loss: 0.010542753359509839\n",
      "[LOG 20200502-15:07:48] epoch: 19509 train-loss: 0.010542743528882662\n",
      "[LOG 20200502-15:07:48] epoch: 19510 train-loss: 0.010542733387814628\n",
      "[LOG 20200502-15:07:48] epoch: 19511 train-loss: 0.010542722936305735\n",
      "[LOG 20200502-15:07:49] epoch: 19512 train-loss: 0.010542712174355984\n",
      "[LOG 20200502-15:07:49] epoch: 19513 train-loss: 0.01054270120544566\n",
      "[LOG 20200502-15:07:49] epoch: 19514 train-loss: 0.01054268992609448\n",
      "[LOG 20200502-15:07:49] epoch: 19515 train-loss: 0.010542678957184156\n",
      "[LOG 20200502-15:07:50] epoch: 19516 train-loss: 0.010542667367392115\n",
      "[LOG 20200502-15:07:50] epoch: 19517 train-loss: 0.010542655053238073\n",
      "[LOG 20200502-15:07:50] epoch: 19518 train-loss: 0.01054264408432775\n",
      "[LOG 20200502-15:07:50] epoch: 19519 train-loss: 0.010542631666693423\n",
      "[LOG 20200502-15:07:51] epoch: 19520 train-loss: 0.010542619973421097\n",
      "[LOG 20200502-15:07:51] epoch: 19521 train-loss: 0.010542608176668486\n",
      "[LOG 20200502-15:07:51] epoch: 19522 train-loss: 0.010542596379915873\n",
      "[LOG 20200502-15:07:51] epoch: 19523 train-loss: 0.01054258406576183\n",
      "[LOG 20200502-15:07:52] epoch: 19524 train-loss: 0.010542572475969791\n",
      "[LOG 20200502-15:07:52] epoch: 19525 train-loss: 0.010542560575736893\n",
      "[LOG 20200502-15:07:52] epoch: 19526 train-loss: 0.01054254908942514\n",
      "[LOG 20200502-15:07:52] epoch: 19527 train-loss: 0.010542537603113387\n",
      "[LOG 20200502-15:07:53] epoch: 19528 train-loss: 0.01054252590984106\n",
      "[LOG 20200502-15:07:53] epoch: 19529 train-loss: 0.010542514630489878\n",
      "[LOG 20200502-15:07:53] epoch: 19530 train-loss: 0.010542503454618983\n",
      "[LOG 20200502-15:07:53] epoch: 19530 new best train-loss: 0.010542503454618983 found\n",
      "[LOG 20200502-15:07:53] epoch: 19531 train-loss: 0.010542492175267803\n",
      "[LOG 20200502-15:07:54] epoch: 19532 train-loss: 0.010542481620278623\n",
      "[LOG 20200502-15:07:54] epoch: 19533 train-loss: 0.010542471065289445\n",
      "[LOG 20200502-15:07:54] epoch: 19534 train-loss: 0.010542460717260838\n",
      "[LOG 20200502-15:07:54] epoch: 19535 train-loss: 0.010542450472712517\n",
      "[LOG 20200502-15:07:55] epoch: 19536 train-loss: 0.010542440849045912\n",
      "[LOG 20200502-15:07:55] epoch: 19537 train-loss: 0.010542429983615875\n",
      "[LOG 20200502-15:07:55] epoch: 19538 train-loss: 0.010542420049508413\n",
      "[LOG 20200502-15:07:55] epoch: 19539 train-loss: 0.010542410529322095\n",
      "[LOG 20200502-15:07:56] epoch: 19540 train-loss: 0.010542400698694918\n",
      "[LOG 20200502-15:07:56] epoch: 19540 new best train-loss: 0.010542400698694918 found\n",
      "[LOG 20200502-15:07:56] epoch: 19541 train-loss: 0.010542390868067741\n",
      "[LOG 20200502-15:07:56] epoch: 19542 train-loss: 0.010542381347881423\n",
      "[LOG 20200502-15:07:56] epoch: 19543 train-loss: 0.01054237193117539\n",
      "[LOG 20200502-15:07:57] epoch: 19544 train-loss: 0.010542362617949644\n",
      "[LOG 20200502-15:07:57] epoch: 19545 train-loss: 0.010542352890802754\n",
      "[LOG 20200502-15:07:57] epoch: 19546 train-loss: 0.01054234326713615\n",
      "[LOG 20200502-15:07:57] epoch: 19547 train-loss: 0.010542333953910403\n",
      "[LOG 20200502-15:07:58] epoch: 19548 train-loss: 0.010542324640684657\n",
      "[LOG 20200502-15:07:58] epoch: 19549 train-loss: 0.010542314913537767\n",
      "[LOG 20200502-15:07:58] epoch: 19550 train-loss: 0.010542304979430305\n",
      "[LOG 20200502-15:07:58] epoch: 19550 new best train-loss: 0.010542304979430305 found\n",
      "[LOG 20200502-15:07:58] epoch: 19551 train-loss: 0.010542295769684844\n",
      "[LOG 20200502-15:07:59] epoch: 19552 train-loss: 0.010542285214695666\n",
      "[LOG 20200502-15:07:59] epoch: 19553 train-loss: 0.010542275694509348\n",
      "[LOG 20200502-15:07:59] epoch: 19554 train-loss: 0.010542265036039881\n",
      "[LOG 20200502-15:07:59] epoch: 19555 train-loss: 0.01054225510193242\n",
      "[LOG 20200502-15:08:00] epoch: 19556 train-loss: 0.010542244339982668\n",
      "[LOG 20200502-15:08:00] epoch: 19557 train-loss: 0.01054223326759206\n",
      "[LOG 20200502-15:08:00] epoch: 19558 train-loss: 0.010542222402162023\n",
      "[LOG 20200502-15:08:00] epoch: 19559 train-loss: 0.01054221112281084\n",
      "[LOG 20200502-15:08:01] epoch: 19560 train-loss: 0.01054219932605823\n",
      "[LOG 20200502-15:08:01] epoch: 19560 new best train-loss: 0.01054219932605823 found\n",
      "[LOG 20200502-15:08:01] epoch: 19561 train-loss: 0.010542187529305616\n",
      "[LOG 20200502-15:08:01] epoch: 19562 train-loss: 0.010542175939513577\n",
      "[LOG 20200502-15:08:01] epoch: 19563 train-loss: 0.010542163728839822\n",
      "[LOG 20200502-15:08:02] epoch: 19564 train-loss: 0.010542150690323777\n",
      "[LOG 20200502-15:08:02] epoch: 19565 train-loss: 0.010542137858768305\n",
      "[LOG 20200502-15:08:02] epoch: 19566 train-loss: 0.010542125234173404\n",
      "[LOG 20200502-15:08:02] epoch: 19567 train-loss: 0.01054211136781507\n",
      "[LOG 20200502-15:08:02] epoch: 19568 train-loss: 0.010542098018858168\n",
      "[LOG 20200502-15:08:03] epoch: 19569 train-loss: 0.010542084152499834\n",
      "[LOG 20200502-15:08:03] epoch: 19570 train-loss: 0.010542070286141502\n",
      "[LOG 20200502-15:08:03] epoch: 19570 new best train-loss: 0.010542070286141502 found\n",
      "[LOG 20200502-15:08:03] epoch: 19571 train-loss: 0.010542055695421167\n",
      "[LOG 20200502-15:08:04] epoch: 19572 train-loss: 0.010542041518621974\n",
      "[LOG 20200502-15:08:04] epoch: 19573 train-loss: 0.010542026203539636\n",
      "[LOG 20200502-15:08:04] epoch: 19574 train-loss: 0.010542011509339014\n",
      "[LOG 20200502-15:08:04] epoch: 19575 train-loss: 0.010541996815138392\n",
      "[LOG 20200502-15:08:04] epoch: 19576 train-loss: 0.010541981707016626\n",
      "[LOG 20200502-15:08:05] epoch: 19577 train-loss: 0.010541965978013145\n",
      "[LOG 20200502-15:08:05] epoch: 19578 train-loss: 0.010541951076851951\n",
      "[LOG 20200502-15:08:05] epoch: 19579 train-loss: 0.010541936382651329\n",
      "[LOG 20200502-15:08:05] epoch: 19580 train-loss: 0.01054192055016756\n",
      "[LOG 20200502-15:08:05] epoch: 19580 new best train-loss: 0.01054192055016756 found\n",
      "[LOG 20200502-15:08:06] epoch: 19581 train-loss: 0.010541905338565508\n",
      "[LOG 20200502-15:08:06] epoch: 19582 train-loss: 0.010541889920002885\n",
      "[LOG 20200502-15:08:06] epoch: 19583 train-loss: 0.010541875122321976\n",
      "[LOG 20200502-15:08:06] epoch: 19584 train-loss: 0.01054186032464107\n",
      "[LOG 20200502-15:08:07] epoch: 19585 train-loss: 0.010541845630440447\n",
      "[LOG 20200502-15:08:07] epoch: 19586 train-loss: 0.010541830936239826\n",
      "[LOG 20200502-15:08:07] epoch: 19587 train-loss: 0.01054181686292092\n",
      "[LOG 20200502-15:08:07] epoch: 19588 train-loss: 0.010541802996562587\n",
      "[LOG 20200502-15:08:08] epoch: 19589 train-loss: 0.010541788612802824\n",
      "[LOG 20200502-15:08:08] epoch: 19590 train-loss: 0.010541775677767064\n",
      "[LOG 20200502-15:08:08] epoch: 19590 new best train-loss: 0.010541775677767064 found\n",
      "[LOG 20200502-15:08:08] epoch: 19591 train-loss: 0.010541762121849589\n",
      "[LOG 20200502-15:08:08] epoch: 19592 train-loss: 0.010541750221616693\n",
      "[LOG 20200502-15:08:09] epoch: 19593 train-loss: 0.010541737286580933\n",
      "[LOG 20200502-15:08:09] epoch: 19594 train-loss: 0.010541725386348035\n",
      "[LOG 20200502-15:08:09] epoch: 19595 train-loss: 0.010541714624398284\n",
      "[LOG 20200502-15:08:09] epoch: 19596 train-loss: 0.0105417026206851\n",
      "[LOG 20200502-15:08:10] epoch: 19597 train-loss: 0.010541692686577639\n",
      "[LOG 20200502-15:08:10] epoch: 19598 train-loss: 0.010541682752470175\n",
      "[LOG 20200502-15:08:10] epoch: 19599 train-loss: 0.010541673232283857\n",
      "[LOG 20200502-15:08:10] epoch: 19600 train-loss: 0.010541663712097539\n",
      "[LOG 20200502-15:08:10] epoch: 19600 new best train-loss: 0.010541663712097539 found\n",
      "[LOG 20200502-15:08:11] epoch: 19601 train-loss: 0.010541655330194367\n",
      "[LOG 20200502-15:08:11] epoch: 19602 train-loss: 0.010541647051771482\n",
      "[LOG 20200502-15:08:11] epoch: 19603 train-loss: 0.010541639497710599\n",
      "[LOG 20200502-15:08:11] epoch: 19604 train-loss: 0.010541631633208858\n",
      "[LOG 20200502-15:08:12] epoch: 19605 train-loss: 0.010541624182628261\n",
      "[LOG 20200502-15:08:12] epoch: 19606 train-loss: 0.010541617766850524\n",
      "[LOG 20200502-15:08:12] epoch: 19607 train-loss: 0.010541612075434791\n",
      "[LOG 20200502-15:08:12] epoch: 19608 train-loss: 0.010541605866617627\n",
      "[LOG 20200502-15:08:12] epoch: 19609 train-loss: 0.010541599864761034\n",
      "[LOG 20200502-15:08:13] epoch: 19610 train-loss: 0.010541594587266445\n",
      "[LOG 20200502-15:08:13] epoch: 19610 new best train-loss: 0.010541594587266445 found\n",
      "[LOG 20200502-15:08:13] epoch: 19611 train-loss: 0.010541589620212713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-15:08:13] epoch: 19612 train-loss: 0.010541585170560412\n",
      "[LOG 20200502-15:08:13] epoch: 19613 train-loss: 0.010541580203506682\n",
      "[LOG 20200502-15:08:14] epoch: 19614 train-loss: 0.010541575753854381\n",
      "[LOG 20200502-15:08:14] epoch: 19615 train-loss: 0.010541572235524654\n",
      "[LOG 20200502-15:08:14] epoch: 19616 train-loss: 0.010541567682392068\n",
      "[LOG 20200502-15:08:14] epoch: 19617 train-loss: 0.010541563336220052\n",
      "[LOG 20200502-15:08:15] epoch: 19618 train-loss: 0.010541559093528323\n",
      "[LOG 20200502-15:08:15] epoch: 19619 train-loss: 0.010541555368238024\n",
      "[LOG 20200502-15:08:15] epoch: 19620 train-loss: 0.010541551332506869\n",
      "[LOG 20200502-15:08:15] epoch: 19620 new best train-loss: 0.010541551332506869 found\n",
      "[LOG 20200502-15:08:16] epoch: 19621 train-loss: 0.010541546779374281\n",
      "[LOG 20200502-15:08:16] epoch: 19622 train-loss: 0.010541543364524841\n",
      "[LOG 20200502-15:08:16] epoch: 19623 train-loss: 0.010541537983549966\n",
      "[LOG 20200502-15:08:16] epoch: 19624 train-loss: 0.010541532809535662\n",
      "[LOG 20200502-15:08:16] epoch: 19625 train-loss: 0.01054152867032422\n",
      "[LOG 20200502-15:08:17] epoch: 19626 train-loss: 0.010541523289349344\n",
      "[LOG 20200502-15:08:17] epoch: 19627 train-loss: 0.01054151811533504\n",
      "[LOG 20200502-15:08:17] epoch: 19628 train-loss: 0.010541512630879879\n",
      "[LOG 20200502-15:08:17] epoch: 19629 train-loss: 0.010541506939464144\n",
      "[LOG 20200502-15:08:18] epoch: 19630 train-loss: 0.010541501144568125\n",
      "[LOG 20200502-15:08:18] epoch: 19630 new best train-loss: 0.010541501144568125 found\n",
      "[LOG 20200502-15:08:18] epoch: 19631 train-loss: 0.010541494521829817\n",
      "[LOG 20200502-15:08:18] epoch: 19632 train-loss: 0.010541488209532367\n",
      "[LOG 20200502-15:08:18] epoch: 19633 train-loss: 0.01054148075895177\n",
      "[LOG 20200502-15:08:19] epoch: 19634 train-loss: 0.010541473411851458\n",
      "[LOG 20200502-15:08:19] epoch: 19635 train-loss: 0.010541465857790576\n",
      "[LOG 20200502-15:08:19] epoch: 19636 train-loss: 0.010541456958485974\n",
      "[LOG 20200502-15:08:19] epoch: 19637 train-loss: 0.010541449197464518\n",
      "[LOG 20200502-15:08:20] epoch: 19638 train-loss: 0.01054144050512049\n",
      "[LOG 20200502-15:08:20] epoch: 19639 train-loss: 0.010541431709296174\n",
      "[LOG 20200502-15:08:20] epoch: 19640 train-loss: 0.010541422396070428\n",
      "[LOG 20200502-15:08:20] epoch: 19640 new best train-loss: 0.010541422396070428 found\n",
      "[LOG 20200502-15:08:20] epoch: 19641 train-loss: 0.010541412979364395\n",
      "[LOG 20200502-15:08:21] epoch: 19642 train-loss: 0.01054140335569779\n",
      "[LOG 20200502-15:08:21] epoch: 19643 train-loss: 0.0105413936285509\n",
      "[LOG 20200502-15:08:21] epoch: 19644 train-loss: 0.010541383694443438\n",
      "[LOG 20200502-15:08:21] epoch: 19645 train-loss: 0.010541373760335974\n",
      "[LOG 20200502-15:08:22] epoch: 19646 train-loss: 0.010541363412307369\n",
      "[LOG 20200502-15:08:22] epoch: 19647 train-loss: 0.010541352753837904\n",
      "[LOG 20200502-15:08:22] epoch: 19648 train-loss: 0.010541342198848724\n",
      "[LOG 20200502-15:08:22] epoch: 19649 train-loss: 0.010541331436898973\n",
      "[LOG 20200502-15:08:23] epoch: 19650 train-loss: 0.010541321399311224\n",
      "[LOG 20200502-15:08:23] epoch: 19650 new best train-loss: 0.010541321399311224 found\n",
      "[LOG 20200502-15:08:23] epoch: 19651 train-loss: 0.010541310844322046\n",
      "[LOG 20200502-15:08:23] epoch: 19652 train-loss: 0.010541300496293439\n",
      "[LOG 20200502-15:08:23] epoch: 19653 train-loss: 0.010541290562185977\n",
      "[LOG 20200502-15:08:24] epoch: 19654 train-loss: 0.010541279903716512\n",
      "[LOG 20200502-15:08:24] epoch: 19655 train-loss: 0.010541269659168191\n",
      "[LOG 20200502-15:08:24] epoch: 19656 train-loss: 0.010541259828541014\n",
      "[LOG 20200502-15:08:24] epoch: 19657 train-loss: 0.010541249790953265\n",
      "[LOG 20200502-15:08:25] epoch: 19658 train-loss: 0.010541239856845804\n",
      "[LOG 20200502-15:08:25] epoch: 19659 train-loss: 0.01054123044013977\n",
      "[LOG 20200502-15:08:25] epoch: 19660 train-loss: 0.010541220816473166\n",
      "[LOG 20200502-15:08:25] epoch: 19660 new best train-loss: 0.010541220816473166 found\n",
      "[LOG 20200502-15:08:25] epoch: 19661 train-loss: 0.010541211917168565\n",
      "[LOG 20200502-15:08:26] epoch: 19662 train-loss: 0.010541202396982245\n",
      "[LOG 20200502-15:08:26] epoch: 19663 train-loss: 0.010541193497677645\n",
      "[LOG 20200502-15:08:26] epoch: 19664 train-loss: 0.01054118439141247\n",
      "[LOG 20200502-15:08:26] epoch: 19665 train-loss: 0.010541175802548727\n",
      "[LOG 20200502-15:08:27] epoch: 19666 train-loss: 0.010541167731086412\n",
      "[LOG 20200502-15:08:27] epoch: 19667 train-loss: 0.010541158935262097\n",
      "[LOG 20200502-15:08:27] epoch: 19668 train-loss: 0.010541150760319497\n",
      "[LOG 20200502-15:08:27] epoch: 19669 train-loss: 0.010541142378416326\n",
      "[LOG 20200502-15:08:28] epoch: 19670 train-loss: 0.010541134203473726\n",
      "[LOG 20200502-15:08:28] epoch: 19670 new best train-loss: 0.010541134203473726 found\n",
      "[LOG 20200502-15:08:28] epoch: 19671 train-loss: 0.0105411262354917\n",
      "[LOG 20200502-15:08:28] epoch: 19672 train-loss: 0.010541117750108242\n",
      "[LOG 20200502-15:08:28] epoch: 19673 train-loss: 0.010541109471685357\n",
      "[LOG 20200502-15:08:29] epoch: 19674 train-loss: 0.010541101814144187\n",
      "[LOG 20200502-15:08:29] epoch: 19675 train-loss: 0.01054109384616216\n",
      "[LOG 20200502-15:08:29] epoch: 19676 train-loss: 0.010541085050337844\n",
      "[LOG 20200502-15:08:29] epoch: 19677 train-loss: 0.010541077185836103\n",
      "[LOG 20200502-15:08:30] epoch: 19678 train-loss: 0.010541068803932931\n",
      "[LOG 20200502-15:08:30] epoch: 19679 train-loss: 0.010541060628990332\n",
      "[LOG 20200502-15:08:30] epoch: 19680 train-loss: 0.010541051419244872\n",
      "[LOG 20200502-15:08:30] epoch: 19680 new best train-loss: 0.010541051419244872 found\n",
      "[LOG 20200502-15:08:30] epoch: 19681 train-loss: 0.010541043658223417\n",
      "[LOG 20200502-15:08:31] epoch: 19682 train-loss: 0.010541034448477957\n",
      "[LOG 20200502-15:08:31] epoch: 19683 train-loss: 0.010541025652653642\n",
      "[LOG 20200502-15:08:31] epoch: 19684 train-loss: 0.010541016546388468\n",
      "[LOG 20200502-15:08:31] epoch: 19685 train-loss: 0.010541007129682435\n",
      "[LOG 20200502-15:08:32] epoch: 19686 train-loss: 0.010540997402535545\n",
      "[LOG 20200502-15:08:32] epoch: 19687 train-loss: 0.01054098777886894\n",
      "[LOG 20200502-15:08:32] epoch: 19688 train-loss: 0.010540977741281191\n",
      "[LOG 20200502-15:08:32] epoch: 19689 train-loss: 0.010540967289772298\n",
      "[LOG 20200502-15:08:33] epoch: 19690 train-loss: 0.010540957355664836\n",
      "[LOG 20200502-15:08:33] epoch: 19690 new best train-loss: 0.010540957355664836 found\n",
      "[LOG 20200502-15:08:33] epoch: 19691 train-loss: 0.01054094617979394\n",
      "[LOG 20200502-15:08:33] epoch: 19692 train-loss: 0.01054093541784419\n",
      "[LOG 20200502-15:08:33] epoch: 19693 train-loss: 0.010540924448933866\n",
      "[LOG 20200502-15:08:34] epoch: 19694 train-loss: 0.010540912341740396\n",
      "[LOG 20200502-15:08:34] epoch: 19695 train-loss: 0.010540900441507498\n",
      "[LOG 20200502-15:08:34] epoch: 19696 train-loss: 0.010540889265636602\n",
      "[LOG 20200502-15:08:34] epoch: 19697 train-loss: 0.010540876848002275\n",
      "[LOG 20200502-15:08:35] epoch: 19698 train-loss: 0.010540864533848233\n",
      "[LOG 20200502-15:08:35] epoch: 19699 train-loss: 0.010540852012733618\n",
      "[LOG 20200502-15:08:35] epoch: 19700 train-loss: 0.010540838974217573\n",
      "[LOG 20200502-15:08:35] epoch: 19700 new best train-loss: 0.010540838974217573 found\n",
      "[LOG 20200502-15:08:35] epoch: 19701 train-loss: 0.010540825935701529\n",
      "[LOG 20200502-15:08:36] epoch: 19702 train-loss: 0.01054081331110663\n",
      "[LOG 20200502-15:08:36] epoch: 19703 train-loss: 0.010540799651708867\n",
      "[LOG 20200502-15:08:36] epoch: 19704 train-loss: 0.010540786820153395\n",
      "[LOG 20200502-15:08:36] epoch: 19705 train-loss: 0.01054077378163735\n",
      "[LOG 20200502-15:08:37] epoch: 19706 train-loss: 0.01054076012223959\n",
      "[LOG 20200502-15:08:37] epoch: 19707 train-loss: 0.010540746876762973\n",
      "[LOG 20200502-15:08:37] epoch: 19708 train-loss: 0.010540733631286357\n",
      "[LOG 20200502-15:08:37] epoch: 19709 train-loss: 0.010540720489290025\n",
      "[LOG 20200502-15:08:38] epoch: 19710 train-loss: 0.010540707657734552\n",
      "[LOG 20200502-15:08:38] epoch: 19710 new best train-loss: 0.010540707657734552 found\n",
      "[LOG 20200502-15:08:38] epoch: 19711 train-loss: 0.010540694929659367\n",
      "[LOG 20200502-15:08:38] epoch: 19712 train-loss: 0.010540681580702463\n",
      "[LOG 20200502-15:08:38] epoch: 19713 train-loss: 0.01054066905958785\n",
      "[LOG 20200502-15:08:39] epoch: 19714 train-loss: 0.01054065664195352\n",
      "[LOG 20200502-15:08:39] epoch: 19715 train-loss: 0.01054064484520091\n",
      "[LOG 20200502-15:08:39] epoch: 19716 train-loss: 0.010540633048448298\n",
      "[LOG 20200502-15:08:39] epoch: 19717 train-loss: 0.0105406211482154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-15:08:40] epoch: 19718 train-loss: 0.010540610386265649\n",
      "[LOG 20200502-15:08:40] epoch: 19719 train-loss: 0.010540599831276469\n",
      "[LOG 20200502-15:08:40] epoch: 19720 train-loss: 0.010540589276287291\n",
      "[LOG 20200502-15:08:40] epoch: 19720 new best train-loss: 0.010540589276287291 found\n",
      "[LOG 20200502-15:08:40] epoch: 19721 train-loss: 0.010540579652620686\n",
      "[LOG 20200502-15:08:41] epoch: 19722 train-loss: 0.010540570442875227\n",
      "[LOG 20200502-15:08:41] epoch: 19723 train-loss: 0.010540561543570625\n",
      "[LOG 20200502-15:08:41] epoch: 19724 train-loss: 0.010540552333825164\n",
      "[LOG 20200502-15:08:41] epoch: 19725 train-loss: 0.010540544779764282\n",
      "[LOG 20200502-15:08:42] epoch: 19726 train-loss: 0.010540537018742826\n",
      "[LOG 20200502-15:08:42] epoch: 19727 train-loss: 0.010540529671642516\n",
      "[LOG 20200502-15:08:42] epoch: 19728 train-loss: 0.010540523566305637\n",
      "[LOG 20200502-15:08:42] epoch: 19729 train-loss: 0.010540517254008187\n",
      "[LOG 20200502-15:08:43] epoch: 19730 train-loss: 0.01054051135563188\n",
      "[LOG 20200502-15:08:43] epoch: 19730 new best train-loss: 0.01054051135563188 found\n",
      "[LOG 20200502-15:08:43] epoch: 19731 train-loss: 0.01054050587117672\n",
      "[LOG 20200502-15:08:43] epoch: 19732 train-loss: 0.010540500697162416\n",
      "[LOG 20200502-15:08:43] epoch: 19733 train-loss: 0.010540496350990402\n",
      "[LOG 20200502-15:08:44] epoch: 19734 train-loss: 0.010540492004818387\n",
      "[LOG 20200502-15:08:44] epoch: 19735 train-loss: 0.010540488383008374\n",
      "[LOG 20200502-15:08:44] epoch: 19736 train-loss: 0.01054048507163922\n",
      "[LOG 20200502-15:08:44] epoch: 19737 train-loss: 0.01054048165678978\n",
      "[LOG 20200502-15:08:45] epoch: 19738 train-loss: 0.010540478345420625\n",
      "[LOG 20200502-15:08:45] epoch: 19739 train-loss: 0.010540475241012044\n",
      "[LOG 20200502-15:08:45] epoch: 19740 train-loss: 0.010540473171406321\n",
      "[LOG 20200502-15:08:45] epoch: 19740 new best train-loss: 0.010540473171406321 found\n",
      "[LOG 20200502-15:08:45] epoch: 19741 train-loss: 0.010540470377438597\n",
      "[LOG 20200502-15:08:46] epoch: 19742 train-loss: 0.010540468204352591\n",
      "[LOG 20200502-15:08:46] epoch: 19743 train-loss: 0.01054046613474687\n",
      "[LOG 20200502-15:08:46] epoch: 19744 train-loss: 0.010540463547739718\n",
      "[LOG 20200502-15:08:46] epoch: 19745 train-loss: 0.010540461788574854\n",
      "[LOG 20200502-15:08:47] epoch: 19746 train-loss: 0.010540459718969133\n",
      "[LOG 20200502-15:08:47] epoch: 19747 train-loss: 0.010540457338922553\n",
      "[LOG 20200502-15:08:47] epoch: 19748 train-loss: 0.01054045506235626\n",
      "[LOG 20200502-15:08:47] epoch: 19749 train-loss: 0.01054045319971111\n",
      "[LOG 20200502-15:08:47] epoch: 19750 train-loss: 0.010540450612703959\n",
      "[LOG 20200502-15:08:47] epoch: 19750 new best train-loss: 0.010540450612703959 found\n",
      "[LOG 20200502-15:08:48] epoch: 19751 train-loss: 0.010540447818736235\n",
      "[LOG 20200502-15:08:48] epoch: 19752 train-loss: 0.010540444610847367\n",
      "[LOG 20200502-15:08:48] epoch: 19753 train-loss: 0.010540441506438784\n",
      "[LOG 20200502-15:08:48] epoch: 19754 train-loss: 0.010540437781148486\n",
      "[LOG 20200502-15:08:49] epoch: 19755 train-loss: 0.010540434262818761\n",
      "[LOG 20200502-15:08:49] epoch: 19756 train-loss: 0.010540430330567889\n",
      "[LOG 20200502-15:08:49] epoch: 19757 train-loss: 0.010540426501797305\n",
      "[LOG 20200502-15:08:49] epoch: 19758 train-loss: 0.010540422259105576\n",
      "[LOG 20200502-15:08:50] epoch: 19759 train-loss: 0.0105404168781307\n",
      "[LOG 20200502-15:08:50] epoch: 19760 train-loss: 0.010540412221517827\n",
      "[LOG 20200502-15:08:50] epoch: 19760 new best train-loss: 0.010540412221517827 found\n",
      "[LOG 20200502-15:08:50] epoch: 19761 train-loss: 0.010540406840542952\n",
      "[LOG 20200502-15:08:50] epoch: 19762 train-loss: 0.010540401252607504\n",
      "[LOG 20200502-15:08:51] epoch: 19763 train-loss: 0.010540394733349482\n",
      "[LOG 20200502-15:08:51] epoch: 19764 train-loss: 0.010540388524532318\n",
      "[LOG 20200502-15:08:51] epoch: 19765 train-loss: 0.010540382005274296\n",
      "[LOG 20200502-15:08:52] epoch: 19766 train-loss: 0.01054037507209513\n",
      "[LOG 20200502-15:08:52] epoch: 19767 train-loss: 0.010540368345876535\n",
      "[LOG 20200502-15:08:52] epoch: 19768 train-loss: 0.010540360688335366\n",
      "[LOG 20200502-15:08:52] epoch: 19769 train-loss: 0.010540353444715342\n",
      "[LOG 20200502-15:08:52] epoch: 19770 train-loss: 0.010540345580213599\n",
      "[LOG 20200502-15:08:52] epoch: 19770 new best train-loss: 0.010540345580213599 found\n",
      "[LOG 20200502-15:08:53] epoch: 19771 train-loss: 0.010540338129633002\n",
      "[LOG 20200502-15:08:53] epoch: 19772 train-loss: 0.010540329954690404\n",
      "[LOG 20200502-15:08:53] epoch: 19773 train-loss: 0.01054032188322809\n",
      "[LOG 20200502-15:08:53] epoch: 19774 train-loss: 0.01054031422568692\n",
      "[LOG 20200502-15:08:54] epoch: 19775 train-loss: 0.01054030553334289\n",
      "[LOG 20200502-15:08:54] epoch: 19776 train-loss: 0.010540297772321437\n",
      "[LOG 20200502-15:08:54] epoch: 19777 train-loss: 0.010540289390418265\n",
      "[LOG 20200502-15:08:54] epoch: 19778 train-loss: 0.010540281732877096\n",
      "[LOG 20200502-15:08:55] epoch: 19779 train-loss: 0.01054027397185564\n",
      "[LOG 20200502-15:08:55] epoch: 19780 train-loss: 0.010540265382991897\n",
      "[LOG 20200502-15:08:55] epoch: 19780 new best train-loss: 0.010540265382991897 found\n",
      "[LOG 20200502-15:08:55] epoch: 19781 train-loss: 0.010540258346332444\n",
      "[LOG 20200502-15:08:55] epoch: 19782 train-loss: 0.010540250585310988\n",
      "[LOG 20200502-15:08:56] epoch: 19783 train-loss: 0.010540243134730391\n",
      "[LOG 20200502-15:08:56] epoch: 19784 train-loss: 0.01054023527022865\n",
      "[LOG 20200502-15:08:56] epoch: 19785 train-loss: 0.0105402289579312\n",
      "[LOG 20200502-15:08:56] epoch: 19786 train-loss: 0.010540221300390031\n",
      "[LOG 20200502-15:08:57] epoch: 19787 train-loss: 0.010540214470691152\n",
      "[LOG 20200502-15:08:57] epoch: 19788 train-loss: 0.010540207744472556\n",
      "[LOG 20200502-15:08:57] epoch: 19789 train-loss: 0.010540201018253962\n",
      "[LOG 20200502-15:08:57] epoch: 19790 train-loss: 0.01054019449899594\n",
      "[LOG 20200502-15:08:57] epoch: 19790 new best train-loss: 0.01054019449899594 found\n",
      "[LOG 20200502-15:08:58] epoch: 19791 train-loss: 0.010540188393659063\n",
      "[LOG 20200502-15:08:58] epoch: 19792 train-loss: 0.010540181667440467\n",
      "[LOG 20200502-15:08:58] epoch: 19793 train-loss: 0.010540176079505019\n",
      "[LOG 20200502-15:08:58] epoch: 19794 train-loss: 0.01054016976720757\n",
      "[LOG 20200502-15:08:59] epoch: 19795 train-loss: 0.01054016345491012\n",
      "[LOG 20200502-15:08:59] epoch: 19796 train-loss: 0.010540157866974672\n",
      "[LOG 20200502-15:08:59] epoch: 19797 train-loss: 0.010540152072078653\n",
      "[LOG 20200502-15:09:00] epoch: 19798 train-loss: 0.010540146277182631\n",
      "[LOG 20200502-15:09:00] epoch: 19799 train-loss: 0.01054014027532604\n",
      "[LOG 20200502-15:09:00] epoch: 19800 train-loss: 0.010540134273469448\n",
      "[LOG 20200502-15:09:00] epoch: 19800 new best train-loss: 0.010540134273469448 found\n",
      "[LOG 20200502-15:09:00] epoch: 19801 train-loss: 0.010540128685534\n",
      "[LOG 20200502-15:09:01] epoch: 19802 train-loss: 0.010540122476716837\n",
      "[LOG 20200502-15:09:01] epoch: 19803 train-loss: 0.010540116371379958\n",
      "[LOG 20200502-15:09:01] epoch: 19804 train-loss: 0.010540110679964224\n",
      "[LOG 20200502-15:09:01] epoch: 19805 train-loss: 0.010540104367666774\n",
      "[LOG 20200502-15:09:01] epoch: 19806 train-loss: 0.01054009764144818\n",
      "[LOG 20200502-15:09:02] epoch: 19807 train-loss: 0.010540091225670444\n",
      "[LOG 20200502-15:09:02] epoch: 19808 train-loss: 0.010540085327294137\n",
      "[LOG 20200502-15:09:02] epoch: 19809 train-loss: 0.010540077980193827\n",
      "[LOG 20200502-15:09:02] epoch: 19810 train-loss: 0.010540070840054087\n",
      "[LOG 20200502-15:09:02] epoch: 19810 new best train-loss: 0.010540070840054087 found\n",
      "[LOG 20200502-15:09:03] epoch: 19811 train-loss: 0.010540064113835493\n",
      "[LOG 20200502-15:09:03] epoch: 19812 train-loss: 0.010540056766735183\n",
      "[LOG 20200502-15:09:03] epoch: 19813 train-loss: 0.010540049109194014\n",
      "[LOG 20200502-15:09:03] epoch: 19814 train-loss: 0.010540041348172558\n",
      "[LOG 20200502-15:09:04] epoch: 19815 train-loss: 0.010540033483670818\n",
      "[LOG 20200502-15:09:04] epoch: 19816 train-loss: 0.010540024687846502\n",
      "[LOG 20200502-15:09:04] epoch: 19817 train-loss: 0.01054001630594333\n",
      "[LOG 20200502-15:09:04] epoch: 19818 train-loss: 0.010540007199678156\n",
      "[LOG 20200502-15:09:05] epoch: 19819 train-loss: 0.010539998196893267\n",
      "[LOG 20200502-15:09:05] epoch: 19820 train-loss: 0.010539989090628095\n",
      "[LOG 20200502-15:09:05] epoch: 19820 new best train-loss: 0.010539989090628095 found\n",
      "[LOG 20200502-15:09:05] epoch: 19821 train-loss: 0.010539979570441775\n",
      "[LOG 20200502-15:09:05] epoch: 19822 train-loss: 0.0105399697398146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-15:09:06] epoch: 19823 train-loss: 0.010539959805707136\n",
      "[LOG 20200502-15:09:06] epoch: 19824 train-loss: 0.01053994945767853\n",
      "[LOG 20200502-15:09:06] epoch: 19825 train-loss: 0.010539939937492212\n",
      "[LOG 20200502-15:09:06] epoch: 19826 train-loss: 0.010539928865101602\n",
      "[LOG 20200502-15:09:07] epoch: 19827 train-loss: 0.010539918620553281\n",
      "[LOG 20200502-15:09:07] epoch: 19828 train-loss: 0.010539907548162673\n",
      "[LOG 20200502-15:09:07] epoch: 19829 train-loss: 0.01053989709665378\n",
      "[LOG 20200502-15:09:07] epoch: 19830 train-loss: 0.010539887059066031\n",
      "[LOG 20200502-15:09:07] epoch: 19830 new best train-loss: 0.010539887059066031 found\n",
      "[LOG 20200502-15:09:08] epoch: 19831 train-loss: 0.010539876193635993\n",
      "[LOG 20200502-15:09:08] epoch: 19832 train-loss: 0.010539865224725671\n",
      "[LOG 20200502-15:09:08] epoch: 19833 train-loss: 0.010539854255815348\n",
      "[LOG 20200502-15:09:08] epoch: 19834 train-loss: 0.010539843804306455\n",
      "[LOG 20200502-15:09:09] epoch: 19835 train-loss: 0.010539833559758134\n",
      "[LOG 20200502-15:09:09] epoch: 19836 train-loss: 0.010539823315209813\n",
      "[LOG 20200502-15:09:09] epoch: 19837 train-loss: 0.01053981338110235\n",
      "[LOG 20200502-15:09:09] epoch: 19838 train-loss: 0.01053980365395546\n",
      "[LOG 20200502-15:09:10] epoch: 19839 train-loss: 0.01053979444421\n",
      "[LOG 20200502-15:09:10] epoch: 19840 train-loss: 0.010539785130984254\n",
      "[LOG 20200502-15:09:10] epoch: 19840 new best train-loss: 0.010539785130984254 found\n",
      "[LOG 20200502-15:09:10] epoch: 19841 train-loss: 0.010539776128199365\n",
      "[LOG 20200502-15:09:10] epoch: 19842 train-loss: 0.010539767125414478\n",
      "[LOG 20200502-15:09:11] epoch: 19843 train-loss: 0.010539759571353594\n",
      "[LOG 20200502-15:09:11] epoch: 19844 train-loss: 0.010539751603371568\n",
      "[LOG 20200502-15:09:11] epoch: 19845 train-loss: 0.010539744463231828\n",
      "[LOG 20200502-15:09:11] epoch: 19846 train-loss: 0.010539737116131518\n",
      "[LOG 20200502-15:09:12] epoch: 19847 train-loss: 0.010539731217755212\n",
      "[LOG 20200502-15:09:12] epoch: 19848 train-loss: 0.010539725526339479\n",
      "[LOG 20200502-15:09:12] epoch: 19849 train-loss: 0.010539719524482885\n",
      "[LOG 20200502-15:09:12] epoch: 19850 train-loss: 0.010539715074830584\n",
      "[LOG 20200502-15:09:12] epoch: 19850 new best train-loss: 0.010539715074830584 found\n",
      "[LOG 20200502-15:09:13] epoch: 19851 train-loss: 0.010539709797335995\n",
      "[LOG 20200502-15:09:13] epoch: 19852 train-loss: 0.010539705865085125\n",
      "[LOG 20200502-15:09:13] epoch: 19853 train-loss: 0.010539701932834255\n",
      "[LOG 20200502-15:09:13] epoch: 19854 train-loss: 0.010539699035386244\n",
      "[LOG 20200502-15:09:14] epoch: 19855 train-loss: 0.010539696344898807\n",
      "[LOG 20200502-15:09:14] epoch: 19856 train-loss: 0.010539693964852227\n",
      "[LOG 20200502-15:09:14] epoch: 19857 train-loss: 0.010539692102207078\n",
      "[LOG 20200502-15:09:15] epoch: 19858 train-loss: 0.010539690239561928\n",
      "[LOG 20200502-15:09:15] epoch: 19859 train-loss: 0.010539688997798495\n",
      "[LOG 20200502-15:09:15] epoch: 19860 train-loss: 0.01053968889431821\n",
      "[LOG 20200502-15:09:15] epoch: 19860 new best train-loss: 0.01053968889431821 found\n",
      "[LOG 20200502-15:09:15] epoch: 19861 train-loss: 0.010539687756035063\n",
      "[LOG 20200502-15:09:15] epoch: 19862 train-loss: 0.01053968734211392\n",
      "[LOG 20200502-15:09:16] epoch: 19863 train-loss: 0.01053968734211392\n",
      "[LOG 20200502-15:09:16] epoch: 19864 train-loss: 0.01053968806647592\n",
      "[LOG 20200502-15:09:16] epoch: 19865 train-loss: 0.010539687549074491\n",
      "[LOG 20200502-15:09:17] epoch: 19866 train-loss: 0.010539688583877351\n",
      "[LOG 20200502-15:09:17] epoch: 19867 train-loss: 0.010539688583877351\n",
      "[LOG 20200502-15:09:17] epoch: 19868 train-loss: 0.010539688997798495\n",
      "[LOG 20200502-15:09:17] epoch: 19869 train-loss: 0.010539690136081643\n",
      "[LOG 20200502-15:09:17] epoch: 19870 train-loss: 0.010539690239561928\n",
      "[LOG 20200502-15:09:18] epoch: 19871 train-loss: 0.01053969075696336\n",
      "[LOG 20200502-15:09:18] epoch: 19872 train-loss: 0.010539691688285934\n",
      "[LOG 20200502-15:09:18] epoch: 19873 train-loss: 0.010539691481325362\n",
      "[LOG 20200502-15:09:18] epoch: 19874 train-loss: 0.010539691791766219\n",
      "[LOG 20200502-15:09:19] epoch: 19875 train-loss: 0.010539691998726793\n",
      "[LOG 20200502-15:09:19] epoch: 19876 train-loss: 0.010539691688285934\n",
      "[LOG 20200502-15:09:19] epoch: 19877 train-loss: 0.010539691791766219\n",
      "[LOG 20200502-15:09:19] epoch: 19878 train-loss: 0.010539690963923931\n",
      "[LOG 20200502-15:09:20] epoch: 19879 train-loss: 0.010539690239561928\n",
      "[LOG 20200502-15:09:20] epoch: 19880 train-loss: 0.010539688687357638\n",
      "[LOG 20200502-15:09:20] epoch: 19880 new best train-loss: 0.010539688687357638 found\n",
      "[LOG 20200502-15:09:20] epoch: 19881 train-loss: 0.01053968703167306\n",
      "[LOG 20200502-15:09:20] epoch: 19882 train-loss: 0.010539685272508197\n",
      "[LOG 20200502-15:09:21] epoch: 19883 train-loss: 0.01053968309942219\n",
      "[LOG 20200502-15:09:21] epoch: 19884 train-loss: 0.010539681133296754\n",
      "[LOG 20200502-15:09:21] epoch: 19885 train-loss: 0.010539678753250174\n",
      "[LOG 20200502-15:09:21] epoch: 19886 train-loss: 0.010539675545361307\n",
      "[LOG 20200502-15:09:22] epoch: 19887 train-loss: 0.010539671923551295\n",
      "[LOG 20200502-15:09:22] epoch: 19888 train-loss: 0.010539668508701854\n",
      "[LOG 20200502-15:09:22] epoch: 19889 train-loss: 0.01053966467993127\n",
      "[LOG 20200502-15:09:22] epoch: 19890 train-loss: 0.010539660644200113\n",
      "[LOG 20200502-15:09:22] epoch: 19890 new best train-loss: 0.010539660644200113 found\n",
      "[LOG 20200502-15:09:23] epoch: 19891 train-loss: 0.010539656918909814\n",
      "[LOG 20200502-15:09:23] epoch: 19892 train-loss: 0.010539652262296941\n",
      "[LOG 20200502-15:09:23] epoch: 19893 train-loss: 0.010539647398723496\n",
      "[LOG 20200502-15:09:23] epoch: 19894 train-loss: 0.010539642431669764\n",
      "[LOG 20200502-15:09:24] epoch: 19895 train-loss: 0.010539637775056891\n",
      "[LOG 20200502-15:09:24] epoch: 19896 train-loss: 0.010539633118444018\n",
      "[LOG 20200502-15:09:24] epoch: 19897 train-loss: 0.010539627633988857\n",
      "[LOG 20200502-15:09:24] epoch: 19898 train-loss: 0.010539622770415412\n",
      "[LOG 20200502-15:09:25] epoch: 19899 train-loss: 0.010539617492920823\n",
      "[LOG 20200502-15:09:25] epoch: 19900 train-loss: 0.01053961200846566\n",
      "[LOG 20200502-15:09:25] epoch: 19900 new best train-loss: 0.01053961200846566 found\n",
      "[LOG 20200502-15:09:25] epoch: 19901 train-loss: 0.010539607869254218\n",
      "[LOG 20200502-15:09:25] epoch: 19902 train-loss: 0.01053960259175963\n",
      "[LOG 20200502-15:09:26] epoch: 19903 train-loss: 0.010539597728186183\n",
      "[LOG 20200502-15:09:26] epoch: 19904 train-loss: 0.010539592968093025\n",
      "[LOG 20200502-15:09:26] epoch: 19905 train-loss: 0.010539588001039293\n",
      "[LOG 20200502-15:09:26] epoch: 19906 train-loss: 0.010539583240946135\n",
      "[LOG 20200502-15:09:27] epoch: 19907 train-loss: 0.010539578687813547\n",
      "[LOG 20200502-15:09:27] epoch: 19908 train-loss: 0.010539574445121817\n",
      "[LOG 20200502-15:09:27] epoch: 19909 train-loss: 0.010539570409390662\n",
      "[LOG 20200502-15:09:27] epoch: 19910 train-loss: 0.010539566684100363\n",
      "[LOG 20200502-15:09:27] epoch: 19910 new best train-loss: 0.010539566684100363 found\n",
      "[LOG 20200502-15:09:28] epoch: 19911 train-loss: 0.010539562648369206\n",
      "[LOG 20200502-15:09:28] epoch: 19912 train-loss: 0.01053955913003948\n",
      "[LOG 20200502-15:09:28] epoch: 19913 train-loss: 0.010539555508229468\n",
      "[LOG 20200502-15:09:29] epoch: 19914 train-loss: 0.0105395523003406\n",
      "[LOG 20200502-15:09:29] epoch: 19915 train-loss: 0.010539548057648871\n",
      "[LOG 20200502-15:09:29] epoch: 19916 train-loss: 0.010539545367161432\n",
      "[LOG 20200502-15:09:29] epoch: 19917 train-loss: 0.01053954205579228\n",
      "[LOG 20200502-15:09:30] epoch: 19918 train-loss: 0.010539538951383697\n",
      "[LOG 20200502-15:09:30] epoch: 19919 train-loss: 0.010539535950455401\n",
      "[LOG 20200502-15:09:30] epoch: 19920 train-loss: 0.01053953336344825\n",
      "[LOG 20200502-15:09:30] epoch: 19920 new best train-loss: 0.01053953336344825 found\n",
      "[LOG 20200502-15:09:30] epoch: 19921 train-loss: 0.010539530155559381\n",
      "[LOG 20200502-15:09:31] epoch: 19922 train-loss: 0.01053952674070994\n",
      "[LOG 20200502-15:09:31] epoch: 19923 train-loss: 0.010539523739781644\n",
      "[LOG 20200502-15:09:31] epoch: 19924 train-loss: 0.010539521049294207\n",
      "[LOG 20200502-15:09:31] epoch: 19925 train-loss: 0.010539517427484194\n",
      "[LOG 20200502-15:09:32] epoch: 19926 train-loss: 0.010539514323075613\n",
      "[LOG 20200502-15:09:32] epoch: 19927 train-loss: 0.010539511322147317\n",
      "[LOG 20200502-15:09:32] epoch: 19928 train-loss: 0.010539508217738735\n",
      "[LOG 20200502-15:09:32] epoch: 19929 train-loss: 0.010539504595928721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200502-15:09:33] epoch: 19930 train-loss: 0.010539501284559568\n",
      "[LOG 20200502-15:09:33] epoch: 19930 new best train-loss: 0.010539501284559568 found\n",
      "[LOG 20200502-15:09:33] epoch: 19931 train-loss: 0.010539496938387552\n",
      "[LOG 20200502-15:09:33] epoch: 19932 train-loss: 0.01053949331657754\n",
      "[LOG 20200502-15:09:33] epoch: 19933 train-loss: 0.010539488453004096\n",
      "[LOG 20200502-15:09:34] epoch: 19934 train-loss: 0.01053948493467437\n",
      "[LOG 20200502-15:09:34] epoch: 19935 train-loss: 0.010539480381541781\n",
      "[LOG 20200502-15:09:34] epoch: 19936 train-loss: 0.010539475414488051\n",
      "[LOG 20200502-15:09:34] epoch: 19937 train-loss: 0.010539470343954034\n",
      "[LOG 20200502-15:09:35] epoch: 19938 train-loss: 0.010539464962979158\n",
      "[LOG 20200502-15:09:35] epoch: 19939 train-loss: 0.010539459582004283\n",
      "[LOG 20200502-15:09:35] epoch: 19940 train-loss: 0.010539453890588548\n",
      "[LOG 20200502-15:09:35] epoch: 19940 new best train-loss: 0.010539453890588548 found\n",
      "[LOG 20200502-15:09:35] epoch: 19941 train-loss: 0.010539447681771385\n",
      "[LOG 20200502-15:09:36] epoch: 19942 train-loss: 0.010539441783395078\n",
      "[LOG 20200502-15:09:36] epoch: 19943 train-loss: 0.0105394356780582\n",
      "[LOG 20200502-15:09:36] epoch: 19944 train-loss: 0.01053942884835932\n",
      "[LOG 20200502-15:09:36] epoch: 19945 train-loss: 0.010539422122140726\n",
      "[LOG 20200502-15:09:37] epoch: 19946 train-loss: 0.010539414878520701\n",
      "[LOG 20200502-15:09:37] epoch: 19947 train-loss: 0.010539407634900676\n",
      "[LOG 20200502-15:09:37] epoch: 19948 train-loss: 0.010539400805201795\n",
      "[LOG 20200502-15:09:37] epoch: 19949 train-loss: 0.010539393251140913\n",
      "[LOG 20200502-15:09:38] epoch: 19950 train-loss: 0.010539385283158885\n",
      "[LOG 20200502-15:09:38] epoch: 19950 new best train-loss: 0.010539385283158885 found\n",
      "[LOG 20200502-15:09:38] epoch: 19951 train-loss: 0.01053937803953886\n",
      "[LOG 20200502-15:09:38] epoch: 19952 train-loss: 0.01053937017503712\n",
      "[LOG 20200502-15:09:38] epoch: 19953 train-loss: 0.01053936251749595\n",
      "[LOG 20200502-15:09:39] epoch: 19954 train-loss: 0.010539354756474495\n",
      "[LOG 20200502-15:09:39] epoch: 19955 train-loss: 0.010539347823295329\n",
      "[LOG 20200502-15:09:39] epoch: 19956 train-loss: 0.010539340062273873\n",
      "[LOG 20200502-15:09:39] epoch: 19957 train-loss: 0.010539332818653848\n",
      "[LOG 20200502-15:09:40] epoch: 19958 train-loss: 0.010539325368073251\n",
      "[LOG 20200502-15:09:40] epoch: 19959 train-loss: 0.010539318745334944\n",
      "[LOG 20200502-15:09:40] epoch: 19960 train-loss: 0.010539311915636063\n",
      "[LOG 20200502-15:09:40] epoch: 19960 new best train-loss: 0.010539311915636063 found\n",
      "[LOG 20200502-15:09:40] epoch: 19961 train-loss: 0.010539305603338612\n",
      "[LOG 20200502-15:09:41] epoch: 19962 train-loss: 0.010539299394521449\n",
      "[LOG 20200502-15:09:41] epoch: 19963 train-loss: 0.010539293392664857\n",
      "[LOG 20200502-15:09:41] epoch: 19964 train-loss: 0.010539287908209695\n",
      "[LOG 20200502-15:09:41] epoch: 19965 train-loss: 0.010539283251596821\n",
      "[LOG 20200502-15:09:42] epoch: 19966 train-loss: 0.010539279008905092\n",
      "[LOG 20200502-15:09:42] epoch: 19967 train-loss: 0.01053927486969365\n",
      "[LOG 20200502-15:09:42] epoch: 19968 train-loss: 0.010539270420041349\n",
      "[LOG 20200502-15:09:42] epoch: 19969 train-loss: 0.010539267729553912\n",
      "[LOG 20200502-15:09:42] epoch: 19970 train-loss: 0.010539264832105901\n",
      "[LOG 20200502-15:09:42] epoch: 19970 new best train-loss: 0.010539264832105901 found\n",
      "[LOG 20200502-15:09:43] epoch: 19971 train-loss: 0.010539262348579036\n",
      "[LOG 20200502-15:09:43] epoch: 19972 train-loss: 0.010539260278973315\n",
      "[LOG 20200502-15:09:43] epoch: 19973 train-loss: 0.01053925934765074\n",
      "[LOG 20200502-15:09:43] epoch: 19974 train-loss: 0.010539258623288738\n",
      "[LOG 20200502-15:09:44] epoch: 19975 train-loss: 0.010539257691966163\n",
      "[LOG 20200502-15:09:44] epoch: 19976 train-loss: 0.010539257795446448\n",
      "[LOG 20200502-15:09:44] epoch: 19977 train-loss: 0.010539258105887307\n",
      "[LOG 20200502-15:09:44] epoch: 19978 train-loss: 0.010539259037209881\n",
      "[LOG 20200502-15:09:45] epoch: 19979 train-loss: 0.0105392603824536\n",
      "[LOG 20200502-15:09:45] epoch: 19980 train-loss: 0.010539262038138177\n",
      "[LOG 20200502-15:09:45] epoch: 19980 new best train-loss: 0.010539262038138177 found\n",
      "[LOG 20200502-15:09:45] epoch: 19981 train-loss: 0.010539263900783327\n",
      "[LOG 20200502-15:09:45] epoch: 19982 train-loss: 0.010539266280829906\n",
      "[LOG 20200502-15:09:46] epoch: 19983 train-loss: 0.010539268764356772\n",
      "[LOG 20200502-15:09:46] epoch: 19984 train-loss: 0.010539271661804782\n",
      "[LOG 20200502-15:09:46] epoch: 19985 train-loss: 0.010539274766213365\n",
      "[LOG 20200502-15:09:46] epoch: 19986 train-loss: 0.010539277974102233\n",
      "[LOG 20200502-15:09:47] epoch: 19987 train-loss: 0.010539281802872816\n",
      "[LOG 20200502-15:09:47] epoch: 19988 train-loss: 0.01053928511424197\n",
      "[LOG 20200502-15:09:47] epoch: 19989 train-loss: 0.010539289149973128\n",
      "[LOG 20200502-15:09:47] epoch: 19990 train-loss: 0.010539292150901424\n",
      "[LOG 20200502-15:09:48] epoch: 19991 train-loss: 0.010539295876191722\n",
      "[LOG 20200502-15:09:48] epoch: 19992 train-loss: 0.010539299808442593\n",
      "[LOG 20200502-15:09:48] epoch: 19993 train-loss: 0.01053930332677232\n",
      "[LOG 20200502-15:09:48] epoch: 19994 train-loss: 0.010539306741621759\n",
      "[LOG 20200502-15:09:49] epoch: 19995 train-loss: 0.0105393101564712\n",
      "[LOG 20200502-15:09:49] epoch: 19996 train-loss: 0.010539313674800925\n",
      "[LOG 20200502-15:09:49] epoch: 19997 train-loss: 0.010539316572248936\n",
      "[LOG 20200502-15:09:49] epoch: 19998 train-loss: 0.010539319262736373\n",
      "[LOG 20200502-15:09:49] epoch: 19999 train-loss: 0.010539321539302668\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Apple_sentiment_lstm_model_19980.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-29c307f256e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Copy best model to a fixed name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_sentiment_lstm_model_{}.pth'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_sentiment_lstm_best_model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.8/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m             \u001b[0;31m# macOS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_HAS_FCOPYFILE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Apple_sentiment_lstm_model_19980.pth'"
     ]
    }
   ],
   "source": [
    "# Specify the training parameters\n",
    "num_epochs = 20000 # number of training epochs\n",
    "mini_batch_size = 128 # size of the mini-batches\n",
    "\n",
    "dataloader = dataloader.DataLoader(train_dataset, batch_size=mini_batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "# init collection of training epoch losses\n",
    "train_epoch_losses = []\n",
    "\n",
    "# set the model in training mode\n",
    "lstm_model.train()\n",
    "\n",
    "# init the best loss\n",
    "best_loss = 100.00\n",
    "\n",
    "# iterate over epochs\n",
    "for epoch in range(0, num_epochs):\n",
    "\n",
    "    # init collection of mini-batch losses\n",
    "    train_mini_batch_losses = []\n",
    "            \n",
    "    # iterate over mini-batches\n",
    "    for sequence_batch, target_batch in dataloader:\n",
    "\n",
    "        # predict sequence output\n",
    "        prediction_batch = lstm_model(sequence_batch)\n",
    "\n",
    "        # calculate batch loss\n",
    "        batch_loss = loss_function(prediction_batch, target_batch)\n",
    "\n",
    "        # run backward gradient calculation\n",
    "        batch_loss.backward()\n",
    "\n",
    "        # update network parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # collect mini-batch loss\n",
    "        train_mini_batch_losses.append(batch_loss.data.item())\n",
    "            \n",
    "    # determine mean min-batch loss of epoch\n",
    "    train_epoch_loss = np.mean(train_mini_batch_losses)\n",
    "    \n",
    "    # print epoch loss\n",
    "    now = dt.datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "    print('[LOG {}] epoch: {} train-loss: {}'.format(str(now), str(epoch), str(train_epoch_loss)))\n",
    "    \n",
    "    # determine mean min-batch loss of epoch\n",
    "    train_epoch_losses.append(train_epoch_loss)\n",
    "        \n",
    "    # print epoch and save models\n",
    "    if epoch % 10 == 0 and epoch > 0:\n",
    "        \n",
    "        # case: new best model trained\n",
    "        if train_epoch_loss < best_loss:\n",
    "                        \n",
    "            # store new best model\n",
    "            model_name = compName + '_sentiment_lstm_model_{}.pth'.format(str(epoch))\n",
    "            torch.save(lstm_model.state_dict(), os.path.join(\"./models\", model_name))\n",
    "            \n",
    "            # update best loss\n",
    "            best_loss = train_epoch_loss\n",
    "            \n",
    "            #keep best model number\n",
    "            best_model_number = epoch\n",
    "            \n",
    "            # print epoch loss\n",
    "            now = dt.datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "            print('[LOG {}] epoch: {} new best train-loss: {} found'.format(str(now), str(epoch), str(train_epoch_loss)))\n",
    "    \n",
    "# Copy best model to a fixed name\n",
    "shutil.copyfile('models' + os.sep' + compName + '_sentiment_lstm_model_{}.pth'.format(str(best_model_number)), 'models' + os.sep' + compName + '_sentiment_lstm_best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_epoch_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ae87e79b0546>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# plot the training epochs vs. the epochs' prediction error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_epoch_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_epoch_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'epoch loss (blue)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# add axis legends\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_epoch_losses' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# plot the training epochs vs. the epochs' prediction error\n",
    "ax.plot(np.array(range(1, len(train_epoch_losses)+1)), train_epoch_losses, label='epoch loss (blue)')\n",
    "\n",
    "# add axis legends\n",
    "ax.set_xlabel(\"[training epoch $e_i$]\", fontsize=10)\n",
    "ax.set_ylabel(\"[Prediction Error $\\mathcal{L}^{MSE}$]\", fontsize=10)\n",
    "\n",
    "# set plot legend\n",
    "plt.legend(loc=\"upper right\", numpoints=1, fancybox=True)\n",
    "\n",
    "# add plot title\n",
    "plt.title('Training Epochs $e_i$ vs. Prediction Error $L^{MSE}$', fontsize=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.3, -0.2, -0.1,  0. ,  0.1,  0.2,  0.3,  0.4,  0.5]),\n",
       " <a list of 9 Text major ticklabel objects>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evalutation of the model\n",
    "\n",
    "model_name = compName + '_sentiment_lstm_best_model.pth'\n",
    "lstm_model.load_state_dict(torch.load(os.path.join(\"./models\", model_name)))\n",
    "\n",
    "# set model in evaluation mode\n",
    "lstm_model.eval()\n",
    "\n",
    "# don't calculate gradients\n",
    "with torch.no_grad():\n",
    "\n",
    "    # predict sequence output\n",
    "    predictions = lstm_model(train_sequences_input)\n",
    "\n",
    "    # collect prediction batch results\n",
    "    predictions_list = predictions.detach().numpy()[:, -1].tolist()\n",
    "\n",
    "    # collect target batch results\n",
    "    targets_list = train_sequences_target.numpy()[:, -1].tolist()\n",
    "\n",
    "    \n",
    "# plot the prediction results\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(pd.to_datetime(sentiment.index).values[0:train_sequences.shape[0],], targets_list, color='C1', label='groundtruth (green)')\n",
    "ax.plot(pd.to_datetime(sentiment.index).values[0:train_sequences.shape[0],], predictions_list, color='C0', label='predictions (blue)')\n",
    "\n",
    "# set y-axis limits\n",
    "ax.set_xlim(pd.to_datetime(sentiment.index).values[0], pd.to_datetime(sentiment.index).values[train_sequences.shape[0]])\n",
    "\n",
    "# set plot legend\n",
    "plt.legend(loc=\"lower right\", numpoints=1, fancybox=True)\n",
    "\n",
    "plt.title('LSTM NN In-Sample Prediction vs. Ground-Truth Sentiment', fontsize=10)\n",
    "plt.xlabel('[time]', fontsize=8)\n",
    "plt.ylabel('[sentiment]', fontsize=8)\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.2, -0.1,  0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6]),\n",
       " <a list of 9 Text major ticklabel objects>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Out-of-sample evaluation of the model\n",
    "\n",
    "# don't calculate gradients\n",
    "with torch.no_grad():\n",
    "\n",
    "    # predict sequence output\n",
    "    predictions = lstm_model(valid_sequences_input)\n",
    "\n",
    "    # collect prediction batch results\n",
    "    predictions_list = predictions.detach().numpy()[:, -1].tolist()\n",
    "\n",
    "    # collect target batch results\n",
    "    targets_list = valid_sequences_target.numpy()[:, -1].tolist()\n",
    "\n",
    "# plot the prediction results\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(pd.to_datetime(sentiment.index).values[train_sequences.shape[0]:train_sequences.shape[0]+valid_sequences.shape[0],], targets_list, color='C1', label='groundtruth (green)')\n",
    "ax.plot(pd.to_datetime(sentiment.index).values[train_sequences.shape[0]:train_sequences.shape[0]+valid_sequences.shape[0],], predictions_list, color='C0', label='predictions (blue)')\n",
    "\n",
    "# set y-axis limits\n",
    "ax.set_xlim(pd.to_datetime(sentiment.index).values[train_sequences.shape[0]], pd.to_datetime(sentiment.index).values[train_sequences.shape[0]+valid_sequences.shape[0]])\n",
    "#ax.set_ylim(0.0, 1.0)\n",
    "\n",
    "# set plot legend\n",
    "plt.legend(loc=\"lower right\", numpoints=1, fancybox=True)\n",
    "\n",
    "plt.title('LSTM NN Out-of-Sample Prediction vs. Ground-Truth Sentiment', fontsize=10)\n",
    "plt.xlabel('[time]', fontsize=8)\n",
    "plt.ylabel('[sentiment]', fontsize=8)\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest of the model\n",
    "\n",
    "sentiment_daily_predictions = pd.DataFrame(predictions_list, columns=['PREDICTIONS'])\n",
    "sentiment_daily_predictions = sentiment_daily_predictions.set_index(pd.to_datetime(sentiment.index).values[train_sequences.shape[0]:train_sequences.shape[0]+valid_sequences.shape[0],])\n",
    "\n",
    "signal_data = pd.DataFrame(np.where(sentiment_daily_predictions['PREDICTIONS'] > 0.0, 1.0, -1.0), columns=['SIGNAL'])\n",
    "signal_data = signal_data.set_index(pd.to_datetime(sentiment.index).values[train_sequences.shape[0]:train_sequences.shape[0]+valid_sequences.shape[0],])\n",
    "\n",
    "\n",
    "stock_market_data = pd.DataFrame(market_data[compTick + ' US Equity'])\n",
    "stock_market_data = stock_market_data.rename(columns={compTick + ' US Equity': 'PRICE'})\n",
    "stock_market_data = stock_market_data.set_index(market_data.index)\n",
    "\n",
    "#sentiment_daily_predictions.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Apple - Daily Historical Stock Closing Prices')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stock_market_data = stock_market_data[stock_market_data.index >= sentiment_daily_predictions.index[0]]\n",
    "stock_market_data = stock_market_data[stock_market_data.index <= sentiment_daily_predictions.index[-1]]\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(stock_market_data['PRICE'], color='#9b59b6')\n",
    "\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "    \n",
    "# set axis labels\n",
    "ax.set_xlabel('[time]', fontsize=10)\n",
    "ax.set_ylabel('[equity %]', fontsize=10)\n",
    "\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "\n",
    "# set axis labels\n",
    "ax.set_xlabel('[time]', fontsize=10)\n",
    "ax.set_ylabel('Price', fontsize=10)\n",
    "\n",
    "# set plot title\n",
    "plt.title(compName + ' - Daily Historical Stock Closing Prices', fontsize=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_lstm_backtest\n",
      "0% [############################# ] 100% | ETA: 00:00:00stock_base_backtest\n",
      "0% [############################# ] 100% | ETA: 00:00:00"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate gain/loss\n",
    "np.abs(stock_market_data.iloc[0]['PRICE'] - stock_market_data.iloc[-1]['PRICE']) / stock_market_data.iloc[0]['PRICE']\n",
    "\n",
    "# Prepare backtest\n",
    "class LSTMStrategy(bt.Algo):\n",
    "    \n",
    "    def __init__(self, signals):\n",
    "        \n",
    "        # set class signals\n",
    "        self.signals = signals\n",
    "        \n",
    "    def __call__(self, target):\n",
    "        \n",
    "        if target.now in self.signals.index[1:]:\n",
    "            \n",
    "            # get actual signal\n",
    "            signal = self.signals.loc[target.now]\n",
    "            \n",
    "            # set target weights according to signal\n",
    "            target.temp['weights'] = dict(PRICE=signal)\n",
    "            \n",
    "        # return True since we want to move on to the next timestep\n",
    "        return True\n",
    "\n",
    "lstm_strategy = bt.Strategy('lstm', [bt.algos.SelectAll(), LSTMStrategy(signal_data['SIGNAL']), bt.algos.Rebalance()])\n",
    "\n",
    "backtest_lstm = bt.Backtest(strategy=lstm_strategy, data=stock_market_data, name='stock_lstm_backtest')\n",
    "\n",
    "signal_data_base = signal_data.copy(deep=True) \n",
    "signal_data_base['SIGNAL'] = 1.0\n",
    "\n",
    "base_strategy = bt.Strategy('base', [bt.algos.SelectAll(), LSTMStrategy(signal_data_base['SIGNAL']), bt.algos.Rebalance()])\n",
    "backtest_base = bt.Backtest(strategy=base_strategy, data=stock_market_data, name='stock_base_backtest')\n",
    "\n",
    "backtest_results = bt.run(backtest_lstm, backtest_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat                 stock_lstm_backtest    stock_base_backtest\n",
      "-------------------  ---------------------  ---------------------\n",
      "Start                2019-11-28             2019-11-28\n",
      "End                  2020-03-26             2020-03-26\n",
      "Risk-free rate       0.00%                  0.00%\n",
      "\n",
      "Total Return         -3.07%                 -3.07%\n",
      "Daily Sharpe         0.10                   0.10\n",
      "Daily Sortino        0.15                   0.15\n",
      "CAGR                 -9.12%                 -9.12%\n",
      "Max Drawdown         -31.42%                -31.42%\n",
      "Calmar Ratio         -0.29                  -0.29\n",
      "\n",
      "MTD                  -5.46%                 -5.46%\n",
      "3m                   -10.64%                -10.64%\n",
      "6m                   -                      -\n",
      "YTD                  -11.78%                -11.78%\n",
      "1Y                   -                      -\n",
      "3Y (ann.)            -9.12%                 -9.12%\n",
      "5Y (ann.)            -                      -\n",
      "10Y (ann.)           -                      -\n",
      "Since Incep. (ann.)  -9.12%                 -9.12%\n",
      "\n",
      "Daily Sharpe         0.10                   0.10\n",
      "Daily Sortino        0.15                   0.15\n",
      "Daily Mean (ann.)    4.76%                  4.76%\n",
      "Daily Vol (ann.)     47.73%                 47.73%\n",
      "Daily Skew           -0.07                  -0.07\n",
      "Daily Kurt           6.60                   6.60\n",
      "Best Day             11.98%                 11.98%\n",
      "Worst Day            -12.86%                -12.86%\n",
      "\n",
      "Monthly Sharpe       -0.15                  -0.15\n",
      "Monthly Sortino      -0.26                  -0.26\n",
      "Monthly Mean (ann.)  -4.95%                 -4.95%\n",
      "Monthly Vol (ann.)   33.90%                 33.90%\n",
      "Monthly Skew         -0.14                  -0.14\n",
      "Monthly Kurt         -3.35                  -3.35\n",
      "Best Month           9.88%                  9.88%\n",
      "Worst Month          -11.47%                -11.47%\n",
      "\n",
      "Yearly Sharpe        -                      -\n",
      "Yearly Sortino       -                      -\n",
      "Yearly Mean          -11.78%                -11.78%\n",
      "Yearly Vol           -                      -\n",
      "Yearly Skew          -                      -\n",
      "Yearly Kurt          -                      -\n",
      "Best Year            -11.78%                -11.78%\n",
      "Worst Year           -11.78%                -11.78%\n",
      "\n",
      "Avg. Drawdown        -4.46%                 -4.46%\n",
      "Avg. Drawdown Days   7.55                   7.55\n",
      "Avg. Up Month        7.64%                  7.64%\n",
      "Avg. Down Month      -8.46%                 -8.46%\n",
      "Win Year %           0.00%                  0.00%\n",
      "Win 12m %            -                      -\n"
     ]
    }
   ],
   "source": [
    "backtest_results.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>% EQUITY</th>\n",
       "      <th>EQUITY</th>\n",
       "      <th>CASH</th>\n",
       "      <th>FEES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-11-28</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>1000000.00</td>\n",
       "      <td>1000000.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-29</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>1000000.00</td>\n",
       "      <td>1000000.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-30</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>1000000.00</td>\n",
       "      <td>186.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>1000000.00</td>\n",
       "      <td>186.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-02</th>\n",
       "      <td>98.844250</td>\n",
       "      <td>988442.50</td>\n",
       "      <td>186.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-03</th>\n",
       "      <td>97.082125</td>\n",
       "      <td>970821.25</td>\n",
       "      <td>186.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-04</th>\n",
       "      <td>97.938625</td>\n",
       "      <td>979386.25</td>\n",
       "      <td>186.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-05</th>\n",
       "      <td>99.375250</td>\n",
       "      <td>993752.50</td>\n",
       "      <td>186.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-06</th>\n",
       "      <td>101.294500</td>\n",
       "      <td>1012945.00</td>\n",
       "      <td>186.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-07</th>\n",
       "      <td>101.294500</td>\n",
       "      <td>1012945.00</td>\n",
       "      <td>186.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              % EQUITY      EQUITY        CASH  FEES\n",
       "2019-11-28  100.000000  1000000.00  1000000.00   0.0\n",
       "2019-11-29  100.000000  1000000.00  1000000.00   0.0\n",
       "2019-11-30  100.000000  1000000.00      186.25   0.0\n",
       "2019-12-01  100.000000  1000000.00      186.25   0.0\n",
       "2019-12-02   98.844250   988442.50      186.25   0.0\n",
       "2019-12-03   97.082125   970821.25      186.25   0.0\n",
       "2019-12-04   97.938625   979386.25      186.25   0.0\n",
       "2019-12-05   99.375250   993752.50      186.25   0.0\n",
       "2019-12-06  101.294500  1012945.00      186.25   0.0\n",
       "2019-12-07  101.294500  1012945.00      186.25   0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backtest_lstm_details = backtest_lstm.strategy.data\n",
    "backtest_lstm_details.columns = ['% EQUITY', 'EQUITY', 'CASH', 'FEES']\n",
    "\n",
    "backtest_lstm_details.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>% EQUITY</th>\n",
       "      <th>EQUITY</th>\n",
       "      <th>CASH</th>\n",
       "      <th>FEES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-11-28</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>1000000.00</td>\n",
       "      <td>1000000.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-29</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>1000000.00</td>\n",
       "      <td>1000000.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-30</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>1000000.00</td>\n",
       "      <td>186.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>1000000.00</td>\n",
       "      <td>186.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-02</th>\n",
       "      <td>98.844250</td>\n",
       "      <td>988442.50</td>\n",
       "      <td>186.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-03</th>\n",
       "      <td>97.082125</td>\n",
       "      <td>970821.25</td>\n",
       "      <td>186.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-04</th>\n",
       "      <td>97.938625</td>\n",
       "      <td>979386.25</td>\n",
       "      <td>186.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-05</th>\n",
       "      <td>99.375250</td>\n",
       "      <td>993752.50</td>\n",
       "      <td>186.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-06</th>\n",
       "      <td>101.294500</td>\n",
       "      <td>1012945.00</td>\n",
       "      <td>186.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-07</th>\n",
       "      <td>101.294500</td>\n",
       "      <td>1012945.00</td>\n",
       "      <td>186.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              % EQUITY      EQUITY        CASH  FEES\n",
       "2019-11-28  100.000000  1000000.00  1000000.00   0.0\n",
       "2019-11-29  100.000000  1000000.00  1000000.00   0.0\n",
       "2019-11-30  100.000000  1000000.00      186.25   0.0\n",
       "2019-12-01  100.000000  1000000.00      186.25   0.0\n",
       "2019-12-02   98.844250   988442.50      186.25   0.0\n",
       "2019-12-03   97.082125   970821.25      186.25   0.0\n",
       "2019-12-04   97.938625   979386.25      186.25   0.0\n",
       "2019-12-05   99.375250   993752.50      186.25   0.0\n",
       "2019-12-06  101.294500  1012945.00      186.25   0.0\n",
       "2019-12-07  101.294500  1012945.00      186.25   0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backtest_base_details = backtest_base.strategy.data\n",
    "backtest_base_details.columns = ['% EQUITY', 'EQUITY', 'CASH', 'FEES']\n",
    "\n",
    "backtest_base_details.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Apple - Backtest % Equity Progression')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(backtest_lstm_details['% EQUITY'], color='C1',lw=1.0, label='lstm strategy (green)')\n",
    "ax.plot(backtest_base_details['% EQUITY'], color='C2',lw=1.0, label='base strategy (red)')\n",
    "\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "    \n",
    "# set axis labels\n",
    "ax.set_xlabel('[time]', fontsize=10)\n",
    "ax.set_ylabel('[equity %]', fontsize=10)\n",
    "\n",
    "# set plot legend\n",
    "plt.legend(loc=\"upper right\", numpoints=1, fancybox=True)\n",
    "\n",
    "# set plot title\n",
    "plt.title(compName + ' - Backtest % Equity Progression', fontsize=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
